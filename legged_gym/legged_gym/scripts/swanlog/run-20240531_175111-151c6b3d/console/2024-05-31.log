swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240531_175111-151c6b3d
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run test_May31_17-51-12 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/ydgdln68vv7ww3obheo68
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.80 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=512, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=512, out_features=256, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=256, out_features=128, bias=True)
  (7): ELU(alpha=1.0)
  (8): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=512, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=512, out_features=256, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=256, out_features=128, bias=True)
  (7): ELU(alpha=1.0)
  (8): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 37003 steps/s (collection: 2.386s, learning 0.271s)
               Value function loss: 0.0568
                    Surrogate loss: 0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.02
                Mean reward (task): 0.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0022
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0013
        Mean episode rew_lin_vel_z: -0.0515
           Mean episode rew_no_fly: 0.0009
       Mean episode rew_smoothness: -0.0062
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0015
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 1.6682
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.66s
                        Total time: 2.66s
                               ETA: 2213 mins 50.3 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 118865 steps/s (collection: 0.602s, learning 0.225s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0256
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0403
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0571
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 1.1189
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.83s
                        Total time: 3.48s
                               ETA: 1451 mins 28.9 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 126459 steps/s (collection: 0.599s, learning 0.178s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0249
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.6722
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.78s
                        Total time: 4.26s
                               ETA: 1183 mins 33.6 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 120765 steps/s (collection: 0.642s, learning 0.172s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0254
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.3645
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.81s
                        Total time: 5.07s
                               ETA: 1057 mins 13.6 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 125628 steps/s (collection: 0.600s, learning 0.183s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0253
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0573
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.1877
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.78s
                        Total time: 5.86s
                               ETA: 976 mins 10.2 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 110671 steps/s (collection: 0.697s, learning 0.191s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0256
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0574
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0967
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.89s
                        Total time: 6.75s
                               ETA: 936 mins 48.9 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 115155 steps/s (collection: 0.672s, learning 0.181s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0249
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0405
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0504
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.85s
                        Total time: 7.60s
                               ETA: 904 mins 34.9 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 121982 steps/s (collection: 0.633s, learning 0.173s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0602
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.81s
                        Total time: 8.41s
                               ETA: 875 mins 25.7 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 121141 steps/s (collection: 0.615s, learning 0.197s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0252
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0405
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.81s
                        Total time: 9.22s
                               ETA: 853 mins 16.1 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 124055 steps/s (collection: 0.619s, learning 0.173s)
               Value function loss: 0.0006
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.79s
                        Total time: 10.01s
                               ETA: 833 mins 57.0 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 121807 steps/s (collection: 0.630s, learning 0.177s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0246
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0576
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.81s
                        Total time: 10.82s
                               ETA: 819 mins 14.9 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 115758 steps/s (collection: 0.666s, learning 0.183s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.85s
                        Total time: 11.67s
                               ETA: 809 mins 55.3 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 128454 steps/s (collection: 0.594s, learning 0.172s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0573
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.77s
                        Total time: 12.43s
                               ETA: 796 mins 39.0 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 123512 steps/s (collection: 0.623s, learning 0.173s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0245
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0557
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.80s
                        Total time: 13.23s
                               ETA: 787 mins 5.7 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 111106 steps/s (collection: 0.687s, learning 0.198s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0240
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.88s
                        Total time: 14.11s
                               ETA: 783 mins 44.8 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 124387 steps/s (collection: 0.618s, learning 0.173s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0243
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.79s
                        Total time: 14.90s
                               ETA: 775 mins 53.9 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 125393 steps/s (collection: 0.611s, learning 0.173s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0241
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0556
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.78s
                        Total time: 15.69s
                               ETA: 768 mins 39.6 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 124083 steps/s (collection: 0.621s, learning 0.172s)
               Value function loss: 0.8119
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0238
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0424
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0551
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.79s
                        Total time: 16.48s
                               ETA: 762 mins 36.4 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 115260 steps/s (collection: 0.681s, learning 0.172s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0547
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.85s
                        Total time: 17.33s
                               ETA: 759 mins 50.9 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 125092 steps/s (collection: 0.613s, learning 0.173s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0551
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.79s
                        Total time: 18.12s
                               ETA: 754 mins 34.4 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 107305 steps/s (collection: 0.735s, learning 0.181s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0576
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.92s
                        Total time: 19.03s
                               ETA: 754 mins 58.0 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 122879 steps/s (collection: 0.626s, learning 0.175s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0241
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0433
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0549
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.80s
                        Total time: 19.83s
                               ETA: 750 mins 55.5 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 113807 steps/s (collection: 0.685s, learning 0.178s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0235
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0556
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.86s
                        Total time: 20.70s
                               ETA: 749 mins 32.7 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 124015 steps/s (collection: 0.621s, learning 0.172s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0547
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.79s
                        Total time: 21.49s
                               ETA: 745 mins 48.6 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 114674 steps/s (collection: 0.665s, learning 0.192s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0237
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0535
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                        Total time: 22.35s
                               ETA: 744 mins 31.5 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 118199 steps/s (collection: 0.651s, learning 0.181s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0527
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.83s
                        Total time: 23.18s
                               ETA: 742 mins 31.0 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 101718 steps/s (collection: 0.768s, learning 0.198s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0237
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.97s
                        Total time: 24.14s
                               ETA: 744 mins 48.9 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 107748 steps/s (collection: 0.719s, learning 0.194s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0537
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.91s
                        Total time: 25.06s
                               ETA: 745 mins 20.3 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 106760 steps/s (collection: 0.730s, learning 0.191s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0240
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0538
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.92s
                        Total time: 25.98s
                               ETA: 746 mins 4.0 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 117782 steps/s (collection: 0.643s, learning 0.192s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0544
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.83s
                        Total time: 26.81s
                               ETA: 744 mins 21.3 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 113231 steps/s (collection: 0.660s, learning 0.208s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0240
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.87s
                        Total time: 27.68s
                               ETA: 743 mins 39.2 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 112879 steps/s (collection: 0.679s, learning 0.192s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0239
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0433
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.87s
                        Total time: 28.55s
                               ETA: 743 mins 3.8 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 119895 steps/s (collection: 0.628s, learning 0.192s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0241
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0559
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.82s
                        Total time: 29.37s
                               ETA: 741 mins 13.5 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 112147 steps/s (collection: 0.684s, learning 0.193s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0238
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0552
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.88s
                        Total time: 30.25s
                               ETA: 740 mins 52.8 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 114952 steps/s (collection: 0.665s, learning 0.190s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0534
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.86s
                        Total time: 31.10s
                               ETA: 740 mins 2.7 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 108084 steps/s (collection: 0.720s, learning 0.189s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0535
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.91s
                        Total time: 32.01s
                               ETA: 740 mins 30.7 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 120014 steps/s (collection: 0.634s, learning 0.185s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0232
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0545
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.82s
                        Total time: 32.83s
                               ETA: 738 mins 55.1 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 108907 steps/s (collection: 0.714s, learning 0.188s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0238
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0448
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0523
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.90s
                        Total time: 33.73s
                               ETA: 739 mins 14.3 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 109778 steps/s (collection: 0.714s, learning 0.181s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0525
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.90s
                        Total time: 34.63s
                               ETA: 739 mins 23.4 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 96685 steps/s (collection: 0.820s, learning 0.196s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0235
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0520
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.02s
                        Total time: 35.65s
                               ETA: 742 mins 3.3 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 111701 steps/s (collection: 0.690s, learning 0.190s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0528
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.88s
                        Total time: 36.53s
                               ETA: 741 mins 48.9 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 113795 steps/s (collection: 0.663s, learning 0.201s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0464
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0546
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.86s
                        Total time: 37.39s
                               ETA: 741 mins 15.9 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 118451 steps/s (collection: 0.638s, learning 0.192s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0521
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.83s
                        Total time: 38.22s
                               ETA: 740 mins 4.9 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 102800 steps/s (collection: 0.759s, learning 0.197s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0448
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0523
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.96s
                        Total time: 39.18s
                               ETA: 741 mins 20.5 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 123266 steps/s (collection: 0.616s, learning 0.182s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0446
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.80s
                        Total time: 39.97s
                               ETA: 739 mins 36.5 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 120942 steps/s (collection: 0.640s, learning 0.173s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0232
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0496
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.81s
                        Total time: 40.79s
                               ETA: 738 mins 13.7 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 120303 steps/s (collection: 0.642s, learning 0.175s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.82s
                        Total time: 41.60s
                               ETA: 736 mins 58.9 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 115955 steps/s (collection: 0.667s, learning 0.181s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0235
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0464
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0526
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.85s
                        Total time: 42.45s
                               ETA: 736 mins 19.0 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 101287 steps/s (collection: 0.788s, learning 0.183s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.97s
                        Total time: 43.42s
                               ETA: 737 mins 46.0 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 114624 steps/s (collection: 0.684s, learning 0.173s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0496
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.86s
                        Total time: 44.28s
                               ETA: 737 mins 16.5 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 127577 steps/s (collection: 0.599s, learning 0.172s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0465
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0528
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.77s
                        Total time: 45.05s
                               ETA: 735 mins 23.0 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 126289 steps/s (collection: 0.605s, learning 0.173s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0511
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.78s
                        Total time: 45.83s
                               ETA: 733 mins 41.3 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 123824 steps/s (collection: 0.621s, learning 0.173s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0495
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.79s
                        Total time: 46.62s
                               ETA: 732 mins 18.0 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 113618 steps/s (collection: 0.682s, learning 0.183s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0487
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.87s
                        Total time: 47.49s
                               ETA: 732 mins 3.8 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 123374 steps/s (collection: 0.620s, learning 0.176s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0223
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0459
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.80s
                        Total time: 48.28s
                               ETA: 730 mins 47.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 122969 steps/s (collection: 0.616s, learning 0.183s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0478
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.80s
                        Total time: 49.08s
                               ETA: 729 mins 37.0 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 119997 steps/s (collection: 0.646s, learning 0.174s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0476
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.82s
                        Total time: 49.90s
                               ETA: 728 mins 45.9 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 125692 steps/s (collection: 0.592s, learning 0.190s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0474
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0492
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.78s
                        Total time: 50.69s
                               ETA: 727 mins 24.6 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 128194 steps/s (collection: 0.594s, learning 0.173s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.77s
                        Total time: 51.45s
                               ETA: 725 mins 53.1 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 127211 steps/s (collection: 0.596s, learning 0.177s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0446
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0450
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.77s
                        Total time: 52.23s
                               ETA: 724 mins 29.6 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 121389 steps/s (collection: 0.637s, learning 0.173s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0467
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.81s
                        Total time: 53.03s
                               ETA: 723 mins 39.1 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 119996 steps/s (collection: 0.633s, learning 0.186s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0468
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.82s
                        Total time: 53.85s
                               ETA: 722 mins 57.8 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 125845 steps/s (collection: 0.608s, learning 0.173s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0444
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.78s
                        Total time: 54.64s
                               ETA: 721 mins 47.6 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 114732 steps/s (collection: 0.680s, learning 0.177s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0464
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.86s
                        Total time: 55.49s
                               ETA: 721 mins 38.6 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 116057 steps/s (collection: 0.674s, learning 0.173s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0454
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.85s
                        Total time: 56.34s
                               ETA: 721 mins 22.3 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 115352 steps/s (collection: 0.666s, learning 0.186s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0442
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.85s
                        Total time: 57.19s
                               ETA: 721 mins 10.5 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 109474 steps/s (collection: 0.716s, learning 0.182s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0458
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0422
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.90s
                        Total time: 58.09s
                               ETA: 721 mins 33.0 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 125806 steps/s (collection: 0.609s, learning 0.173s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0453
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.78s
                        Total time: 58.87s
                               ETA: 720 mins 29.3 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 123143 steps/s (collection: 0.621s, learning 0.178s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0451
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.80s
                        Total time: 59.67s
                               ETA: 719 mins 39.6 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 128702 steps/s (collection: 0.591s, learning 0.173s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0453
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.76s
                        Total time: 60.43s
                               ETA: 718 mins 26.7 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 123975 steps/s (collection: 0.621s, learning 0.172s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0436
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.79s
                        Total time: 61.23s
                               ETA: 717 mins 36.4 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 128197 steps/s (collection: 0.594s, learning 0.173s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0441
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.77s
                        Total time: 61.99s
                               ETA: 716 mins 29.3 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 128795 steps/s (collection: 0.590s, learning 0.173s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0406
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.76s
                        Total time: 62.76s
                               ETA: 715 mins 21.5 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 131880 steps/s (collection: 0.574s, learning 0.172s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0430
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.75s
                        Total time: 63.50s
                               ETA: 714 mins 3.6 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 108979 steps/s (collection: 0.726s, learning 0.176s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0451
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.90s
                        Total time: 64.40s
                               ETA: 714 mins 32.0 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 128767 steps/s (collection: 0.583s, learning 0.181s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0405
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.76s
                        Total time: 65.17s
                               ETA: 713 mins 28.5 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 126343 steps/s (collection: 0.606s, learning 0.172s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0446
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0439
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.78s
                        Total time: 65.94s
                               ETA: 712 mins 36.2 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 120037 steps/s (collection: 0.645s, learning 0.174s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0441
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.82s
                        Total time: 66.76s
                               ETA: 712 mins 11.3 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 123149 steps/s (collection: 0.626s, learning 0.173s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0449
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.80s
                        Total time: 67.56s
                               ETA: 711 mins 34.0 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 117902 steps/s (collection: 0.657s, learning 0.176s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0438
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.83s
                        Total time: 68.40s
                               ETA: 711 mins 19.8 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 118061 steps/s (collection: 0.656s, learning 0.176s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0417
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.83s
                        Total time: 69.23s
                               ETA: 711 mins 5.2 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 121848 steps/s (collection: 0.631s, learning 0.176s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0431
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.81s
                        Total time: 70.04s
                               ETA: 710 mins 35.2 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 112175 steps/s (collection: 0.705s, learning 0.171s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0438
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.88s
                        Total time: 70.91s
                               ETA: 710 mins 47.7 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 109604 steps/s (collection: 0.710s, learning 0.187s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0420
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.90s
                        Total time: 71.81s
                               ETA: 711 mins 12.1 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 121242 steps/s (collection: 0.617s, learning 0.194s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0409
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.81s
                        Total time: 72.62s
                               ETA: 710 mins 45.4 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 125612 steps/s (collection: 0.591s, learning 0.192s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0427
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.78s
                        Total time: 73.40s
                               ETA: 710 mins 2.9 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 120107 steps/s (collection: 0.611s, learning 0.207s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0411
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.82s
                        Total time: 74.22s
                               ETA: 709 mins 42.0 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 118551 steps/s (collection: 0.645s, learning 0.184s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0421
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.83s
                        Total time: 75.05s
                               ETA: 709 mins 27.5 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 127837 steps/s (collection: 0.598s, learning 0.171s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0463
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0442
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.77s
                        Total time: 75.82s
                               ETA: 708 mins 39.7 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 119484 steps/s (collection: 0.644s, learning 0.178s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0392
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.82s
                        Total time: 76.64s
                               ETA: 708 mins 22.6 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 120825 steps/s (collection: 0.641s, learning 0.172s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0423
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.81s
                        Total time: 77.45s
                               ETA: 708 mins 1.0 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 116136 steps/s (collection: 0.663s, learning 0.184s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0419
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.85s
                        Total time: 78.30s
                               ETA: 707 mins 57.6 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 121374 steps/s (collection: 0.637s, learning 0.173s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0419
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.81s
                        Total time: 79.11s
                               ETA: 707 mins 34.6 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 129803 steps/s (collection: 0.583s, learning 0.175s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0399
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.76s
                        Total time: 79.87s
                               ETA: 706 mins 44.2 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 129058 steps/s (collection: 0.571s, learning 0.190s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0403
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.76s
                        Total time: 80.63s
                               ETA: 705 mins 57.2 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 127741 steps/s (collection: 0.579s, learning 0.190s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0417
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.77s
                        Total time: 81.40s
                               ETA: 705 mins 15.1 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 126099 steps/s (collection: 0.594s, learning 0.186s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0440
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.78s
                        Total time: 82.18s
                               ETA: 704 mins 39.1 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 127403 steps/s (collection: 0.584s, learning 0.188s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0377
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.77s
                        Total time: 82.95s
                               ETA: 703 mins 59.8 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 127300 steps/s (collection: 0.583s, learning 0.190s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0411
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.77s
                        Total time: 83.72s
                               ETA: 703 mins 21.5 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 113455 steps/s (collection: 0.694s, learning 0.173s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0434
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.87s
                        Total time: 84.59s
                               ETA: 703 mins 31.0 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 125148 steps/s (collection: 0.613s, learning 0.173s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0444
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0398
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.79s
                        Total time: 85.38s
                               ETA: 703 mins 0.4 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 130119 steps/s (collection: 0.583s, learning 0.173s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0415
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.76s
                        Total time: 86.13s
                               ETA: 702 mins 15.6 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 128191 steps/s (collection: 0.581s, learning 0.186s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0425
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.77s
                        Total time: 86.90s
                               ETA: 701 mins 37.2 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 120724 steps/s (collection: 0.622s, learning 0.192s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0448
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0393
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.81s
                        Total time: 87.71s
                               ETA: 701 mins 22.2 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 124962 steps/s (collection: 0.595s, learning 0.192s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0401
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.79s
                        Total time: 88.50s
                               ETA: 700 mins 54.4 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 129265 steps/s (collection: 0.584s, learning 0.176s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0439
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0383
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.76s
                        Total time: 89.26s
                               ETA: 700 mins 14.8 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 128197 steps/s (collection: 0.595s, learning 0.172s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0404
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.77s
                        Total time: 90.03s
                               ETA: 699 mins 38.9 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 126584 steps/s (collection: 0.604s, learning 0.172s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0377
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.78s
                        Total time: 90.80s
                               ETA: 699 mins 8.1 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 126204 steps/s (collection: 0.593s, learning 0.186s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0402
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.78s
                        Total time: 91.58s
                               ETA: 698 mins 39.0 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 122612 steps/s (collection: 0.628s, learning 0.174s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0403
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.80s
                        Total time: 92.38s
                               ETA: 698 mins 20.7 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 116388 steps/s (collection: 0.647s, learning 0.197s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0439
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0381
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.84s
                        Total time: 93.23s
                               ETA: 698 mins 22.0 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 121885 steps/s (collection: 0.615s, learning 0.191s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0367
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.81s
                        Total time: 94.03s
                               ETA: 698 mins 6.3 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 120421 steps/s (collection: 0.618s, learning 0.198s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0444
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0388
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.82s
                        Total time: 94.85s
                               ETA: 697 mins 55.2 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 127878 steps/s (collection: 0.579s, learning 0.190s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0393
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.77s
                        Total time: 95.62s
                               ETA: 697 mins 23.4 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 124652 steps/s (collection: 0.599s, learning 0.190s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0375
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.79s
                        Total time: 96.41s
                               ETA: 697 mins 0.8 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 118920 steps/s (collection: 0.653s, learning 0.173s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0438
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0357
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.83s
                        Total time: 97.23s
                               ETA: 696 mins 55.0 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 131259 steps/s (collection: 0.575s, learning 0.174s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0433
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0368
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.75s
                        Total time: 97.98s
                               ETA: 696 mins 16.1 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 125545 steps/s (collection: 0.610s, learning 0.173s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0380
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.78s
                        Total time: 98.77s
                               ETA: 695 mins 52.2 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 132005 steps/s (collection: 0.573s, learning 0.172s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0374
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.74s
                        Total time: 99.51s
                               ETA: 695 mins 12.7 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 130945 steps/s (collection: 0.570s, learning 0.181s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0368
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.75s
                        Total time: 100.26s
                               ETA: 694 mins 36.3 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 130088 steps/s (collection: 0.578s, learning 0.177s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0353
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.76s
                        Total time: 101.02s
                               ETA: 694 mins 2.6 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 130066 steps/s (collection: 0.580s, learning 0.176s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0434
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0370
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.76s
                        Total time: 101.77s
                               ETA: 693 mins 29.4 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 133812 steps/s (collection: 0.560s, learning 0.175s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0359
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.73s
                        Total time: 102.51s
                               ETA: 692 mins 48.2 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 125008 steps/s (collection: 0.601s, learning 0.185s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0374
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.79s
                        Total time: 103.29s
                               ETA: 692 mins 28.4 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 119502 steps/s (collection: 0.641s, learning 0.181s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0363
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.82s
                        Total time: 104.12s
                               ETA: 692 mins 23.5 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 112879 steps/s (collection: 0.684s, learning 0.187s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0362
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.87s
                        Total time: 104.99s
                               ETA: 692 mins 37.6 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 123330 steps/s (collection: 0.613s, learning 0.184s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0376
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.80s
                        Total time: 105.78s
                               ETA: 692 mins 22.6 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 111102 steps/s (collection: 0.701s, learning 0.184s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0429
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0368
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.88s
                        Total time: 106.67s
                               ETA: 692 mins 42.0 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 118851 steps/s (collection: 0.645s, learning 0.183s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0424
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0343
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.83s
                        Total time: 107.50s
                               ETA: 692 mins 38.7 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 127946 steps/s (collection: 0.593s, learning 0.175s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0430
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0345
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.77s
                        Total time: 108.27s
                               ETA: 692 mins 13.0 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 119871 steps/s (collection: 0.641s, learning 0.179s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.82s
                        Total time: 109.09s
                               ETA: 692 mins 7.3 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 117627 steps/s (collection: 0.658s, learning 0.177s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0415
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.84s
                        Total time: 109.92s
                               ETA: 692 mins 7.6 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 121449 steps/s (collection: 0.634s, learning 0.176s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0355
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.81s
                        Total time: 110.73s
                               ETA: 691 mins 58.0 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 112715 steps/s (collection: 0.675s, learning 0.197s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0357
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.87s
                        Total time: 111.60s
                               ETA: 692 mins 11.9 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 121623 steps/s (collection: 0.619s, learning 0.190s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.81s
                        Total time: 112.41s
                               ETA: 692 mins 2.0 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 107614 steps/s (collection: 0.722s, learning 0.191s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0350
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.91s
                        Total time: 113.32s
                               ETA: 692 mins 30.8 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 128196 steps/s (collection: 0.585s, learning 0.181s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0335
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.77s
                        Total time: 114.09s
                               ETA: 692 mins 5.8 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 124899 steps/s (collection: 0.602s, learning 0.185s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0337
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.79s
                        Total time: 114.88s
                               ETA: 691 mins 48.4 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 121148 steps/s (collection: 0.631s, learning 0.180s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.81s
                        Total time: 115.69s
                               ETA: 691 mins 40.1 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 121369 steps/s (collection: 0.635s, learning 0.175s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.81s
                        Total time: 116.50s
                               ETA: 691 mins 31.3 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 130442 steps/s (collection: 0.581s, learning 0.172s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0422
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0342
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.75s
                        Total time: 117.25s
                               ETA: 691 mins 2.7 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 130727 steps/s (collection: 0.579s, learning 0.173s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0341
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.75s
                        Total time: 118.01s
                               ETA: 690 mins 33.9 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 120361 steps/s (collection: 0.625s, learning 0.192s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0328
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.82s
                        Total time: 118.82s
                               ETA: 690 mins 28.1 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 118966 steps/s (collection: 0.637s, learning 0.190s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0321
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.83s
                        Total time: 119.65s
                               ETA: 690 mins 25.7 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 117734 steps/s (collection: 0.653s, learning 0.182s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0346
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.83s
                        Total time: 120.48s
                               ETA: 690 mins 26.2 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 117623 steps/s (collection: 0.655s, learning 0.180s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0333
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.84s
                        Total time: 121.32s
                               ETA: 690 mins 27.1 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 110921 steps/s (collection: 0.707s, learning 0.180s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0312
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.89s
                        Total time: 122.21s
                               ETA: 690 mins 45.0 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 115444 steps/s (collection: 0.666s, learning 0.186s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0342
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.85s
                        Total time: 123.06s
                               ETA: 690 mins 50.9 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 117253 steps/s (collection: 0.655s, learning 0.183s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.84s
                        Total time: 123.89s
                               ETA: 690 mins 52.4 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 121768 steps/s (collection: 0.628s, learning 0.180s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0309
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.81s
                        Total time: 124.70s
                               ETA: 690 mins 43.6 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 132019 steps/s (collection: 0.565s, learning 0.180s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.74s
                        Total time: 125.45s
                               ETA: 690 mins 14.1 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 121902 steps/s (collection: 0.625s, learning 0.181s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0267
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.81s
                        Total time: 126.25s
                               ETA: 690 mins 5.3 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 120533 steps/s (collection: 0.643s, learning 0.173s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.82s
                        Total time: 127.07s
                               ETA: 689 mins 59.5 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 121687 steps/s (collection: 0.634s, learning 0.173s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.81s
                        Total time: 127.88s
                               ETA: 689 mins 51.4 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 132615 steps/s (collection: 0.547s, learning 0.195s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0396
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0265
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.74s
                        Total time: 128.62s
                               ETA: 689 mins 21.9 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 135414 steps/s (collection: 0.553s, learning 0.173s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0295
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0265
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.73s
                        Total time: 129.34s
                               ETA: 688 mins 47.9 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 135210 steps/s (collection: 0.555s, learning 0.172s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0312
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.73s
                        Total time: 130.07s
                               ETA: 688 mins 14.6 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 136137 steps/s (collection: 0.543s, learning 0.179s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.72s
                        Total time: 130.79s
                               ETA: 687 mins 40.2 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 128830 steps/s (collection: 0.575s, learning 0.188s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0267
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.76s
                        Total time: 131.56s
                               ETA: 687 mins 19.1 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 111143 steps/s (collection: 0.702s, learning 0.182s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.88s
                        Total time: 132.44s
                               ETA: 687 mins 36.1 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 118685 steps/s (collection: 0.647s, learning 0.181s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.83s
                        Total time: 133.27s
                               ETA: 687 mins 35.4 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 127014 steps/s (collection: 0.595s, learning 0.179s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.77s
                        Total time: 134.04s
                               ETA: 687 mins 18.0 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 117936 steps/s (collection: 0.648s, learning 0.186s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0321
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.83s
                        Total time: 134.88s
                               ETA: 687 mins 19.1 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 14384 steps/s (collection: 6.655s, learning 0.179s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 6.83s
                        Total time: 141.71s
                               ETA: 717 mins 43.5 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 36891 steps/s (collection: 2.490s, learning 0.175s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0332
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0281
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.66s
                        Total time: 144.37s
                               ETA: 726 mins 46.5 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 120438 steps/s (collection: 0.643s, learning 0.173s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0312
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0278
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.82s
                        Total time: 145.19s
                               ETA: 726 mins 28.0 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 122608 steps/s (collection: 0.616s, learning 0.186s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0310
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.80s
                        Total time: 145.99s
                               ETA: 726 mins 5.4 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 124708 steps/s (collection: 0.615s, learning 0.173s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0311
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.79s
                        Total time: 146.78s
                               ETA: 725 mins 39.0 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 121901 steps/s (collection: 0.627s, learning 0.179s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0317
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.81s
                        Total time: 147.59s
                               ETA: 725 mins 18.3 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 122244 steps/s (collection: 0.621s, learning 0.183s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0321
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.80s
                        Total time: 148.39s
                               ETA: 724 mins 57.1 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 125110 steps/s (collection: 0.594s, learning 0.192s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.79s
                        Total time: 149.18s
                               ETA: 724 mins 30.9 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 121603 steps/s (collection: 0.614s, learning 0.194s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0285
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.81s
                        Total time: 149.99s
                               ETA: 724 mins 11.5 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 129305 steps/s (collection: 0.551s, learning 0.209s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0282
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.76s
                        Total time: 150.75s
                               ETA: 723 mins 38.4 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 120523 steps/s (collection: 0.623s, learning 0.193s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0389
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.82s
                        Total time: 151.56s
                               ETA: 723 mins 21.6 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 120090 steps/s (collection: 0.627s, learning 0.192s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.82s
                        Total time: 152.38s
                               ETA: 723 mins 5.8 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 134798 steps/s (collection: 0.556s, learning 0.174s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0286
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.73s
                        Total time: 153.11s
                               ETA: 722 mins 24.8 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 128526 steps/s (collection: 0.584s, learning 0.181s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0285
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.76s
                        Total time: 153.87s
                               ETA: 721 mins 54.4 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 135175 steps/s (collection: 0.554s, learning 0.173s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0389
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0301
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.73s
                        Total time: 154.60s
                               ETA: 721 mins 13.7 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 120100 steps/s (collection: 0.630s, learning 0.189s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0396
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0312
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.82s
                        Total time: 155.42s
                               ETA: 720 mins 58.9 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 121855 steps/s (collection: 0.617s, learning 0.190s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0306
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.81s
                        Total time: 156.23s
                               ETA: 720 mins 41.0 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 116744 steps/s (collection: 0.656s, learning 0.186s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.84s
                        Total time: 157.07s
                               ETA: 720 mins 33.1 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 131183 steps/s (collection: 0.568s, learning 0.182s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.75s
                        Total time: 157.82s
                               ETA: 719 mins 59.8 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 124495 steps/s (collection: 0.610s, learning 0.180s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0332
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.79s
                        Total time: 158.61s
                               ETA: 719 mins 37.8 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 127588 steps/s (collection: 0.597s, learning 0.174s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0288
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.77s
                        Total time: 159.38s
                               ETA: 719 mins 10.9 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 130543 steps/s (collection: 0.580s, learning 0.173s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.75s
                        Total time: 160.13s
                               ETA: 718 mins 39.5 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 119490 steps/s (collection: 0.637s, learning 0.185s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.82s
                        Total time: 160.95s
                               ETA: 718 mins 27.2 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 131209 steps/s (collection: 0.575s, learning 0.174s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.75s
                        Total time: 161.70s
                               ETA: 717 mins 55.4 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 121880 steps/s (collection: 0.632s, learning 0.175s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.81s
                        Total time: 162.51s
                               ETA: 717 mins 39.1 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 113617 steps/s (collection: 0.681s, learning 0.184s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0323
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.87s
                        Total time: 163.38s
                               ETA: 717 mins 38.5 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 121724 steps/s (collection: 0.614s, learning 0.193s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0384
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0311
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0298
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.81s
                        Total time: 164.18s
                               ETA: 717 mins 22.7 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 116086 steps/s (collection: 0.658s, learning 0.189s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0298
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.85s
                        Total time: 165.03s
                               ETA: 717 mins 17.3 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 129330 steps/s (collection: 0.586s, learning 0.174s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0303
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.76s
                        Total time: 165.79s
                               ETA: 716 mins 49.5 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 109987 steps/s (collection: 0.707s, learning 0.187s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0317
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0304
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.89s
                        Total time: 166.68s
                               ETA: 716 mins 56.5 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 133419 steps/s (collection: 0.563s, learning 0.174s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0375
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.74s
                        Total time: 167.42s
                               ETA: 716 mins 23.0 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 128292 steps/s (collection: 0.593s, learning 0.173s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.77s
                        Total time: 168.19s
                               ETA: 715 mins 57.5 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 119592 steps/s (collection: 0.641s, learning 0.181s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.82s
                        Total time: 169.01s
                               ETA: 715 mins 46.3 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 123208 steps/s (collection: 0.616s, learning 0.182s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.80s
                        Total time: 169.81s
                               ETA: 715 mins 29.1 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 119840 steps/s (collection: 0.646s, learning 0.174s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0304
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.82s
                        Total time: 170.63s
                               ETA: 715 mins 17.8 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 120551 steps/s (collection: 0.641s, learning 0.175s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0307
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.82s
                        Total time: 171.44s
                               ETA: 715 mins 5.4 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 118363 steps/s (collection: 0.643s, learning 0.188s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0375
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0304
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.83s
                        Total time: 172.27s
                               ETA: 714 mins 56.8 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 133211 steps/s (collection: 0.560s, learning 0.178s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.74s
                        Total time: 173.01s
                               ETA: 714 mins 25.3 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 128875 steps/s (collection: 0.582s, learning 0.181s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0309
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.76s
                        Total time: 173.77s
                               ETA: 714 mins 0.3 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 129522 steps/s (collection: 0.570s, learning 0.189s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0378
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0304
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.76s
                        Total time: 174.53s
                               ETA: 713 mins 34.6 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 114877 steps/s (collection: 0.664s, learning 0.191s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0375
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.86s
                        Total time: 175.39s
                               ETA: 713 mins 32.8 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 128648 steps/s (collection: 0.576s, learning 0.188s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0310
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.76s
                        Total time: 176.15s
                               ETA: 713 mins 8.7 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 132422 steps/s (collection: 0.569s, learning 0.173s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0372
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.74s
                        Total time: 176.89s
                               ETA: 712 mins 39.6 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 121427 steps/s (collection: 0.637s, learning 0.173s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0310
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.81s
                        Total time: 177.70s
                               ETA: 712 mins 26.9 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 125627 steps/s (collection: 0.610s, learning 0.173s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.78s
                        Total time: 178.49s
                               ETA: 712 mins 7.8 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 115532 steps/s (collection: 0.677s, learning 0.174s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0320
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.85s
                        Total time: 179.34s
                               ETA: 712 mins 5.3 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 117784 steps/s (collection: 0.645s, learning 0.189s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0310
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.83s
                        Total time: 180.17s
                               ETA: 711 mins 58.8 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 110689 steps/s (collection: 0.688s, learning 0.200s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.89s
                        Total time: 181.06s
                               ETA: 712 mins 5.1 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 124431 steps/s (collection: 0.617s, learning 0.173s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0323
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.79s
                        Total time: 181.85s
                               ETA: 711 mins 48.2 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 125664 steps/s (collection: 0.609s, learning 0.173s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.78s
                        Total time: 182.63s
                               ETA: 711 mins 29.7 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 115538 steps/s (collection: 0.669s, learning 0.182s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0316
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.85s
                        Total time: 183.48s
                               ETA: 711 mins 27.3 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 119399 steps/s (collection: 0.623s, learning 0.200s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.82s
                        Total time: 184.31s
                               ETA: 711 mins 18.6 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 113600 steps/s (collection: 0.672s, learning 0.193s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.87s
                        Total time: 185.17s
                               ETA: 711 mins 19.6 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 117030 steps/s (collection: 0.649s, learning 0.191s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0285
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.84s
                        Total time: 186.01s
                               ETA: 711 mins 14.8 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 122106 steps/s (collection: 0.610s, learning 0.195s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0320
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.81s
                        Total time: 186.82s
                               ETA: 711 mins 2.0 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 129060 steps/s (collection: 0.576s, learning 0.186s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0301
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.76s
                        Total time: 187.58s
                               ETA: 710 mins 39.5 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 130799 steps/s (collection: 0.578s, learning 0.173s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0281
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.75s
                        Total time: 188.33s
                               ETA: 710 mins 14.9 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 134005 steps/s (collection: 0.557s, learning 0.177s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0320
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.73s
                        Total time: 189.06s
                               ETA: 709 mins 46.5 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 111572 steps/s (collection: 0.701s, learning 0.181s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.88s
                        Total time: 189.95s
                               ETA: 709 mins 51.3 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 123250 steps/s (collection: 0.611s, learning 0.186s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0323
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.80s
                        Total time: 190.74s
                               ETA: 709 mins 37.5 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 121528 steps/s (collection: 0.630s, learning 0.179s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.81s
                        Total time: 191.55s
                               ETA: 709 mins 26.3 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 122356 steps/s (collection: 0.613s, learning 0.190s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.80s
                        Total time: 192.35s
                               ETA: 709 mins 14.1 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 107038 steps/s (collection: 0.714s, learning 0.205s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.92s
                        Total time: 193.27s
                               ETA: 709 mins 27.2 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 128544 steps/s (collection: 0.574s, learning 0.191s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.76s
                        Total time: 194.04s
                               ETA: 709 mins 6.5 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 119451 steps/s (collection: 0.632s, learning 0.191s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.82s
                        Total time: 194.86s
                               ETA: 708 mins 58.7 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 115965 steps/s (collection: 0.638s, learning 0.209s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.85s
                        Total time: 195.71s
                               ETA: 708 mins 56.3 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 115823 steps/s (collection: 0.645s, learning 0.203s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0301
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.85s
                        Total time: 196.56s
                               ETA: 708 mins 54.2 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 121167 steps/s (collection: 0.621s, learning 0.190s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0327
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.81s
                        Total time: 197.37s
                               ETA: 708 mins 44.0 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 130134 steps/s (collection: 0.566s, learning 0.190s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.76s
                        Total time: 198.12s
                               ETA: 708 mins 21.9 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 121446 steps/s (collection: 0.625s, learning 0.185s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.81s
                        Total time: 198.93s
                               ETA: 708 mins 11.6 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 118223 steps/s (collection: 0.657s, learning 0.174s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.83s
                        Total time: 199.77s
                               ETA: 708 mins 6.0 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 130081 steps/s (collection: 0.581s, learning 0.175s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.76s
                        Total time: 200.52s
                               ETA: 707 mins 44.4 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 122247 steps/s (collection: 0.630s, learning 0.174s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.80s
                        Total time: 201.33s
                               ETA: 707 mins 33.1 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 132613 steps/s (collection: 0.568s, learning 0.174s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.74s
                        Total time: 202.07s
                               ETA: 707 mins 8.8 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 130567 steps/s (collection: 0.563s, learning 0.190s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.75s
                        Total time: 202.82s
                               ETA: 706 mins 47.1 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 114142 steps/s (collection: 0.671s, learning 0.190s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.86s
                        Total time: 203.68s
                               ETA: 706 mins 48.1 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 125866 steps/s (collection: 0.595s, learning 0.186s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.78s
                        Total time: 204.46s
                               ETA: 706 mins 32.5 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 130982 steps/s (collection: 0.577s, learning 0.173s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.75s
                        Total time: 205.21s
                               ETA: 706 mins 10.7 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 121580 steps/s (collection: 0.629s, learning 0.179s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0337
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.81s
                        Total time: 206.02s
                               ETA: 706 mins 1.1 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 121079 steps/s (collection: 0.639s, learning 0.173s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0335
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.81s
                        Total time: 206.83s
                               ETA: 705 mins 52.1 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 124492 steps/s (collection: 0.616s, learning 0.174s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.79s
                        Total time: 207.62s
                               ETA: 705 mins 38.7 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 132370 steps/s (collection: 0.569s, learning 0.173s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.74s
                        Total time: 208.36s
                               ETA: 705 mins 15.9 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 124482 steps/s (collection: 0.604s, learning 0.186s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0335
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.79s
                        Total time: 209.15s
                               ETA: 705 mins 2.8 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 127023 steps/s (collection: 0.591s, learning 0.183s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.77s
                        Total time: 209.93s
                               ETA: 704 mins 46.5 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 115897 steps/s (collection: 0.654s, learning 0.194s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.85s
                        Total time: 210.78s
                               ETA: 704 mins 45.3 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 128534 steps/s (collection: 0.572s, learning 0.193s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.76s
                        Total time: 211.54s
                               ETA: 704 mins 27.5 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 117703 steps/s (collection: 0.622s, learning 0.213s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.84s
                        Total time: 212.38s
                               ETA: 704 mins 23.8 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 127056 steps/s (collection: 0.581s, learning 0.193s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.77s
                        Total time: 213.15s
                               ETA: 704 mins 7.9 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 124143 steps/s (collection: 0.606s, learning 0.186s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.79s
                        Total time: 213.94s
                               ETA: 703 mins 55.7 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 132170 steps/s (collection: 0.569s, learning 0.175s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.74s
                        Total time: 214.69s
                               ETA: 703 mins 34.2 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 125112 steps/s (collection: 0.606s, learning 0.180s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.79s
                        Total time: 215.47s
                               ETA: 703 mins 21.0 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 127100 steps/s (collection: 0.584s, learning 0.189s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.77s
                        Total time: 216.25s
                               ETA: 703 mins 5.6 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 113277 steps/s (collection: 0.687s, learning 0.181s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.87s
                        Total time: 217.11s
                               ETA: 703 mins 8.6 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 118190 steps/s (collection: 0.645s, learning 0.186s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.83s
                        Total time: 217.94s
                               ETA: 703 mins 4.6 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 115653 steps/s (collection: 0.667s, learning 0.183s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0351
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.85s
                        Total time: 218.79s
                               ETA: 703 mins 4.1 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 127018 steps/s (collection: 0.590s, learning 0.184s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.77s
                        Total time: 219.57s
                               ETA: 702 mins 49.0 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 132120 steps/s (collection: 0.572s, learning 0.172s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0350
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.74s
                        Total time: 220.31s
                               ETA: 702 mins 28.3 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 114947 steps/s (collection: 0.667s, learning 0.188s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.86s
                        Total time: 221.17s
                               ETA: 702 mins 29.0 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 131746 steps/s (collection: 0.573s, learning 0.173s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.75s
                        Total time: 221.91s
                               ETA: 702 mins 8.9 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 121490 steps/s (collection: 0.635s, learning 0.174s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.81s
                        Total time: 222.72s
                               ETA: 702 mins 0.9 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 123093 steps/s (collection: 0.612s, learning 0.187s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0350
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.80s
                        Total time: 223.52s
                               ETA: 701 mins 51.0 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 124299 steps/s (collection: 0.617s, learning 0.174s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.79s
                        Total time: 224.31s
                               ETA: 701 mins 39.7 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 116614 steps/s (collection: 0.656s, learning 0.187s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.84s
                        Total time: 225.16s
                               ETA: 701 mins 38.2 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 121073 steps/s (collection: 0.639s, learning 0.173s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.81s
                        Total time: 225.97s
                               ETA: 701 mins 30.9 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 128101 steps/s (collection: 0.590s, learning 0.178s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0354
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.77s
                        Total time: 226.73s
                               ETA: 701 mins 15.4 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 121721 steps/s (collection: 0.634s, learning 0.174s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.81s
                        Total time: 227.54s
                               ETA: 701 mins 7.4 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 128271 steps/s (collection: 0.580s, learning 0.186s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0354
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.77s
                        Total time: 228.31s
                               ETA: 700 mins 51.9 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 118124 steps/s (collection: 0.636s, learning 0.196s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.83s
                        Total time: 229.14s
                               ETA: 700 mins 48.6 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 111572 steps/s (collection: 0.694s, learning 0.187s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.88s
                        Total time: 230.02s
                               ETA: 700 mins 54.3 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 110130 steps/s (collection: 0.694s, learning 0.199s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.89s
                        Total time: 230.91s
                               ETA: 701 mins 2.0 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 122057 steps/s (collection: 0.614s, learning 0.191s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.81s
                        Total time: 231.72s
                               ETA: 700 mins 53.8 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 110147 steps/s (collection: 0.708s, learning 0.185s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.89s
                        Total time: 232.61s
                               ETA: 701 mins 1.4 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 114403 steps/s (collection: 0.678s, learning 0.181s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.86s
                        Total time: 233.47s
                               ETA: 701 mins 3.0 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 122099 steps/s (collection: 0.620s, learning 0.185s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0357
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.81s
                        Total time: 234.28s
                               ETA: 700 mins 54.8 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 118528 steps/s (collection: 0.649s, learning 0.181s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.83s
                        Total time: 235.11s
                               ETA: 700 mins 51.1 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 124921 steps/s (collection: 0.613s, learning 0.174s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0354
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.79s
                        Total time: 235.89s
                               ETA: 700 mins 39.7 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 114197 steps/s (collection: 0.687s, learning 0.174s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.86s
                        Total time: 236.75s
                               ETA: 700 mins 41.6 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 128458 steps/s (collection: 0.591s, learning 0.175s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.77s
                        Total time: 237.52s
                               ETA: 700 mins 26.6 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 126450 steps/s (collection: 0.604s, learning 0.174s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0371
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.78s
                        Total time: 238.30s
                               ETA: 700 mins 13.7 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 122550 steps/s (collection: 0.628s, learning 0.174s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.80s
                        Total time: 239.10s
                               ETA: 700 mins 5.4 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 113574 steps/s (collection: 0.685s, learning 0.180s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.87s
                        Total time: 239.96s
                               ETA: 700 mins 8.1 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 116892 steps/s (collection: 0.659s, learning 0.182s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.84s
                        Total time: 240.81s
                               ETA: 700 mins 6.6 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 112539 steps/s (collection: 0.694s, learning 0.180s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0370
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.87s
                        Total time: 241.68s
                               ETA: 700 mins 10.7 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 110636 steps/s (collection: 0.715s, learning 0.174s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.89s
                        Total time: 242.57s
                               ETA: 700 mins 17.4 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 120577 steps/s (collection: 0.619s, learning 0.196s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.82s
                        Total time: 243.38s
                               ETA: 700 mins 11.4 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 122528 steps/s (collection: 0.625s, learning 0.177s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.80s
                        Total time: 244.19s
                               ETA: 700 mins 3.2 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 120003 steps/s (collection: 0.642s, learning 0.177s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.82s
                        Total time: 245.00s
                               ETA: 699 mins 57.9 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 124427 steps/s (collection: 0.600s, learning 0.190s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.79s
                        Total time: 245.79s
                               ETA: 699 mins 47.7 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 106541 steps/s (collection: 0.734s, learning 0.189s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.92s
                        Total time: 246.72s
                               ETA: 700 mins 0.2 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 122932 steps/s (collection: 0.626s, learning 0.174s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.80s
                        Total time: 247.52s
                               ETA: 699 mins 51.7 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 115062 steps/s (collection: 0.668s, learning 0.186s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.85s
                        Total time: 248.37s
                               ETA: 699 mins 52.4 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 115996 steps/s (collection: 0.672s, learning 0.176s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0370
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.85s
                        Total time: 249.22s
                               ETA: 699 mins 52.0 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 116867 steps/s (collection: 0.667s, learning 0.174s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.84s
                        Total time: 250.06s
                               ETA: 699 mins 50.6 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 124293 steps/s (collection: 0.610s, learning 0.181s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.79s
                        Total time: 250.85s
                               ETA: 699 mins 40.7 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 114594 steps/s (collection: 0.683s, learning 0.175s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.86s
                        Total time: 251.71s
                               ETA: 699 mins 42.1 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 111912 steps/s (collection: 0.699s, learning 0.179s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.88s
                        Total time: 252.59s
                               ETA: 699 mins 46.8 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 128165 steps/s (collection: 0.577s, learning 0.190s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.77s
                        Total time: 253.35s
                               ETA: 699 mins 33.1 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 122681 steps/s (collection: 0.611s, learning 0.190s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.80s
                        Total time: 254.16s
                               ETA: 699 mins 25.1 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 118697 steps/s (collection: 0.639s, learning 0.189s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.83s
                        Total time: 254.98s
                               ETA: 699 mins 21.6 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 122368 steps/s (collection: 0.623s, learning 0.180s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.80s
                        Total time: 255.79s
                               ETA: 699 mins 14.1 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 113120 steps/s (collection: 0.688s, learning 0.181s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0373
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.87s
                        Total time: 256.66s
                               ETA: 699 mins 17.3 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 126390 steps/s (collection: 0.604s, learning 0.174s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.78s
                        Total time: 257.43s
                               ETA: 699 mins 5.6 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 112566 steps/s (collection: 0.699s, learning 0.175s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0284
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.87s
                        Total time: 258.31s
                               ETA: 699 mins 9.5 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 118134 steps/s (collection: 0.643s, learning 0.189s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.83s
                        Total time: 259.14s
                               ETA: 699 mins 6.7 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 125185 steps/s (collection: 0.612s, learning 0.174s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.79s
                        Total time: 259.92s
                               ETA: 698 mins 56.4 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 118975 steps/s (collection: 0.653s, learning 0.174s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.83s
                        Total time: 260.75s
                               ETA: 698 mins 52.7 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 118230 steps/s (collection: 0.651s, learning 0.180s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.83s
                        Total time: 261.58s
                               ETA: 698 mins 49.9 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 127829 steps/s (collection: 0.596s, learning 0.173s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0373
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.77s
                        Total time: 262.35s
                               ETA: 698 mins 37.1 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 121479 steps/s (collection: 0.631s, learning 0.178s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.81s
                        Total time: 263.16s
                               ETA: 698 mins 30.8 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 122220 steps/s (collection: 0.613s, learning 0.191s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.80s
                        Total time: 263.96s
                               ETA: 698 mins 23.7 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 121161 steps/s (collection: 0.618s, learning 0.193s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.81s
                        Total time: 264.78s
                               ETA: 698 mins 17.8 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 115582 steps/s (collection: 0.664s, learning 0.186s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.85s
                        Total time: 265.63s
                               ETA: 698 mins 18.1 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 122945 steps/s (collection: 0.625s, learning 0.175s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0382
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.80s
                        Total time: 266.43s
                               ETA: 698 mins 10.4 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 114634 steps/s (collection: 0.676s, learning 0.181s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.86s
                        Total time: 267.28s
                               ETA: 698 mins 11.8 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 122359 steps/s (collection: 0.630s, learning 0.173s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.80s
                        Total time: 268.09s
                               ETA: 698 mins 4.8 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 111834 steps/s (collection: 0.699s, learning 0.180s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0377
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.88s
                        Total time: 268.97s
                               ETA: 698 mins 9.5 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 113133 steps/s (collection: 0.677s, learning 0.191s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.87s
                        Total time: 269.83s
                               ETA: 698 mins 12.7 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 110390 steps/s (collection: 0.710s, learning 0.180s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.89s
                        Total time: 270.73s
                               ETA: 698 mins 19.2 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 127075 steps/s (collection: 0.601s, learning 0.173s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.77s
                        Total time: 271.50s
                               ETA: 698 mins 7.5 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 110159 steps/s (collection: 0.684s, learning 0.209s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.89s
                        Total time: 272.39s
                               ETA: 698 mins 14.3 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 117751 steps/s (collection: 0.634s, learning 0.200s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.83s
                        Total time: 273.23s
                               ETA: 698 mins 12.1 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 114955 steps/s (collection: 0.652s, learning 0.203s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.86s
                        Total time: 274.08s
                               ETA: 698 mins 13.1 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 113192 steps/s (collection: 0.689s, learning 0.179s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.87s
                        Total time: 274.95s
                               ETA: 698 mins 16.1 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 116926 steps/s (collection: 0.667s, learning 0.173s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0384
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.84s
                        Total time: 275.79s
                               ETA: 698 mins 14.8 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 116343 steps/s (collection: 0.671s, learning 0.174s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.84s
                        Total time: 276.64s
                               ETA: 698 mins 14.2 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 117854 steps/s (collection: 0.648s, learning 0.186s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.83s
                        Total time: 277.47s
                               ETA: 698 mins 12.0 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 127852 steps/s (collection: 0.595s, learning 0.174s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.77s
                        Total time: 278.24s
                               ETA: 697 mins 59.9 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 117429 steps/s (collection: 0.661s, learning 0.176s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0390
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.84s
                        Total time: 279.08s
                               ETA: 697 mins 58.2 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 114357 steps/s (collection: 0.669s, learning 0.191s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.86s
                        Total time: 279.94s
                               ETA: 697 mins 59.8 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 118884 steps/s (collection: 0.637s, learning 0.190s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.83s
                        Total time: 280.76s
                               ETA: 697 mins 56.5 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 126016 steps/s (collection: 0.606s, learning 0.174s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.78s
                        Total time: 281.54s
                               ETA: 697 mins 46.3 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 113693 steps/s (collection: 0.671s, learning 0.193s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0390
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.86s
                        Total time: 282.41s
                               ETA: 697 mins 48.7 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 113307 steps/s (collection: 0.682s, learning 0.186s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.87s
                        Total time: 283.27s
                               ETA: 697 mins 51.5 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 116799 steps/s (collection: 0.651s, learning 0.190s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.84s
                        Total time: 284.12s
                               ETA: 697 mins 50.4 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 125767 steps/s (collection: 0.601s, learning 0.181s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.78s
                        Total time: 284.90s
                               ETA: 697 mins 40.6 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 119803 steps/s (collection: 0.629s, learning 0.191s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0382
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.82s
                        Total time: 285.72s
                               ETA: 697 mins 36.4 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 114449 steps/s (collection: 0.666s, learning 0.193s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.86s
                        Total time: 286.58s
                               ETA: 697 mins 37.9 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 110029 steps/s (collection: 0.710s, learning 0.183s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.89s
                        Total time: 287.47s
                               ETA: 697 mins 44.5 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 124897 steps/s (collection: 0.614s, learning 0.174s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.79s
                        Total time: 288.26s
                               ETA: 697 mins 35.5 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 105697 steps/s (collection: 0.731s, learning 0.199s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.93s
                        Total time: 289.19s
                               ETA: 697 mins 47.3 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 115187 steps/s (collection: 0.662s, learning 0.191s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.85s
                        Total time: 290.04s
                               ETA: 697 mins 47.9 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 120800 steps/s (collection: 0.622s, learning 0.192s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.81s
                        Total time: 290.85s
                               ETA: 697 mins 42.9 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 107607 steps/s (collection: 0.711s, learning 0.203s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.91s
                        Total time: 291.77s
                               ETA: 697 mins 52.1 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 114485 steps/s (collection: 0.660s, learning 0.199s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.86s
                        Total time: 292.63s
                               ETA: 697 mins 53.5 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 127296 steps/s (collection: 0.598s, learning 0.174s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.77s
                        Total time: 293.40s
                               ETA: 697 mins 42.5 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 111042 steps/s (collection: 0.697s, learning 0.188s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.89s
                        Total time: 294.28s
                               ETA: 697 mins 47.7 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 117311 steps/s (collection: 0.655s, learning 0.183s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.84s
                        Total time: 295.12s
                               ETA: 697 mins 46.1 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 117649 steps/s (collection: 0.662s, learning 0.174s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.84s
                        Total time: 295.96s
                               ETA: 697 mins 44.2 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 117857 steps/s (collection: 0.660s, learning 0.174s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0405
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.83s
                        Total time: 296.79s
                               ETA: 697 mins 42.0 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 120737 steps/s (collection: 0.631s, learning 0.183s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.81s
                        Total time: 297.61s
                               ETA: 697 mins 37.1 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 125521 steps/s (collection: 0.604s, learning 0.179s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.78s
                        Total time: 298.39s
                               ETA: 697 mins 27.9 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 116056 steps/s (collection: 0.645s, learning 0.202s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.85s
                        Total time: 299.24s
                               ETA: 697 mins 27.6 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 115491 steps/s (collection: 0.655s, learning 0.197s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.85s
                        Total time: 300.09s
                               ETA: 697 mins 27.9 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 118824 steps/s (collection: 0.637s, learning 0.190s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.83s
                        Total time: 300.92s
                               ETA: 697 mins 24.9 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 121541 steps/s (collection: 0.618s, learning 0.190s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.81s
                        Total time: 301.72s
                               ETA: 697 mins 19.3 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 126568 steps/s (collection: 0.604s, learning 0.173s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.78s
                        Total time: 302.50s
                               ETA: 697 mins 9.3 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 111343 steps/s (collection: 0.691s, learning 0.192s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.88s
                        Total time: 303.38s
                               ETA: 697 mins 14.0 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 107371 steps/s (collection: 0.730s, learning 0.186s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.92s
                        Total time: 304.30s
                               ETA: 697 mins 23.2 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 123404 steps/s (collection: 0.623s, learning 0.174s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.80s
                        Total time: 305.10s
                               ETA: 697 mins 16.0 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 118377 steps/s (collection: 0.633s, learning 0.198s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.83s
                        Total time: 305.93s
                               ETA: 697 mins 13.5 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 117150 steps/s (collection: 0.628s, learning 0.211s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.84s
                        Total time: 306.77s
                               ETA: 697 mins 12.1 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 122373 steps/s (collection: 0.612s, learning 0.191s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.80s
                        Total time: 307.57s
                               ETA: 697 mins 5.9 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 114581 steps/s (collection: 0.684s, learning 0.174s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.86s
                        Total time: 308.43s
                               ETA: 697 mins 7.2 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 111857 steps/s (collection: 0.705s, learning 0.174s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.88s
                        Total time: 309.31s
                               ETA: 697 mins 11.2 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 112675 steps/s (collection: 0.689s, learning 0.184s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.87s
                        Total time: 310.18s
                               ETA: 697 mins 14.4 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 124828 steps/s (collection: 0.613s, learning 0.174s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.79s
                        Total time: 310.97s
                               ETA: 697 mins 6.1 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 118183 steps/s (collection: 0.656s, learning 0.176s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0405
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.83s
                        Total time: 311.80s
                               ETA: 697 mins 3.8 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 124041 steps/s (collection: 0.617s, learning 0.175s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.79s
                        Total time: 312.59s
                               ETA: 696 mins 56.2 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 108862 steps/s (collection: 0.724s, learning 0.179s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.90s
                        Total time: 313.49s
                               ETA: 697 mins 3.4 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 125848 steps/s (collection: 0.608s, learning 0.173s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.78s
                        Total time: 314.27s
                               ETA: 696 mins 54.4 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 123997 steps/s (collection: 0.618s, learning 0.175s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0405
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.79s
                        Total time: 315.07s
                               ETA: 696 mins 46.9 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 125897 steps/s (collection: 0.608s, learning 0.173s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.78s
                        Total time: 315.85s
                               ETA: 696 mins 38.0 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 117476 steps/s (collection: 0.654s, learning 0.183s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.84s
                        Total time: 316.68s
                               ETA: 696 mins 36.4 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 127736 steps/s (collection: 0.596s, learning 0.173s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.77s
                        Total time: 317.45s
                               ETA: 696 mins 26.0 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 112716 steps/s (collection: 0.690s, learning 0.182s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.87s
                        Total time: 318.33s
                               ETA: 696 mins 29.1 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 120663 steps/s (collection: 0.641s, learning 0.174s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.81s
                        Total time: 319.14s
                               ETA: 696 mins 24.7 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 124595 steps/s (collection: 0.615s, learning 0.174s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.79s
                        Total time: 319.93s
                               ETA: 696 mins 16.9 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 115399 steps/s (collection: 0.672s, learning 0.179s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.85s
                        Total time: 320.78s
                               ETA: 696 mins 17.3 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 110850 steps/s (collection: 0.713s, learning 0.173s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.89s
                        Total time: 321.67s
                               ETA: 696 mins 22.3 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 111702 steps/s (collection: 0.700s, learning 0.180s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0407
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.88s
                        Total time: 322.55s
                               ETA: 696 mins 26.4 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 108018 steps/s (collection: 0.729s, learning 0.181s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.91s
                        Total time: 323.46s
                               ETA: 696 mins 34.3 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 113924 steps/s (collection: 0.662s, learning 0.201s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.86s
                        Total time: 324.32s
                               ETA: 696 mins 36.1 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 111924 steps/s (collection: 0.676s, learning 0.202s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.88s
                        Total time: 325.20s
                               ETA: 696 mins 39.9 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 113911 steps/s (collection: 0.670s, learning 0.193s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0419
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.86s
                        Total time: 326.06s
                               ETA: 696 mins 41.7 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 109497 steps/s (collection: 0.695s, learning 0.203s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.90s
                        Total time: 326.96s
                               ETA: 696 mins 47.9 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 120757 steps/s (collection: 0.624s, learning 0.191s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.81s
                        Total time: 327.77s
                               ETA: 696 mins 43.4 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 121537 steps/s (collection: 0.616s, learning 0.192s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.81s
                        Total time: 328.58s
                               ETA: 696 mins 38.3 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 123622 steps/s (collection: 0.605s, learning 0.191s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.80s
                        Total time: 329.38s
                               ETA: 696 mins 31.4 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 121330 steps/s (collection: 0.619s, learning 0.192s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.81s
                        Total time: 330.19s
                               ETA: 696 mins 26.5 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 123320 steps/s (collection: 0.606s, learning 0.191s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.80s
                        Total time: 330.99s
                               ETA: 696 mins 20.0 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 122235 steps/s (collection: 0.613s, learning 0.191s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0420
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.80s
                        Total time: 331.79s
                               ETA: 696 mins 14.4 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 121874 steps/s (collection: 0.615s, learning 0.192s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0418
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.81s
                        Total time: 332.60s
                               ETA: 696 mins 9.1 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 119623 steps/s (collection: 0.630s, learning 0.192s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0420
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.82s
                        Total time: 333.42s
                               ETA: 696 mins 5.7 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 121968 steps/s (collection: 0.613s, learning 0.193s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.81s
                        Total time: 334.22s
                               ETA: 696 mins 0.3 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 120464 steps/s (collection: 0.625s, learning 0.191s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0406
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.82s
                        Total time: 335.04s
                               ETA: 695 mins 56.3 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 121290 steps/s (collection: 0.617s, learning 0.193s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0425
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.81s
                        Total time: 335.85s
                               ETA: 695 mins 51.5 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 113996 steps/s (collection: 0.665s, learning 0.197s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0418
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.86s
                        Total time: 336.71s
                               ETA: 695 mins 53.3 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 123609 steps/s (collection: 0.621s, learning 0.175s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.80s
                        Total time: 337.51s
                               ETA: 695 mins 46.7 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 123695 steps/s (collection: 0.620s, learning 0.174s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0418
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.79s
                        Total time: 338.30s
                               ETA: 695 mins 40.0 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 115960 steps/s (collection: 0.674s, learning 0.173s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.85s
                        Total time: 339.15s
                               ETA: 695 mins 40.0 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 105013 steps/s (collection: 0.741s, learning 0.195s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.94s
                        Total time: 340.09s
                               ETA: 695 mins 50.7 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 113239 steps/s (collection: 0.672s, learning 0.196s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0421
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.87s
                        Total time: 340.95s
                               ETA: 695 mins 53.1 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 121583 steps/s (collection: 0.617s, learning 0.192s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0417
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.81s
                        Total time: 341.76s
                               ETA: 695 mins 48.2 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 102859 steps/s (collection: 0.750s, learning 0.206s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.96s
                        Total time: 342.72s
                               ETA: 696 mins 1.2 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 108504 steps/s (collection: 0.709s, learning 0.197s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0426
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.91s
                        Total time: 343.63s
                               ETA: 696 mins 8.2 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 122711 steps/s (collection: 0.610s, learning 0.191s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0283
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.80s
                        Total time: 344.43s
                               ETA: 696 mins 2.3 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 117629 steps/s (collection: 0.640s, learning 0.195s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0424
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.84s
                        Total time: 345.26s
                               ETA: 696 mins 0.7 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 111425 steps/s (collection: 0.679s, learning 0.203s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.88s
                        Total time: 346.14s
                               ETA: 696 mins 4.7 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 124365 steps/s (collection: 0.611s, learning 0.180s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0424
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.79s
                        Total time: 346.93s
                               ETA: 695 mins 57.6 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 106598 steps/s (collection: 0.734s, learning 0.188s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.92s
                        Total time: 347.86s
                               ETA: 696 mins 6.4 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 115855 steps/s (collection: 0.675s, learning 0.174s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0420
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.85s
                        Total time: 348.71s
                               ETA: 696 mins 6.3 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 114332 steps/s (collection: 0.667s, learning 0.193s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.86s
                        Total time: 349.57s
                               ETA: 696 mins 7.6 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 122358 steps/s (collection: 0.614s, learning 0.189s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.80s
                        Total time: 350.37s
                               ETA: 696 mins 2.1 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 120657 steps/s (collection: 0.631s, learning 0.184s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.81s
                        Total time: 351.18s
                               ETA: 695 mins 58.0 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 114435 steps/s (collection: 0.668s, learning 0.191s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0424
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.86s
                        Total time: 352.04s
                               ETA: 695 mins 59.1 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 125570 steps/s (collection: 0.610s, learning 0.173s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.78s
                        Total time: 352.83s
                               ETA: 695 mins 51.3 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 121793 steps/s (collection: 0.628s, learning 0.179s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.81s
                        Total time: 353.63s
                               ETA: 695 mins 46.3 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 128676 steps/s (collection: 0.590s, learning 0.173s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.76s
                        Total time: 354.40s
                               ETA: 695 mins 36.3 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 121035 steps/s (collection: 0.631s, learning 0.181s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0426
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.81s
                        Total time: 355.21s
                               ETA: 695 mins 32.0 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 127325 steps/s (collection: 0.598s, learning 0.174s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.77s
                        Total time: 355.98s
                               ETA: 695 mins 22.9 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 113367 steps/s (collection: 0.677s, learning 0.190s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.87s
                        Total time: 356.85s
                               ETA: 695 mins 25.1 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 123162 steps/s (collection: 0.624s, learning 0.174s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.80s
                        Total time: 357.65s
                               ETA: 695 mins 19.2 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 126734 steps/s (collection: 0.595s, learning 0.181s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.78s
                        Total time: 358.42s
                               ETA: 695 mins 10.7 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 116234 steps/s (collection: 0.672s, learning 0.173s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.85s
                        Total time: 359.27s
                               ETA: 695 mins 10.3 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 118331 steps/s (collection: 0.657s, learning 0.173s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.83s
                        Total time: 360.10s
                               ETA: 695 mins 8.3 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 125981 steps/s (collection: 0.607s, learning 0.173s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.78s
                        Total time: 360.88s
                               ETA: 695 mins 0.4 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 115202 steps/s (collection: 0.661s, learning 0.192s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.85s
                        Total time: 361.73s
                               ETA: 695 mins 0.9 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 110084 steps/s (collection: 0.705s, learning 0.188s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0433
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.89s
                        Total time: 362.62s
                               ETA: 695 mins 6.0 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 111136 steps/s (collection: 0.708s, learning 0.177s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.88s
                        Total time: 363.51s
                               ETA: 695 mins 10.2 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 111575 steps/s (collection: 0.704s, learning 0.177s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.88s
                        Total time: 364.39s
                               ETA: 695 mins 13.8 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 123256 steps/s (collection: 0.623s, learning 0.174s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.80s
                        Total time: 365.19s
                               ETA: 695 mins 8.0 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 109457 steps/s (collection: 0.709s, learning 0.189s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0436
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.90s
                        Total time: 366.09s
                               ETA: 695 mins 13.6 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 112510 steps/s (collection: 0.694s, learning 0.180s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0445
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.87s
                        Total time: 366.96s
                               ETA: 695 mins 16.4 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 111670 steps/s (collection: 0.707s, learning 0.173s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0438
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.88s
                        Total time: 367.84s
                               ETA: 695 mins 20.0 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 117579 steps/s (collection: 0.650s, learning 0.186s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.84s
                        Total time: 368.68s
                               ETA: 695 mins 18.5 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 125068 steps/s (collection: 0.612s, learning 0.174s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0441
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.79s
                        Total time: 369.46s
                               ETA: 695 mins 11.3 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 120499 steps/s (collection: 0.631s, learning 0.185s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.82s
                        Total time: 370.28s
                               ETA: 695 mins 7.6 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 119824 steps/s (collection: 0.646s, learning 0.174s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0438
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.82s
                        Total time: 371.10s
                               ETA: 695 mins 4.4 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 108933 steps/s (collection: 0.714s, learning 0.189s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.90s
                        Total time: 372.00s
                               ETA: 695 mins 10.4 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 125895 steps/s (collection: 0.607s, learning 0.174s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0444
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.78s
                        Total time: 372.78s
                               ETA: 695 mins 2.7 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 111866 steps/s (collection: 0.706s, learning 0.173s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.88s
                        Total time: 373.66s
                               ETA: 695 mins 6.0 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 116207 steps/s (collection: 0.662s, learning 0.183s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.85s
                        Total time: 374.51s
                               ETA: 695 mins 5.7 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 122140 steps/s (collection: 0.631s, learning 0.174s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0446
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.80s
                        Total time: 375.31s
                               ETA: 695 mins 0.8 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 125806 steps/s (collection: 0.608s, learning 0.173s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.78s
                        Total time: 376.09s
                               ETA: 694 mins 53.3 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 119208 steps/s (collection: 0.652s, learning 0.173s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0432
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.82s
                        Total time: 376.92s
                               ETA: 694 mins 50.6 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 110671 steps/s (collection: 0.705s, learning 0.184s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.89s
                        Total time: 377.81s
                               ETA: 694 mins 54.9 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 119815 steps/s (collection: 0.647s, learning 0.173s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.82s
                        Total time: 378.63s
                               ETA: 694 mins 51.7 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 111435 steps/s (collection: 0.702s, learning 0.180s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.88s
                        Total time: 379.51s
                               ETA: 694 mins 55.4 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 110323 steps/s (collection: 0.693s, learning 0.198s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0445
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.89s
                        Total time: 380.40s
                               ETA: 694 mins 60.0 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 116747 steps/s (collection: 0.650s, learning 0.192s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0448
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.84s
                        Total time: 381.24s
                               ETA: 694 mins 59.2 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 122058 steps/s (collection: 0.611s, learning 0.194s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.81s
                        Total time: 382.05s
                               ETA: 694 mins 54.4 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 108752 steps/s (collection: 0.731s, learning 0.173s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0444
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.90s
                        Total time: 382.95s
                               ETA: 695 mins 0.3 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 117491 steps/s (collection: 0.658s, learning 0.179s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0440
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.84s
                        Total time: 383.79s
                               ETA: 694 mins 59.0 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 123166 steps/s (collection: 0.612s, learning 0.187s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.80s
                        Total time: 384.59s
                               ETA: 694 mins 53.4 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 124252 steps/s (collection: 0.620s, learning 0.171s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0444
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.79s
                        Total time: 385.38s
                               ETA: 694 mins 47.1 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 120236 steps/s (collection: 0.644s, learning 0.174s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.82s
                        Total time: 386.19s
                               ETA: 694 mins 43.7 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 113646 steps/s (collection: 0.691s, learning 0.174s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0444
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.86s
                        Total time: 387.06s
                               ETA: 694 mins 45.4 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 120986 steps/s (collection: 0.638s, learning 0.174s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0432
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.81s
                        Total time: 387.87s
                               ETA: 694 mins 41.5 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 122865 steps/s (collection: 0.627s, learning 0.173s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.80s
                        Total time: 388.67s
                               ETA: 694 mins 36.2 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 120903 steps/s (collection: 0.623s, learning 0.190s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0450
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.81s
                        Total time: 389.48s
                               ETA: 694 mins 32.3 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 103574 steps/s (collection: 0.764s, learning 0.186s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.95s
                        Total time: 390.43s
                               ETA: 694 mins 43.0 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 119604 steps/s (collection: 0.648s, learning 0.174s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0450
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.82s
                        Total time: 391.26s
                               ETA: 694 mins 40.1 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 124254 steps/s (collection: 0.616s, learning 0.175s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.79s
                        Total time: 392.05s
                               ETA: 694 mins 33.9 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 119171 steps/s (collection: 0.651s, learning 0.174s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0457
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.82s
                        Total time: 392.87s
                               ETA: 694 mins 31.3 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 124758 steps/s (collection: 0.614s, learning 0.174s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0436
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.79s
                        Total time: 393.66s
                               ETA: 694 mins 24.8 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 118427 steps/s (collection: 0.656s, learning 0.174s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.83s
                        Total time: 394.49s
                               ETA: 694 mins 22.8 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 120219 steps/s (collection: 0.639s, learning 0.179s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0456
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.82s
                        Total time: 395.31s
                               ETA: 694 mins 19.5 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 115119 steps/s (collection: 0.682s, learning 0.172s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0456
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.85s
                        Total time: 396.16s
                               ETA: 694 mins 20.0 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 115304 steps/s (collection: 0.647s, learning 0.205s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0451
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.85s
                        Total time: 397.01s
                               ETA: 694 mins 20.4 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 115459 steps/s (collection: 0.671s, learning 0.181s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.85s
                        Total time: 397.87s
                               ETA: 694 mins 20.6 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 113553 steps/s (collection: 0.677s, learning 0.189s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0454
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.87s
                        Total time: 398.73s
                               ETA: 694 mins 22.3 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 126368 steps/s (collection: 0.604s, learning 0.173s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0445
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.78s
                        Total time: 399.51s
                               ETA: 694 mins 14.9 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 126522 steps/s (collection: 0.603s, learning 0.174s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.78s
                        Total time: 400.29s
                               ETA: 694 mins 7.4 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 116284 steps/s (collection: 0.669s, learning 0.177s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0457
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.85s
                        Total time: 401.13s
                               ETA: 694 mins 7.0 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 119538 steps/s (collection: 0.648s, learning 0.175s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0448
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.82s
                        Total time: 401.95s
                               ETA: 694 mins 4.2 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 111706 steps/s (collection: 0.697s, learning 0.183s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.88s
                        Total time: 402.83s
                               ETA: 694 mins 7.4 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 128196 steps/s (collection: 0.593s, learning 0.174s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.77s
                        Total time: 403.60s
                               ETA: 693 mins 59.0 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 110950 steps/s (collection: 0.707s, learning 0.179s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0454
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.89s
                        Total time: 404.49s
                               ETA: 694 mins 2.8 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 112815 steps/s (collection: 0.697s, learning 0.174s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0448
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.87s
                        Total time: 405.36s
                               ETA: 694 mins 5.1 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 122665 steps/s (collection: 0.627s, learning 0.174s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0450
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.80s
                        Total time: 406.16s
                               ETA: 694 mins 0.2 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 121769 steps/s (collection: 0.627s, learning 0.180s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0445
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.81s
                        Total time: 406.97s
                               ETA: 693 mins 55.9 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 113007 steps/s (collection: 0.687s, learning 0.183s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.87s
                        Total time: 407.84s
                               ETA: 693 mins 58.0 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 126320 steps/s (collection: 0.604s, learning 0.174s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.78s
                        Total time: 408.61s
                               ETA: 693 mins 50.8 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 122058 steps/s (collection: 0.632s, learning 0.174s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0457
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.81s
                        Total time: 409.42s
                               ETA: 693 mins 46.3 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 125149 steps/s (collection: 0.612s, learning 0.173s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.79s
                        Total time: 410.21s
                               ETA: 693 mins 39.9 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 113981 steps/s (collection: 0.689s, learning 0.173s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.86s
                        Total time: 411.07s
                               ETA: 693 mins 41.3 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 124839 steps/s (collection: 0.614s, learning 0.174s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.79s
                        Total time: 411.86s
                               ETA: 693 mins 35.0 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 120151 steps/s (collection: 0.645s, learning 0.173s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0463
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.82s
                        Total time: 412.67s
                               ETA: 693 mins 32.0 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 126491 steps/s (collection: 0.604s, learning 0.173s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.78s
                        Total time: 413.45s
                               ETA: 693 mins 24.7 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 111492 steps/s (collection: 0.688s, learning 0.194s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.88s
                        Total time: 414.33s
                               ETA: 693 mins 28.1 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 117087 steps/s (collection: 0.656s, learning 0.184s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.84s
                        Total time: 415.17s
                               ETA: 693 mins 27.1 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 127164 steps/s (collection: 0.599s, learning 0.174s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.77s
                        Total time: 415.95s
                               ETA: 693 mins 19.5 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 125004 steps/s (collection: 0.614s, learning 0.172s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.79s
                        Total time: 416.73s
                               ETA: 693 mins 13.3 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 107516 steps/s (collection: 0.733s, learning 0.181s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0459
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.91s
                        Total time: 417.65s
                               ETA: 693 mins 19.9 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 111026 steps/s (collection: 0.707s, learning 0.178s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0466
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.89s
                        Total time: 418.53s
                               ETA: 693 mins 23.5 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 113577 steps/s (collection: 0.685s, learning 0.181s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.87s
                        Total time: 419.40s
                               ETA: 693 mins 25.2 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 113434 steps/s (collection: 0.683s, learning 0.183s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.87s
                        Total time: 420.26s
                               ETA: 693 mins 26.9 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 125915 steps/s (collection: 0.607s, learning 0.174s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.78s
                        Total time: 421.04s
                               ETA: 693 mins 20.2 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 107974 steps/s (collection: 0.720s, learning 0.190s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.91s
                        Total time: 421.95s
                               ETA: 693 mins 26.2 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 123484 steps/s (collection: 0.622s, learning 0.174s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.80s
                        Total time: 422.75s
                               ETA: 693 mins 21.0 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 114617 steps/s (collection: 0.684s, learning 0.174s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.86s
                        Total time: 423.61s
                               ETA: 693 mins 21.9 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 123418 steps/s (collection: 0.623s, learning 0.173s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.80s
                        Total time: 424.40s
                               ETA: 693 mins 16.7 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 116316 steps/s (collection: 0.656s, learning 0.189s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.85s
                        Total time: 425.25s
                               ETA: 693 mins 16.3 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 112099 steps/s (collection: 0.691s, learning 0.185s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.88s
                        Total time: 426.13s
                               ETA: 693 mins 19.1 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 116753 steps/s (collection: 0.652s, learning 0.190s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.84s
                        Total time: 426.97s
                               ETA: 693 mins 18.4 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 111454 steps/s (collection: 0.696s, learning 0.186s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.88s
                        Total time: 427.85s
                               ETA: 693 mins 21.6 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 106976 steps/s (collection: 0.744s, learning 0.175s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.92s
                        Total time: 428.77s
                               ETA: 693 mins 28.3 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 119579 steps/s (collection: 0.639s, learning 0.183s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0466
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.82s
                        Total time: 429.59s
                               ETA: 693 mins 25.7 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 120727 steps/s (collection: 0.636s, learning 0.178s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.81s
                        Total time: 430.41s
                               ETA: 693 mins 22.3 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 126169 steps/s (collection: 0.605s, learning 0.174s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.78s
                        Total time: 431.19s
                               ETA: 693 mins 15.5 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 110293 steps/s (collection: 0.718s, learning 0.173s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.89s
                        Total time: 432.08s
                               ETA: 693 mins 19.6 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 121835 steps/s (collection: 0.633s, learning 0.174s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0463
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.81s
                        Total time: 432.88s
                               ETA: 693 mins 15.5 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 113118 steps/s (collection: 0.675s, learning 0.194s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0470
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.87s
                        Total time: 433.75s
                               ETA: 693 mins 17.4 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 105793 steps/s (collection: 0.732s, learning 0.197s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.93s
                        Total time: 434.68s
                               ETA: 693 mins 25.0 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 107680 steps/s (collection: 0.722s, learning 0.191s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.91s
                        Total time: 435.59s
                               ETA: 693 mins 31.1 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 112239 steps/s (collection: 0.667s, learning 0.209s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.88s
                        Total time: 436.47s
                               ETA: 693 mins 33.5 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 110855 steps/s (collection: 0.678s, learning 0.209s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.89s
                        Total time: 437.36s
                               ETA: 693 mins 37.1 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 111369 steps/s (collection: 0.690s, learning 0.192s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0477
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.88s
                        Total time: 438.24s
                               ETA: 693 mins 40.2 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 122760 steps/s (collection: 0.610s, learning 0.191s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.80s
                        Total time: 439.04s
                               ETA: 693 mins 35.5 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 118501 steps/s (collection: 0.629s, learning 0.200s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.83s
                        Total time: 439.87s
                               ETA: 693 mins 33.6 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 115088 steps/s (collection: 0.666s, learning 0.188s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.85s
                        Total time: 440.72s
                               ETA: 693 mins 34.0 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 125157 steps/s (collection: 0.598s, learning 0.187s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.79s
                        Total time: 441.51s
                               ETA: 693 mins 27.9 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 115676 steps/s (collection: 0.676s, learning 0.174s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.85s
                        Total time: 442.36s
                               ETA: 693 mins 27.9 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 124081 steps/s (collection: 0.606s, learning 0.186s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.79s
                        Total time: 443.15s
                               ETA: 693 mins 22.5 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 107245 steps/s (collection: 0.679s, learning 0.237s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.92s
                        Total time: 444.07s
                               ETA: 693 mins 28.7 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 107971 steps/s (collection: 0.719s, learning 0.192s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.91s
                        Total time: 444.98s
                               ETA: 693 mins 34.4 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 122800 steps/s (collection: 0.610s, learning 0.191s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.80s
                        Total time: 445.78s
                               ETA: 693 mins 29.7 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 119121 steps/s (collection: 0.634s, learning 0.191s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0471
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.83s
                        Total time: 446.60s
                               ETA: 693 mins 27.4 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 106089 steps/s (collection: 0.720s, learning 0.206s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.93s
                        Total time: 447.53s
                               ETA: 693 mins 34.5 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 110746 steps/s (collection: 0.691s, learning 0.197s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.89s
                        Total time: 448.42s
                               ETA: 693 mins 38.0 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 105766 steps/s (collection: 0.729s, learning 0.201s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.93s
                        Total time: 449.35s
                               ETA: 693 mins 45.3 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 117314 steps/s (collection: 0.641s, learning 0.197s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.84s
                        Total time: 450.19s
                               ETA: 693 mins 44.1 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 121641 steps/s (collection: 0.618s, learning 0.190s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0478
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.81s
                        Total time: 450.99s
                               ETA: 693 mins 40.2 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 117327 steps/s (collection: 0.647s, learning 0.191s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.84s
                        Total time: 451.83s
                               ETA: 693 mins 39.1 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 109791 steps/s (collection: 0.702s, learning 0.193s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.90s
                        Total time: 452.73s
                               ETA: 693 mins 43.2 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 120579 steps/s (collection: 0.626s, learning 0.189s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.82s
                        Total time: 453.54s
                               ETA: 693 mins 39.9 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 117945 steps/s (collection: 0.655s, learning 0.178s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0480
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.83s
                        Total time: 454.38s
                               ETA: 693 mins 38.4 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 123761 steps/s (collection: 0.618s, learning 0.176s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.79s
                        Total time: 455.17s
                               ETA: 693 mins 33.2 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 121001 steps/s (collection: 0.639s, learning 0.173s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.81s
                        Total time: 455.98s
                               ETA: 693 mins 29.7 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 110343 steps/s (collection: 0.712s, learning 0.179s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.89s
                        Total time: 456.87s
                               ETA: 693 mins 33.4 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 125759 steps/s (collection: 0.606s, learning 0.176s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0480
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.78s
                        Total time: 457.66s
                               ETA: 693 mins 27.1 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 116078 steps/s (collection: 0.668s, learning 0.179s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.85s
                        Total time: 458.50s
                               ETA: 693 mins 26.8 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 122660 steps/s (collection: 0.608s, learning 0.194s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.80s
                        Total time: 459.30s
                               ETA: 693 mins 22.4 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 124359 steps/s (collection: 0.609s, learning 0.181s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.79s
                        Total time: 460.09s
                               ETA: 693 mins 16.9 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 108253 steps/s (collection: 0.717s, learning 0.191s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.91s
                        Total time: 461.00s
                               ETA: 693 mins 22.1 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 126100 steps/s (collection: 0.606s, learning 0.173s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.78s
                        Total time: 461.78s
                               ETA: 693 mins 15.7 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 113526 steps/s (collection: 0.692s, learning 0.174s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0488
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.87s
                        Total time: 462.65s
                               ETA: 693 mins 17.1 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 120177 steps/s (collection: 0.642s, learning 0.176s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0480
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.82s
                        Total time: 463.47s
                               ETA: 693 mins 14.2 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 112903 steps/s (collection: 0.673s, learning 0.198s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.87s
                        Total time: 464.34s
                               ETA: 693 mins 16.0 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 108615 steps/s (collection: 0.708s, learning 0.197s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0490
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.91s
                        Total time: 465.24s
                               ETA: 693 mins 20.9 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 121153 steps/s (collection: 0.606s, learning 0.205s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0486
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.81s
                        Total time: 466.05s
                               ETA: 693 mins 17.4 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 111514 steps/s (collection: 0.670s, learning 0.212s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.88s
                        Total time: 466.93s
                               ETA: 693 mins 20.1 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 114109 steps/s (collection: 0.670s, learning 0.192s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0486
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.86s
                        Total time: 467.80s
                               ETA: 693 mins 21.1 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 122146 steps/s (collection: 0.618s, learning 0.187s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.80s
                        Total time: 468.60s
                               ETA: 693 mins 17.0 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 120150 steps/s (collection: 0.627s, learning 0.191s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0490
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.82s
                        Total time: 469.42s
                               ETA: 693 mins 14.1 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 120957 steps/s (collection: 0.636s, learning 0.177s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0491
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.81s
                        Total time: 470.23s
                               ETA: 693 mins 10.7 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 124696 steps/s (collection: 0.614s, learning 0.175s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.79s
                        Total time: 471.02s
                               ETA: 693 mins 5.2 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 111504 steps/s (collection: 0.697s, learning 0.185s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.88s
                        Total time: 471.90s
                               ETA: 693 mins 7.9 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 123898 steps/s (collection: 0.596s, learning 0.197s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.79s
                        Total time: 472.70s
                               ETA: 693 mins 2.9 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 119572 steps/s (collection: 0.649s, learning 0.173s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.82s
                        Total time: 473.52s
                               ETA: 693 mins 0.4 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 114620 steps/s (collection: 0.670s, learning 0.188s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.86s
                        Total time: 474.38s
                               ETA: 693 mins 1.0 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 120253 steps/s (collection: 0.645s, learning 0.173s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.82s
                        Total time: 475.19s
                               ETA: 692 mins 58.1 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 116720 steps/s (collection: 0.666s, learning 0.176s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0487
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.84s
                        Total time: 476.03s
                               ETA: 692 mins 57.4 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 122101 steps/s (collection: 0.632s, learning 0.173s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0487
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.81s
                        Total time: 476.84s
                               ETA: 692 mins 53.4 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 110708 steps/s (collection: 0.710s, learning 0.178s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.89s
                        Total time: 477.73s
                               ETA: 692 mins 56.6 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 124320 steps/s (collection: 0.616s, learning 0.175s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0490
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.79s
                        Total time: 478.52s
                               ETA: 692 mins 51.4 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 119433 steps/s (collection: 0.649s, learning 0.174s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0492
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.82s
                        Total time: 479.34s
                               ETA: 692 mins 49.0 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 126138 steps/s (collection: 0.606s, learning 0.173s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0496
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.78s
                        Total time: 480.12s
                               ETA: 692 mins 42.8 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 127464 steps/s (collection: 0.597s, learning 0.174s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0490
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.77s
                        Total time: 480.89s
                               ETA: 692 mins 36.0 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 122385 steps/s (collection: 0.616s, learning 0.187s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0487
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.80s
                        Total time: 481.70s
                               ETA: 692 mins 31.9 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 117126 steps/s (collection: 0.665s, learning 0.174s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.84s
                        Total time: 482.53s
                               ETA: 692 mins 31.0 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 125171 steps/s (collection: 0.596s, learning 0.189s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.79s
                        Total time: 483.32s
                               ETA: 692 mins 25.4 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 106464 steps/s (collection: 0.729s, learning 0.194s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0488
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.92s
                        Total time: 484.24s
                               ETA: 692 mins 31.6 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 120198 steps/s (collection: 0.616s, learning 0.202s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0496
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.82s
                        Total time: 485.06s
                               ETA: 692 mins 28.8 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 127426 steps/s (collection: 0.598s, learning 0.174s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.77s
                        Total time: 485.83s
                               ETA: 692 mins 22.1 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 116011 steps/s (collection: 0.673s, learning 0.174s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.85s
                        Total time: 486.68s
                               ETA: 692 mins 21.8 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 127187 steps/s (collection: 0.599s, learning 0.174s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.77s
                        Total time: 487.45s
                               ETA: 692 mins 15.2 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 120422 steps/s (collection: 0.641s, learning 0.176s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.82s
                        Total time: 488.27s
                               ETA: 692 mins 12.3 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 127958 steps/s (collection: 0.591s, learning 0.178s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.77s
                        Total time: 489.04s
                               ETA: 692 mins 5.3 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 124032 steps/s (collection: 0.620s, learning 0.173s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0491
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.79s
                        Total time: 489.83s
                               ETA: 692 mins 0.5 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 113344 steps/s (collection: 0.677s, learning 0.190s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.87s
                        Total time: 490.70s
                               ETA: 692 mins 1.9 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 114342 steps/s (collection: 0.677s, learning 0.183s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0511
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.86s
                        Total time: 491.56s
                               ETA: 692 mins 2.7 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 120933 steps/s (collection: 0.631s, learning 0.182s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0500
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.81s
                        Total time: 492.37s
                               ETA: 691 mins 59.6 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 123366 steps/s (collection: 0.611s, learning 0.186s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.80s
                        Total time: 493.17s
                               ETA: 691 mins 55.1 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 110559 steps/s (collection: 0.715s, learning 0.174s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.89s
                        Total time: 494.06s
                               ETA: 691 mins 58.4 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 124706 steps/s (collection: 0.613s, learning 0.175s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.79s
                        Total time: 494.84s
                               ETA: 691 mins 53.2 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 113896 steps/s (collection: 0.677s, learning 0.186s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.86s
                        Total time: 495.71s
                               ETA: 691 mins 54.2 s

################################################################################
                      Learning iteration 590/50000                      

                       Computation: 114065 steps/s (collection: 0.689s, learning 0.173s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0503
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.86s
                        Total time: 496.57s
                               ETA: 691 mins 55.2 s

################################################################################
                      Learning iteration 591/50000                      

                       Computation: 115292 steps/s (collection: 0.680s, learning 0.172s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.85s
                        Total time: 497.42s
                               ETA: 691 mins 55.4 s

################################################################################
                      Learning iteration 592/50000                      

                       Computation: 114719 steps/s (collection: 0.662s, learning 0.195s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.86s
                        Total time: 498.28s
                               ETA: 691 mins 56.0 s

################################################################################
                      Learning iteration 593/50000                      

                       Computation: 121283 steps/s (collection: 0.619s, learning 0.191s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.81s
                        Total time: 499.09s
                               ETA: 691 mins 52.6 s

################################################################################
                      Learning iteration 594/50000                      

                       Computation: 108467 steps/s (collection: 0.709s, learning 0.197s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.91s
                        Total time: 500.00s
                               ETA: 691 mins 57.3 s

################################################################################
                      Learning iteration 595/50000                      

                       Computation: 121007 steps/s (collection: 0.640s, learning 0.172s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.81s
                        Total time: 500.81s
                               ETA: 691 mins 54.1 s

################################################################################
                      Learning iteration 596/50000                      

                       Computation: 124824 steps/s (collection: 0.614s, learning 0.173s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.79s
                        Total time: 501.60s
                               ETA: 691 mins 48.9 s

################################################################################
                      Learning iteration 597/50000                      

                       Computation: 122697 steps/s (collection: 0.625s, learning 0.176s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0511
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.80s
                        Total time: 502.40s
                               ETA: 691 mins 44.9 s

################################################################################
                      Learning iteration 598/50000                      

                       Computation: 117845 steps/s (collection: 0.645s, learning 0.189s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.83s
                        Total time: 503.23s
                               ETA: 691 mins 43.5 s

################################################################################
                      Learning iteration 599/50000                      

                       Computation: 120691 steps/s (collection: 0.634s, learning 0.180s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0511
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.81s
                        Total time: 504.05s
                               ETA: 691 mins 40.6 s

################################################################################
                      Learning iteration 600/50000                      

                       Computation: 118623 steps/s (collection: 0.656s, learning 0.173s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0516
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.83s
                        Total time: 504.87s
                               ETA: 691 mins 38.8 s

################################################################################
                      Learning iteration 601/50000                      

                       Computation: 120866 steps/s (collection: 0.629s, learning 0.185s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0503
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.81s
                        Total time: 505.69s
                               ETA: 691 mins 35.8 s

################################################################################
                      Learning iteration 602/50000                      

                       Computation: 110261 steps/s (collection: 0.718s, learning 0.174s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.89s
                        Total time: 506.58s
                               ETA: 691 mins 39.2 s

################################################################################
                      Learning iteration 603/50000                      

                       Computation: 122052 steps/s (collection: 0.626s, learning 0.179s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.81s
                        Total time: 507.38s
                               ETA: 691 mins 35.5 s

################################################################################
                      Learning iteration 604/50000                      

                       Computation: 118108 steps/s (collection: 0.650s, learning 0.182s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.83s
                        Total time: 508.22s
                               ETA: 691 mins 34.0 s

################################################################################
                      Learning iteration 605/50000                      

                       Computation: 124184 steps/s (collection: 0.615s, learning 0.177s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.79s
                        Total time: 509.01s
                               ETA: 691 mins 29.2 s

################################################################################
                      Learning iteration 606/50000                      

                       Computation: 126524 steps/s (collection: 0.603s, learning 0.174s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0521
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.78s
                        Total time: 509.79s
                               ETA: 691 mins 23.3 s

################################################################################
                      Learning iteration 607/50000                      

                       Computation: 127828 steps/s (collection: 0.597s, learning 0.172s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0510
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.77s
                        Total time: 510.55s
                               ETA: 691 mins 16.7 s

################################################################################
                      Learning iteration 608/50000                      

                       Computation: 123256 steps/s (collection: 0.625s, learning 0.173s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0517
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.80s
                        Total time: 511.35s
                               ETA: 691 mins 12.4 s

################################################################################
                      Learning iteration 609/50000                      

                       Computation: 123369 steps/s (collection: 0.621s, learning 0.176s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0510
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.80s
                        Total time: 512.15s
                               ETA: 691 mins 8.1 s

################################################################################
                      Learning iteration 610/50000                      

                       Computation: 122074 steps/s (collection: 0.632s, learning 0.173s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.81s
                        Total time: 512.95s
                               ETA: 691 mins 4.5 s

################################################################################
                      Learning iteration 611/50000                      

                       Computation: 124619 steps/s (collection: 0.616s, learning 0.173s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.79s
                        Total time: 513.74s
                               ETA: 690 mins 59.6 s

################################################################################
                      Learning iteration 612/50000                      

                       Computation: 114090 steps/s (collection: 0.687s, learning 0.174s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0513
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.86s
                        Total time: 514.60s
                               ETA: 691 mins 0.5 s

################################################################################
                      Learning iteration 613/50000                      

                       Computation: 117865 steps/s (collection: 0.656s, learning 0.178s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0517
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.83s
                        Total time: 515.44s
                               ETA: 690 mins 59.2 s

################################################################################
                      Learning iteration 614/50000                      

                       Computation: 118554 steps/s (collection: 0.655s, learning 0.174s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.83s
                        Total time: 516.27s
                               ETA: 690 mins 57.6 s

################################################################################
                      Learning iteration 615/50000                      

                       Computation: 107164 steps/s (collection: 0.726s, learning 0.191s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.92s
                        Total time: 517.19s
                               ETA: 691 mins 3.0 s

################################################################################
                      Learning iteration 616/50000                      

                       Computation: 111193 steps/s (collection: 0.703s, learning 0.181s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.88s
                        Total time: 518.07s
                               ETA: 691 mins 5.7 s

################################################################################
                      Learning iteration 617/50000                      

                       Computation: 123212 steps/s (collection: 0.625s, learning 0.173s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.80s
                        Total time: 518.87s
                               ETA: 691 mins 1.5 s

################################################################################
                      Learning iteration 618/50000                      

                       Computation: 112236 steps/s (collection: 0.696s, learning 0.180s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.88s
                        Total time: 519.74s
                               ETA: 691 mins 3.6 s

################################################################################
                      Learning iteration 619/50000                      

                       Computation: 125721 steps/s (collection: 0.608s, learning 0.174s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.78s
                        Total time: 520.52s
                               ETA: 690 mins 58.1 s

################################################################################
                      Learning iteration 620/50000                      

                       Computation: 111216 steps/s (collection: 0.696s, learning 0.188s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0528
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.88s
                        Total time: 521.41s
                               ETA: 691 mins 0.8 s

################################################################################
                      Learning iteration 621/50000                      

                       Computation: 127848 steps/s (collection: 0.595s, learning 0.173s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.77s
                        Total time: 522.18s
                               ETA: 690 mins 54.4 s

################################################################################
                      Learning iteration 622/50000                      

                       Computation: 128259 steps/s (collection: 0.592s, learning 0.174s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.77s
                        Total time: 522.94s
                               ETA: 690 mins 47.7 s

################################################################################
                      Learning iteration 623/50000                      

                       Computation: 115974 steps/s (collection: 0.655s, learning 0.193s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.85s
                        Total time: 523.79s
                               ETA: 690 mins 47.5 s

################################################################################
                      Learning iteration 624/50000                      

                       Computation: 108563 steps/s (collection: 0.706s, learning 0.200s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.91s
                        Total time: 524.70s
                               ETA: 690 mins 51.9 s

################################################################################
                      Learning iteration 625/50000                      

                       Computation: 118038 steps/s (collection: 0.635s, learning 0.198s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0520
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.83s
                        Total time: 525.53s
                               ETA: 690 mins 50.6 s

################################################################################
                      Learning iteration 626/50000                      

                       Computation: 105972 steps/s (collection: 0.747s, learning 0.180s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0526
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.93s
                        Total time: 526.46s
                               ETA: 690 mins 56.7 s

################################################################################
                      Learning iteration 627/50000                      

                       Computation: 117145 steps/s (collection: 0.665s, learning 0.174s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.84s
                        Total time: 527.30s
                               ETA: 690 mins 55.8 s

################################################################################
                      Learning iteration 628/50000                      

                       Computation: 113158 steps/s (collection: 0.695s, learning 0.174s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.87s
                        Total time: 528.17s
                               ETA: 690 mins 57.2 s

################################################################################
                      Learning iteration 629/50000                      

                       Computation: 127137 steps/s (collection: 0.599s, learning 0.174s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.77s
                        Total time: 528.94s
                               ETA: 690 mins 51.2 s

################################################################################
                      Learning iteration 630/50000                      

                       Computation: 125274 steps/s (collection: 0.607s, learning 0.177s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0517
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.78s
                        Total time: 529.72s
                               ETA: 690 mins 46.0 s

################################################################################
                      Learning iteration 631/50000                      

                       Computation: 114819 steps/s (collection: 0.682s, learning 0.174s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0531
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.86s
                        Total time: 530.58s
                               ETA: 690 mins 46.5 s

################################################################################
                      Learning iteration 632/50000                      

                       Computation: 110938 steps/s (collection: 0.703s, learning 0.183s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0524
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.89s
                        Total time: 531.47s
                               ETA: 690 mins 49.3 s

################################################################################
                      Learning iteration 633/50000                      

                       Computation: 108532 steps/s (collection: 0.723s, learning 0.183s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0529
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.91s
                        Total time: 532.37s
                               ETA: 690 mins 53.6 s

################################################################################
                      Learning iteration 634/50000                      

                       Computation: 113646 steps/s (collection: 0.691s, learning 0.174s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0526
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.86s
                        Total time: 533.24s
                               ETA: 690 mins 54.7 s

################################################################################
                      Learning iteration 635/50000                      

                       Computation: 126520 steps/s (collection: 0.603s, learning 0.174s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.78s
                        Total time: 534.01s
                               ETA: 690 mins 49.0 s

################################################################################
                      Learning iteration 636/50000                      

                       Computation: 127402 steps/s (collection: 0.597s, learning 0.174s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.77s
                        Total time: 534.79s
                               ETA: 690 mins 42.9 s

################################################################################
                      Learning iteration 637/50000                      

                       Computation: 115581 steps/s (collection: 0.670s, learning 0.181s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0537
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.85s
                        Total time: 535.64s
                               ETA: 690 mins 42.9 s

################################################################################
                      Learning iteration 638/50000                      

                       Computation: 106857 steps/s (collection: 0.738s, learning 0.182s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0529
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.92s
                        Total time: 536.56s
                               ETA: 690 mins 48.3 s

################################################################################
                      Learning iteration 639/50000                      

                       Computation: 126628 steps/s (collection: 0.604s, learning 0.172s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0537
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.78s
                        Total time: 537.33s
                               ETA: 690 mins 42.6 s

################################################################################
                      Learning iteration 640/50000                      

                       Computation: 112650 steps/s (collection: 0.676s, learning 0.196s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.87s
                        Total time: 538.20s
                               ETA: 690 mins 44.3 s

################################################################################
                      Learning iteration 641/50000                      

                       Computation: 119510 steps/s (collection: 0.648s, learning 0.174s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.82s
                        Total time: 539.03s
                               ETA: 690 mins 42.1 s

################################################################################
                      Learning iteration 642/50000                      

                       Computation: 114229 steps/s (collection: 0.680s, learning 0.180s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.86s
                        Total time: 539.89s
                               ETA: 690 mins 42.9 s

################################################################################
                      Learning iteration 643/50000                      

                       Computation: 113769 steps/s (collection: 0.692s, learning 0.172s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.86s
                        Total time: 540.75s
                               ETA: 690 mins 43.9 s

################################################################################
                      Learning iteration 644/50000                      

                       Computation: 109334 steps/s (collection: 0.719s, learning 0.180s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0536
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.90s
                        Total time: 541.65s
                               ETA: 690 mins 47.6 s

################################################################################
                      Learning iteration 645/50000                      

                       Computation: 123683 steps/s (collection: 0.621s, learning 0.174s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0535
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.79s
                        Total time: 542.45s
                               ETA: 690 mins 43.4 s

################################################################################
                      Learning iteration 646/50000                      

                       Computation: 116339 steps/s (collection: 0.646s, learning 0.199s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0517
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.84s
                        Total time: 543.29s
                               ETA: 690 mins 42.9 s

################################################################################
                      Learning iteration 647/50000                      

                       Computation: 111580 steps/s (collection: 0.680s, learning 0.201s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0536
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.88s
                        Total time: 544.17s
                               ETA: 690 mins 45.2 s

################################################################################
                      Learning iteration 648/50000                      

                       Computation: 120718 steps/s (collection: 0.633s, learning 0.181s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0531
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.81s
                        Total time: 544.99s
                               ETA: 690 mins 42.4 s

################################################################################
                      Learning iteration 649/50000                      

                       Computation: 125353 steps/s (collection: 0.613s, learning 0.172s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.78s
                        Total time: 545.77s
                               ETA: 690 mins 37.4 s

################################################################################
                      Learning iteration 650/50000                      

                       Computation: 113879 steps/s (collection: 0.679s, learning 0.184s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.86s
                        Total time: 546.63s
                               ETA: 690 mins 38.3 s

################################################################################
                      Learning iteration 651/50000                      

                       Computation: 118313 steps/s (collection: 0.658s, learning 0.173s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0535
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.83s
                        Total time: 547.46s
                               ETA: 690 mins 36.8 s

################################################################################
                      Learning iteration 652/50000                      

                       Computation: 122061 steps/s (collection: 0.632s, learning 0.174s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.81s
                        Total time: 548.27s
                               ETA: 690 mins 33.4 s

################################################################################
                      Learning iteration 653/50000                      

                       Computation: 123628 steps/s (collection: 0.622s, learning 0.174s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0536
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.80s
                        Total time: 549.06s
                               ETA: 690 mins 29.2 s

################################################################################
                      Learning iteration 654/50000                      

                       Computation: 128425 steps/s (collection: 0.592s, learning 0.174s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0534
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.77s
                        Total time: 549.83s
                               ETA: 690 mins 22.8 s

################################################################################
                      Learning iteration 655/50000                      

                       Computation: 120519 steps/s (collection: 0.642s, learning 0.174s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0549
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.82s
                        Total time: 550.65s
                               ETA: 690 mins 20.2 s

################################################################################
                      Learning iteration 656/50000                      

                       Computation: 113593 steps/s (collection: 0.691s, learning 0.174s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.87s
                        Total time: 551.51s
                               ETA: 690 mins 21.3 s

################################################################################
                      Learning iteration 657/50000                      

                       Computation: 122396 steps/s (collection: 0.628s, learning 0.175s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0528
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.80s
                        Total time: 552.31s
                               ETA: 690 mins 17.7 s

################################################################################
                      Learning iteration 658/50000                      

                       Computation: 113260 steps/s (collection: 0.689s, learning 0.179s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0546
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.87s
                        Total time: 553.18s
                               ETA: 690 mins 19.0 s

################################################################################
                      Learning iteration 659/50000                      

                       Computation: 124923 steps/s (collection: 0.613s, learning 0.174s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0545
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.79s
                        Total time: 553.97s
                               ETA: 690 mins 14.2 s

################################################################################
                      Learning iteration 660/50000                      

                       Computation: 116543 steps/s (collection: 0.664s, learning 0.179s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0528
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.84s
                        Total time: 554.81s
                               ETA: 690 mins 13.7 s

################################################################################
                      Learning iteration 661/50000                      

                       Computation: 114160 steps/s (collection: 0.687s, learning 0.174s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0534
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.86s
                        Total time: 555.67s
                               ETA: 690 mins 14.5 s

################################################################################
                      Learning iteration 662/50000                      

                       Computation: 121138 steps/s (collection: 0.637s, learning 0.174s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.81s
                        Total time: 556.49s
                               ETA: 690 mins 11.6 s

################################################################################
                      Learning iteration 663/50000                      

                       Computation: 121025 steps/s (collection: 0.639s, learning 0.173s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.81s
                        Total time: 557.30s
                               ETA: 690 mins 8.7 s

################################################################################
                      Learning iteration 664/50000                      

                       Computation: 111672 steps/s (collection: 0.700s, learning 0.181s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.88s
                        Total time: 558.18s
                               ETA: 690 mins 10.9 s

################################################################################
                      Learning iteration 665/50000                      

                       Computation: 115724 steps/s (collection: 0.667s, learning 0.183s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.85s
                        Total time: 559.03s
                               ETA: 690 mins 10.8 s

################################################################################
                      Learning iteration 666/50000                      

                       Computation: 122472 steps/s (collection: 0.627s, learning 0.176s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 0.80s
                        Total time: 559.83s
                               ETA: 690 mins 7.3 s

################################################################################
                      Learning iteration 667/50000                      

                       Computation: 123298 steps/s (collection: 0.623s, learning 0.174s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 0.80s
                        Total time: 560.63s
                               ETA: 690 mins 3.3 s

################################################################################
                      Learning iteration 668/50000                      

                       Computation: 121931 steps/s (collection: 0.632s, learning 0.174s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0533
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 0.81s
                        Total time: 561.43s
                               ETA: 690 mins 0.1 s

################################################################################
                      Learning iteration 669/50000                      

                       Computation: 121115 steps/s (collection: 0.628s, learning 0.184s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 0.81s
                        Total time: 562.25s
                               ETA: 689 mins 57.2 s

################################################################################
                      Learning iteration 670/50000                      

                       Computation: 113327 steps/s (collection: 0.688s, learning 0.179s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 0.87s
                        Total time: 563.11s
                               ETA: 689 mins 58.4 s

################################################################################
                      Learning iteration 671/50000                      

                       Computation: 122011 steps/s (collection: 0.631s, learning 0.174s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 0.81s
                        Total time: 563.92s
                               ETA: 689 mins 55.1 s

################################################################################
                      Learning iteration 672/50000                      

                       Computation: 119886 steps/s (collection: 0.640s, learning 0.180s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0549
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 0.82s
                        Total time: 564.74s
                               ETA: 689 mins 52.9 s

################################################################################
                      Learning iteration 673/50000                      

                       Computation: 119320 steps/s (collection: 0.650s, learning 0.174s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 0.82s
                        Total time: 565.56s
                               ETA: 689 mins 50.9 s

################################################################################
                      Learning iteration 674/50000                      

                       Computation: 124748 steps/s (collection: 0.614s, learning 0.174s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0543
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 0.79s
                        Total time: 566.35s
                               ETA: 689 mins 46.4 s

################################################################################
                      Learning iteration 675/50000                      

                       Computation: 122388 steps/s (collection: 0.620s, learning 0.183s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 0.80s
                        Total time: 567.15s
                               ETA: 689 mins 42.9 s

################################################################################
                      Learning iteration 676/50000                      

                       Computation: 117733 steps/s (collection: 0.654s, learning 0.181s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0548
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.83s
                        Total time: 567.99s
                               ETA: 689 mins 41.8 s

################################################################################
                      Learning iteration 677/50000                      

                       Computation: 128105 steps/s (collection: 0.594s, learning 0.174s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0560
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.77s
                        Total time: 568.76s
                               ETA: 689 mins 35.7 s

################################################################################
                      Learning iteration 678/50000                      

                       Computation: 110388 steps/s (collection: 0.702s, learning 0.188s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.89s
                        Total time: 569.65s
                               ETA: 689 mins 38.6 s

################################################################################
                      Learning iteration 679/50000                      

                       Computation: 128650 steps/s (collection: 0.591s, learning 0.173s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.76s
                        Total time: 570.41s
                               ETA: 689 mins 32.4 s

################################################################################
                      Learning iteration 680/50000                      

                       Computation: 121887 steps/s (collection: 0.617s, learning 0.190s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0545
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.81s
                        Total time: 571.22s
                               ETA: 689 mins 29.2 s

################################################################################
                      Learning iteration 681/50000                      

                       Computation: 123781 steps/s (collection: 0.603s, learning 0.191s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.79s
                        Total time: 572.01s
                               ETA: 689 mins 25.1 s

################################################################################
                      Learning iteration 682/50000                      

                       Computation: 127389 steps/s (collection: 0.582s, learning 0.190s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.77s
                        Total time: 572.78s
                               ETA: 689 mins 19.4 s

################################################################################
                      Learning iteration 683/50000                      

                       Computation: 110194 steps/s (collection: 0.702s, learning 0.190s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.89s
                        Total time: 573.67s
                               ETA: 689 mins 22.5 s

################################################################################
                      Learning iteration 684/50000                      

                       Computation: 127396 steps/s (collection: 0.583s, learning 0.188s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0551
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.77s
                        Total time: 574.45s
                               ETA: 689 mins 16.8 s

################################################################################
                      Learning iteration 685/50000                      

                       Computation: 130289 steps/s (collection: 0.580s, learning 0.174s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.75s
                        Total time: 575.20s
                               ETA: 689 mins 9.9 s

################################################################################
                      Learning iteration 686/50000                      

                       Computation: 110894 steps/s (collection: 0.713s, learning 0.173s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.89s
                        Total time: 576.09s
                               ETA: 689 mins 12.5 s

################################################################################
                      Learning iteration 687/50000                      

                       Computation: 117675 steps/s (collection: 0.645s, learning 0.190s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0560
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.84s
                        Total time: 576.92s
                               ETA: 689 mins 11.5 s

################################################################################
                      Learning iteration 688/50000                      

                       Computation: 127774 steps/s (collection: 0.592s, learning 0.178s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0547
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.77s
                        Total time: 577.69s
                               ETA: 689 mins 5.7 s

################################################################################
                      Learning iteration 689/50000                      

                       Computation: 108578 steps/s (collection: 0.716s, learning 0.189s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.91s
                        Total time: 578.60s
                               ETA: 689 mins 9.6 s

################################################################################
                      Learning iteration 690/50000                      

                       Computation: 102866 steps/s (collection: 0.754s, learning 0.202s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.96s
                        Total time: 579.55s
                               ETA: 689 mins 17.1 s

################################################################################
                      Learning iteration 691/50000                      

                       Computation: 127212 steps/s (collection: 0.599s, learning 0.174s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.77s
                        Total time: 580.33s
                               ETA: 689 mins 11.6 s

################################################################################
                      Learning iteration 692/50000                      

                       Computation: 117004 steps/s (collection: 0.662s, learning 0.178s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0554
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.84s
                        Total time: 581.17s
                               ETA: 689 mins 10.9 s

################################################################################
                      Learning iteration 693/50000                      

                       Computation: 131611 steps/s (collection: 0.573s, learning 0.174s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0554
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.75s
                        Total time: 581.91s
                               ETA: 689 mins 3.5 s

################################################################################
                      Learning iteration 694/50000                      

                       Computation: 131651 steps/s (collection: 0.574s, learning 0.172s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.75s
                        Total time: 582.66s
                               ETA: 688 mins 56.2 s

################################################################################
                      Learning iteration 695/50000                      

                       Computation: 126718 steps/s (collection: 0.602s, learning 0.174s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.78s
                        Total time: 583.44s
                               ETA: 688 mins 50.9 s

################################################################################
                      Learning iteration 696/50000                      

                       Computation: 109786 steps/s (collection: 0.707s, learning 0.188s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.90s
                        Total time: 584.33s
                               ETA: 688 mins 54.1 s

################################################################################
                      Learning iteration 697/50000                      

                       Computation: 123927 steps/s (collection: 0.620s, learning 0.174s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.79s
                        Total time: 585.12s
                               ETA: 688 mins 50.1 s

################################################################################
                      Learning iteration 698/50000                      

                       Computation: 106907 steps/s (collection: 0.730s, learning 0.190s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.92s
                        Total time: 586.04s
                               ETA: 688 mins 55.0 s

################################################################################
                      Learning iteration 699/50000                      

                       Computation: 110630 steps/s (collection: 0.704s, learning 0.185s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.89s
                        Total time: 586.93s
                               ETA: 688 mins 57.6 s

################################################################################
                      Learning iteration 700/50000                      

                       Computation: 131344 steps/s (collection: 0.575s, learning 0.174s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.75s
                        Total time: 587.68s
                               ETA: 688 mins 50.5 s

################################################################################
                      Learning iteration 701/50000                      

                       Computation: 120739 steps/s (collection: 0.634s, learning 0.180s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0562
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.81s
                        Total time: 588.49s
                               ETA: 688 mins 47.9 s

################################################################################
                      Learning iteration 702/50000                      

                       Computation: 116138 steps/s (collection: 0.661s, learning 0.186s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0562
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.85s
                        Total time: 589.34s
                               ETA: 688 mins 47.7 s

################################################################################
                      Learning iteration 703/50000                      

                       Computation: 114063 steps/s (collection: 0.684s, learning 0.178s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.86s
                        Total time: 590.20s
                               ETA: 688 mins 48.5 s

################################################################################
                      Learning iteration 704/50000                      

                       Computation: 127960 steps/s (collection: 0.593s, learning 0.175s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.77s
                        Total time: 590.97s
                               ETA: 688 mins 42.7 s

################################################################################
                      Learning iteration 705/50000                      

                       Computation: 122636 steps/s (collection: 0.628s, learning 0.174s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0580
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.80s
                        Total time: 591.77s
                               ETA: 688 mins 39.3 s

################################################################################
                      Learning iteration 706/50000                      

                       Computation: 110422 steps/s (collection: 0.698s, learning 0.192s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0562
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.89s
                        Total time: 592.66s
                               ETA: 688 mins 42.1 s

################################################################################
                      Learning iteration 707/50000                      

                       Computation: 110991 steps/s (collection: 0.682s, learning 0.203s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.89s
                        Total time: 593.55s
                               ETA: 688 mins 44.6 s

################################################################################
                      Learning iteration 708/50000                      

                       Computation: 121762 steps/s (collection: 0.626s, learning 0.181s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.81s
                        Total time: 594.36s
                               ETA: 688 mins 41.6 s

################################################################################
                      Learning iteration 709/50000                      

                       Computation: 128399 steps/s (collection: 0.593s, learning 0.173s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0560
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.77s
                        Total time: 595.12s
                               ETA: 688 mins 35.7 s

################################################################################
                      Learning iteration 710/50000                      

                       Computation: 123736 steps/s (collection: 0.621s, learning 0.173s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0566
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.79s
                        Total time: 595.92s
                               ETA: 688 mins 31.8 s

################################################################################
                      Learning iteration 711/50000                      

                       Computation: 114292 steps/s (collection: 0.674s, learning 0.186s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.86s
                        Total time: 596.78s
                               ETA: 688 mins 32.5 s

################################################################################
                      Learning iteration 712/50000                      

                       Computation: 122263 steps/s (collection: 0.630s, learning 0.174s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.80s
                        Total time: 597.58s
                               ETA: 688 mins 29.3 s

################################################################################
                      Learning iteration 713/50000                      

                       Computation: 124971 steps/s (collection: 0.603s, learning 0.184s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.79s
                        Total time: 598.37s
                               ETA: 688 mins 24.9 s

################################################################################
                      Learning iteration 714/50000                      

                       Computation: 121542 steps/s (collection: 0.623s, learning 0.186s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.81s
                        Total time: 599.18s
                               ETA: 688 mins 22.1 s

################################################################################
                      Learning iteration 715/50000                      

                       Computation: 126721 steps/s (collection: 0.599s, learning 0.177s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.78s
                        Total time: 599.95s
                               ETA: 688 mins 17.0 s

################################################################################
                      Learning iteration 716/50000                      

                       Computation: 108023 steps/s (collection: 0.721s, learning 0.189s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0570
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.91s
                        Total time: 600.86s
                               ETA: 688 mins 21.1 s

################################################################################
                      Learning iteration 717/50000                      

                       Computation: 109901 steps/s (collection: 0.657s, learning 0.238s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.89s
                        Total time: 601.76s
                               ETA: 688 mins 24.1 s

################################################################################
                      Learning iteration 718/50000                      

                       Computation: 109153 steps/s (collection: 0.713s, learning 0.187s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0558
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.90s
                        Total time: 602.66s
                               ETA: 688 mins 27.6 s

################################################################################
                      Learning iteration 719/50000                      

                       Computation: 118049 steps/s (collection: 0.644s, learning 0.189s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0558
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.83s
                        Total time: 603.49s
                               ETA: 688 mins 26.3 s

################################################################################
                      Learning iteration 720/50000                      

                       Computation: 116880 steps/s (collection: 0.646s, learning 0.195s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.84s
                        Total time: 604.33s
                               ETA: 688 mins 25.7 s

################################################################################
                      Learning iteration 721/50000                      

                       Computation: 127072 steps/s (collection: 0.599s, learning 0.175s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0555
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.77s
                        Total time: 605.10s
                               ETA: 688 mins 20.5 s

################################################################################
                      Learning iteration 722/50000                      

                       Computation: 15873 steps/s (collection: 6.014s, learning 0.179s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0555
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 6.19s
                        Total time: 611.30s
                               ETA: 694 mins 24.6 s

################################################################################
                      Learning iteration 723/50000                      

                       Computation: 17858 steps/s (collection: 5.331s, learning 0.173s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 5.50s
                        Total time: 616.80s
                               ETA: 699 mins 40.9 s

################################################################################
                      Learning iteration 724/50000                      

                       Computation: 72080 steps/s (collection: 1.182s, learning 0.181s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0573
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.36s
                        Total time: 618.17s
                               ETA: 700 mins 14.8 s

################################################################################
                      Learning iteration 725/50000                      

                       Computation: 83230 steps/s (collection: 1.003s, learning 0.178s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 1.18s
                        Total time: 619.35s
                               ETA: 700 mins 36.2 s

################################################################################
                      Learning iteration 726/50000                      

                       Computation: 82317 steps/s (collection: 1.019s, learning 0.175s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.19s
                        Total time: 620.54s
                               ETA: 700 mins 58.5 s

################################################################################
                      Learning iteration 727/50000                      

                       Computation: 84829 steps/s (collection: 0.986s, learning 0.173s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 1.16s
                        Total time: 621.70s
                               ETA: 701 mins 18.3 s

################################################################################
                      Learning iteration 728/50000                      

                       Computation: 84094 steps/s (collection: 0.995s, learning 0.174s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.17s
                        Total time: 622.87s
                               ETA: 701 mins 38.8 s

################################################################################
                      Learning iteration 729/50000                      

                       Computation: 83428 steps/s (collection: 1.004s, learning 0.174s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 1.18s
                        Total time: 624.05s
                               ETA: 701 mins 59.8 s

################################################################################
                      Learning iteration 730/50000                      

                       Computation: 78935 steps/s (collection: 1.069s, learning 0.176s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0579
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 1.25s
                        Total time: 625.29s
                               ETA: 702 mins 25.2 s

################################################################################
                      Learning iteration 731/50000                      

                       Computation: 81586 steps/s (collection: 1.023s, learning 0.182s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 1.20s
                        Total time: 626.50s
                               ETA: 702 mins 47.9 s

################################################################################
                      Learning iteration 732/50000                      

                       Computation: 84140 steps/s (collection: 0.994s, learning 0.174s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 1.17s
                        Total time: 627.67s
                               ETA: 703 mins 8.0 s

################################################################################
                      Learning iteration 733/50000                      

                       Computation: 85391 steps/s (collection: 0.977s, learning 0.174s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0579
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 1.15s
                        Total time: 628.82s
                               ETA: 703 mins 27.0 s

################################################################################
                      Learning iteration 734/50000                      

                       Computation: 85147 steps/s (collection: 0.981s, learning 0.174s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0584
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 1.15s
                        Total time: 629.97s
                               ETA: 703 mins 46.1 s

################################################################################
                      Learning iteration 735/50000                      

                       Computation: 81645 steps/s (collection: 1.031s, learning 0.173s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0593
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 1.20s
                        Total time: 631.18s
                               ETA: 704 mins 8.5 s

################################################################################
                      Learning iteration 736/50000                      

                       Computation: 80975 steps/s (collection: 1.040s, learning 0.174s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 1.21s
                        Total time: 632.39s
                               ETA: 704 mins 31.4 s

################################################################################
                      Learning iteration 737/50000                      

                       Computation: 78814 steps/s (collection: 1.061s, learning 0.187s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 1.25s
                        Total time: 633.64s
                               ETA: 704 mins 56.5 s

################################################################################
                      Learning iteration 738/50000                      

                       Computation: 80958 steps/s (collection: 1.041s, learning 0.174s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0560
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 1.21s
                        Total time: 634.85s
                               ETA: 705 mins 19.4 s

################################################################################
                      Learning iteration 739/50000                      

                       Computation: 78801 steps/s (collection: 1.062s, learning 0.185s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0594
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 1.25s
                        Total time: 636.10s
                               ETA: 705 mins 44.4 s

################################################################################
                      Learning iteration 740/50000                      

                       Computation: 85822 steps/s (collection: 0.973s, learning 0.172s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 1.15s
                        Total time: 637.24s
                               ETA: 706 mins 2.5 s

################################################################################
                      Learning iteration 741/50000                      

                       Computation: 76134 steps/s (collection: 1.108s, learning 0.183s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 1.29s
                        Total time: 638.54s
                               ETA: 706 mins 30.3 s

################################################################################
                      Learning iteration 742/50000                      

                       Computation: 85617 steps/s (collection: 0.974s, learning 0.174s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0603
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 1.15s
                        Total time: 639.68s
                               ETA: 706 mins 48.5 s

################################################################################
                      Learning iteration 743/50000                      

                       Computation: 71893 steps/s (collection: 1.175s, learning 0.192s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 1.37s
                        Total time: 641.05s
                               ETA: 707 mins 21.2 s

################################################################################
                      Learning iteration 744/50000                      

                       Computation: 81840 steps/s (collection: 1.027s, learning 0.175s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 1.20s
                        Total time: 642.25s
                               ETA: 707 mins 42.8 s

################################################################################
                      Learning iteration 745/50000                      

                       Computation: 83485 steps/s (collection: 0.982s, learning 0.195s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 1.18s
                        Total time: 643.43s
                               ETA: 708 mins 2.7 s

################################################################################
                      Learning iteration 746/50000                      

                       Computation: 83843 steps/s (collection: 0.982s, learning 0.190s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 1.17s
                        Total time: 644.60s
                               ETA: 708 mins 22.3 s

################################################################################
                      Learning iteration 747/50000                      

                       Computation: 85534 steps/s (collection: 0.975s, learning 0.174s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 1.15s
                        Total time: 645.75s
                               ETA: 708 mins 40.3 s

################################################################################
                      Learning iteration 748/50000                      

                       Computation: 80882 steps/s (collection: 1.042s, learning 0.173s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 1.22s
                        Total time: 646.97s
                               ETA: 709 mins 2.6 s

################################################################################
                      Learning iteration 749/50000                      

                       Computation: 85940 steps/s (collection: 0.970s, learning 0.174s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0596
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 1.14s
                        Total time: 648.11s
                               ETA: 709 mins 20.1 s

################################################################################
                      Learning iteration 750/50000                      

                       Computation: 81865 steps/s (collection: 1.017s, learning 0.184s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 1.20s
                        Total time: 649.31s
                               ETA: 709 mins 41.3 s

################################################################################
                      Learning iteration 751/50000                      

                       Computation: 84959 steps/s (collection: 0.983s, learning 0.174s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0591
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 1.16s
                        Total time: 650.47s
                               ETA: 709 mins 59.6 s

################################################################################
                      Learning iteration 752/50000                      

                       Computation: 85072 steps/s (collection: 0.972s, learning 0.184s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 1.16s
                        Total time: 651.62s
                               ETA: 710 mins 17.8 s

################################################################################
                      Learning iteration 753/50000                      

                       Computation: 77712 steps/s (collection: 1.093s, learning 0.172s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 1.26s
                        Total time: 652.89s
                               ETA: 710 mins 43.0 s

################################################################################
                      Learning iteration 754/50000                      

                       Computation: 86133 steps/s (collection: 0.967s, learning 0.174s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0181
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 1.14s
                        Total time: 654.03s
                               ETA: 711 mins 0.1 s

################################################################################
                      Learning iteration 755/50000                      

                       Computation: 116334 steps/s (collection: 0.653s, learning 0.192s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.85s
                        Total time: 654.88s
                               ETA: 710 mins 57.8 s

################################################################################
                      Learning iteration 756/50000                      

                       Computation: 106314 steps/s (collection: 0.741s, learning 0.184s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0593
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.92s
                        Total time: 655.80s
                               ETA: 711 mins 0.8 s

################################################################################
                      Learning iteration 757/50000                      

                       Computation: 113429 steps/s (collection: 0.676s, learning 0.191s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.87s
                        Total time: 656.67s
                               ETA: 710 mins 59.9 s

################################################################################
                      Learning iteration 758/50000                      

                       Computation: 123870 steps/s (collection: 0.619s, learning 0.174s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.79s
                        Total time: 657.46s
                               ETA: 710 mins 54.3 s

################################################################################
                      Learning iteration 759/50000                      

                       Computation: 116498 steps/s (collection: 0.663s, learning 0.181s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.84s
                        Total time: 658.30s
                               ETA: 710 mins 52.0 s

################################################################################
                      Learning iteration 760/50000                      

                       Computation: 106552 steps/s (collection: 0.717s, learning 0.206s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.92s
                        Total time: 659.23s
                               ETA: 710 mins 54.8 s

################################################################################
                      Learning iteration 761/50000                      

                       Computation: 126645 steps/s (collection: 0.589s, learning 0.187s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.78s
                        Total time: 660.00s
                               ETA: 710 mins 48.1 s

################################################################################
                      Learning iteration 762/50000                      

                       Computation: 119919 steps/s (collection: 0.646s, learning 0.174s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.82s
                        Total time: 660.82s
                               ETA: 710 mins 44.3 s

################################################################################
                      Learning iteration 763/50000                      

                       Computation: 119011 steps/s (collection: 0.653s, learning 0.173s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.83s
                        Total time: 661.65s
                               ETA: 710 mins 40.8 s

################################################################################
                      Learning iteration 764/50000                      

                       Computation: 127330 steps/s (collection: 0.598s, learning 0.174s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.77s
                        Total time: 662.42s
                               ETA: 710 mins 33.9 s

################################################################################
                      Learning iteration 765/50000                      

                       Computation: 129278 steps/s (collection: 0.586s, learning 0.174s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.76s
                        Total time: 663.18s
                               ETA: 710 mins 26.2 s

################################################################################
                      Learning iteration 766/50000                      

                       Computation: 126589 steps/s (collection: 0.604s, learning 0.172s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0603
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.78s
                        Total time: 663.96s
                               ETA: 710 mins 19.7 s

################################################################################
                      Learning iteration 767/50000                      

                       Computation: 120547 steps/s (collection: 0.634s, learning 0.182s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0589
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.82s
                        Total time: 664.77s
                               ETA: 710 mins 15.6 s

################################################################################
                      Learning iteration 768/50000                      

                       Computation: 123549 steps/s (collection: 0.622s, learning 0.174s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0588
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.80s
                        Total time: 665.57s
                               ETA: 710 mins 10.2 s

################################################################################
                      Learning iteration 769/50000                      

                       Computation: 129138 steps/s (collection: 0.588s, learning 0.173s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0589
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.76s
                        Total time: 666.33s
                               ETA: 710 mins 2.7 s

################################################################################
                      Learning iteration 770/50000                      

                       Computation: 114982 steps/s (collection: 0.667s, learning 0.187s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0602
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.85s
                        Total time: 667.18s
                               ETA: 710 mins 1.2 s

################################################################################
                      Learning iteration 771/50000                      

                       Computation: 111560 steps/s (collection: 0.707s, learning 0.174s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.88s
                        Total time: 668.07s
                               ETA: 710 mins 1.3 s

################################################################################
                      Learning iteration 772/50000                      

                       Computation: 126839 steps/s (collection: 0.591s, learning 0.184s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.78s
                        Total time: 668.84s
                               ETA: 709 mins 54.7 s

################################################################################
                      Learning iteration 773/50000                      

                       Computation: 128645 steps/s (collection: 0.592s, learning 0.172s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0605
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.76s
                        Total time: 669.61s
                               ETA: 709 mins 47.4 s

################################################################################
                      Learning iteration 774/50000                      

                       Computation: 119402 steps/s (collection: 0.637s, learning 0.186s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0613
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.82s
                        Total time: 670.43s
                               ETA: 709 mins 43.9 s

################################################################################
                      Learning iteration 775/50000                      

                       Computation: 117448 steps/s (collection: 0.655s, learning 0.182s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0589
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.84s
                        Total time: 671.27s
                               ETA: 709 mins 41.2 s

################################################################################
                      Learning iteration 776/50000                      

                       Computation: 131982 steps/s (collection: 0.572s, learning 0.173s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.74s
                        Total time: 672.01s
                               ETA: 709 mins 32.8 s

################################################################################
                      Learning iteration 777/50000                      

                       Computation: 130836 steps/s (collection: 0.577s, learning 0.174s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.75s
                        Total time: 672.76s
                               ETA: 709 mins 24.7 s

################################################################################
                      Learning iteration 778/50000                      

                       Computation: 128432 steps/s (collection: 0.592s, learning 0.174s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0612
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.77s
                        Total time: 673.53s
                               ETA: 709 mins 17.6 s

################################################################################
                      Learning iteration 779/50000                      

                       Computation: 129845 steps/s (collection: 0.583s, learning 0.174s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0611
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.76s
                        Total time: 674.28s
                               ETA: 709 mins 9.9 s

################################################################################
                      Learning iteration 780/50000                      

                       Computation: 131465 steps/s (collection: 0.573s, learning 0.174s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.75s
                        Total time: 675.03s
                               ETA: 709 mins 1.7 s

################################################################################
                      Learning iteration 781/50000                      

                       Computation: 131794 steps/s (collection: 0.572s, learning 0.174s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.75s
                        Total time: 675.78s
                               ETA: 708 mins 53.4 s

################################################################################
                      Learning iteration 782/50000                      

                       Computation: 130254 steps/s (collection: 0.581s, learning 0.174s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0596
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.75s
                        Total time: 676.53s
                               ETA: 708 mins 45.6 s

################################################################################
                      Learning iteration 783/50000                      

                       Computation: 130983 steps/s (collection: 0.577s, learning 0.174s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.75s
                        Total time: 677.28s
                               ETA: 708 mins 37.6 s

################################################################################
                      Learning iteration 784/50000                      

                       Computation: 131647 steps/s (collection: 0.573s, learning 0.174s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.75s
                        Total time: 678.03s
                               ETA: 708 mins 29.4 s

################################################################################
                      Learning iteration 785/50000                      

                       Computation: 130801 steps/s (collection: 0.577s, learning 0.175s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0617
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.75s
                        Total time: 678.78s
                               ETA: 708 mins 21.5 s

################################################################################
                      Learning iteration 786/50000                      

                       Computation: 129832 steps/s (collection: 0.584s, learning 0.173s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0618
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.76s
                        Total time: 679.54s
                               ETA: 708 mins 14.0 s

################################################################################
                      Learning iteration 787/50000                      

                       Computation: 129472 steps/s (collection: 0.586s, learning 0.173s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0621
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.76s
                        Total time: 680.30s
                               ETA: 708 mins 6.7 s

################################################################################
                      Learning iteration 788/50000                      

                       Computation: 129804 steps/s (collection: 0.583s, learning 0.174s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.76s
                        Total time: 681.05s
                               ETA: 707 mins 59.2 s

################################################################################
                      Learning iteration 789/50000                      

                       Computation: 130255 steps/s (collection: 0.581s, learning 0.174s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.75s
                        Total time: 681.81s
                               ETA: 707 mins 51.6 s

################################################################################
                      Learning iteration 790/50000                      

                       Computation: 127237 steps/s (collection: 0.600s, learning 0.173s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.77s
                        Total time: 682.58s
                               ETA: 707 mins 45.1 s

################################################################################
                      Learning iteration 791/50000                      

                       Computation: 131310 steps/s (collection: 0.576s, learning 0.173s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0620
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.75s
                        Total time: 683.33s
                               ETA: 707 mins 37.1 s

################################################################################
                      Learning iteration 792/50000                      

                       Computation: 131891 steps/s (collection: 0.572s, learning 0.173s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.75s
                        Total time: 684.08s
                               ETA: 707 mins 29.0 s

################################################################################
                      Learning iteration 793/50000                      

                       Computation: 129796 steps/s (collection: 0.584s, learning 0.174s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0618
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.76s
                        Total time: 684.83s
                               ETA: 707 mins 21.6 s

################################################################################
                      Learning iteration 794/50000                      

                       Computation: 127824 steps/s (collection: 0.597s, learning 0.173s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0613
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.77s
                        Total time: 685.60s
                               ETA: 707 mins 14.9 s

################################################################################
                      Learning iteration 795/50000                      

                       Computation: 130856 steps/s (collection: 0.577s, learning 0.174s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0630
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.75s
                        Total time: 686.35s
                               ETA: 707 mins 7.2 s

################################################################################
                      Learning iteration 796/50000                      

                       Computation: 130612 steps/s (collection: 0.580s, learning 0.173s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0631
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.75s
                        Total time: 687.11s
                               ETA: 706 mins 59.6 s

################################################################################
                      Learning iteration 797/50000                      

                       Computation: 130243 steps/s (collection: 0.582s, learning 0.173s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0613
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.75s
                        Total time: 687.86s
                               ETA: 706 mins 52.1 s

################################################################################
                      Learning iteration 798/50000                      

                       Computation: 127745 steps/s (collection: 0.597s, learning 0.173s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0626
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.77s
                        Total time: 688.63s
                               ETA: 706 mins 45.5 s

################################################################################
                      Learning iteration 799/50000                      

                       Computation: 131660 steps/s (collection: 0.573s, learning 0.174s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.75s
                        Total time: 689.38s
                               ETA: 706 mins 37.6 s

################################################################################
                      Learning iteration 800/50000                      

                       Computation: 128997 steps/s (collection: 0.588s, learning 0.174s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0180
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0621
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.76s
                        Total time: 690.14s
                               ETA: 706 mins 30.6 s

################################################################################
                      Learning iteration 801/50000                      

                       Computation: 116938 steps/s (collection: 0.593s, learning 0.248s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.84s
                        Total time: 690.98s
                               ETA: 706 mins 28.4 s

################################################################################
                      Learning iteration 802/50000                      

                       Computation: 128402 steps/s (collection: 0.589s, learning 0.176s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.77s
                        Total time: 691.75s
                               ETA: 706 mins 21.7 s

################################################################################
                      Learning iteration 803/50000                      

                       Computation: 130834 steps/s (collection: 0.579s, learning 0.172s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0638
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.75s
                        Total time: 692.50s
                               ETA: 706 mins 14.1 s

################################################################################
                      Learning iteration 804/50000                      

                       Computation: 130650 steps/s (collection: 0.580s, learning 0.172s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0605
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.75s
                        Total time: 693.25s
                               ETA: 706 mins 6.6 s

################################################################################
                      Learning iteration 805/50000                      

                       Computation: 128252 steps/s (collection: 0.589s, learning 0.177s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0620
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.77s
                        Total time: 694.02s
                               ETA: 705 mins 59.9 s

################################################################################
                      Learning iteration 806/50000                      

                       Computation: 130020 steps/s (collection: 0.582s, learning 0.174s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0617
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.76s
                        Total time: 694.77s
                               ETA: 705 mins 52.7 s

################################################################################
                      Learning iteration 807/50000                      

                       Computation: 129186 steps/s (collection: 0.587s, learning 0.174s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0630
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.76s
                        Total time: 695.53s
                               ETA: 705 mins 45.7 s

################################################################################
                      Learning iteration 808/50000                      

                       Computation: 128332 steps/s (collection: 0.593s, learning 0.173s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.77s
                        Total time: 696.30s
                               ETA: 705 mins 39.1 s

################################################################################
                      Learning iteration 809/50000                      

                       Computation: 126806 steps/s (collection: 0.584s, learning 0.191s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.78s
                        Total time: 697.07s
                               ETA: 705 mins 33.1 s

################################################################################
                      Learning iteration 810/50000                      

                       Computation: 128349 steps/s (collection: 0.575s, learning 0.191s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.77s
                        Total time: 697.84s
                               ETA: 705 mins 26.5 s

################################################################################
                      Learning iteration 811/50000                      

                       Computation: 127124 steps/s (collection: 0.583s, learning 0.190s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.77s
                        Total time: 698.61s
                               ETA: 705 mins 20.3 s

################################################################################
                      Learning iteration 812/50000                      

                       Computation: 130982 steps/s (collection: 0.574s, learning 0.176s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.75s
                        Total time: 699.36s
                               ETA: 705 mins 12.8 s

################################################################################
                      Learning iteration 813/50000                      

                       Computation: 118358 steps/s (collection: 0.646s, learning 0.185s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0620
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.83s
                        Total time: 700.19s
                               ETA: 705 mins 10.2 s

################################################################################
                      Learning iteration 814/50000                      

                       Computation: 129633 steps/s (collection: 0.586s, learning 0.173s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.76s
                        Total time: 700.95s
                               ETA: 705 mins 3.1 s

################################################################################
                      Learning iteration 815/50000                      

                       Computation: 118074 steps/s (collection: 0.659s, learning 0.173s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0180
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0646
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.83s
                        Total time: 701.79s
                               ETA: 705 mins 0.6 s

################################################################################
                      Learning iteration 816/50000                      

                       Computation: 129701 steps/s (collection: 0.585s, learning 0.173s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0179
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0638
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.76s
                        Total time: 702.54s
                               ETA: 704 mins 53.6 s

################################################################################
                      Learning iteration 817/50000                      

                       Computation: 125669 steps/s (collection: 0.600s, learning 0.182s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0624
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.78s
                        Total time: 703.33s
                               ETA: 704 mins 48.1 s

################################################################################
                      Learning iteration 818/50000                      

                       Computation: 129065 steps/s (collection: 0.588s, learning 0.173s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0630
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.76s
                        Total time: 704.09s
                               ETA: 704 mins 41.3 s

################################################################################
                      Learning iteration 819/50000                      

                       Computation: 112725 steps/s (collection: 0.689s, learning 0.183s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0620
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.87s
                        Total time: 704.96s
                               ETA: 704 mins 41.2 s

################################################################################
                      Learning iteration 820/50000                      

                       Computation: 124252 steps/s (collection: 0.617s, learning 0.174s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0617
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.79s
                        Total time: 705.75s
                               ETA: 704 mins 36.3 s

################################################################################
                      Learning iteration 821/50000                      

                       Computation: 117743 steps/s (collection: 0.659s, learning 0.176s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0638
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.83s
                        Total time: 706.59s
                               ETA: 704 mins 33.9 s

################################################################################
                      Learning iteration 822/50000                      

                       Computation: 128785 steps/s (collection: 0.590s, learning 0.173s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.76s
                        Total time: 707.35s
                               ETA: 704 mins 27.3 s

################################################################################
                      Learning iteration 823/50000                      

                       Computation: 129938 steps/s (collection: 0.585s, learning 0.172s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0637
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.76s
                        Total time: 708.11s
                               ETA: 704 mins 20.3 s

################################################################################
                      Learning iteration 824/50000                      

                       Computation: 128543 steps/s (collection: 0.588s, learning 0.177s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.76s
                        Total time: 708.87s
                               ETA: 704 mins 13.8 s

################################################################################
                      Learning iteration 825/50000                      

                       Computation: 112965 steps/s (collection: 0.693s, learning 0.177s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0626
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.87s
                        Total time: 709.74s
                               ETA: 704 mins 13.6 s

################################################################################
                      Learning iteration 826/50000                      

                       Computation: 129090 steps/s (collection: 0.589s, learning 0.173s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.76s
                        Total time: 710.50s
                               ETA: 704 mins 6.9 s

################################################################################
                      Learning iteration 827/50000                      

                       Computation: 123449 steps/s (collection: 0.623s, learning 0.174s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0176
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.80s
                        Total time: 711.30s
                               ETA: 704 mins 2.3 s

################################################################################
                      Learning iteration 828/50000                      

                       Computation: 119040 steps/s (collection: 0.645s, learning 0.181s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.83s
                        Total time: 712.12s
                               ETA: 703 mins 59.5 s

################################################################################
                      Learning iteration 829/50000                      

                       Computation: 121871 steps/s (collection: 0.632s, learning 0.175s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0631
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.81s
                        Total time: 712.93s
                               ETA: 703 mins 55.5 s

################################################################################
                      Learning iteration 830/50000                      

                       Computation: 114559 steps/s (collection: 0.667s, learning 0.191s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.86s
                        Total time: 713.79s
                               ETA: 703 mins 54.6 s

################################################################################
                      Learning iteration 831/50000                      

                       Computation: 117375 steps/s (collection: 0.629s, learning 0.209s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.84s
                        Total time: 714.63s
                               ETA: 703 mins 52.5 s

################################################################################
                      Learning iteration 832/50000                      

                       Computation: 125619 steps/s (collection: 0.590s, learning 0.192s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.78s
                        Total time: 715.41s
                               ETA: 703 mins 47.1 s

################################################################################
                      Learning iteration 833/50000                      

                       Computation: 119298 steps/s (collection: 0.628s, learning 0.196s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0180
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.82s
                        Total time: 716.23s
                               ETA: 703 mins 44.2 s

################################################################################
                      Learning iteration 834/50000                      

                       Computation: 113761 steps/s (collection: 0.660s, learning 0.204s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.86s
                        Total time: 717.10s
                               ETA: 703 mins 43.7 s

################################################################################
                      Learning iteration 835/50000                      

                       Computation: 105165 steps/s (collection: 0.737s, learning 0.197s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0180
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.93s
                        Total time: 718.03s
                               ETA: 703 mins 47.3 s

################################################################################
                      Learning iteration 836/50000                      

                       Computation: 118736 steps/s (collection: 0.635s, learning 0.193s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.83s
                        Total time: 718.86s
                               ETA: 703 mins 44.6 s

################################################################################
                      Learning iteration 837/50000                      

                       Computation: 116798 steps/s (collection: 0.639s, learning 0.203s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.84s
                        Total time: 719.70s
                               ETA: 703 mins 42.7 s

################################################################################
                      Learning iteration 838/50000                      

                       Computation: 125225 steps/s (collection: 0.595s, learning 0.191s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.79s
                        Total time: 720.49s
                               ETA: 703 mins 37.6 s

################################################################################
                      Learning iteration 839/50000                      

                       Computation: 112376 steps/s (collection: 0.673s, learning 0.201s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.87s
                        Total time: 721.36s
                               ETA: 703 mins 37.6 s

################################################################################
                      Learning iteration 840/50000                      

                       Computation: 126149 steps/s (collection: 0.587s, learning 0.192s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0179
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0643
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.78s
                        Total time: 722.14s
                               ETA: 703 mins 32.1 s

################################################################################
                      Learning iteration 841/50000                      

                       Computation: 112798 steps/s (collection: 0.667s, learning 0.205s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0092
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.87s
                        Total time: 723.01s
                               ETA: 703 mins 32.0 s

################################################################################
                      Learning iteration 842/50000                      

                       Computation: 123950 steps/s (collection: 0.599s, learning 0.194s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.79s
                        Total time: 723.80s
                               ETA: 703 mins 27.3 s

################################################################################
                      Learning iteration 843/50000                      

                       Computation: 111623 steps/s (collection: 0.688s, learning 0.192s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0645
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.88s
                        Total time: 724.69s
                               ETA: 703 mins 27.8 s

################################################################################
                      Learning iteration 844/50000                      

                       Computation: 116993 steps/s (collection: 0.660s, learning 0.180s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.84s
                        Total time: 725.53s
                               ETA: 703 mins 25.8 s

################################################################################
                      Learning iteration 845/50000                      

                       Computation: 117140 steps/s (collection: 0.659s, learning 0.180s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.84s
                        Total time: 726.36s
                               ETA: 703 mins 23.9 s

################################################################################
                      Learning iteration 846/50000                      

                       Computation: 120254 steps/s (collection: 0.644s, learning 0.174s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.82s
                        Total time: 727.18s
                               ETA: 703 mins 20.6 s

################################################################################
                      Learning iteration 847/50000                      

                       Computation: 122025 steps/s (collection: 0.621s, learning 0.185s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.81s
                        Total time: 727.99s
                               ETA: 703 mins 16.7 s

################################################################################
                      Learning iteration 848/50000                      

                       Computation: 121394 steps/s (collection: 0.628s, learning 0.182s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0177
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.81s
                        Total time: 728.80s
                               ETA: 703 mins 13.0 s

################################################################################
                      Learning iteration 849/50000                      

                       Computation: 116678 steps/s (collection: 0.658s, learning 0.184s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.84s
                        Total time: 729.64s
                               ETA: 703 mins 11.2 s

################################################################################
                      Learning iteration 850/50000                      

                       Computation: 117116 steps/s (collection: 0.654s, learning 0.185s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.84s
                        Total time: 730.48s
                               ETA: 703 mins 9.3 s

################################################################################
                      Learning iteration 851/50000                      

                       Computation: 116635 steps/s (collection: 0.663s, learning 0.180s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.84s
                        Total time: 731.32s
                               ETA: 703 mins 7.5 s

################################################################################
                      Learning iteration 852/50000                      

                       Computation: 125706 steps/s (collection: 0.608s, learning 0.174s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.78s
                        Total time: 732.10s
                               ETA: 703 mins 2.3 s

################################################################################
                      Learning iteration 853/50000                      

                       Computation: 124623 steps/s (collection: 0.616s, learning 0.173s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0188
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.79s
                        Total time: 732.89s
                               ETA: 702 mins 57.4 s

################################################################################
                      Learning iteration 854/50000                      

                       Computation: 130496 steps/s (collection: 0.581s, learning 0.172s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.75s
                        Total time: 733.65s
                               ETA: 702 mins 50.5 s

################################################################################
                      Learning iteration 855/50000                      

                       Computation: 119978 steps/s (collection: 0.631s, learning 0.189s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0650
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.82s
                        Total time: 734.47s
                               ETA: 702 mins 47.4 s

################################################################################
                      Learning iteration 856/50000                      

                       Computation: 120745 steps/s (collection: 0.622s, learning 0.192s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0175
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0637
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.81s
                        Total time: 735.28s
                               ETA: 702 mins 44.1 s

################################################################################
                      Learning iteration 857/50000                      

                       Computation: 110130 steps/s (collection: 0.713s, learning 0.180s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0181
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0654
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.89s
                        Total time: 736.17s
                               ETA: 702 mins 45.2 s

################################################################################
                      Learning iteration 858/50000                      

                       Computation: 122204 steps/s (collection: 0.630s, learning 0.174s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.80s
                        Total time: 736.98s
                               ETA: 702 mins 41.3 s

################################################################################
                      Learning iteration 859/50000                      

                       Computation: 129647 steps/s (collection: 0.585s, learning 0.174s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0092
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0667
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.76s
                        Total time: 737.74s
                               ETA: 702 mins 34.7 s

################################################################################
                      Learning iteration 860/50000                      

                       Computation: 111550 steps/s (collection: 0.699s, learning 0.182s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.88s
                        Total time: 738.62s
                               ETA: 702 mins 35.2 s

################################################################################
                      Learning iteration 861/50000                      

                       Computation: 114071 steps/s (collection: 0.669s, learning 0.193s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.86s
                        Total time: 739.48s
                               ETA: 702 mins 34.6 s

################################################################################
                      Learning iteration 862/50000                      

                       Computation: 121513 steps/s (collection: 0.630s, learning 0.179s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0664
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.81s
                        Total time: 740.29s
                               ETA: 702 mins 30.9 s

################################################################################
                      Learning iteration 863/50000                      

                       Computation: 120997 steps/s (collection: 0.631s, learning 0.181s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.81s
                        Total time: 741.10s
                               ETA: 702 mins 27.5 s

################################################################################
                      Learning iteration 864/50000                      

                       Computation: 123524 steps/s (collection: 0.604s, learning 0.192s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0181
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0643
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.80s
                        Total time: 741.90s
                               ETA: 702 mins 23.1 s

################################################################################
                      Learning iteration 865/50000                      

                       Computation: 108296 steps/s (collection: 0.713s, learning 0.194s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0676
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.91s
                        Total time: 742.80s
                               ETA: 702 mins 25.1 s

################################################################################
                      Learning iteration 866/50000                      

                       Computation: 125774 steps/s (collection: 0.603s, learning 0.178s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0641
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.78s
                        Total time: 743.58s
                               ETA: 702 mins 19.9 s

################################################################################
                      Learning iteration 867/50000                      

                       Computation: 119267 steps/s (collection: 0.651s, learning 0.173s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0663
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.82s
                        Total time: 744.41s
                               ETA: 702 mins 17.2 s

################################################################################
                      Learning iteration 868/50000                      

                       Computation: 128434 steps/s (collection: 0.588s, learning 0.178s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.77s
                        Total time: 745.17s
                               ETA: 702 mins 11.1 s

################################################################################
                      Learning iteration 869/50000                      

                       Computation: 130029 steps/s (collection: 0.583s, learning 0.173s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0672
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.76s
                        Total time: 745.93s
                               ETA: 702 mins 4.5 s

################################################################################
                      Learning iteration 870/50000                      

                       Computation: 125016 steps/s (collection: 0.614s, learning 0.173s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0168
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.79s
                        Total time: 746.72s
                               ETA: 701 mins 59.6 s

################################################################################
                      Learning iteration 871/50000                      

                       Computation: 123353 steps/s (collection: 0.623s, learning 0.174s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.80s
                        Total time: 747.51s
                               ETA: 701 mins 55.4 s

################################################################################
                      Learning iteration 872/50000                      

                       Computation: 123482 steps/s (collection: 0.621s, learning 0.175s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0092
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.80s
                        Total time: 748.31s
                               ETA: 701 mins 51.1 s

################################################################################
                      Learning iteration 873/50000                      

                       Computation: 126080 steps/s (collection: 0.593s, learning 0.186s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0181
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0637
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.78s
                        Total time: 749.09s
                               ETA: 701 mins 45.9 s

################################################################################
                      Learning iteration 874/50000                      

                       Computation: 111573 steps/s (collection: 0.696s, learning 0.185s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0650
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.88s
                        Total time: 749.97s
                               ETA: 701 mins 46.4 s

################################################################################
                      Learning iteration 875/50000                      

                       Computation: 129410 steps/s (collection: 0.587s, learning 0.173s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0645
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.76s
                        Total time: 750.73s
                               ETA: 701 mins 40.0 s

################################################################################
                      Learning iteration 876/50000                      

                       Computation: 123152 steps/s (collection: 0.621s, learning 0.177s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.80s
                        Total time: 751.53s
                               ETA: 701 mins 35.9 s

################################################################################
                      Learning iteration 877/50000                      

                       Computation: 113990 steps/s (collection: 0.671s, learning 0.191s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.86s
                        Total time: 752.39s
                               ETA: 701 mins 35.3 s

################################################################################
                      Learning iteration 878/50000                      

                       Computation: 130087 steps/s (collection: 0.584s, learning 0.172s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.76s
                        Total time: 753.15s
                               ETA: 701 mins 28.8 s

################################################################################
                      Learning iteration 879/50000                      

                       Computation: 119330 steps/s (collection: 0.628s, learning 0.196s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0643
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.82s
                        Total time: 753.97s
                               ETA: 701 mins 26.1 s

################################################################################
                      Learning iteration 880/50000                      

                       Computation: 114690 steps/s (collection: 0.655s, learning 0.202s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0654
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.86s
                        Total time: 754.83s
                               ETA: 701 mins 25.3 s

################################################################################
                      Learning iteration 881/50000                      

                       Computation: 109480 steps/s (collection: 0.721s, learning 0.177s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0178
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.90s
                        Total time: 755.73s
                               ETA: 701 mins 26.7 s

################################################################################
                      Learning iteration 882/50000                      

                       Computation: 117122 steps/s (collection: 0.666s, learning 0.174s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0657
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.84s
                        Total time: 756.56s
                               ETA: 701 mins 24.9 s

################################################################################
                      Learning iteration 883/50000                      

                       Computation: 124156 steps/s (collection: 0.618s, learning 0.174s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0646
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.79s
                        Total time: 757.36s
                               ETA: 701 mins 20.4 s

################################################################################
                      Learning iteration 884/50000                      

                       Computation: 122993 steps/s (collection: 0.625s, learning 0.174s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0674
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.80s
                        Total time: 758.16s
                               ETA: 701 mins 16.4 s

################################################################################
                      Learning iteration 885/50000                      

                       Computation: 120691 steps/s (collection: 0.630s, learning 0.185s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.81s
                        Total time: 758.97s
                               ETA: 701 mins 13.2 s

################################################################################
                      Learning iteration 886/50000                      

                       Computation: 115761 steps/s (collection: 0.665s, learning 0.185s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0188
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0657
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.85s
                        Total time: 759.82s
                               ETA: 701 mins 11.9 s

################################################################################
                      Learning iteration 887/50000                      

                       Computation: 130794 steps/s (collection: 0.579s, learning 0.173s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0662
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.75s
                        Total time: 760.57s
                               ETA: 701 mins 5.2 s

################################################################################
                      Learning iteration 888/50000                      

                       Computation: 126855 steps/s (collection: 0.603s, learning 0.172s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0651
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.77s
                        Total time: 761.35s
                               ETA: 700 mins 59.9 s

################################################################################
                      Learning iteration 889/50000                      

                       Computation: 117139 steps/s (collection: 0.655s, learning 0.184s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0655
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.84s
                        Total time: 762.19s
                               ETA: 700 mins 58.1 s

################################################################################
                      Learning iteration 890/50000                      

                       Computation: 131225 steps/s (collection: 0.578s, learning 0.171s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0662
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.75s
                        Total time: 762.93s
                               ETA: 700 mins 51.3 s

################################################################################
                      Learning iteration 891/50000                      

                       Computation: 130574 steps/s (collection: 0.579s, learning 0.174s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0188
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.75s
                        Total time: 763.69s
                               ETA: 700 mins 44.7 s

################################################################################
                      Learning iteration 892/50000                      

                       Computation: 124974 steps/s (collection: 0.612s, learning 0.175s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.79s
                        Total time: 764.47s
                               ETA: 700 mins 40.1 s

################################################################################
                      Learning iteration 893/50000                      

                       Computation: 120084 steps/s (collection: 0.627s, learning 0.191s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0124
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0171
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0643
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.82s
                        Total time: 765.29s
                               ETA: 700 mins 37.2 s

################################################################################
                      Learning iteration 894/50000                      

                       Computation: 130362 steps/s (collection: 0.582s, learning 0.172s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0662
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.75s
                        Total time: 766.05s
                               ETA: 700 mins 30.7 s

################################################################################
                      Learning iteration 895/50000                      

                       Computation: 115923 steps/s (collection: 0.671s, learning 0.177s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0175
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.85s
                        Total time: 766.89s
                               ETA: 700 mins 29.4 s

################################################################################
                      Learning iteration 896/50000                      

                       Computation: 127992 steps/s (collection: 0.595s, learning 0.173s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0122
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0179
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.77s
                        Total time: 767.66s
                               ETA: 700 mins 23.7 s

################################################################################
                      Learning iteration 897/50000                      

                       Computation: 129160 steps/s (collection: 0.588s, learning 0.174s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.76s
                        Total time: 768.42s
                               ETA: 700 mins 17.7 s

################################################################################
                      Learning iteration 898/50000                      

                       Computation: 118527 steps/s (collection: 0.654s, learning 0.175s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0185
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.83s
                        Total time: 769.25s
                               ETA: 700 mins 15.4 s

################################################################################
                      Learning iteration 899/50000                      

                       Computation: 117606 steps/s (collection: 0.649s, learning 0.187s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.84s
                        Total time: 770.09s
                               ETA: 700 mins 13.5 s

################################################################################
                      Learning iteration 900/50000                      

                       Computation: 116998 steps/s (collection: 0.650s, learning 0.191s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.84s
                        Total time: 770.93s
                               ETA: 700 mins 11.8 s

################################################################################
                      Learning iteration 901/50000                      

                       Computation: 121818 steps/s (collection: 0.630s, learning 0.177s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0123
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0179
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.81s
                        Total time: 771.74s
                               ETA: 700 mins 8.3 s

################################################################################
                      Learning iteration 902/50000                      

                       Computation: 127870 steps/s (collection: 0.595s, learning 0.174s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0123
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0175
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0664
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.77s
                        Total time: 772.50s
                               ETA: 700 mins 2.7 s

################################################################################
                      Learning iteration 903/50000                      

                       Computation: 125379 steps/s (collection: 0.605s, learning 0.180s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0124
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.78s
                        Total time: 773.29s
                               ETA: 699 mins 58.0 s

################################################################################
                      Learning iteration 904/50000                      

                       Computation: 115782 steps/s (collection: 0.654s, learning 0.195s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0171
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.85s
                        Total time: 774.14s
                               ETA: 699 mins 56.8 s

################################################################################
                      Learning iteration 905/50000                      

                       Computation: 108658 steps/s (collection: 0.713s, learning 0.192s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0122
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.90s
                        Total time: 775.04s
                               ETA: 699 mins 58.6 s

################################################################################
                      Learning iteration 906/50000                      

                       Computation: 109594 steps/s (collection: 0.687s, learning 0.210s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0124
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0175
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.90s
                        Total time: 775.94s
                               ETA: 699 mins 60.0 s

################################################################################
                      Learning iteration 907/50000                      

                       Computation: 122608 steps/s (collection: 0.610s, learning 0.191s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0122
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0178
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.80s
                        Total time: 776.74s
                               ETA: 699 mins 56.2 s

################################################################################
                      Learning iteration 908/50000                      

                       Computation: 127189 steps/s (collection: 0.574s, learning 0.199s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.77s
                        Total time: 777.51s
                               ETA: 699 mins 50.9 s

################################################################################
                      Learning iteration 909/50000                      

                       Computation: 112860 steps/s (collection: 0.680s, learning 0.191s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.87s
                        Total time: 778.39s
                               ETA: 699 mins 50.9 s

################################################################################
                      Learning iteration 910/50000                      

                       Computation: 126551 steps/s (collection: 0.586s, learning 0.191s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.78s
                        Total time: 779.16s
                               ETA: 699 mins 45.8 s

################################################################################
                      Learning iteration 911/50000                      

                       Computation: 127727 steps/s (collection: 0.578s, learning 0.191s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0160
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.77s
                        Total time: 779.93s
                               ETA: 699 mins 40.3 s

################################################################################
                      Learning iteration 912/50000                      

                       Computation: 120357 steps/s (collection: 0.618s, learning 0.198s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0695
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.82s
                        Total time: 780.75s
                               ETA: 699 mins 37.4 s

################################################################################
                      Learning iteration 913/50000                      

                       Computation: 114100 steps/s (collection: 0.653s, learning 0.209s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.98
                Mean reward (task): 0.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.86s
                        Total time: 781.61s
                               ETA: 699 mins 36.9 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/ydgdln68vv7ww3obheo68
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    