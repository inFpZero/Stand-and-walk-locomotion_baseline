swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_125451-4ec781cf
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yh_gym_Jun01_12-54-51 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/j3ij3qrmvowfu1wm3skzd
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.22 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 25500 steps/s (collection: 3.676s, learning 0.179s)
               Value function loss: 3.2652
                    Surrogate loss: 0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0166
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0007
       Mean episode rew_smoothness: -0.0061
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1359
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 1.6333
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 3.85s
                        Total time: 3.85s
                               ETA: 3212 mins 28.6 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 151716 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 3.7621
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0219
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0586
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 1.0161
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.65s
                        Total time: 4.50s
                               ETA: 1876 mins 10.6 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 141720 steps/s (collection: 0.557s, learning 0.136s)
               Value function loss: 3.1690
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0198
       Mean episode rew_ang_vel_xy: -0.0220
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.5610
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.69s
                        Total time: 5.20s
                               ETA: 1443 mins 25.9 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 139815 steps/s (collection: 0.576s, learning 0.127s)
               Value function loss: 2.6286
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0201
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0597
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.2829
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.70s
                        Total time: 5.90s
                               ETA: 1229 mins 1.3 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 142980 steps/s (collection: 0.564s, learning 0.124s)
               Value function loss: 2.0252
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0203
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0577
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.1381
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.69s
                        Total time: 6.59s
                               ETA: 1097 mins 46.7 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 129410 steps/s (collection: 0.636s, learning 0.123s)
               Value function loss: 1.6599
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0205
       Mean episode rew_ang_vel_xy: -0.0218
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0601
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0683
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.76s
                        Total time: 7.35s
                               ETA: 1020 mins 17.4 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 131908 steps/s (collection: 0.619s, learning 0.126s)
               Value function loss: 1.1777
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0209
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0590
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.75s
                        Total time: 8.09s
                               ETA: 963 mins 13.5 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 138378 steps/s (collection: 0.588s, learning 0.122s)
               Value function loss: 0.8434
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0611
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.71s
                        Total time: 8.80s
                               ETA: 916 mins 47.7 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 125718 steps/s (collection: 0.656s, learning 0.126s)
               Value function loss: 0.6877
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0593
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.78s
                        Total time: 9.58s
                               ETA: 887 mins 18.2 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 140180 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.4316
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0591
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0127
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.70s
                        Total time: 10.29s
                               ETA: 856 mins 59.1 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 121621 steps/s (collection: 0.669s, learning 0.139s)
               Value function loss: 0.3365
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0589
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.81s
                        Total time: 11.09s
                               ETA: 840 mins 16.9 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 126857 steps/s (collection: 0.648s, learning 0.127s)
               Value function loss: 0.2169
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0219
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0605
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.77s
                        Total time: 11.87s
                               ETA: 824 mins 2.7 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 140967 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.1003
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0429
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0603
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0132
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.70s
                        Total time: 12.57s
                               ETA: 805 mins 20.0 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 141863 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0865
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0430
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0583
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0133
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.69s
                        Total time: 13.26s
                               ETA: 789 mins 1.8 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 141748 steps/s (collection: 0.571s, learning 0.123s)
               Value function loss: 0.0445
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0228
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0587
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0135
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.69s
                        Total time: 13.95s
                               ETA: 774 mins 55.9 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 138112 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0567
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.71s
                        Total time: 14.66s
                               ETA: 763 mins 32.6 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 136094 steps/s (collection: 0.599s, learning 0.123s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0591
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.72s
                        Total time: 15.39s
                               ETA: 754 mins 0.7 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 138728 steps/s (collection: 0.588s, learning 0.121s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0239
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0582
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0141
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.71s
                        Total time: 16.10s
                               ETA: 744 mins 54.1 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 134132 steps/s (collection: 0.611s, learning 0.122s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0244
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0144
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.73s
                        Total time: 16.83s
                               ETA: 737 mins 48.9 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 137619 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0475
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0589
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0147
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.71s
                        Total time: 17.54s
                               ETA: 730 mins 39.8 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 143465 steps/s (collection: 0.563s, learning 0.122s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0255
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0478
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0151
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.69s
                        Total time: 18.23s
                               ETA: 723 mins 2.1 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 126990 steps/s (collection: 0.651s, learning 0.123s)
               Value function loss: 0.0144
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0481
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0546
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0154
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.77s
                        Total time: 19.00s
                               ETA: 719 mins 27.9 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 128776 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0265
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0482
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0547
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0158
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.76s
                        Total time: 19.77s
                               ETA: 715 mins 49.0 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 123998 steps/s (collection: 0.669s, learning 0.124s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0191
          Mean episode rew_dof_acc: -0.0487
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0539
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.79s
                        Total time: 20.56s
                               ETA: 713 mins 29.5 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 138872 steps/s (collection: 0.586s, learning 0.122s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0504
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0543
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0163
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.71s
                        Total time: 21.27s
                               ETA: 708 mins 31.4 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 131719 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0499
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0521
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0167
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.75s
                        Total time: 22.01s
                               ETA: 705 mins 10.0 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 141140 steps/s (collection: 0.566s, learning 0.130s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0512
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0534
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0170
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.70s
                        Total time: 22.71s
                               ETA: 700 mins 31.3 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 136670 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0526
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0546
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0174
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.72s
                        Total time: 23.43s
                               ETA: 696 mins 53.1 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 134097 steps/s (collection: 0.611s, learning 0.122s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0538
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0543
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0177
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.73s
                        Total time: 24.16s
                               ETA: 693 mins 53.7 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 142698 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0302
       Mean episode rew_ang_vel_xy: -0.0191
          Mean episode rew_dof_acc: -0.0531
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.69s
                        Total time: 24.85s
                               ETA: 689 mins 52.6 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 139522 steps/s (collection: 0.577s, learning 0.127s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0530
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0543
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0181
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.70s
                        Total time: 25.55s
                               ETA: 686 mins 32.3 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 142612 steps/s (collection: 0.568s, learning 0.122s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0307
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0546
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0517
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0183
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.69s
                        Total time: 26.24s
                               ETA: 683 mins 0.6 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 136942 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0315
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0546
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0504
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0188
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.72s
                        Total time: 26.96s
                               ETA: 680 mins 24.9 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 125721 steps/s (collection: 0.659s, learning 0.123s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0548
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0189
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.78s
                        Total time: 27.74s
                               ETA: 679 mins 32.5 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 140245 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0180
          Mean episode rew_dof_acc: -0.0542
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0193
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.70s
                        Total time: 28.44s
                               ETA: 676 mins 47.4 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 134067 steps/s (collection: 0.611s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0187
          Mean episode rew_dof_acc: -0.0556
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0196
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.73s
                        Total time: 29.18s
                               ETA: 674 mins 56.3 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 133158 steps/s (collection: 0.615s, learning 0.124s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0334
       Mean episode rew_ang_vel_xy: -0.0185
          Mean episode rew_dof_acc: -0.0558
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0475
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0200
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.74s
                        Total time: 29.92s
                               ETA: 673 mins 18.0 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 138207 steps/s (collection: 0.589s, learning 0.122s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0337
       Mean episode rew_ang_vel_xy: -0.0183
          Mean episode rew_dof_acc: -0.0557
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0486
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0201
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.71s
                        Total time: 30.63s
                               ETA: 671 mins 9.3 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 144926 steps/s (collection: 0.556s, learning 0.122s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0338
       Mean episode rew_ang_vel_xy: -0.0182
          Mean episode rew_dof_acc: -0.0564
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0500
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0203
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.68s
                        Total time: 31.31s
                               ETA: 668 mins 24.9 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 127794 steps/s (collection: 0.629s, learning 0.140s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0348
       Mean episode rew_ang_vel_xy: -0.0182
          Mean episode rew_dof_acc: -0.0567
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0490
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0208
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.77s
                        Total time: 32.07s
                               ETA: 667 mins 42.3 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 130138 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0353
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0581
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0496
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0210
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.76s
                        Total time: 32.83s
                               ETA: 666 mins 44.8 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 144487 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0356
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0590
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0212
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.68s
                        Total time: 33.51s
                               ETA: 664 mins 20.9 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 132222 steps/s (collection: 0.622s, learning 0.121s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0364
       Mean episode rew_ang_vel_xy: -0.0187
          Mean episode rew_dof_acc: -0.0582
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0476
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0216
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.74s
                        Total time: 34.25s
                               ETA: 663 mins 16.9 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 144763 steps/s (collection: 0.557s, learning 0.122s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0365
       Mean episode rew_ang_vel_xy: -0.0187
          Mean episode rew_dof_acc: -0.0582
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0490
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0218
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.68s
                        Total time: 34.93s
                               ETA: 661 mins 2.6 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 136180 steps/s (collection: 0.600s, learning 0.122s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0372
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0585
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0492
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0222
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.72s
                        Total time: 35.66s
                               ETA: 659 mins 41.8 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 137162 steps/s (collection: 0.594s, learning 0.122s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0375
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0592
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0496
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0223
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.72s
                        Total time: 36.37s
                               ETA: 658 mins 18.9 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 119101 steps/s (collection: 0.695s, learning 0.130s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0384
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0600
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0479
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0229
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.83s
                        Total time: 37.20s
                               ETA: 658 mins 55.0 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 133047 steps/s (collection: 0.615s, learning 0.124s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0390
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0611
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0477
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0232
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.74s
                        Total time: 37.94s
                               ETA: 657 mins 59.5 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 141179 steps/s (collection: 0.574s, learning 0.122s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0391
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0600
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0233
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.70s
                        Total time: 38.63s
                               ETA: 656 mins 22.8 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 143714 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0396
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0610
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0491
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0236
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.68s
                        Total time: 39.32s
                               ETA: 654 mins 37.7 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 135268 steps/s (collection: 0.604s, learning 0.122s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0403
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0610
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0500
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0239
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.73s
                        Total time: 40.04s
                               ETA: 653 mins 38.6 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 130948 steps/s (collection: 0.628s, learning 0.122s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0409
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0620
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0242
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.75s
                        Total time: 40.79s
                               ETA: 653 mins 4.7 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 143673 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0415
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0629
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0245
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.68s
                        Total time: 41.48s
                               ETA: 651 mins 29.4 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 138076 steps/s (collection: 0.578s, learning 0.134s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0421
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0618
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0250
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.71s
                        Total time: 42.19s
                               ETA: 650 mins 23.3 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 129208 steps/s (collection: 0.639s, learning 0.122s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0429
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0638
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0527
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0254
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.76s
                        Total time: 42.95s
                               ETA: 650 mins 3.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 127736 steps/s (collection: 0.641s, learning 0.129s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0433
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0647
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0254
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.77s
                        Total time: 43.72s
                               ETA: 649 mins 53.0 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 125259 steps/s (collection: 0.659s, learning 0.125s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0442
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0642
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0259
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.78s
                        Total time: 44.51s
                               ETA: 649 mins 55.8 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 137257 steps/s (collection: 0.594s, learning 0.123s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0445
       Mean episode rew_ang_vel_xy: -0.0207
          Mean episode rew_dof_acc: -0.0649
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0531
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0260
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.72s
                        Total time: 45.22s
                               ETA: 648 mins 59.4 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 138654 steps/s (collection: 0.584s, learning 0.125s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0450
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0644
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0498
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0263
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.71s
                        Total time: 45.93s
                               ETA: 647 mins 58.8 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 137526 steps/s (collection: 0.593s, learning 0.122s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0459
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0651
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0503
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0268
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.71s
                        Total time: 46.65s
                               ETA: 647 mins 5.0 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 144184 steps/s (collection: 0.558s, learning 0.124s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0461
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0656
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0268
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.68s
                        Total time: 47.33s
                               ETA: 645 mins 46.0 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 132084 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0472
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0652
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0276
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.74s
                        Total time: 48.07s
                               ETA: 645 mins 19.7 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 136498 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0475
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0658
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0534
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0278
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.72s
                        Total time: 48.79s
                               ETA: 644 mins 35.2 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 123708 steps/s (collection: 0.673s, learning 0.122s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0481
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0664
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0281
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.79s
                        Total time: 49.59s
                               ETA: 644 mins 50.2 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 135683 steps/s (collection: 0.600s, learning 0.125s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0481
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0661
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0520
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0282
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.72s
                        Total time: 50.31s
                               ETA: 644 mins 10.8 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 143257 steps/s (collection: 0.563s, learning 0.124s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0491
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0670
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0287
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.69s
                        Total time: 51.00s
                               ETA: 643 mins 3.6 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 133244 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0500
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0690
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0293
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.74s
                        Total time: 51.73s
                               ETA: 642 mins 36.8 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 140248 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0504
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0677
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0295
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.70s
                        Total time: 52.44s
                               ETA: 641 mins 43.7 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 137543 steps/s (collection: 0.593s, learning 0.122s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0510
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0685
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0298
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.71s
                        Total time: 53.15s
                               ETA: 641 mins 2.2 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 143555 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0520
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0699
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0303
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.68s
                        Total time: 53.83s
                               ETA: 640 mins 0.4 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 124481 steps/s (collection: 0.649s, learning 0.140s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0520
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0688
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0528
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0304
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.79s
                        Total time: 54.62s
                               ETA: 640 mins 14.1 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 131237 steps/s (collection: 0.628s, learning 0.121s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0529
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0687
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0539
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0309
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.75s
                        Total time: 55.37s
                               ETA: 639 mins 59.3 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 138336 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0533
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0699
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0310
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.71s
                        Total time: 56.08s
                               ETA: 639 mins 18.5 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 143464 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0533
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0693
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0311
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.69s
                        Total time: 56.77s
                               ETA: 638 mins 21.7 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 137343 steps/s (collection: 0.594s, learning 0.122s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0547
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0698
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0318
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.72s
                        Total time: 57.49s
                               ETA: 637 mins 46.7 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 138070 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0551
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0703
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0559
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0320
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.71s
                        Total time: 58.20s
                               ETA: 637 mins 10.2 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 135132 steps/s (collection: 0.606s, learning 0.121s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0559
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0707
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0553
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0324
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.73s
                        Total time: 58.92s
                               ETA: 636 mins 44.6 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 125270 steps/s (collection: 0.637s, learning 0.148s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0563
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0709
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0575
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0326
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.78s
                        Total time: 59.71s
                               ETA: 636 mins 56.3 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 134287 steps/s (collection: 0.608s, learning 0.124s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0574
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0728
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0571
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0331
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.73s
                        Total time: 60.44s
                               ETA: 636 mins 34.4 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 136418 steps/s (collection: 0.598s, learning 0.123s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0582
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0715
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0562
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0337
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.72s
                        Total time: 61.16s
                               ETA: 636 mins 5.9 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 137636 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0581
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0719
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0574
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0335
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.71s
                        Total time: 61.88s
                               ETA: 635 mins 34.1 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 131249 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0592
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0729
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0548
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0343
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.75s
                        Total time: 62.63s
                               ETA: 635 mins 24.3 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 141806 steps/s (collection: 0.572s, learning 0.121s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0593
       Mean episode rew_ang_vel_xy: -0.0218
          Mean episode rew_dof_acc: -0.0716
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0577
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0342
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.69s
                        Total time: 63.32s
                               ETA: 634 mins 41.1 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 142894 steps/s (collection: 0.566s, learning 0.122s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0602
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0721
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0592
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0347
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.69s
                        Total time: 64.01s
                               ETA: 633 mins 55.8 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 142534 steps/s (collection: 0.565s, learning 0.125s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0609
       Mean episode rew_ang_vel_xy: -0.0224
          Mean episode rew_dof_acc: -0.0741
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0603
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0353
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.69s
                        Total time: 64.70s
                               ETA: 633 mins 12.6 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 135227 steps/s (collection: 0.603s, learning 0.124s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0616
       Mean episode rew_ang_vel_xy: -0.0228
          Mean episode rew_dof_acc: -0.0751
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0616
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0355
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.73s
                        Total time: 65.42s
                               ETA: 632 mins 52.0 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 134176 steps/s (collection: 0.609s, learning 0.124s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0622
       Mean episode rew_ang_vel_xy: -0.0220
          Mean episode rew_dof_acc: -0.0737
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0574
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0359
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.73s
                        Total time: 66.16s
                               ETA: 632 mins 35.1 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 143821 steps/s (collection: 0.562s, learning 0.121s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0632
       Mean episode rew_ang_vel_xy: -0.0221
          Mean episode rew_dof_acc: -0.0749
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0565
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0363
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.68s
                        Total time: 66.84s
                               ETA: 631 mins 50.8 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 142744 steps/s (collection: 0.568s, learning 0.121s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0643
       Mean episode rew_ang_vel_xy: -0.0226
          Mean episode rew_dof_acc: -0.0760
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0586
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0369
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.69s
                        Total time: 67.53s
                               ETA: 631 mins 10.2 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 134394 steps/s (collection: 0.609s, learning 0.122s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0642
       Mean episode rew_ang_vel_xy: -0.0221
          Mean episode rew_dof_acc: -0.0747
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0591
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0368
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.73s
                        Total time: 68.26s
                               ETA: 630 mins 54.4 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 140243 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0649
       Mean episode rew_ang_vel_xy: -0.0232
          Mean episode rew_dof_acc: -0.0762
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0602
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0371
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.70s
                        Total time: 68.96s
                               ETA: 630 mins 22.1 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 135128 steps/s (collection: 0.580s, learning 0.147s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0652
       Mean episode rew_ang_vel_xy: -0.0229
          Mean episode rew_dof_acc: -0.0775
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0622
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0373
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.73s
                        Total time: 69.69s
                               ETA: 630 mins 4.9 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 143647 steps/s (collection: 0.563s, learning 0.121s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0659
       Mean episode rew_ang_vel_xy: -0.0223
          Mean episode rew_dof_acc: -0.0756
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0609
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0379
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.68s
                        Total time: 70.37s
                               ETA: 629 mins 24.9 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 140252 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0663
       Mean episode rew_ang_vel_xy: -0.0230
          Mean episode rew_dof_acc: -0.0777
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0612
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0383
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.70s
                        Total time: 71.07s
                               ETA: 628 mins 54.5 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 139502 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0672
       Mean episode rew_ang_vel_xy: -0.0226
          Mean episode rew_dof_acc: -0.0768
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0626
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0388
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.70s
                        Total time: 71.78s
                               ETA: 628 mins 26.7 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 140154 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0678
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0772
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0389
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.70s
                        Total time: 72.48s
                               ETA: 627 mins 57.8 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 134610 steps/s (collection: 0.609s, learning 0.122s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0694
       Mean episode rew_ang_vel_xy: -0.0227
          Mean episode rew_dof_acc: -0.0779
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0591
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0400
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.73s
                        Total time: 73.21s
                               ETA: 627 mins 44.3 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 139526 steps/s (collection: 0.584s, learning 0.121s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0702
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0784
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0633
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0403
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.70s
                        Total time: 73.91s
                               ETA: 627 mins 18.0 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 142076 steps/s (collection: 0.564s, learning 0.128s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0704
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0779
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0651
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0403
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.69s
                        Total time: 74.61s
                               ETA: 626 mins 45.9 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 122903 steps/s (collection: 0.667s, learning 0.133s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0712
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0800
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0658
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0409
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.80s
                        Total time: 75.41s
                               ETA: 627 mins 8.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 142506 steps/s (collection: 0.568s, learning 0.122s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0721
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0789
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0632
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0413
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.69s
                        Total time: 76.10s
                               ETA: 626 mins 35.7 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 127674 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0728
       Mean episode rew_ang_vel_xy: -0.0233
          Mean episode rew_dof_acc: -0.0804
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0625
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0417
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.77s
                        Total time: 76.87s
                               ETA: 626 mins 43.0 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 133286 steps/s (collection: 0.605s, learning 0.133s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0730
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0809
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0639
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0418
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.74s
                        Total time: 77.60s
                               ETA: 626 mins 34.5 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 137593 steps/s (collection: 0.590s, learning 0.125s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0737
       Mean episode rew_ang_vel_xy: -0.0235
          Mean episode rew_dof_acc: -0.0811
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0624
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0424
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.71s
                        Total time: 78.32s
                               ETA: 626 mins 15.1 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 139386 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0752
       Mean episode rew_ang_vel_xy: -0.0234
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0433
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.71s
                        Total time: 79.02s
                               ETA: 625 mins 51.6 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 129828 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0740
       Mean episode rew_ang_vel_xy: -0.0238
          Mean episode rew_dof_acc: -0.0796
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0424
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.76s
                        Total time: 79.78s
                               ETA: 625 mins 53.0 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 139942 steps/s (collection: 0.576s, learning 0.127s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0766
       Mean episode rew_ang_vel_xy: -0.0238
          Mean episode rew_dof_acc: -0.0819
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0638
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0441
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.70s
                        Total time: 80.48s
                               ETA: 625 mins 28.9 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 144440 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0841
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0667
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0445
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.68s
                        Total time: 81.16s
                               ETA: 624 mins 55.0 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 134790 steps/s (collection: 0.606s, learning 0.123s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0764
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0835
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0728
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0438
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.73s
                        Total time: 81.89s
                               ETA: 624 mins 44.1 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 138879 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0784
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0826
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0634
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0450
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.71s
                        Total time: 82.60s
                               ETA: 624 mins 23.7 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 15527 steps/s (collection: 6.208s, learning 0.123s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0803
       Mean episode rew_ang_vel_xy: -0.0238
          Mean episode rew_dof_acc: -0.0843
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0649
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0464
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 6.33s
                        Total time: 88.93s
                               ETA: 666 mins 10.9 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 72521 steps/s (collection: 1.233s, learning 0.122s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0805
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0839
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0649
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0464
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.36s
                        Total time: 90.29s
                               ETA: 670 mins 17.0 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 130361 steps/s (collection: 0.631s, learning 0.123s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0795
       Mean episode rew_ang_vel_xy: -0.0239
          Mean episode rew_dof_acc: -0.0836
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0680
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0457
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.75s
                        Total time: 91.04s
                               ETA: 669 mins 53.2 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 132071 steps/s (collection: 0.618s, learning 0.127s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0846
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0653
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0477
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.74s
                        Total time: 91.78s
                               ETA: 669 mins 25.6 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 143435 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0814
       Mean episode rew_ang_vel_xy: -0.0238
          Mean episode rew_dof_acc: -0.0835
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0651
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0470
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.69s
                        Total time: 92.47s
                               ETA: 668 mins 32.8 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 138277 steps/s (collection: 0.590s, learning 0.121s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0829
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0840
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0643
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0480
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.71s
                        Total time: 93.18s
                               ETA: 667 mins 51.9 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 141067 steps/s (collection: 0.571s, learning 0.125s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0835
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0836
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0677
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0484
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.70s
                        Total time: 93.88s
                               ETA: 667 mins 5.7 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 148622 steps/s (collection: 0.538s, learning 0.123s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0848
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0861
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0653
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0491
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.66s
                        Total time: 94.54s
                               ETA: 666 mins 5.4 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 130508 steps/s (collection: 0.610s, learning 0.144s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0853
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0857
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0673
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0493
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.75s
                        Total time: 95.29s
                               ETA: 665 mins 44.5 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 137067 steps/s (collection: 0.593s, learning 0.124s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0864
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0741
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0502
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.72s
                        Total time: 96.01s
                               ETA: 665 mins 8.9 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 142318 steps/s (collection: 0.568s, learning 0.122s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0873
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0867
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0653
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0506
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.69s
                        Total time: 96.70s
                               ETA: 664 mins 23.0 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 144208 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0872
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0877
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0642
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0507
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.68s
                        Total time: 97.38s
                               ETA: 663 mins 34.2 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 144960 steps/s (collection: 0.554s, learning 0.124s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0884
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0878
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0688
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0509
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.68s
                        Total time: 98.06s
                               ETA: 662 mins 44.7 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 143800 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0899
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0870
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0675
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0520
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.68s
                        Total time: 98.74s
                               ETA: 661 mins 58.2 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 137934 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0898
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0867
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0681
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0519
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.71s
                        Total time: 99.46s
                               ETA: 661 mins 24.0 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 131807 steps/s (collection: 0.607s, learning 0.139s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0901
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0870
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0701
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0523
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.75s
                        Total time: 100.20s
                               ETA: 661 mins 3.5 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 135070 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0914
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0881
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0674
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0529
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.73s
                        Total time: 100.93s
                               ETA: 660 mins 36.2 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 136083 steps/s (collection: 0.596s, learning 0.127s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0910
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0883
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0674
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0527
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.72s
                        Total time: 101.65s
                               ETA: 660 mins 7.2 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 139793 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0926
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0890
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0695
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0532
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.70s
                        Total time: 102.36s
                               ETA: 659 mins 31.3 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 129156 steps/s (collection: 0.633s, learning 0.128s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0928
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0886
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0660
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0533
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.76s
                        Total time: 103.12s
                               ETA: 659 mins 18.1 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 142093 steps/s (collection: 0.569s, learning 0.122s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0953
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0925
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0707
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0549
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.69s
                        Total time: 103.81s
                               ETA: 658 mins 38.7 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 133101 steps/s (collection: 0.589s, learning 0.150s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0949
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0897
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0714
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0544
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.74s
                        Total time: 104.55s
                               ETA: 658 mins 17.6 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 138563 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0951
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0915
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0722
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0547
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.71s
                        Total time: 105.26s
                               ETA: 657 mins 45.8 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 142974 steps/s (collection: 0.564s, learning 0.124s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0955
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0916
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0720
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0549
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.69s
                        Total time: 105.94s
                               ETA: 657 mins 6.4 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 140717 steps/s (collection: 0.577s, learning 0.122s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0983
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0924
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0732
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0562
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.70s
                        Total time: 106.64s
                               ETA: 656 mins 31.6 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 141650 steps/s (collection: 0.571s, learning 0.123s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0991
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0911
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0682
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0564
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.69s
                        Total time: 107.34s
                               ETA: 655 mins 55.6 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 145136 steps/s (collection: 0.554s, learning 0.123s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0982
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0913
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0705
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0564
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.68s
                        Total time: 108.01s
                               ETA: 655 mins 14.1 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 142999 steps/s (collection: 0.557s, learning 0.130s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1002
       Mean episode rew_ang_vel_xy: -0.0260
          Mean episode rew_dof_acc: -0.0918
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0718
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0574
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.69s
                        Total time: 108.70s
                               ETA: 654 mins 36.8 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 146071 steps/s (collection: 0.549s, learning 0.124s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0999
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0926
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0715
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0570
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.67s
                        Total time: 109.37s
                               ETA: 653 mins 54.9 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 144018 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1007
       Mean episode rew_ang_vel_xy: -0.0261
          Mean episode rew_dof_acc: -0.0918
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0736
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0575
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.68s
                        Total time: 110.06s
                               ETA: 653 mins 16.9 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 126184 steps/s (collection: 0.628s, learning 0.151s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1028
       Mean episode rew_ang_vel_xy: -0.0263
          Mean episode rew_dof_acc: -0.0931
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0701
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0586
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.78s
                        Total time: 110.84s
                               ETA: 653 mins 13.6 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 143968 steps/s (collection: 0.560s, learning 0.123s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1031
       Mean episode rew_ang_vel_xy: -0.0264
          Mean episode rew_dof_acc: -0.0937
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0787
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0586
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.68s
                        Total time: 111.52s
                               ETA: 652 mins 36.6 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 144722 steps/s (collection: 0.556s, learning 0.123s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1036
       Mean episode rew_ang_vel_xy: -0.0266
          Mean episode rew_dof_acc: -0.0927
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0757
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0590
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.68s
                        Total time: 112.20s
                               ETA: 651 mins 58.8 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 141430 steps/s (collection: 0.572s, learning 0.124s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1048
       Mean episode rew_ang_vel_xy: -0.0272
          Mean episode rew_dof_acc: -0.0957
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0805
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0597
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.70s
                        Total time: 112.89s
                               ETA: 651 mins 27.0 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 149070 steps/s (collection: 0.537s, learning 0.123s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1058
       Mean episode rew_ang_vel_xy: -0.0271
          Mean episode rew_dof_acc: -0.0937
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0751
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0601
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.66s
                        Total time: 113.55s
                               ETA: 650 mins 43.4 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 127663 steps/s (collection: 0.641s, learning 0.129s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1077
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.0974
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0808
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0609
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.77s
                        Total time: 114.32s
                               ETA: 650 mins 38.2 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 147539 steps/s (collection: 0.544s, learning 0.122s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1093
       Mean episode rew_ang_vel_xy: -0.0273
          Mean episode rew_dof_acc: -0.0954
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0735
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0619
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.67s
                        Total time: 114.99s
                               ETA: 649 mins 57.8 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 135951 steps/s (collection: 0.588s, learning 0.135s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1095
       Mean episode rew_ang_vel_xy: -0.0276
          Mean episode rew_dof_acc: -0.0960
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0760
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0619
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.72s
                        Total time: 115.71s
                               ETA: 649 mins 37.1 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 140407 steps/s (collection: 0.578s, learning 0.122s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1112
       Mean episode rew_ang_vel_xy: -0.0270
          Mean episode rew_dof_acc: -0.0974
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0770
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0625
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.70s
                        Total time: 116.41s
                               ETA: 649 mins 9.0 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 137152 steps/s (collection: 0.594s, learning 0.123s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1118
       Mean episode rew_ang_vel_xy: -0.0273
          Mean episode rew_dof_acc: -0.0967
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0776
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0628
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.72s
                        Total time: 117.13s
                               ETA: 648 mins 46.7 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 137004 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1137
       Mean episode rew_ang_vel_xy: -0.0271
          Mean episode rew_dof_acc: -0.0969
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0740
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0637
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.72s
                        Total time: 117.85s
                               ETA: 648 mins 25.1 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 146950 steps/s (collection: 0.547s, learning 0.122s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1131
       Mean episode rew_ang_vel_xy: -0.0273
          Mean episode rew_dof_acc: -0.0984
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0760
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0638
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.67s
                        Total time: 118.52s
                               ETA: 647 mins 47.7 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 143288 steps/s (collection: 0.563s, learning 0.124s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1153
       Mean episode rew_ang_vel_xy: -0.0279
          Mean episode rew_dof_acc: -0.0987
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0801
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0647
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.69s
                        Total time: 119.20s
                               ETA: 647 mins 16.4 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 147255 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1159
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.0977
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0796
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0655
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.67s
                        Total time: 119.87s
                               ETA: 646 mins 39.5 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 144351 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1166
       Mean episode rew_ang_vel_xy: -0.0283
          Mean episode rew_dof_acc: -0.0984
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0800
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0655
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.68s
                        Total time: 120.55s
                               ETA: 646 mins 7.5 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 130632 steps/s (collection: 0.625s, learning 0.127s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1177
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.0996
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0811
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0661
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.75s
                        Total time: 121.30s
                               ETA: 645 mins 58.6 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 147513 steps/s (collection: 0.544s, learning 0.122s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1181
       Mean episode rew_ang_vel_xy: -0.0272
          Mean episode rew_dof_acc: -0.0989
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0803
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0663
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.67s
                        Total time: 121.97s
                               ETA: 645 mins 22.5 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 138200 steps/s (collection: 0.581s, learning 0.130s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1217
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.1016
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0792
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0686
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.71s
                        Total time: 122.68s
                               ETA: 645 mins 1.1 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 139966 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1198
       Mean episode rew_ang_vel_xy: -0.0280
          Mean episode rew_dof_acc: -0.1019
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0856
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0670
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.70s
                        Total time: 123.38s
                               ETA: 644 mins 37.1 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 140193 steps/s (collection: 0.568s, learning 0.134s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1248
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.1024
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0818
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0704
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.70s
                        Total time: 124.08s
                               ETA: 644 mins 13.0 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 144436 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1251
       Mean episode rew_ang_vel_xy: -0.0281
          Mean episode rew_dof_acc: -0.1019
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0806
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0704
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.68s
                        Total time: 124.76s
                               ETA: 643 mins 42.8 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 133906 steps/s (collection: 0.604s, learning 0.131s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1252
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.1015
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0841
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0706
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.73s
                        Total time: 125.50s
                               ETA: 643 mins 29.5 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 147007 steps/s (collection: 0.546s, learning 0.122s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1251
       Mean episode rew_ang_vel_xy: -0.0282
          Mean episode rew_dof_acc: -0.1023
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0808
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0706
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.67s
                        Total time: 126.17s
                               ETA: 642 mins 56.3 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 136977 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1275
       Mean episode rew_ang_vel_xy: -0.0287
          Mean episode rew_dof_acc: -0.1028
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0810
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0717
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.72s
                        Total time: 126.89s
                               ETA: 642 mins 38.4 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 133478 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1277
       Mean episode rew_ang_vel_xy: -0.0283
          Mean episode rew_dof_acc: -0.1041
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0855
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0715
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.74s
                        Total time: 127.62s
                               ETA: 642 mins 26.4 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 146905 steps/s (collection: 0.545s, learning 0.124s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1300
       Mean episode rew_ang_vel_xy: -0.0281
          Mean episode rew_dof_acc: -0.1033
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0817
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0726
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.67s
                        Total time: 128.29s
                               ETA: 641 mins 54.3 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 136263 steps/s (collection: 0.587s, learning 0.135s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1310
       Mean episode rew_ang_vel_xy: -0.0282
          Mean episode rew_dof_acc: -0.1041
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0817
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0734
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.72s
                        Total time: 129.01s
                               ETA: 641 mins 38.2 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 135702 steps/s (collection: 0.587s, learning 0.138s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1345
       Mean episode rew_ang_vel_xy: -0.0283
          Mean episode rew_dof_acc: -0.1045
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0784
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0750
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.72s
                        Total time: 129.74s
                               ETA: 641 mins 23.2 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 127243 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1314
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1064
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0897
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0731
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.77s
                        Total time: 130.51s
                               ETA: 641 mins 22.5 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 148237 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1324
       Mean episode rew_ang_vel_xy: -0.0286
          Mean episode rew_dof_acc: -0.1035
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0833
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0743
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.66s
                        Total time: 131.17s
                               ETA: 640 mins 49.7 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 119439 steps/s (collection: 0.689s, learning 0.134s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1350
       Mean episode rew_ang_vel_xy: -0.0284
          Mean episode rew_dof_acc: -0.1040
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0848
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0752
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.82s
                        Total time: 132.00s
                               ETA: 641 mins 4.0 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 148811 steps/s (collection: 0.538s, learning 0.123s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1359
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.1074
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0841
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0761
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.66s
                        Total time: 132.66s
                               ETA: 640 mins 30.9 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 144737 steps/s (collection: 0.557s, learning 0.122s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1401
       Mean episode rew_ang_vel_xy: -0.0293
          Mean episode rew_dof_acc: -0.1058
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0811
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0782
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.68s
                        Total time: 133.34s
                               ETA: 640 mins 3.7 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 141392 steps/s (collection: 0.573s, learning 0.123s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1384
       Mean episode rew_ang_vel_xy: -0.0291
          Mean episode rew_dof_acc: -0.1054
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0829
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0772
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.70s
                        Total time: 134.03s
                               ETA: 639 mins 41.3 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 142532 steps/s (collection: 0.566s, learning 0.124s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1396
       Mean episode rew_ang_vel_xy: -0.0292
          Mean episode rew_dof_acc: -0.1062
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0793
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0780
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.69s
                        Total time: 134.72s
                               ETA: 639 mins 17.6 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 150951 steps/s (collection: 0.529s, learning 0.122s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1419
       Mean episode rew_ang_vel_xy: -0.0291
          Mean episode rew_dof_acc: -0.1074
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0797
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0795
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.65s
                        Total time: 135.37s
                               ETA: 638 mins 43.2 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 129603 steps/s (collection: 0.628s, learning 0.130s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1403
       Mean episode rew_ang_vel_xy: -0.0289
          Mean episode rew_dof_acc: -0.1065
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0843
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0791
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.76s
                        Total time: 136.13s
                               ETA: 638 mins 39.4 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 138654 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1421
       Mean episode rew_ang_vel_xy: -0.0297
          Mean episode rew_dof_acc: -0.1071
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0845
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0795
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.71s
                        Total time: 136.84s
                               ETA: 638 mins 21.8 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 143183 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1417
       Mean episode rew_ang_vel_xy: -0.0286
          Mean episode rew_dof_acc: -0.1047
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0785
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0793
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.69s
                        Total time: 137.53s
                               ETA: 637 mins 58.2 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 133595 steps/s (collection: 0.614s, learning 0.122s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1439
       Mean episode rew_ang_vel_xy: -0.0295
          Mean episode rew_dof_acc: -0.1061
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0840
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0804
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.74s
                        Total time: 138.26s
                               ETA: 637 mins 48.4 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 138900 steps/s (collection: 0.586s, learning 0.122s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1456
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1076
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0861
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0819
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.71s
                        Total time: 138.97s
                               ETA: 637 mins 31.1 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 134226 steps/s (collection: 0.600s, learning 0.132s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1450
       Mean episode rew_ang_vel_xy: -0.0288
          Mean episode rew_dof_acc: -0.1075
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0820
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0809
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.73s
                        Total time: 139.70s
                               ETA: 637 mins 20.6 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 137573 steps/s (collection: 0.592s, learning 0.122s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1472
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.1063
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0812
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0824
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.71s
                        Total time: 140.42s
                               ETA: 637 mins 5.4 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 139417 steps/s (collection: 0.568s, learning 0.137s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1491
       Mean episode rew_ang_vel_xy: -0.0296
          Mean episode rew_dof_acc: -0.1091
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0830
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0842
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.71s
                        Total time: 141.12s
                               ETA: 636 mins 47.8 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 138108 steps/s (collection: 0.590s, learning 0.122s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1491
       Mean episode rew_ang_vel_xy: -0.0303
          Mean episode rew_dof_acc: -0.1108
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0851
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0835
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.71s
                        Total time: 141.83s
                               ETA: 636 mins 32.2 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 124374 steps/s (collection: 0.659s, learning 0.132s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1488
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1066
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0813
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0832
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.79s
                        Total time: 142.62s
                               ETA: 636 mins 37.7 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 139168 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1510
       Mean episode rew_ang_vel_xy: -0.0293
          Mean episode rew_dof_acc: -0.1078
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0860
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0846
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.71s
                        Total time: 143.33s
                               ETA: 636 mins 20.9 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 130157 steps/s (collection: 0.634s, learning 0.122s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1528
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.1096
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0868
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0861
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.76s
                        Total time: 144.08s
                               ETA: 636 mins 17.1 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 142720 steps/s (collection: 0.564s, learning 0.125s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1542
       Mean episode rew_ang_vel_xy: -0.0291
          Mean episode rew_dof_acc: -0.1092
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0829
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0870
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.69s
                        Total time: 144.77s
                               ETA: 635 mins 55.9 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 124211 steps/s (collection: 0.670s, learning 0.122s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1584
       Mean episode rew_ang_vel_xy: -0.0303
          Mean episode rew_dof_acc: -0.1127
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0850
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0892
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.79s
                        Total time: 145.57s
                               ETA: 636 mins 1.8 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 141262 steps/s (collection: 0.562s, learning 0.134s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1556
       Mean episode rew_ang_vel_xy: -0.0299
          Mean episode rew_dof_acc: -0.1103
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0881
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0877
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.70s
                        Total time: 146.26s
                               ETA: 635 mins 42.7 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 139761 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1563
       Mean episode rew_ang_vel_xy: -0.0292
          Mean episode rew_dof_acc: -0.1113
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0875
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0882
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.70s
                        Total time: 146.96s
                               ETA: 635 mins 25.8 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 148233 steps/s (collection: 0.538s, learning 0.125s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1591
       Mean episode rew_ang_vel_xy: -0.0293
          Mean episode rew_dof_acc: -0.1103
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0845
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0897
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.66s
                        Total time: 147.63s
                               ETA: 634 mins 58.6 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 132225 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1597
       Mean episode rew_ang_vel_xy: -0.0300
          Mean episode rew_dof_acc: -0.1109
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0886
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0913
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.74s
                        Total time: 148.37s
                               ETA: 634 mins 52.4 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 131719 steps/s (collection: 0.614s, learning 0.132s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1604
       Mean episode rew_ang_vel_xy: -0.0295
          Mean episode rew_dof_acc: -0.1106
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0888
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0911
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.75s
                        Total time: 149.12s
                               ETA: 634 mins 46.9 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 144537 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1647
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.1127
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0855
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0936
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.68s
                        Total time: 149.80s
                               ETA: 634 mins 24.6 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 139455 steps/s (collection: 0.584s, learning 0.121s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1637
       Mean episode rew_ang_vel_xy: -0.0297
          Mean episode rew_dof_acc: -0.1143
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0914
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0926
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.70s
                        Total time: 150.50s
                               ETA: 634 mins 8.8 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 148572 steps/s (collection: 0.538s, learning 0.123s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1666
       Mean episode rew_ang_vel_xy: -0.0309
          Mean episode rew_dof_acc: -0.1139
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0917
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0943
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.66s
                        Total time: 151.16s
                               ETA: 633 mins 42.3 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 145137 steps/s (collection: 0.556s, learning 0.121s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1671
       Mean episode rew_ang_vel_xy: -0.0303
          Mean episode rew_dof_acc: -0.1118
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0866
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0948
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.68s
                        Total time: 151.84s
                               ETA: 633 mins 20.0 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 144939 steps/s (collection: 0.557s, learning 0.122s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1685
       Mean episode rew_ang_vel_xy: -0.0312
          Mean episode rew_dof_acc: -0.1147
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0891
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0956
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.68s
                        Total time: 152.52s
                               ETA: 632 mins 58.1 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 121145 steps/s (collection: 0.676s, learning 0.136s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1730
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1171
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0949
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0981
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.81s
                        Total time: 153.33s
                               ETA: 633 mins 9.5 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 129715 steps/s (collection: 0.635s, learning 0.122s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1699
       Mean episode rew_ang_vel_xy: -0.0307
          Mean episode rew_dof_acc: -0.1145
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0875
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0966
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.76s
                        Total time: 154.09s
                               ETA: 633 mins 7.5 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 138945 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1727
       Mean episode rew_ang_vel_xy: -0.0309
          Mean episode rew_dof_acc: -0.1150
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0951
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0981
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.71s
                        Total time: 154.80s
                               ETA: 632 mins 53.2 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 143283 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1702
       Mean episode rew_ang_vel_xy: -0.0305
          Mean episode rew_dof_acc: -0.1158
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0868
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0975
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.69s
                        Total time: 155.48s
                               ETA: 632 mins 33.7 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 140155 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1745
       Mean episode rew_ang_vel_xy: -0.0306
          Mean episode rew_dof_acc: -0.1165
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0914
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0999
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.70s
                        Total time: 156.18s
                               ETA: 632 mins 18.2 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 139163 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1748
       Mean episode rew_ang_vel_xy: -0.0303
          Mean episode rew_dof_acc: -0.1162
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0883
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.1002
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.71s
                        Total time: 156.89s
                               ETA: 632 mins 4.0 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 147199 steps/s (collection: 0.546s, learning 0.122s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1757
       Mean episode rew_ang_vel_xy: -0.0303
          Mean episode rew_dof_acc: -0.1151
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0915
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.1002
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.67s
                        Total time: 157.56s
                               ETA: 631 mins 40.7 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 144622 steps/s (collection: 0.556s, learning 0.124s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1792
       Mean episode rew_ang_vel_xy: -0.0313
          Mean episode rew_dof_acc: -0.1165
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0903
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.1022
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.68s
                        Total time: 158.24s
                               ETA: 631 mins 20.5 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 142737 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1792
       Mean episode rew_ang_vel_xy: -0.0309
          Mean episode rew_dof_acc: -0.1181
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0922
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.1022
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.69s
                        Total time: 158.93s
                               ETA: 631 mins 2.5 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 135089 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1812
       Mean episode rew_ang_vel_xy: -0.0304
          Mean episode rew_dof_acc: -0.1168
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0891
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1042
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.73s
                        Total time: 159.65s
                               ETA: 630 mins 54.0 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 145558 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1839
       Mean episode rew_ang_vel_xy: -0.0315
          Mean episode rew_dof_acc: -0.1204
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0938
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1052
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.68s
                        Total time: 160.33s
                               ETA: 630 mins 33.2 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 144759 steps/s (collection: 0.555s, learning 0.124s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1811
       Mean episode rew_ang_vel_xy: -0.0300
          Mean episode rew_dof_acc: -0.1166
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0902
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1044
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.68s
                        Total time: 161.01s
                               ETA: 630 mins 13.5 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 135419 steps/s (collection: 0.601s, learning 0.125s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1838
       Mean episode rew_ang_vel_xy: -0.0304
          Mean episode rew_dof_acc: -0.1174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0924
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1049
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.73s
                        Total time: 161.73s
                               ETA: 630 mins 4.9 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 140361 steps/s (collection: 0.576s, learning 0.124s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1887
       Mean episode rew_ang_vel_xy: -0.0309
          Mean episode rew_dof_acc: -0.1174
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0901
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1075
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.70s
                        Total time: 162.43s
                               ETA: 629 mins 50.4 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 144430 steps/s (collection: 0.560s, learning 0.121s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1848
       Mean episode rew_ang_vel_xy: -0.0317
          Mean episode rew_dof_acc: -0.1199
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0920
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.1058
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.68s
                        Total time: 163.12s
                               ETA: 629 mins 31.5 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 139336 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1877
       Mean episode rew_ang_vel_xy: -0.0318
          Mean episode rew_dof_acc: -0.1199
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0927
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1072
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.71s
                        Total time: 163.82s
                               ETA: 629 mins 18.5 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 137324 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1872
       Mean episode rew_ang_vel_xy: -0.0313
          Mean episode rew_dof_acc: -0.1193
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0966
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1072
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.72s
                        Total time: 164.54s
                               ETA: 629 mins 8.0 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 139264 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1906
       Mean episode rew_ang_vel_xy: -0.0315
          Mean episode rew_dof_acc: -0.1203
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0941
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1086
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.71s
                        Total time: 165.24s
                               ETA: 628 mins 55.3 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 137810 steps/s (collection: 0.590s, learning 0.124s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1881
       Mean episode rew_ang_vel_xy: -0.0314
          Mean episode rew_dof_acc: -0.1192
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0921
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1076
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.71s
                        Total time: 165.96s
                               ETA: 628 mins 44.3 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 143921 steps/s (collection: 0.559s, learning 0.124s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1927
       Mean episode rew_ang_vel_xy: -0.0319
          Mean episode rew_dof_acc: -0.1197
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.68s
                        Total time: 166.64s
                               ETA: 628 mins 26.7 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 134114 steps/s (collection: 0.610s, learning 0.123s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1964
       Mean episode rew_ang_vel_xy: -0.0321
          Mean episode rew_dof_acc: -0.1198
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0923
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1125
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.73s
                        Total time: 167.37s
                               ETA: 628 mins 20.4 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 144444 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1976
       Mean episode rew_ang_vel_xy: -0.0326
          Mean episode rew_dof_acc: -0.1218
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0991
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1129
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.68s
                        Total time: 168.05s
                               ETA: 628 mins 2.4 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 138374 steps/s (collection: 0.585s, learning 0.126s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1938
       Mean episode rew_ang_vel_xy: -0.0314
          Mean episode rew_dof_acc: -0.1198
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0866
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.71s
                        Total time: 168.76s
                               ETA: 627 mins 51.3 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 143022 steps/s (collection: 0.563s, learning 0.124s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1955
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1232
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0985
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1119
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.69s
                        Total time: 169.45s
                               ETA: 627 mins 35.1 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 139558 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1970
       Mean episode rew_ang_vel_xy: -0.0317
          Mean episode rew_dof_acc: -0.1201
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0904
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.1127
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.70s
                        Total time: 170.15s
                               ETA: 627 mins 22.8 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 143155 steps/s (collection: 0.561s, learning 0.125s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2006
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.1231
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0935
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1150
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.69s
                        Total time: 170.84s
                               ETA: 627 mins 6.7 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 137434 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2040
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.1222
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0929
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1172
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.72s
                        Total time: 171.56s
                               ETA: 626 mins 57.1 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 145021 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2056
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1252
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0951
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1188
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.68s
                        Total time: 172.23s
                               ETA: 626 mins 39.3 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 141472 steps/s (collection: 0.569s, learning 0.126s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2063
       Mean episode rew_ang_vel_xy: -0.0331
          Mean episode rew_dof_acc: -0.1246
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0984
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1189
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.69s
                        Total time: 172.93s
                               ETA: 626 mins 25.4 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 144926 steps/s (collection: 0.556s, learning 0.123s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2049
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.1255
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1000
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1178
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.68s
                        Total time: 173.61s
                               ETA: 626 mins 8.0 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 126897 steps/s (collection: 0.627s, learning 0.148s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2072
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.1236
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0931
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1186
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.77s
                        Total time: 174.38s
                               ETA: 626 mins 11.5 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 139401 steps/s (collection: 0.569s, learning 0.137s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2122
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.1268
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0932
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1218
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.71s
                        Total time: 175.09s
                               ETA: 626 mins 0.1 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 127473 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2061
       Mean episode rew_ang_vel_xy: -0.0320
          Mean episode rew_dof_acc: -0.1257
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0943
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1180
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.77s
                        Total time: 175.86s
                               ETA: 626 mins 2.9 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 139821 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2118
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.1246
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0951
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1224
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.70s
                        Total time: 176.56s
                               ETA: 625 mins 51.1 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 136838 steps/s (collection: 0.596s, learning 0.122s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2104
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.1234
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0955
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1203
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.72s
                        Total time: 177.28s
                               ETA: 625 mins 42.7 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 135251 steps/s (collection: 0.605s, learning 0.122s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2130
       Mean episode rew_ang_vel_xy: -0.0325
          Mean episode rew_dof_acc: -0.1282
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1215
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.73s
                        Total time: 178.01s
                               ETA: 625 mins 36.1 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 143615 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2153
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.1257
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0911
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1233
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.68s
                        Total time: 178.69s
                               ETA: 625 mins 20.7 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 139241 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2182
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1272
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0951
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1253
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.71s
                        Total time: 179.40s
                               ETA: 625 mins 10.0 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 129446 steps/s (collection: 0.628s, learning 0.132s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2159
       Mean episode rew_ang_vel_xy: -0.0329
          Mean episode rew_dof_acc: -0.1258
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0982
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1237
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.76s
                        Total time: 180.16s
                               ETA: 625 mins 10.4 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 132831 steps/s (collection: 0.611s, learning 0.129s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2204
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1268
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0967
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1263
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.74s
                        Total time: 180.90s
                               ETA: 625 mins 6.8 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 135825 steps/s (collection: 0.593s, learning 0.131s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2213
       Mean episode rew_ang_vel_xy: -0.0335
          Mean episode rew_dof_acc: -0.1297
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0987
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1259
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.72s
                        Total time: 181.62s
                               ETA: 624 mins 59.8 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 134399 steps/s (collection: 0.589s, learning 0.142s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2217
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.1272
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0966
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1270
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.73s
                        Total time: 182.35s
                               ETA: 624 mins 54.5 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 124132 steps/s (collection: 0.645s, learning 0.147s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2281
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.1293
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0980
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1302
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.79s
                        Total time: 183.14s
                               ETA: 625 mins 1.6 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 120142 steps/s (collection: 0.680s, learning 0.138s)
               Value function loss: 0.0442
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2263
       Mean episode rew_ang_vel_xy: -0.0337
          Mean episode rew_dof_acc: -0.1278
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0963
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1286
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.82s
                        Total time: 183.96s
                               ETA: 625 mins 14.0 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 120493 steps/s (collection: 0.676s, learning 0.140s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2272
       Mean episode rew_ang_vel_xy: -0.0337
          Mean episode rew_dof_acc: -0.1292
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0973
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.1295
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.82s
                        Total time: 184.78s
                               ETA: 625 mins 25.8 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 117518 steps/s (collection: 0.689s, learning 0.147s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2344
       Mean episode rew_ang_vel_xy: -0.0342
          Mean episode rew_dof_acc: -0.1291
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0995
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1337
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.84s
                        Total time: 185.61s
                               ETA: 625 mins 41.7 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 129782 steps/s (collection: 0.632s, learning 0.126s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2301
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.1280
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1310
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.76s
                        Total time: 186.37s
                               ETA: 625 mins 41.6 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 124801 steps/s (collection: 0.644s, learning 0.144s)
               Value function loss: 0.0452
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2296
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1276
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0967
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1317
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.79s
                        Total time: 187.16s
                               ETA: 625 mins 47.5 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 126593 steps/s (collection: 0.639s, learning 0.137s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2366
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.1305
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1009
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1354
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0187
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.78s
                        Total time: 187.94s
                               ETA: 625 mins 51.1 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 121014 steps/s (collection: 0.679s, learning 0.134s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2353
       Mean episode rew_ang_vel_xy: -0.0345
          Mean episode rew_dof_acc: -0.1309
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1007
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1342
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.81s
                        Total time: 188.75s
                               ETA: 626 mins 1.8 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 133544 steps/s (collection: 0.606s, learning 0.131s)
               Value function loss: 0.0442
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2394
       Mean episode rew_ang_vel_xy: -0.0342
          Mean episode rew_dof_acc: -0.1321
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1011
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1382
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.74s
                        Total time: 189.48s
                               ETA: 625 mins 57.3 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 133206 steps/s (collection: 0.607s, learning 0.131s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2396
       Mean episode rew_ang_vel_xy: -0.0342
          Mean episode rew_dof_acc: -0.1308
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0978
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1379
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.74s
                        Total time: 190.22s
                               ETA: 625 mins 53.2 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 144374 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2454
       Mean episode rew_ang_vel_xy: -0.0335
          Mean episode rew_dof_acc: -0.1327
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0994
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1408
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.68s
                        Total time: 190.90s
                               ETA: 625 mins 37.9 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 133296 steps/s (collection: 0.615s, learning 0.122s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2417
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.1342
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0989
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1394
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.74s
                        Total time: 191.64s
                               ETA: 625 mins 33.8 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 127406 steps/s (collection: 0.642s, learning 0.129s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2449
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.1330
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1030
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1417
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.77s
                        Total time: 192.41s
                               ETA: 625 mins 36.4 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 144525 steps/s (collection: 0.557s, learning 0.123s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2486
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.1350
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0981
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1428
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.68s
                        Total time: 193.09s
                               ETA: 625 mins 21.2 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 139375 steps/s (collection: 0.570s, learning 0.135s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2452
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.1309
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0983
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1407
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.71s
                        Total time: 193.80s
                               ETA: 625 mins 10.9 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 145410 steps/s (collection: 0.552s, learning 0.124s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2504
       Mean episode rew_ang_vel_xy: -0.0344
          Mean episode rew_dof_acc: -0.1338
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1447
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.68s
                        Total time: 194.47s
                               ETA: 624 mins 55.1 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 139632 steps/s (collection: 0.583s, learning 0.121s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2508
       Mean episode rew_ang_vel_xy: -0.0334
          Mean episode rew_dof_acc: -0.1340
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0973
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1439
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.70s
                        Total time: 195.18s
                               ETA: 624 mins 44.8 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 134728 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2517
       Mean episode rew_ang_vel_xy: -0.0345
          Mean episode rew_dof_acc: -0.1351
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0970
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1448
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.73s
                        Total time: 195.91s
                               ETA: 624 mins 39.5 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 130730 steps/s (collection: 0.629s, learning 0.123s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2565
       Mean episode rew_ang_vel_xy: -0.0345
          Mean episode rew_dof_acc: -0.1380
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0995
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1470
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.75s
                        Total time: 196.66s
                               ETA: 624 mins 38.4 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 141993 steps/s (collection: 0.570s, learning 0.122s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2612
       Mean episode rew_ang_vel_xy: -0.0352
          Mean episode rew_dof_acc: -0.1360
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0993
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1501
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.69s
                        Total time: 197.35s
                               ETA: 624 mins 26.1 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 133232 steps/s (collection: 0.616s, learning 0.122s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2571
       Mean episode rew_ang_vel_xy: -0.0344
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1013
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1474
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.74s
                        Total time: 198.09s
                               ETA: 624 mins 22.4 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 139304 steps/s (collection: 0.581s, learning 0.124s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2598
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.1375
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1007
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1495
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.71s
                        Total time: 198.80s
                               ETA: 624 mins 12.7 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 143385 steps/s (collection: 0.561s, learning 0.124s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2610
       Mean episode rew_ang_vel_xy: -0.0340
          Mean episode rew_dof_acc: -0.1343
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0962
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1502
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.69s
                        Total time: 199.48s
                               ETA: 623 mins 59.3 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 135645 steps/s (collection: 0.579s, learning 0.146s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2648
       Mean episode rew_ang_vel_xy: -0.0337
          Mean episode rew_dof_acc: -0.1348
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0980
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1518
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.72s
                        Total time: 200.21s
                               ETA: 623 mins 53.3 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 147572 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2725
       Mean episode rew_ang_vel_xy: -0.0345
          Mean episode rew_dof_acc: -0.1412
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0963
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1565
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0187
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.67s
                        Total time: 200.87s
                               ETA: 623 mins 36.4 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 138037 steps/s (collection: 0.588s, learning 0.124s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2666
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.1387
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1017
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1543
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.71s
                        Total time: 201.58s
                               ETA: 623 mins 28.2 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 137931 steps/s (collection: 0.587s, learning 0.125s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2750
       Mean episode rew_ang_vel_xy: -0.0342
          Mean episode rew_dof_acc: -0.1369
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0987
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1586
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.71s
                        Total time: 202.30s
                               ETA: 623 mins 20.2 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 136705 steps/s (collection: 0.597s, learning 0.122s)
               Value function loss: 0.0484
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2767
       Mean episode rew_ang_vel_xy: -0.0347
          Mean episode rew_dof_acc: -0.1364
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0976
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1598
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.72s
                        Total time: 203.02s
                               ETA: 623 mins 13.4 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 124186 steps/s (collection: 0.652s, learning 0.139s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2787
       Mean episode rew_ang_vel_xy: -0.0353
          Mean episode rew_dof_acc: -0.1412
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1008
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1605
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.79s
                        Total time: 203.81s
                               ETA: 623 mins 19.9 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 137887 steps/s (collection: 0.592s, learning 0.121s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2745
       Mean episode rew_ang_vel_xy: -0.0354
          Mean episode rew_dof_acc: -0.1384
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1577
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.71s
                        Total time: 204.52s
                               ETA: 623 mins 12.0 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 144791 steps/s (collection: 0.554s, learning 0.125s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2848
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.1404
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0993
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1655
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.68s
                        Total time: 205.20s
                               ETA: 622 mins 57.9 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 144079 steps/s (collection: 0.558s, learning 0.124s)
               Value function loss: 0.0517
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2771
       Mean episode rew_ang_vel_xy: -0.0352
          Mean episode rew_dof_acc: -0.1376
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1058
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1607
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.68s
                        Total time: 205.88s
                               ETA: 622 mins 44.6 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 139559 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2868
       Mean episode rew_ang_vel_xy: -0.0350
          Mean episode rew_dof_acc: -0.1407
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0971
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1671
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.70s
                        Total time: 206.59s
                               ETA: 622 mins 35.3 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 144473 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0519
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2803
       Mean episode rew_ang_vel_xy: -0.0351
          Mean episode rew_dof_acc: -0.1387
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0991
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1627
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.68s
                        Total time: 207.27s
                               ETA: 622 mins 21.8 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 133974 steps/s (collection: 0.610s, learning 0.124s)
               Value function loss: 0.0515
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2843
       Mean episode rew_ang_vel_xy: -0.0362
          Mean episode rew_dof_acc: -0.1407
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1080
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1658
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.73s
                        Total time: 208.00s
                               ETA: 622 mins 18.0 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 143881 steps/s (collection: 0.559s, learning 0.124s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2971
       Mean episode rew_ang_vel_xy: -0.0355
          Mean episode rew_dof_acc: -0.1435
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.0974
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1713
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.68s
                        Total time: 208.68s
                               ETA: 622 mins 5.1 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 143268 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2863
       Mean episode rew_ang_vel_xy: -0.0356
          Mean episode rew_dof_acc: -0.1410
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1031
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1682
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.69s
                        Total time: 209.37s
                               ETA: 621 mins 52.9 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 126769 steps/s (collection: 0.651s, learning 0.124s)
               Value function loss: 0.0516
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2890
       Mean episode rew_ang_vel_xy: -0.0351
          Mean episode rew_dof_acc: -0.1413
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1010
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1686
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0192
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.78s
                        Total time: 210.15s
                               ETA: 621 mins 56.6 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 125055 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2937
       Mean episode rew_ang_vel_xy: -0.0347
          Mean episode rew_dof_acc: -0.1405
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.0998
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1707
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.79s
                        Total time: 210.93s
                               ETA: 622 mins 2.1 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 142707 steps/s (collection: 0.564s, learning 0.125s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2935
       Mean episode rew_ang_vel_xy: -0.0354
          Mean episode rew_dof_acc: -0.1430
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0998
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1708
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.69s
                        Total time: 211.62s
                               ETA: 621 mins 50.5 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 134457 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2946
       Mean episode rew_ang_vel_xy: -0.0354
          Mean episode rew_dof_acc: -0.1422
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.0993
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1719
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.73s
                        Total time: 212.35s
                               ETA: 621 mins 46.3 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 144265 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2953
       Mean episode rew_ang_vel_xy: -0.0349
          Mean episode rew_dof_acc: -0.1402
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0977
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1715
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.68s
                        Total time: 213.03s
                               ETA: 621 mins 33.5 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 127423 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3035
       Mean episode rew_ang_vel_xy: -0.0360
          Mean episode rew_dof_acc: -0.1430
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1042
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1767
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.77s
                        Total time: 213.80s
                               ETA: 621 mins 36.5 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 139136 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3069
       Mean episode rew_ang_vel_xy: -0.0367
          Mean episode rew_dof_acc: -0.1445
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1019
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1796
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.71s
                        Total time: 214.51s
                               ETA: 621 mins 28.2 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 139613 steps/s (collection: 0.580s, learning 0.124s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2997
       Mean episode rew_ang_vel_xy: -0.0363
          Mean episode rew_dof_acc: -0.1441
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1034
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1745
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.70s
                        Total time: 215.22s
                               ETA: 621 mins 19.4 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 131110 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3112
       Mean episode rew_ang_vel_xy: -0.0370
          Mean episode rew_dof_acc: -0.1479
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1091
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1811
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.75s
                        Total time: 215.96s
                               ETA: 621 mins 18.7 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 138520 steps/s (collection: 0.583s, learning 0.127s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3003
       Mean episode rew_ang_vel_xy: -0.0358
          Mean episode rew_dof_acc: -0.1427
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1029
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1760
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.71s
                        Total time: 216.67s
                               ETA: 621 mins 11.0 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 136151 steps/s (collection: 0.598s, learning 0.124s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3093
       Mean episode rew_ang_vel_xy: -0.0365
          Mean episode rew_dof_acc: -0.1446
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1044
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1808
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0236
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.72s
                        Total time: 217.40s
                               ETA: 621 mins 5.5 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 124737 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3054
       Mean episode rew_ang_vel_xy: -0.0363
          Mean episode rew_dof_acc: -0.1446
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1033
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1785
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.79s
                        Total time: 218.18s
                               ETA: 621 mins 11.3 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 143753 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0542
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3116
       Mean episode rew_ang_vel_xy: -0.0367
          Mean episode rew_dof_acc: -0.1467
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1096
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1817
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.68s
                        Total time: 218.87s
                               ETA: 620 mins 59.4 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 126912 steps/s (collection: 0.630s, learning 0.144s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3183
       Mean episode rew_ang_vel_xy: -0.0361
          Mean episode rew_dof_acc: -0.1461
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.0985
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1872
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.77s
                        Total time: 219.64s
                               ETA: 621 mins 2.9 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 122761 steps/s (collection: 0.664s, learning 0.137s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3199
       Mean episode rew_ang_vel_xy: -0.0377
          Mean episode rew_dof_acc: -0.1489
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1061
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1873
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.80s
                        Total time: 220.44s
                               ETA: 621 mins 10.7 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 141248 steps/s (collection: 0.571s, learning 0.125s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3201
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.1463
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1092
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1874
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0281
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.70s
                        Total time: 221.14s
                               ETA: 621 mins 0.9 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 143311 steps/s (collection: 0.562s, learning 0.124s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3181
       Mean episode rew_ang_vel_xy: -0.0370
          Mean episode rew_dof_acc: -0.1470
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1056
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1862
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.69s
                        Total time: 221.83s
                               ETA: 620 mins 49.5 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 129422 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3184
       Mean episode rew_ang_vel_xy: -0.0362
          Mean episode rew_dof_acc: -0.1478
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1032
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1869
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.76s
                        Total time: 222.59s
                               ETA: 620 mins 50.4 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 143284 steps/s (collection: 0.561s, learning 0.125s)
               Value function loss: 0.0542
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3232
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1442
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1004
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1898
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0260
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.69s
                        Total time: 223.27s
                               ETA: 620 mins 39.1 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 136889 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0544
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3301
       Mean episode rew_ang_vel_xy: -0.0368
          Mean episode rew_dof_acc: -0.1482
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1056
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1928
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.72s
                        Total time: 223.99s
                               ETA: 620 mins 33.2 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 128926 steps/s (collection: 0.625s, learning 0.138s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3188
       Mean episode rew_ang_vel_xy: -0.0358
          Mean episode rew_dof_acc: -0.1429
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1035
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1862
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.76s
                        Total time: 224.75s
                               ETA: 620 mins 34.7 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 139048 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3318
       Mean episode rew_ang_vel_xy: -0.0364
          Mean episode rew_dof_acc: -0.1465
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1016
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1940
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.71s
                        Total time: 225.46s
                               ETA: 620 mins 26.9 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 140445 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3307
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1468
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1926
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.70s
                        Total time: 226.16s
                               ETA: 620 mins 18.1 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 141865 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3242
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1463
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1092
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1902
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.69s
                        Total time: 226.85s
                               ETA: 620 mins 8.2 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 127089 steps/s (collection: 0.642s, learning 0.132s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3301
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.1480
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1015
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1927
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.77s
                        Total time: 227.63s
                               ETA: 620 mins 11.5 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 142266 steps/s (collection: 0.567s, learning 0.124s)
               Value function loss: 0.0559
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3419
       Mean episode rew_ang_vel_xy: -0.0367
          Mean episode rew_dof_acc: -0.1478
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1050
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2026
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.69s
                        Total time: 228.32s
                               ETA: 620 mins 1.3 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 124077 steps/s (collection: 0.668s, learning 0.124s)
               Value function loss: 0.0572
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3329
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1479
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1028
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1953
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0226
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.79s
                        Total time: 229.11s
                               ETA: 620 mins 7.7 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 131087 steps/s (collection: 0.623s, learning 0.127s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3423
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1489
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1015
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2005
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.75s
                        Total time: 229.86s
                               ETA: 620 mins 7.1 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 142211 steps/s (collection: 0.568s, learning 0.123s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3479
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1496
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1039
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2038
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.69s
                        Total time: 230.55s
                               ETA: 619 mins 57.1 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 129226 steps/s (collection: 0.610s, learning 0.151s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3399
       Mean episode rew_ang_vel_xy: -0.0370
          Mean episode rew_dof_acc: -0.1484
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1039
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1990
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.76s
                        Total time: 231.31s
                               ETA: 619 mins 58.3 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 133904 steps/s (collection: 0.611s, learning 0.124s)
               Value function loss: 0.0556
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3453
       Mean episode rew_ang_vel_xy: -0.0373
          Mean episode rew_dof_acc: -0.1503
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2034
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.73s
                        Total time: 232.04s
                               ETA: 619 mins 55.3 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 134712 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3325
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.1461
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1018
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1949
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.73s
                        Total time: 232.77s
                               ETA: 619 mins 51.5 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 139398 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3440
       Mean episode rew_ang_vel_xy: -0.0370
          Mean episode rew_dof_acc: -0.1497
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1089
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2014
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.71s
                        Total time: 233.48s
                               ETA: 619 mins 43.9 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 125065 steps/s (collection: 0.653s, learning 0.133s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3492
       Mean episode rew_ang_vel_xy: -0.0376
          Mean episode rew_dof_acc: -0.1507
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1066
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2057
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.79s
                        Total time: 234.27s
                               ETA: 619 mins 49.1 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 130179 steps/s (collection: 0.631s, learning 0.124s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3440
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.1486
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1062
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2024
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.76s
                        Total time: 235.02s
                               ETA: 619 mins 49.4 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 133615 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3556
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1512
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1066
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2091
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0239
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.74s
                        Total time: 235.76s
                               ETA: 619 mins 46.6 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 136267 steps/s (collection: 0.599s, learning 0.122s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3573
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1532
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1106
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2092
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0247
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.72s
                        Total time: 236.48s
                               ETA: 619 mins 41.6 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 140497 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3539
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1508
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1056
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2081
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.70s
                        Total time: 237.18s
                               ETA: 619 mins 33.3 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 125227 steps/s (collection: 0.659s, learning 0.126s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3587
       Mean episode rew_ang_vel_xy: -0.0380
          Mean episode rew_dof_acc: -0.1534
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1079
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2105
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.79s
                        Total time: 237.96s
                               ETA: 619 mins 38.3 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 133523 steps/s (collection: 0.613s, learning 0.124s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3660
       Mean episode rew_ang_vel_xy: -0.0368
          Mean episode rew_dof_acc: -0.1530
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1096
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2138
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0256
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.74s
                        Total time: 238.70s
                               ETA: 619 mins 35.6 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 125169 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3660
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1557
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1028
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2117
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.79s
                        Total time: 239.48s
                               ETA: 619 mins 40.7 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 127292 steps/s (collection: 0.641s, learning 0.131s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3643
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1533
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1044
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2123
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0266
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.77s
                        Total time: 240.26s
                               ETA: 619 mins 43.6 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 139007 steps/s (collection: 0.583s, learning 0.124s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3662
       Mean episode rew_ang_vel_xy: -0.0385
          Mean episode rew_dof_acc: -0.1529
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1032
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2134
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.71s
                        Total time: 240.96s
                               ETA: 619 mins 36.5 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 130640 steps/s (collection: 0.630s, learning 0.122s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3724
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1548
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1049
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2169
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.75s
                        Total time: 241.72s
                               ETA: 619 mins 36.4 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 126120 steps/s (collection: 0.656s, learning 0.124s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3707
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1521
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1090
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2151
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.78s
                        Total time: 242.50s
                               ETA: 619 mins 40.4 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 122448 steps/s (collection: 0.673s, learning 0.130s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3717
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1515
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1103
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2162
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.80s
                        Total time: 243.30s
                               ETA: 619 mins 48.0 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 128661 steps/s (collection: 0.639s, learning 0.125s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3811
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1567
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1136
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2233
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0249
        Mean episode terrain_level: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.76s
                        Total time: 244.06s
                               ETA: 619 mins 49.6 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 130888 steps/s (collection: 0.629s, learning 0.122s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3791
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1564
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1094
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2208
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.75s
                        Total time: 244.81s
                               ETA: 619 mins 49.2 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 125092 steps/s (collection: 0.662s, learning 0.124s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3786
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1557
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1126
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2205
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.79s
                        Total time: 245.60s
                               ETA: 619 mins 54.1 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 137830 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0629
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3862
       Mean episode rew_ang_vel_xy: -0.0393
          Mean episode rew_dof_acc: -0.1559
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1104
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2254
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0278
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.71s
                        Total time: 246.31s
                               ETA: 619 mins 47.9 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 140221 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3859
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1524
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1065
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2243
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.70s
                        Total time: 247.01s
                               ETA: 619 mins 40.0 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 131853 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3930
       Mean episode rew_ang_vel_xy: -0.0402
          Mean episode rew_dof_acc: -0.1599
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1137
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2289
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.75s
                        Total time: 247.76s
                               ETA: 619 mins 38.8 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 132113 steps/s (collection: 0.609s, learning 0.135s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3839
       Mean episode rew_ang_vel_xy: -0.0381
          Mean episode rew_dof_acc: -0.1544
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1037
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2231
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.74s
                        Total time: 248.50s
                               ETA: 619 mins 37.4 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 134291 steps/s (collection: 0.599s, learning 0.133s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3965
       Mean episode rew_ang_vel_xy: -0.0386
          Mean episode rew_dof_acc: -0.1576
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1089
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2314
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.73s
                        Total time: 249.24s
                               ETA: 619 mins 34.2 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 124013 steps/s (collection: 0.672s, learning 0.121s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3884
       Mean episode rew_ang_vel_xy: -0.0392
          Mean episode rew_dof_acc: -0.1573
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1083
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2243
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.79s
                        Total time: 250.03s
                               ETA: 619 mins 40.0 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 137673 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3970
       Mean episode rew_ang_vel_xy: -0.0388
          Mean episode rew_dof_acc: -0.1555
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1054
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2303
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0293
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.71s
                        Total time: 250.74s
                               ETA: 619 mins 34.2 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 122341 steps/s (collection: 0.681s, learning 0.123s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3965
       Mean episode rew_ang_vel_xy: -0.0390
          Mean episode rew_dof_acc: -0.1565
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1119
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2301
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0276
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.80s
                        Total time: 251.55s
                               ETA: 619 mins 41.6 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 134981 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3972
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.1537
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1135
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2308
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0269
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.73s
                        Total time: 252.27s
                               ETA: 619 mins 37.8 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 140341 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4158
       Mean episode rew_ang_vel_xy: -0.0401
          Mean episode rew_dof_acc: -0.1611
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1139
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2418
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0272
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.70s
                        Total time: 252.97s
                               ETA: 619 mins 30.0 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 138853 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4085
       Mean episode rew_ang_vel_xy: -0.0393
          Mean episode rew_dof_acc: -0.1597
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1138
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2379
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.71s
                        Total time: 253.68s
                               ETA: 619 mins 23.3 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 139237 steps/s (collection: 0.584s, learning 0.122s)
               Value function loss: 0.0590
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4055
       Mean episode rew_ang_vel_xy: -0.0393
          Mean episode rew_dof_acc: -0.1565
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1130
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2340
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.71s
                        Total time: 254.39s
                               ETA: 619 mins 16.4 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 131005 steps/s (collection: 0.608s, learning 0.143s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4140
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1615
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1116
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2410
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0277
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.75s
                        Total time: 255.14s
                               ETA: 619 mins 16.0 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 136396 steps/s (collection: 0.596s, learning 0.125s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4146
       Mean episode rew_ang_vel_xy: -0.0392
          Mean episode rew_dof_acc: -0.1609
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1122
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2411
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0302
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.72s
                        Total time: 255.86s
                               ETA: 619 mins 11.2 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 140721 steps/s (collection: 0.575s, learning 0.123s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4148
       Mean episode rew_ang_vel_xy: -0.0403
          Mean episode rew_dof_acc: -0.1593
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1115
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2397
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.70s
                        Total time: 256.56s
                               ETA: 619 mins 3.3 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 126486 steps/s (collection: 0.648s, learning 0.129s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4221
       Mean episode rew_ang_vel_xy: -0.0403
          Mean episode rew_dof_acc: -0.1648
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1170
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2453
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.78s
                        Total time: 257.34s
                               ETA: 619 mins 6.8 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 137340 steps/s (collection: 0.586s, learning 0.129s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4226
       Mean episode rew_ang_vel_xy: -0.0401
          Mean episode rew_dof_acc: -0.1591
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1130
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2473
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0332
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.72s
                        Total time: 258.05s
                               ETA: 619 mins 1.4 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 134881 steps/s (collection: 0.595s, learning 0.134s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4154
       Mean episode rew_ang_vel_xy: -0.0400
          Mean episode rew_dof_acc: -0.1601
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1067
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2424
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0315
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.73s
                        Total time: 258.78s
                               ETA: 618 mins 57.9 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 141340 steps/s (collection: 0.573s, learning 0.123s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4193
       Mean episode rew_ang_vel_xy: -0.0403
          Mean episode rew_dof_acc: -0.1577
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1139
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2443
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.70s
                        Total time: 259.48s
                               ETA: 618 mins 49.6 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 131231 steps/s (collection: 0.627s, learning 0.122s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4107
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1574
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1111
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2393
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.75s
                        Total time: 260.22s
                               ETA: 618 mins 49.1 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 140935 steps/s (collection: 0.575s, learning 0.123s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4368
       Mean episode rew_ang_vel_xy: -0.0394
          Mean episode rew_dof_acc: -0.1629
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1123
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2550
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.70s
                        Total time: 260.92s
                               ETA: 618 mins 41.2 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 137539 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4206
       Mean episode rew_ang_vel_xy: -0.0398
          Mean episode rew_dof_acc: -0.1574
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1134
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2428
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.71s
                        Total time: 261.64s
                               ETA: 618 mins 35.7 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 133805 steps/s (collection: 0.611s, learning 0.124s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4275
       Mean episode rew_ang_vel_xy: -0.0397
          Mean episode rew_dof_acc: -0.1630
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1152
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2454
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.73s
                        Total time: 262.37s
                               ETA: 618 mins 33.2 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 126689 steps/s (collection: 0.645s, learning 0.131s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4291
       Mean episode rew_ang_vel_xy: -0.0398
          Mean episode rew_dof_acc: -0.1601
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1118
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2469
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0260
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.78s
                        Total time: 263.15s
                               ETA: 618 mins 36.4 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 124623 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4409
       Mean episode rew_ang_vel_xy: -0.0405
          Mean episode rew_dof_acc: -0.1626
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1146
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2553
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.79s
                        Total time: 263.94s
                               ETA: 618 mins 41.5 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 134664 steps/s (collection: 0.607s, learning 0.123s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4395
       Mean episode rew_ang_vel_xy: -0.0405
          Mean episode rew_dof_acc: -0.1632
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1174
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2542
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.73s
                        Total time: 264.67s
                               ETA: 618 mins 38.3 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 130620 steps/s (collection: 0.606s, learning 0.146s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4560
       Mean episode rew_ang_vel_xy: -0.0412
          Mean episode rew_dof_acc: -0.1642
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1152
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2659
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0265
        Mean episode terrain_level: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.75s
                        Total time: 265.42s
                               ETA: 618 mins 38.2 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 133719 steps/s (collection: 0.595s, learning 0.140s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4456
       Mean episode rew_ang_vel_xy: -0.0408
          Mean episode rew_dof_acc: -0.1648
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1093
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2579
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.74s
                        Total time: 266.15s
                               ETA: 618 mins 35.7 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 128006 steps/s (collection: 0.632s, learning 0.136s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4535
       Mean episode rew_ang_vel_xy: -0.0415
          Mean episode rew_dof_acc: -0.1623
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1163
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2650
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0289
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.77s
                        Total time: 266.92s
                               ETA: 618 mins 37.8 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 122800 steps/s (collection: 0.661s, learning 0.139s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4564
       Mean episode rew_ang_vel_xy: -0.0418
          Mean episode rew_dof_acc: -0.1663
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1165
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2643
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0295
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.80s
                        Total time: 267.72s
                               ETA: 618 mins 44.4 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 120097 steps/s (collection: 0.666s, learning 0.153s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4477
       Mean episode rew_ang_vel_xy: -0.0411
          Mean episode rew_dof_acc: -0.1637
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1118
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2585
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.82s
                        Total time: 268.54s
                               ETA: 618 mins 53.4 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 132324 steps/s (collection: 0.611s, learning 0.132s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4440
       Mean episode rew_ang_vel_xy: -0.0411
          Mean episode rew_dof_acc: -0.1616
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1172
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2587
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.74s
                        Total time: 269.28s
                               ETA: 618 mins 52.0 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 127093 steps/s (collection: 0.648s, learning 0.125s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4473
       Mean episode rew_ang_vel_xy: -0.0407
          Mean episode rew_dof_acc: -0.1624
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1175
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2602
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.77s
                        Total time: 270.06s
                               ETA: 618 mins 54.7 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 130455 steps/s (collection: 0.618s, learning 0.135s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4489
       Mean episode rew_ang_vel_xy: -0.0407
          Mean episode rew_dof_acc: -0.1683
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1165
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2607
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.75s
                        Total time: 270.81s
                               ETA: 618 mins 54.7 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 130279 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4568
       Mean episode rew_ang_vel_xy: -0.0427
          Mean episode rew_dof_acc: -0.1672
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1171
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2671
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0322
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.75s
                        Total time: 271.57s
                               ETA: 618 mins 54.8 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 138267 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4655
       Mean episode rew_ang_vel_xy: -0.0416
          Mean episode rew_dof_acc: -0.1684
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1159
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2739
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.71s
                        Total time: 272.28s
                               ETA: 618 mins 49.0 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 136849 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4733
       Mean episode rew_ang_vel_xy: -0.0424
          Mean episode rew_dof_acc: -0.1691
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1087
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2770
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.72s
                        Total time: 272.99s
                               ETA: 618 mins 44.2 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 134963 steps/s (collection: 0.592s, learning 0.136s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4663
       Mean episode rew_ang_vel_xy: -0.0417
          Mean episode rew_dof_acc: -0.1672
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1114
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2713
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.73s
                        Total time: 273.72s
                               ETA: 618 mins 40.8 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 116768 steps/s (collection: 0.712s, learning 0.130s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4499
       Mean episode rew_ang_vel_xy: -0.0420
          Mean episode rew_dof_acc: -0.1642
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1117
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2633
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0353
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.84s
                        Total time: 274.56s
                               ETA: 618 mins 52.8 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 140309 steps/s (collection: 0.576s, learning 0.124s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4722
       Mean episode rew_ang_vel_xy: -0.0421
          Mean episode rew_dof_acc: -0.1681
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1087
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2765
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.70s
                        Total time: 275.27s
                               ETA: 618 mins 45.7 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 139266 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4600
       Mean episode rew_ang_vel_xy: -0.0416
          Mean episode rew_dof_acc: -0.1674
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1133
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2689
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.71s
                        Total time: 275.97s
                               ETA: 618 mins 39.2 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 132388 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4800
       Mean episode rew_ang_vel_xy: -0.0422
          Mean episode rew_dof_acc: -0.1693
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1135
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2847
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0324
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.74s
                        Total time: 276.71s
                               ETA: 618 mins 37.8 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 122984 steps/s (collection: 0.677s, learning 0.122s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4811
       Mean episode rew_ang_vel_xy: -0.0423
          Mean episode rew_dof_acc: -0.1734
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1167
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2817
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.80s
                        Total time: 277.51s
                               ETA: 618 mins 43.9 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 138375 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4740
       Mean episode rew_ang_vel_xy: -0.0422
          Mean episode rew_dof_acc: -0.1660
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1124
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2785
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0295
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.71s
                        Total time: 278.22s
                               ETA: 618 mins 38.1 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 135764 steps/s (collection: 0.601s, learning 0.123s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4756
       Mean episode rew_ang_vel_xy: -0.0423
          Mean episode rew_dof_acc: -0.1677
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1102
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2760
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.72s
                        Total time: 278.95s
                               ETA: 618 mins 34.2 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 134766 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4800
       Mean episode rew_ang_vel_xy: -0.0408
          Mean episode rew_dof_acc: -0.1666
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1134
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2828
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.73s
                        Total time: 279.68s
                               ETA: 618 mins 31.0 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 138973 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4833
       Mean episode rew_ang_vel_xy: -0.0414
          Mean episode rew_dof_acc: -0.1668
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1120
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2829
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.71s
                        Total time: 280.38s
                               ETA: 618 mins 24.9 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 139661 steps/s (collection: 0.580s, learning 0.124s)
               Value function loss: 0.0653
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4617
       Mean episode rew_ang_vel_xy: -0.0410
          Mean episode rew_dof_acc: -0.1661
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1175
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2678
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0314
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.70s
                        Total time: 281.09s
                               ETA: 618 mins 18.4 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 123950 steps/s (collection: 0.652s, learning 0.142s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5049
       Mean episode rew_ang_vel_xy: -0.0428
          Mean episode rew_dof_acc: -0.1696
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1099
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2947
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0286
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.79s
                        Total time: 281.88s
                               ETA: 618 mins 23.7 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 137820 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4937
       Mean episode rew_ang_vel_xy: -0.0432
          Mean episode rew_dof_acc: -0.1731
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1171
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2892
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.71s
                        Total time: 282.59s
                               ETA: 618 mins 18.4 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 135472 steps/s (collection: 0.603s, learning 0.123s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4866
       Mean episode rew_ang_vel_xy: -0.0418
          Mean episode rew_dof_acc: -0.1703
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1147
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2844
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.73s
                        Total time: 283.32s
                               ETA: 618 mins 14.8 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 131542 steps/s (collection: 0.606s, learning 0.141s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5018
       Mean episode rew_ang_vel_xy: -0.0432
          Mean episode rew_dof_acc: -0.1744
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2939
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.75s
                        Total time: 284.07s
                               ETA: 618 mins 14.0 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 132194 steps/s (collection: 0.619s, learning 0.125s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4867
       Mean episode rew_ang_vel_xy: -0.0419
          Mean episode rew_dof_acc: -0.1699
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1117
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2839
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.74s
                        Total time: 284.81s
                               ETA: 618 mins 12.7 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 137437 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4950
       Mean episode rew_ang_vel_xy: -0.0415
          Mean episode rew_dof_acc: -0.1687
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1159
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2921
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.72s
                        Total time: 285.53s
                               ETA: 618 mins 7.8 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 122695 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4984
       Mean episode rew_ang_vel_xy: -0.0425
          Mean episode rew_dof_acc: -0.1700
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1180
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2949
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.80s
                        Total time: 286.33s
                               ETA: 618 mins 14.0 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 126864 steps/s (collection: 0.651s, learning 0.124s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5131
       Mean episode rew_ang_vel_xy: -0.0424
          Mean episode rew_dof_acc: -0.1723
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1119
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3031
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.77s
                        Total time: 287.10s
                               ETA: 618 mins 16.8 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 124863 steps/s (collection: 0.658s, learning 0.129s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4998
       Mean episode rew_ang_vel_xy: -0.0426
          Mean episode rew_dof_acc: -0.1686
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1115
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2932
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.79s
                        Total time: 287.89s
                               ETA: 618 mins 21.1 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 128904 steps/s (collection: 0.638s, learning 0.125s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5124
       Mean episode rew_ang_vel_xy: -0.0426
          Mean episode rew_dof_acc: -0.1734
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1162
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3019
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0293
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.76s
                        Total time: 288.65s
                               ETA: 618 mins 22.3 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 131947 steps/s (collection: 0.617s, learning 0.129s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5033
       Mean episode rew_ang_vel_xy: -0.0416
          Mean episode rew_dof_acc: -0.1715
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1112
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2969
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0278
        Mean episode terrain_level: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.75s
                        Total time: 289.40s
                               ETA: 618 mins 21.2 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 134527 steps/s (collection: 0.599s, learning 0.131s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5086
       Mean episode rew_ang_vel_xy: -0.0432
          Mean episode rew_dof_acc: -0.1717
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1188
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2996
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.73s
                        Total time: 290.13s
                               ETA: 618 mins 18.3 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 123358 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4984
       Mean episode rew_ang_vel_xy: -0.0443
          Mean episode rew_dof_acc: -0.1732
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2916
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.80s
                        Total time: 290.93s
                               ETA: 618 mins 23.8 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 137067 steps/s (collection: 0.593s, learning 0.124s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5188
       Mean episode rew_ang_vel_xy: -0.0430
          Mean episode rew_dof_acc: -0.1743
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1129
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3059
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0330
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.72s
                        Total time: 291.64s
                               ETA: 618 mins 19.1 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 123801 steps/s (collection: 0.657s, learning 0.137s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5245
       Mean episode rew_ang_vel_xy: -0.0428
          Mean episode rew_dof_acc: -0.1747
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1158
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3087
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0308
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.79s
                        Total time: 292.44s
                               ETA: 618 mins 24.3 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 133463 steps/s (collection: 0.610s, learning 0.126s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5344
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1773
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1156
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3162
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0298
        Mean episode terrain_level: 0.0315
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.74s
                        Total time: 293.17s
                               ETA: 618 mins 22.1 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 134018 steps/s (collection: 0.609s, learning 0.125s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5149
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1729
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1188
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3015
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.73s
                        Total time: 293.91s
                               ETA: 618 mins 19.5 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 134213 steps/s (collection: 0.609s, learning 0.124s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5197
       Mean episode rew_ang_vel_xy: -0.0423
          Mean episode rew_dof_acc: -0.1712
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1096
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3065
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0281
        Mean episode terrain_level: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.73s
                        Total time: 294.64s
                               ETA: 618 mins 16.8 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 134156 steps/s (collection: 0.605s, learning 0.128s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5203
       Mean episode rew_ang_vel_xy: -0.0436
          Mean episode rew_dof_acc: -0.1758
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1177
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3048
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.73s
                        Total time: 295.37s
                               ETA: 618 mins 14.2 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 120655 steps/s (collection: 0.691s, learning 0.124s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5154
       Mean episode rew_ang_vel_xy: -0.0430
          Mean episode rew_dof_acc: -0.1715
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1161
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3026
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.81s
                        Total time: 296.19s
                               ETA: 618 mins 21.8 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 122147 steps/s (collection: 0.681s, learning 0.124s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5263
       Mean episode rew_ang_vel_xy: -0.0434
          Mean episode rew_dof_acc: -0.1740
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1149
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3111
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0358
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.80s
                        Total time: 296.99s
                               ETA: 618 mins 28.2 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 123328 steps/s (collection: 0.658s, learning 0.139s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5216
       Mean episode rew_ang_vel_xy: -0.0436
          Mean episode rew_dof_acc: -0.1734
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1132
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3055
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.80s
                        Total time: 297.79s
                               ETA: 618 mins 33.5 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 128580 steps/s (collection: 0.628s, learning 0.136s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5332
       Mean episode rew_ang_vel_xy: -0.0440
          Mean episode rew_dof_acc: -0.1759
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1200
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3130
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0282
        Mean episode terrain_level: 0.0340
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.76s
                        Total time: 298.55s
                               ETA: 618 mins 34.8 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 123702 steps/s (collection: 0.667s, learning 0.127s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5481
       Mean episode rew_ang_vel_xy: -0.0435
          Mean episode rew_dof_acc: -0.1770
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1133
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3262
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.79s
                        Total time: 299.35s
                               ETA: 618 mins 39.8 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 120482 steps/s (collection: 0.685s, learning 0.131s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5415
       Mean episode rew_ang_vel_xy: -0.0451
          Mean episode rew_dof_acc: -0.1782
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1208
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3180
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0348
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.82s
                        Total time: 300.16s
                               ETA: 618 mins 47.4 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 118245 steps/s (collection: 0.687s, learning 0.144s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5477
       Mean episode rew_ang_vel_xy: -0.0433
          Mean episode rew_dof_acc: -0.1809
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1172
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3190
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0357
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.83s
                        Total time: 300.99s
                               ETA: 618 mins 56.9 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 122526 steps/s (collection: 0.676s, learning 0.127s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5365
       Mean episode rew_ang_vel_xy: -0.0428
          Mean episode rew_dof_acc: -0.1735
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1143
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3188
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0349
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.80s
                        Total time: 301.80s
                               ETA: 619 mins 2.7 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 128441 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5408
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1727
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1141
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3196
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.77s
                        Total time: 302.56s
                               ETA: 619 mins 4.0 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 120345 steps/s (collection: 0.693s, learning 0.124s)
               Value function loss: 0.0676
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5349
       Mean episode rew_ang_vel_xy: -0.0441
          Mean episode rew_dof_acc: -0.1731
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1165
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3142
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0329
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.82s
                        Total time: 303.38s
                               ETA: 619 mins 11.6 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 117864 steps/s (collection: 0.711s, learning 0.123s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5562
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1786
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1114
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3315
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0295
        Mean episode terrain_level: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.83s
                        Total time: 304.21s
                               ETA: 619 mins 21.2 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 136972 steps/s (collection: 0.596s, learning 0.122s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5430
       Mean episode rew_ang_vel_xy: -0.0446
          Mean episode rew_dof_acc: -0.1734
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1191
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3193
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0323
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.72s
                        Total time: 304.93s
                               ETA: 619 mins 16.6 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 120713 steps/s (collection: 0.690s, learning 0.124s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5630
       Mean episode rew_ang_vel_xy: -0.0443
          Mean episode rew_dof_acc: -0.1794
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1164
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3333
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.81s
                        Total time: 305.75s
                               ETA: 619 mins 23.8 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 125721 steps/s (collection: 0.649s, learning 0.133s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5486
       Mean episode rew_ang_vel_xy: -0.0438
          Mean episode rew_dof_acc: -0.1766
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1215
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3242
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.78s
                        Total time: 306.53s
                               ETA: 619 mins 27.0 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 126025 steps/s (collection: 0.636s, learning 0.144s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5600
       Mean episode rew_ang_vel_xy: -0.0440
          Mean episode rew_dof_acc: -0.1765
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1135
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3296
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0320
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.78s
                        Total time: 307.31s
                               ETA: 619 mins 29.9 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 126876 steps/s (collection: 0.651s, learning 0.124s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5670
       Mean episode rew_ang_vel_xy: -0.0438
          Mean episode rew_dof_acc: -0.1757
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3331
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0298
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.77s
                        Total time: 308.08s
                               ETA: 619 mins 32.2 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 135325 steps/s (collection: 0.602s, learning 0.124s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5610
       Mean episode rew_ang_vel_xy: -0.0430
          Mean episode rew_dof_acc: -0.1771
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1139
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3308
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.73s
                        Total time: 308.81s
                               ETA: 619 mins 28.7 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 127458 steps/s (collection: 0.639s, learning 0.132s)
               Value function loss: 0.0655
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5466
       Mean episode rew_ang_vel_xy: -0.0434
          Mean episode rew_dof_acc: -0.1758
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1156
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3202
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.77s
                        Total time: 309.58s
                               ETA: 619 mins 30.6 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 117507 steps/s (collection: 0.705s, learning 0.131s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5783
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1784
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1142
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3387
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.84s
                        Total time: 310.42s
                               ETA: 619 mins 40.2 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 124549 steps/s (collection: 0.658s, learning 0.131s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5529
       Mean episode rew_ang_vel_xy: -0.0440
          Mean episode rew_dof_acc: -0.1744
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1156
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3235
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.79s
                        Total time: 311.21s
                               ETA: 619 mins 44.2 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 123539 steps/s (collection: 0.658s, learning 0.137s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5685
       Mean episode rew_ang_vel_xy: -0.0441
          Mean episode rew_dof_acc: -0.1772
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1160
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3334
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0285
        Mean episode terrain_level: 0.0360
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.80s
                        Total time: 312.00s
                               ETA: 619 mins 48.9 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 129916 steps/s (collection: 0.633s, learning 0.124s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5534
       Mean episode rew_ang_vel_xy: -0.0418
          Mean episode rew_dof_acc: -0.1721
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1124
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3246
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.76s
                        Total time: 312.76s
                               ETA: 619 mins 48.9 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 132396 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5702
       Mean episode rew_ang_vel_xy: -0.0440
          Mean episode rew_dof_acc: -0.1761
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1185
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3343
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.74s
                        Total time: 313.50s
                               ETA: 619 mins 47.3 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 119755 steps/s (collection: 0.698s, learning 0.123s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5648
       Mean episode rew_ang_vel_xy: -0.0445
          Mean episode rew_dof_acc: -0.1774
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1181
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3328
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.82s
                        Total time: 314.32s
                               ETA: 619 mins 54.9 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 123436 steps/s (collection: 0.653s, learning 0.144s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5781
       Mean episode rew_ang_vel_xy: -0.0438
          Mean episode rew_dof_acc: -0.1779
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1172
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3399
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0286
        Mean episode terrain_level: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.80s
                        Total time: 315.12s
                               ETA: 619 mins 59.7 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 136932 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6063
       Mean episode rew_ang_vel_xy: -0.0453
          Mean episode rew_dof_acc: -0.1857
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3585
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.72s
                        Total time: 315.84s
                               ETA: 619 mins 55.1 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 135821 steps/s (collection: 0.602s, learning 0.121s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5938
       Mean episode rew_ang_vel_xy: -0.0446
          Mean episode rew_dof_acc: -0.1789
   Mean episode rew_dof_pos_limits: -0.0094
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1222
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3507
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0299
        Mean episode terrain_level: 0.0351
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.72s
                        Total time: 316.56s
                               ETA: 619 mins 51.2 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 137700 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6036
       Mean episode rew_ang_vel_xy: -0.0458
          Mean episode rew_dof_acc: -0.1817
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1219
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3531
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0295
        Mean episode terrain_level: 0.0352
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.71s
                        Total time: 317.27s
                               ETA: 619 mins 46.2 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 125403 steps/s (collection: 0.661s, learning 0.123s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5855
       Mean episode rew_ang_vel_xy: -0.0441
          Mean episode rew_dof_acc: -0.1791
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1196
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3436
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0282
        Mean episode terrain_level: 0.0391
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.78s
                        Total time: 318.06s
                               ETA: 619 mins 49.4 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 118088 steps/s (collection: 0.707s, learning 0.125s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5967
       Mean episode rew_ang_vel_xy: -0.0452
          Mean episode rew_dof_acc: -0.1814
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1166
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3529
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0414
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.83s
                        Total time: 318.89s
                               ETA: 619 mins 58.3 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 129968 steps/s (collection: 0.613s, learning 0.143s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6009
       Mean episode rew_ang_vel_xy: -0.0456
          Mean episode rew_dof_acc: -0.1821
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1193
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3516
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0426
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.76s
                        Total time: 319.65s
                               ETA: 619 mins 58.2 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 119908 steps/s (collection: 0.693s, learning 0.127s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6055
       Mean episode rew_ang_vel_xy: -0.0450
          Mean episode rew_dof_acc: -0.1815
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1172
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3566
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0410
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.82s
                        Total time: 320.47s
                               ETA: 620 mins 5.6 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 132058 steps/s (collection: 0.620s, learning 0.124s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6027
       Mean episode rew_ang_vel_xy: -0.0447
          Mean episode rew_dof_acc: -0.1817
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1115
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3541
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0311
        Mean episode terrain_level: 0.0400
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.74s
                        Total time: 321.21s
                               ETA: 620 mins 4.1 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 134904 steps/s (collection: 0.605s, learning 0.124s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6111
       Mean episode rew_ang_vel_xy: -0.0454
          Mean episode rew_dof_acc: -0.1820
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1167
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3575
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0399
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.73s
                        Total time: 321.94s
                               ETA: 620 mins 0.8 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 126590 steps/s (collection: 0.640s, learning 0.136s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5909
       Mean episode rew_ang_vel_xy: -0.0436
          Mean episode rew_dof_acc: -0.1786
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1161
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3480
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0397
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.78s
                        Total time: 322.72s
                               ETA: 620 mins 3.1 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 129311 steps/s (collection: 0.635s, learning 0.125s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5973
       Mean episode rew_ang_vel_xy: -0.0455
          Mean episode rew_dof_acc: -0.1800
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1183
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3504
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0394
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.76s
                        Total time: 323.48s
                               ETA: 620 mins 3.5 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 131050 steps/s (collection: 0.626s, learning 0.125s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6033
       Mean episode rew_ang_vel_xy: -0.0453
          Mean episode rew_dof_acc: -0.1771
   Mean episode rew_dof_pos_limits: -0.0094
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1137
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3526
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0362
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.75s
                        Total time: 324.23s
                               ETA: 620 mins 2.7 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 127317 steps/s (collection: 0.648s, learning 0.124s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6129
       Mean episode rew_ang_vel_xy: -0.0450
          Mean episode rew_dof_acc: -0.1802
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1155
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3574
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0374
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.77s
                        Total time: 325.00s
                               ETA: 620 mins 4.4 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 126949 steps/s (collection: 0.646s, learning 0.129s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6213
       Mean episode rew_ang_vel_xy: -0.0460
          Mean episode rew_dof_acc: -0.1840
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1147
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3632
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.77s
                        Total time: 325.77s
                               ETA: 620 mins 6.4 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 118279 steps/s (collection: 0.709s, learning 0.122s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6262
       Mean episode rew_ang_vel_xy: -0.0457
          Mean episode rew_dof_acc: -0.1863
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3657
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0311
        Mean episode terrain_level: 0.0359
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.83s
                        Total time: 326.60s
                               ETA: 620 mins 14.8 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 132821 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6149
       Mean episode rew_ang_vel_xy: -0.0458
          Mean episode rew_dof_acc: -0.1807
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3581
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0286
        Mean episode terrain_level: 0.0359
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.74s
                        Total time: 327.34s
                               ETA: 620 mins 12.8 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 126736 steps/s (collection: 0.654s, learning 0.122s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6387
       Mean episode rew_ang_vel_xy: -0.0461
          Mean episode rew_dof_acc: -0.1850
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1186
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3725
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.78s
                        Total time: 328.12s
                               ETA: 620 mins 14.9 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 121240 steps/s (collection: 0.664s, learning 0.147s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6230
       Mean episode rew_ang_vel_xy: -0.0442
          Mean episode rew_dof_acc: -0.1783
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1179
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3633
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0363
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.81s
                        Total time: 328.93s
                               ETA: 620 mins 20.9 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 115409 steps/s (collection: 0.716s, learning 0.136s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6270
       Mean episode rew_ang_vel_xy: -0.0451
          Mean episode rew_dof_acc: -0.1864
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1185
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3643
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0297
        Mean episode terrain_level: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.85s
                        Total time: 329.78s
                               ETA: 620 mins 31.5 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 128365 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6285
       Mean episode rew_ang_vel_xy: -0.0451
          Mean episode rew_dof_acc: -0.1802
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1184
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3654
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0376
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.77s
                        Total time: 330.55s
                               ETA: 620 mins 32.4 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 121884 steps/s (collection: 0.661s, learning 0.145s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6418
       Mean episode rew_ang_vel_xy: -0.0461
          Mean episode rew_dof_acc: -0.1824
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1220
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3742
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0386
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.81s
                        Total time: 331.35s
                               ETA: 620 mins 37.9 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 129273 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6411
       Mean episode rew_ang_vel_xy: -0.0458
          Mean episode rew_dof_acc: -0.1876
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3752
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0288
        Mean episode terrain_level: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.76s
                        Total time: 332.11s
                               ETA: 620 mins 38.2 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 113904 steps/s (collection: 0.741s, learning 0.122s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6475
       Mean episode rew_ang_vel_xy: -0.0467
          Mean episode rew_dof_acc: -0.1844
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1216
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3782
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0362
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.86s
                        Total time: 332.98s
                               ETA: 620 mins 49.9 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 132581 steps/s (collection: 0.618s, learning 0.123s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6438
       Mean episode rew_ang_vel_xy: -0.0452
          Mean episode rew_dof_acc: -0.1818
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1189
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3749
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0304
        Mean episode terrain_level: 0.0349
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.74s
                        Total time: 333.72s
                               ETA: 620 mins 48.0 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 133440 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6370
       Mean episode rew_ang_vel_xy: -0.0461
          Mean episode rew_dof_acc: -0.1821
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1161
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3711
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0309
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.74s
                        Total time: 334.46s
                               ETA: 620 mins 45.6 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 133545 steps/s (collection: 0.613s, learning 0.124s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6498
       Mean episode rew_ang_vel_xy: -0.0468
          Mean episode rew_dof_acc: -0.1900
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1166
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3752
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0327
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.74s
                        Total time: 335.19s
                               ETA: 620 mins 43.1 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 133488 steps/s (collection: 0.614s, learning 0.122s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6565
       Mean episode rew_ang_vel_xy: -0.0455
          Mean episode rew_dof_acc: -0.1848
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1134
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3825
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.74s
                        Total time: 335.93s
                               ETA: 620 mins 40.7 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 125696 steps/s (collection: 0.660s, learning 0.122s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6685
       Mean episode rew_ang_vel_xy: -0.0459
          Mean episode rew_dof_acc: -0.1883
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1197
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3882
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0324
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.78s
                        Total time: 336.71s
                               ETA: 620 mins 43.3 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 131691 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6293
       Mean episode rew_ang_vel_xy: -0.0458
          Mean episode rew_dof_acc: -0.1792
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1196
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3612
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.75s
                        Total time: 337.46s
                               ETA: 620 mins 42.0 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 127592 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6728
       Mean episode rew_ang_vel_xy: -0.0482
          Mean episode rew_dof_acc: -0.1911
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1297
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3876
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0311
        Mean episode terrain_level: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.77s
                        Total time: 338.23s
                               ETA: 620 mins 43.3 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 122180 steps/s (collection: 0.665s, learning 0.139s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6560
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1846
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1207
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3785
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0368
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.80s
                        Total time: 339.03s
                               ETA: 620 mins 48.4 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 127676 steps/s (collection: 0.618s, learning 0.151s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6573
       Mean episode rew_ang_vel_xy: -0.0448
          Mean episode rew_dof_acc: -0.1849
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1141
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3849
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0399
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.77s
                        Total time: 339.80s
                               ETA: 620 mins 49.6 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 125646 steps/s (collection: 0.661s, learning 0.121s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6753
       Mean episode rew_ang_vel_xy: -0.0462
          Mean episode rew_dof_acc: -0.1933
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1218
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3895
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0391
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.78s
                        Total time: 340.58s
                               ETA: 620 mins 52.2 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 120853 steps/s (collection: 0.691s, learning 0.123s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6687
       Mean episode rew_ang_vel_xy: -0.0468
          Mean episode rew_dof_acc: -0.1860
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1188
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3831
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0378
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.81s
                        Total time: 341.40s
                               ETA: 620 mins 58.2 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 122285 steps/s (collection: 0.680s, learning 0.123s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6887
       Mean episode rew_ang_vel_xy: -0.0467
          Mean episode rew_dof_acc: -0.1931
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1218
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3984
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0393
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.80s
                        Total time: 342.20s
                               ETA: 621 mins 3.1 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 131899 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6622
       Mean episode rew_ang_vel_xy: -0.0468
          Mean episode rew_dof_acc: -0.1859
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1304
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3808
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0374
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.75s
                        Total time: 342.95s
                               ETA: 621 mins 1.6 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 115819 steps/s (collection: 0.722s, learning 0.126s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6963
       Mean episode rew_ang_vel_xy: -0.0470
          Mean episode rew_dof_acc: -0.1913
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1247
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4012
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.85s
                        Total time: 343.80s
                               ETA: 621 mins 11.4 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 125813 steps/s (collection: 0.641s, learning 0.140s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6829
       Mean episode rew_ang_vel_xy: -0.0460
          Mean episode rew_dof_acc: -0.1868
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3937
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0299
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.78s
                        Total time: 344.58s
                               ETA: 621 mins 13.7 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 121222 steps/s (collection: 0.671s, learning 0.140s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6870
       Mean episode rew_ang_vel_xy: -0.0470
          Mean episode rew_dof_acc: -0.1933
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1227
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3972
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.81s
                        Total time: 345.39s
                               ETA: 621 mins 19.3 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 128069 steps/s (collection: 0.636s, learning 0.132s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6818
       Mean episode rew_ang_vel_xy: -0.0464
          Mean episode rew_dof_acc: -0.1902
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3960
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.77s
                        Total time: 346.16s
                               ETA: 621 mins 20.2 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 128464 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7041
       Mean episode rew_ang_vel_xy: -0.0468
          Mean episode rew_dof_acc: -0.1890
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4037
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.77s
                        Total time: 346.92s
                               ETA: 621 mins 20.8 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 127067 steps/s (collection: 0.650s, learning 0.124s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7076
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.1935
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1263
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4100
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.77s
                        Total time: 347.69s
                               ETA: 621 mins 22.3 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 123510 steps/s (collection: 0.662s, learning 0.134s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7015
       Mean episode rew_ang_vel_xy: -0.0470
          Mean episode rew_dof_acc: -0.1934
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1217
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4093
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0288
        Mean episode terrain_level: 0.0414
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.80s
                        Total time: 348.49s
                               ETA: 621 mins 26.2 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 130635 steps/s (collection: 0.628s, learning 0.125s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7190
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.2017
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1283
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4186
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0439
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.75s
                        Total time: 349.24s
                               ETA: 621 mins 25.4 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 132188 steps/s (collection: 0.619s, learning 0.124s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7194
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1961
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4210
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0441
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.74s
                        Total time: 349.99s
                               ETA: 621 mins 23.7 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 122718 steps/s (collection: 0.668s, learning 0.133s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7181
       Mean episode rew_ang_vel_xy: -0.0473
          Mean episode rew_dof_acc: -0.1929
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1211
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4143
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0451
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.80s
                        Total time: 350.79s
                               ETA: 621 mins 28.1 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 119519 steps/s (collection: 0.683s, learning 0.140s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7014
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1933
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1250
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4097
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0320
        Mean episode terrain_level: 0.0448
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.82s
                        Total time: 351.61s
                               ETA: 621 mins 34.7 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 117621 steps/s (collection: 0.700s, learning 0.136s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7182
       Mean episode rew_ang_vel_xy: -0.0464
          Mean episode rew_dof_acc: -0.1973
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1210
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4214
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0448
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.84s
                        Total time: 352.45s
                               ETA: 621 mins 42.8 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 130855 steps/s (collection: 0.627s, learning 0.124s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7019
       Mean episode rew_ang_vel_xy: -0.0461
          Mean episode rew_dof_acc: -0.1873
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1207
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4073
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0321
        Mean episode terrain_level: 0.0422
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.75s
                        Total time: 353.20s
                               ETA: 621 mins 41.8 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 120094 steps/s (collection: 0.696s, learning 0.123s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7141
       Mean episode rew_ang_vel_xy: -0.0465
          Mean episode rew_dof_acc: -0.1957
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1225
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4170
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0385
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.82s
                        Total time: 354.02s
                               ETA: 621 mins 48.0 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 123245 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7405
       Mean episode rew_ang_vel_xy: -0.0465
          Mean episode rew_dof_acc: -0.1981
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1216
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4305
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.80s
                        Total time: 354.81s
                               ETA: 621 mins 51.9 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 128342 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7407
       Mean episode rew_ang_vel_xy: -0.0477
          Mean episode rew_dof_acc: -0.2004
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1212
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4291
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0373
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.77s
                        Total time: 355.58s
                               ETA: 621 mins 52.5 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 127423 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7088
       Mean episode rew_ang_vel_xy: -0.0472
          Mean episode rew_dof_acc: -0.1902
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1283
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4061
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.77s
                        Total time: 356.35s
                               ETA: 621 mins 53.6 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 122453 steps/s (collection: 0.670s, learning 0.133s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7320
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1938
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1176
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4226
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.80s
                        Total time: 357.15s
                               ETA: 621 mins 58.0 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 121705 steps/s (collection: 0.673s, learning 0.134s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7020
       Mean episode rew_ang_vel_xy: -0.0460
          Mean episode rew_dof_acc: -0.1918
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1247
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.4009
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0297
        Mean episode terrain_level: 0.0381
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.81s
                        Total time: 357.96s
                               ETA: 622 mins 2.9 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 120493 steps/s (collection: 0.681s, learning 0.135s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7376
       Mean episode rew_ang_vel_xy: -0.0472
          Mean episode rew_dof_acc: -0.1954
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1232
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4278
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0337
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.82s
                        Total time: 358.78s
                               ETA: 622 mins 8.6 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 114986 steps/s (collection: 0.704s, learning 0.151s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7505
       Mean episode rew_ang_vel_xy: -0.0480
          Mean episode rew_dof_acc: -0.1986
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4353
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.85s
                        Total time: 359.63s
                               ETA: 622 mins 18.4 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 115947 steps/s (collection: 0.721s, learning 0.127s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7230
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1919
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1209
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4124
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0369
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.85s
                        Total time: 360.48s
                               ETA: 622 mins 27.4 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 130235 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7300
       Mean episode rew_ang_vel_xy: -0.0467
          Mean episode rew_dof_acc: -0.1924
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1215
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4169
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0353
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.75s
                        Total time: 361.23s
                               ETA: 622 mins 26.7 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 112165 steps/s (collection: 0.748s, learning 0.128s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7599
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1981
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1215
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4355
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0337
        Mean episode terrain_level: 0.0364
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.88s
                        Total time: 362.11s
                               ETA: 622 mins 38.5 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 123236 steps/s (collection: 0.660s, learning 0.138s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7398
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1927
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4253
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0377
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.80s
                        Total time: 362.91s
                               ETA: 622 mins 42.2 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 117334 steps/s (collection: 0.715s, learning 0.123s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7710
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1980
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1143
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4420
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.84s
                        Total time: 363.75s
                               ETA: 622 mins 50.0 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 128775 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7673
       Mean episode rew_ang_vel_xy: -0.0472
          Mean episode rew_dof_acc: -0.1992
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1261
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4431
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0403
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.76s
                        Total time: 364.51s
                               ETA: 622 mins 50.2 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 130028 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7510
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.2011
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1240
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4290
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0391
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.76s
                        Total time: 365.27s
                               ETA: 622 mins 49.6 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 124691 steps/s (collection: 0.655s, learning 0.133s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7791
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1998
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1209
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4447
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0395
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.79s
                        Total time: 366.05s
                               ETA: 622 mins 52.2 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 129632 steps/s (collection: 0.633s, learning 0.125s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7558
       Mean episode rew_ang_vel_xy: -0.0471
          Mean episode rew_dof_acc: -0.1943
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1223
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4302
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0437
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.76s
                        Total time: 366.81s
                               ETA: 622 mins 51.9 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 127475 steps/s (collection: 0.636s, learning 0.135s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7652
       Mean episode rew_ang_vel_xy: -0.0484
          Mean episode rew_dof_acc: -0.2002
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1273
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4369
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0457
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.77s
                        Total time: 367.58s
                               ETA: 622 mins 52.8 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 124140 steps/s (collection: 0.660s, learning 0.132s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7795
       Mean episode rew_ang_vel_xy: -0.0492
          Mean episode rew_dof_acc: -0.2009
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1241
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4384
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0464
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.79s
                        Total time: 368.38s
                               ETA: 622 mins 55.8 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 125200 steps/s (collection: 0.661s, learning 0.124s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7661
       Mean episode rew_ang_vel_xy: -0.0468
          Mean episode rew_dof_acc: -0.1931
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1175
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4355
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0483
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.79s
                        Total time: 369.16s
                               ETA: 622 mins 58.1 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 118298 steps/s (collection: 0.703s, learning 0.128s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7820
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1972
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1187
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4396
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0316
        Mean episode terrain_level: 0.0468
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.83s
                        Total time: 369.99s
                               ETA: 623 mins 5.0 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 113932 steps/s (collection: 0.731s, learning 0.132s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7966
       Mean episode rew_ang_vel_xy: -0.0472
          Mean episode rew_dof_acc: -0.2033
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1205
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4518
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0466
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.86s
                        Total time: 370.85s
                               ETA: 623 mins 15.1 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 106400 steps/s (collection: 0.784s, learning 0.140s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8076
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.2035
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1253
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4588
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0375
        Mean episode terrain_level: 0.0466
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.92s
                        Total time: 371.78s
                               ETA: 623 mins 31.3 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 114857 steps/s (collection: 0.715s, learning 0.140s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7828
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1979
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1251
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4398
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0318
        Mean episode terrain_level: 0.0461
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.86s
                        Total time: 372.63s
                               ETA: 623 mins 40.6 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 103620 steps/s (collection: 0.802s, learning 0.147s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7789
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1975
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1239
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4389
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0338
        Mean episode terrain_level: 0.0467
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.95s
                        Total time: 373.58s
                               ETA: 623 mins 59.2 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 122583 steps/s (collection: 0.656s, learning 0.146s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8097
       Mean episode rew_ang_vel_xy: -0.0488
          Mean episode rew_dof_acc: -0.2010
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1196
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4515
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0464
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.80s
                        Total time: 374.38s
                               ETA: 624 mins 3.0 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 112352 steps/s (collection: 0.729s, learning 0.146s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7963
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.1974
   Mean episode rew_dof_pos_limits: -0.0122
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1199
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4441
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0479
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.87s
                        Total time: 375.26s
                               ETA: 624 mins 14.1 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 122734 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7632
       Mean episode rew_ang_vel_xy: -0.0471
          Mean episode rew_dof_acc: -0.1934
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1267
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4242
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0320
        Mean episode terrain_level: 0.0507
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.80s
                        Total time: 376.06s
                               ETA: 624 mins 17.8 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 125568 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.94
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8003
       Mean episode rew_ang_vel_xy: -0.0477
          Mean episode rew_dof_acc: -0.1960
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1192
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4498
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0506
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.78s
                        Total time: 376.84s
                               ETA: 624 mins 19.6 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 126868 steps/s (collection: 0.643s, learning 0.132s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.94
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8134
       Mean episode rew_ang_vel_xy: -0.0481
          Mean episode rew_dof_acc: -0.1988
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1162
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4602
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0505
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.77s
                        Total time: 377.62s
                               ETA: 624 mins 20.7 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 122606 steps/s (collection: 0.679s, learning 0.123s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.94
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8208
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.2084
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1231
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4600
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0335
        Mean episode terrain_level: 0.0511
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.80s
                        Total time: 378.42s
                               ETA: 624 mins 24.4 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 115282 steps/s (collection: 0.707s, learning 0.146s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8414
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.2066
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.4720
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0508
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.85s
                        Total time: 379.27s
                               ETA: 624 mins 33.1 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 128376 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8225
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.2031
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1226
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4616
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0521
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.77s
                        Total time: 380.04s
                               ETA: 624 mins 33.2 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 122084 steps/s (collection: 0.684s, learning 0.121s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8567
       Mean episode rew_ang_vel_xy: -0.0487
          Mean episode rew_dof_acc: -0.2105
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1232
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.4848
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0521
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.81s
                        Total time: 380.84s
                               ETA: 624 mins 37.2 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 121218 steps/s (collection: 0.688s, learning 0.123s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8100
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1991
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1137
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4556
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0520
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.81s
                        Total time: 381.66s
                               ETA: 624 mins 41.7 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 119208 steps/s (collection: 0.696s, learning 0.129s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7798
       Mean episode rew_ang_vel_xy: -0.0475
          Mean episode rew_dof_acc: -0.1912
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1203
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4305
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0304
        Mean episode terrain_level: 0.0501
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.82s
                        Total time: 382.48s
                               ETA: 624 mins 47.5 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 119561 steps/s (collection: 0.695s, learning 0.127s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8372
       Mean episode rew_ang_vel_xy: -0.0487
          Mean episode rew_dof_acc: -0.2069
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1166
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4656
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0468
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.82s
                        Total time: 383.30s
                               ETA: 624 mins 53.1 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 119336 steps/s (collection: 0.687s, learning 0.137s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8513
       Mean episode rew_ang_vel_xy: -0.0485
          Mean episode rew_dof_acc: -0.2074
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1225
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4770
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0493
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.82s
                        Total time: 384.13s
                               ETA: 624 mins 58.8 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 108872 steps/s (collection: 0.774s, learning 0.129s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8217
       Mean episode rew_ang_vel_xy: -0.0487
          Mean episode rew_dof_acc: -0.1992
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1221
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4590
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0489
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.90s
                        Total time: 385.03s
                               ETA: 625 mins 12.2 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 116583 steps/s (collection: 0.719s, learning 0.124s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8641
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.2091
   Mean episode rew_dof_pos_limits: -0.0126
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1230
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4821
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0481
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.84s
                        Total time: 385.87s
                               ETA: 625 mins 19.8 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 116285 steps/s (collection: 0.720s, learning 0.125s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8385
       Mean episode rew_ang_vel_xy: -0.0481
          Mean episode rew_dof_acc: -0.2023
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1190
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4648
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0486
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.85s
                        Total time: 386.72s
                               ETA: 625 mins 27.5 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 120670 steps/s (collection: 0.691s, learning 0.124s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8022
       Mean episode rew_ang_vel_xy: -0.0461
          Mean episode rew_dof_acc: -0.1943
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1191
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4493
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0478
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.81s
                        Total time: 387.53s
                               ETA: 625 mins 32.2 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 123894 steps/s (collection: 0.671s, learning 0.122s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8188
       Mean episode rew_ang_vel_xy: -0.0481
          Mean episode rew_dof_acc: -0.2016
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.4551
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0459
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.79s
                        Total time: 388.33s
                               ETA: 625 mins 34.8 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 122404 steps/s (collection: 0.671s, learning 0.132s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8641
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.2065
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1256
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4829
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0496
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.80s
                        Total time: 389.13s
                               ETA: 625 mins 38.4 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 118586 steps/s (collection: 0.705s, learning 0.124s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8716
       Mean episode rew_ang_vel_xy: -0.0485
          Mean episode rew_dof_acc: -0.2072
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1199
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4858
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0370
        Mean episode terrain_level: 0.0499
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.83s
                        Total time: 389.96s
                               ETA: 625 mins 44.4 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 122393 steps/s (collection: 0.673s, learning 0.130s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8452
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.2032
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1243
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4736
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0499
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.80s
                        Total time: 390.76s
                               ETA: 625 mins 47.9 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 116521 steps/s (collection: 0.708s, learning 0.136s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8911
       Mean episode rew_ang_vel_xy: -0.0491
          Mean episode rew_dof_acc: -0.2152
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1237
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.4946
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0513
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.84s
                        Total time: 391.60s
                               ETA: 625 mins 55.3 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 127655 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8582
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.2070
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1205
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4789
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0516
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.77s
                        Total time: 392.37s
                               ETA: 625 mins 55.6 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 126692 steps/s (collection: 0.653s, learning 0.123s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8761
       Mean episode rew_ang_vel_xy: -0.0491
          Mean episode rew_dof_acc: -0.2085
   Mean episode rew_dof_pos_limits: -0.0122
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.4912
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0554
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.78s
                        Total time: 393.15s
                               ETA: 625 mins 56.5 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 125049 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8686
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.2057
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1256
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4821
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0557
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.79s
                        Total time: 393.94s
                               ETA: 625 mins 58.3 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 121442 steps/s (collection: 0.686s, learning 0.123s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8691
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.2046
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1169
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4798
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0578
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.81s
                        Total time: 394.75s
                               ETA: 626 mins 2.3 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 120564 steps/s (collection: 0.689s, learning 0.127s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8934
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.2145
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1234
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5004
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0382
        Mean episode terrain_level: 0.0568
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.82s
                        Total time: 395.56s
                               ETA: 626 mins 6.9 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 110905 steps/s (collection: 0.760s, learning 0.126s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8706
       Mean episode rew_ang_vel_xy: -0.0492
          Mean episode rew_dof_acc: -0.2036
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1183
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4871
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0552
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.89s
                        Total time: 396.45s
                               ETA: 626 mins 18.2 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 114225 steps/s (collection: 0.733s, learning 0.128s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9109
       Mean episode rew_ang_vel_xy: -0.0498
          Mean episode rew_dof_acc: -0.2129
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1199
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5082
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0537
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.86s
                        Total time: 397.31s
                               ETA: 626 mins 27.0 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 117361 steps/s (collection: 0.705s, learning 0.132s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8660
       Mean episode rew_ang_vel_xy: -0.0480
          Mean episode rew_dof_acc: -0.2068
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1147
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4822
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0351
        Mean episode terrain_level: 0.0515
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.84s
                        Total time: 398.15s
                               ETA: 626 mins 33.6 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 122497 steps/s (collection: 0.679s, learning 0.123s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9003
       Mean episode rew_ang_vel_xy: -0.0497
          Mean episode rew_dof_acc: -0.2082
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1197
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5053
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0534
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.80s
                        Total time: 398.95s
                               ETA: 626 mins 36.9 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 118354 steps/s (collection: 0.709s, learning 0.122s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8727
       Mean episode rew_ang_vel_xy: -0.0492
          Mean episode rew_dof_acc: -0.2043
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1174
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4891
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0540
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.83s
                        Total time: 399.78s
                               ETA: 626 mins 42.8 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 124477 steps/s (collection: 0.666s, learning 0.124s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8682
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.2018
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1148
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.4868
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0507
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.79s
                        Total time: 400.57s
                               ETA: 626 mins 44.8 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 118607 steps/s (collection: 0.687s, learning 0.142s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9467
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.2182
   Mean episode rew_dof_pos_limits: -0.0127
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5310
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0495
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.83s
                        Total time: 401.40s
                               ETA: 626 mins 50.5 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 105513 steps/s (collection: 0.788s, learning 0.143s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.00
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8888
       Mean episode rew_ang_vel_xy: -0.0487
          Mean episode rew_dof_acc: -0.2076
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1164
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.4998
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0489
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.93s
                        Total time: 402.33s
                               ETA: 627 mins 5.7 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 108782 steps/s (collection: 0.774s, learning 0.129s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.00
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9353
       Mean episode rew_ang_vel_xy: -0.0501
          Mean episode rew_dof_acc: -0.2192
   Mean episode rew_dof_pos_limits: -0.0127
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1240
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5268
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0350
        Mean episode terrain_level: 0.0510
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.90s
                        Total time: 403.23s
                               ETA: 627 mins 18.3 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 115836 steps/s (collection: 0.725s, learning 0.123s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.00
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9044
       Mean episode rew_ang_vel_xy: -0.0495
          Mean episode rew_dof_acc: -0.2115
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1192
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.5084
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0523
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.85s
                        Total time: 404.08s
                               ETA: 627 mins 25.8 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 123460 steps/s (collection: 0.671s, learning 0.125s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8995
       Mean episode rew_ang_vel_xy: -0.0486
          Mean episode rew_dof_acc: -0.2091
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1183
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5028
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0519
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.80s
                        Total time: 404.88s
                               ETA: 627 mins 28.3 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 114284 steps/s (collection: 0.726s, learning 0.134s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9479
       Mean episode rew_ang_vel_xy: -0.0499
          Mean episode rew_dof_acc: -0.2139
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1213
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5397
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0390
        Mean episode terrain_level: 0.0526
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.86s
                        Total time: 405.74s
                               ETA: 627 mins 36.7 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 113902 steps/s (collection: 0.737s, learning 0.126s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9588
       Mean episode rew_ang_vel_xy: -0.0500
          Mean episode rew_dof_acc: -0.2121
   Mean episode rew_dof_pos_limits: -0.0143
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1218
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5456
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0417
        Mean episode terrain_level: 0.0531
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.86s
                        Total time: 406.60s
                               ETA: 627 mins 45.4 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 125672 steps/s (collection: 0.659s, learning 0.124s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9531
       Mean episode rew_ang_vel_xy: -0.0504
          Mean episode rew_dof_acc: -0.2171
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1261
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5389
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0373
        Mean episode terrain_level: 0.0561
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.78s
                        Total time: 407.38s
                               ETA: 627 mins 46.6 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 120952 steps/s (collection: 0.688s, learning 0.124s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9131
       Mean episode rew_ang_vel_xy: -0.0486
          Mean episode rew_dof_acc: -0.2063
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1216
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5152
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0586
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.81s
                        Total time: 408.20s
                               ETA: 627 mins 50.5 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 124264 steps/s (collection: 0.658s, learning 0.133s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9111
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.2092
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1188
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5099
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0593
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.79s
                        Total time: 408.99s
                               ETA: 627 mins 52.5 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 121557 steps/s (collection: 0.676s, learning 0.133s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9302
       Mean episode rew_ang_vel_xy: -0.0494
          Mean episode rew_dof_acc: -0.2118
   Mean episode rew_dof_pos_limits: -0.0136
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1219
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5214
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0606
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.81s
                        Total time: 409.80s
                               ETA: 627 mins 56.1 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 125431 steps/s (collection: 0.661s, learning 0.123s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9268
       Mean episode rew_ang_vel_xy: -0.0494
          Mean episode rew_dof_acc: -0.2096
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1191
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5242
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0605
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.78s
                        Total time: 410.58s
                               ETA: 627 mins 57.3 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 123571 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9563
       Mean episode rew_ang_vel_xy: -0.0506
          Mean episode rew_dof_acc: -0.2128
   Mean episode rew_dof_pos_limits: -0.0137
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1149
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5341
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0564
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.80s
                        Total time: 411.37s
                               ETA: 627 mins 59.6 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 120530 steps/s (collection: 0.676s, learning 0.140s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9628
       Mean episode rew_ang_vel_xy: -0.0509
          Mean episode rew_dof_acc: -0.2118
   Mean episode rew_dof_pos_limits: -0.0140
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5417
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0562
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.82s
                        Total time: 412.19s
                               ETA: 628 mins 3.8 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 114057 steps/s (collection: 0.734s, learning 0.128s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9475
       Mean episode rew_ang_vel_xy: -0.0481
          Mean episode rew_dof_acc: -0.2061
   Mean episode rew_dof_pos_limits: -0.0142
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1134
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5387
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0562
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.86s
                        Total time: 413.05s
                               ETA: 628 mins 12.2 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 116381 steps/s (collection: 0.700s, learning 0.145s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9419
       Mean episode rew_ang_vel_xy: -0.0486
          Mean episode rew_dof_acc: -0.2098
   Mean episode rew_dof_pos_limits: -0.0122
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1204
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5293
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0349
        Mean episode terrain_level: 0.0564
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.84s
                        Total time: 413.90s
                               ETA: 628 mins 18.9 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 126269 steps/s (collection: 0.656s, learning 0.123s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9384
       Mean episode rew_ang_vel_xy: -0.0508
          Mean episode rew_dof_acc: -0.2129
   Mean episode rew_dof_pos_limits: -0.0126
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1220
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5286
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0552
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.78s
                        Total time: 414.68s
                               ETA: 628 mins 19.6 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 115340 steps/s (collection: 0.729s, learning 0.123s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9458
       Mean episode rew_ang_vel_xy: -0.0503
          Mean episode rew_dof_acc: -0.2074
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1204
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5312
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0557
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.85s
                        Total time: 415.53s
                               ETA: 628 mins 27.0 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 119823 steps/s (collection: 0.696s, learning 0.124s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9575
       Mean episode rew_ang_vel_xy: -0.0492
          Mean episode rew_dof_acc: -0.2070
   Mean episode rew_dof_pos_limits: -0.0141
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1199
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5400
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0549
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.82s
                        Total time: 416.35s
                               ETA: 628 mins 31.5 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 124346 steps/s (collection: 0.668s, learning 0.123s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9727
       Mean episode rew_ang_vel_xy: -0.0509
          Mean episode rew_dof_acc: -0.2131
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1190
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5484
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0544
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.79s
                        Total time: 417.14s
                               ETA: 628 mins 33.3 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 122468 steps/s (collection: 0.677s, learning 0.126s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9307
       Mean episode rew_ang_vel_xy: -0.0490
          Mean episode rew_dof_acc: -0.2067
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1205
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.5249
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0573
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.80s
                        Total time: 417.94s
                               ETA: 628 mins 36.2 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 115438 steps/s (collection: 0.725s, learning 0.126s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9687
       Mean episode rew_ang_vel_xy: -0.0505
          Mean episode rew_dof_acc: -0.2129
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1191
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5455
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0565
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.85s
                        Total time: 418.79s
                               ETA: 628 mins 43.4 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 117352 steps/s (collection: 0.714s, learning 0.124s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9896
       Mean episode rew_ang_vel_xy: -0.0515
          Mean episode rew_dof_acc: -0.2150
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1229
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5538
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0579
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.84s
                        Total time: 419.63s
                               ETA: 628 mins 49.4 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 120391 steps/s (collection: 0.693s, learning 0.124s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9770
       Mean episode rew_ang_vel_xy: -0.0505
          Mean episode rew_dof_acc: -0.2125
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1214
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5490
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0600
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.82s
                        Total time: 420.45s
                               ETA: 628 mins 53.4 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 117852 steps/s (collection: 0.700s, learning 0.135s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9814
       Mean episode rew_ang_vel_xy: -0.0505
          Mean episode rew_dof_acc: -0.2108
   Mean episode rew_dof_pos_limits: -0.0138
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1245
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5544
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.83s
                        Total time: 421.28s
                               ETA: 628 mins 59.0 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 119979 steps/s (collection: 0.679s, learning 0.141s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9996
       Mean episode rew_ang_vel_xy: -0.0504
          Mean episode rew_dof_acc: -0.2240
   Mean episode rew_dof_pos_limits: -0.0143
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1261
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.5605
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0599
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.82s
                        Total time: 422.10s
                               ETA: 629 mins 3.3 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 104532 steps/s (collection: 0.803s, learning 0.137s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9858
       Mean episode rew_ang_vel_xy: -0.0509
          Mean episode rew_dof_acc: -0.2118
   Mean episode rew_dof_pos_limits: -0.0147
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1238
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5570
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0611
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.94s
                        Total time: 423.04s
                               ETA: 629 mins 18.3 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 124566 steps/s (collection: 0.660s, learning 0.129s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9503
       Mean episode rew_ang_vel_xy: -0.0495
          Mean episode rew_dof_acc: -0.2022
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1185
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.5329
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0634
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.79s
                        Total time: 423.83s
                               ETA: 629 mins 19.8 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 124570 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9919
       Mean episode rew_ang_vel_xy: -0.0494
          Mean episode rew_dof_acc: -0.2124
   Mean episode rew_dof_pos_limits: -0.0147
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1151
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5600
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0634
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.79s
                        Total time: 424.62s
                               ETA: 629 mins 21.3 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 123756 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0602
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.2198
   Mean episode rew_dof_pos_limits: -0.0166
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1211
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.5997
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0633
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.79s
                        Total time: 425.41s
                               ETA: 629 mins 23.3 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 112183 steps/s (collection: 0.751s, learning 0.125s)
               Value function loss: 0.0893
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0281
       Mean episode rew_ang_vel_xy: -0.0514
          Mean episode rew_dof_acc: -0.2208
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1148
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5805
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0629
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.88s
                        Total time: 426.29s
                               ETA: 629 mins 32.5 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 124162 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.0911
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0121
       Mean episode rew_ang_vel_xy: -0.0521
          Mean episode rew_dof_acc: -0.2115
   Mean episode rew_dof_pos_limits: -0.0147
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5677
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0638
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.79s
                        Total time: 427.08s
                               ETA: 629 mins 34.2 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 114896 steps/s (collection: 0.720s, learning 0.136s)
               Value function loss: 0.0865
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9865
       Mean episode rew_ang_vel_xy: -0.0496
          Mean episode rew_dof_acc: -0.2086
   Mean episode rew_dof_pos_limits: -0.0143
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1220
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5561
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0595
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.86s
                        Total time: 427.94s
                               ETA: 629 mins 41.5 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 125568 steps/s (collection: 0.658s, learning 0.125s)
               Value function loss: 0.0907
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0187
       Mean episode rew_ang_vel_xy: -0.0516
          Mean episode rew_dof_acc: -0.2149
   Mean episode rew_dof_pos_limits: -0.0150
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1283
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.5711
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0584
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.78s
                        Total time: 428.72s
                               ETA: 629 mins 42.4 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 122408 steps/s (collection: 0.679s, learning 0.124s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0525
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.2206
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1181
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5933
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0579
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.80s
                        Total time: 429.52s
                               ETA: 629 mins 45.1 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 115293 steps/s (collection: 0.728s, learning 0.125s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0033
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.2080
   Mean episode rew_dof_pos_limits: -0.0156
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1164
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5613
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0558
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.85s
                        Total time: 430.38s
                               ETA: 629 mins 52.1 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 111818 steps/s (collection: 0.740s, learning 0.139s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0043
       Mean episode rew_ang_vel_xy: -0.0514
          Mean episode rew_dof_acc: -0.2108
   Mean episode rew_dof_pos_limits: -0.0149
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1224
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5622
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0553
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.88s
                        Total time: 431.26s
                               ETA: 630 mins 1.3 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 117035 steps/s (collection: 0.718s, learning 0.122s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0498
       Mean episode rew_ang_vel_xy: -0.0518
          Mean episode rew_dof_acc: -0.2153
   Mean episode rew_dof_pos_limits: -0.0145
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1178
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5903
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0553
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.84s
                        Total time: 432.10s
                               ETA: 630 mins 7.2 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 109166 steps/s (collection: 0.755s, learning 0.146s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0915
       Mean episode rew_ang_vel_xy: -0.0525
          Mean episode rew_dof_acc: -0.2210
   Mean episode rew_dof_pos_limits: -0.0143
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1238
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6148
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0418
        Mean episode terrain_level: 0.0566
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.90s
                        Total time: 433.00s
                               ETA: 630 mins 18.3 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 125037 steps/s (collection: 0.654s, learning 0.133s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0598
       Mean episode rew_ang_vel_xy: -0.0528
          Mean episode rew_dof_acc: -0.2206
   Mean episode rew_dof_pos_limits: -0.0155
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1243
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6005
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0588
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.79s
                        Total time: 433.78s
                               ETA: 630 mins 19.3 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 111836 steps/s (collection: 0.736s, learning 0.143s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0429
       Mean episode rew_ang_vel_xy: -0.0528
          Mean episode rew_dof_acc: -0.2173
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1249
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.5880
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0627
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.88s
                        Total time: 434.66s
                               ETA: 630 mins 28.5 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 121335 steps/s (collection: 0.689s, learning 0.121s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0429
       Mean episode rew_ang_vel_xy: -0.0522
          Mean episode rew_dof_acc: -0.2215
   Mean episode rew_dof_pos_limits: -0.0140
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1227
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5906
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0651
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.81s
                        Total time: 435.47s
                               ETA: 630 mins 31.6 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 118831 steps/s (collection: 0.704s, learning 0.123s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0425
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.2097
   Mean episode rew_dof_pos_limits: -0.0140
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1209
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.5858
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0630
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.83s
                        Total time: 436.30s
                               ETA: 630 mins 36.2 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 120951 steps/s (collection: 0.679s, learning 0.134s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0761
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.2189
   Mean episode rew_dof_pos_limits: -0.0141
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1162
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.6139
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0618
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.81s
                        Total time: 437.11s
                               ETA: 630 mins 39.6 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 111517 steps/s (collection: 0.758s, learning 0.124s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0932
       Mean episode rew_ang_vel_xy: -0.0532
          Mean episode rew_dof_acc: -0.2256
   Mean episode rew_dof_pos_limits: -0.0175
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1221
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.6151
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0455
        Mean episode terrain_level: 0.0614
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.88s
                        Total time: 437.99s
                               ETA: 630 mins 48.8 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 112940 steps/s (collection: 0.745s, learning 0.125s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1466
       Mean episode rew_ang_vel_xy: -0.0552
          Mean episode rew_dof_acc: -0.2283
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1255
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6520
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0425
        Mean episode terrain_level: 0.0609
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.87s
                        Total time: 438.86s
                               ETA: 630 mins 57.1 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 122610 steps/s (collection: 0.678s, learning 0.124s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0836
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.2197
   Mean episode rew_dof_pos_limits: -0.0147
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1189
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.6157
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0420
        Mean episode terrain_level: 0.0593
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.80s
                        Total time: 439.66s
                               ETA: 630 mins 59.4 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 118479 steps/s (collection: 0.708s, learning 0.121s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0797
       Mean episode rew_ang_vel_xy: -0.0522
          Mean episode rew_dof_acc: -0.2229
   Mean episode rew_dof_pos_limits: -0.0151
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1226
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6098
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0407
        Mean episode terrain_level: 0.0586
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.83s
                        Total time: 440.49s
                               ETA: 631 mins 4.1 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 117860 steps/s (collection: 0.707s, learning 0.127s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1153
       Mean episode rew_ang_vel_xy: -0.0544
          Mean episode rew_dof_acc: -0.2213
   Mean episode rew_dof_pos_limits: -0.0151
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1263
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.6280
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0610
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.83s
                        Total time: 441.33s
                               ETA: 631 mins 9.2 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 112900 steps/s (collection: 0.747s, learning 0.124s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.09
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0807
       Mean episode rew_ang_vel_xy: -0.0525
          Mean episode rew_dof_acc: -0.2198
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6075
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0637
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.87s
                        Total time: 442.20s
                               ETA: 631 mins 17.4 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 114470 steps/s (collection: 0.713s, learning 0.146s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.09
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0959
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.2266
   Mean episode rew_dof_pos_limits: -0.0137
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1223
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.6202
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0644
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.86s
                        Total time: 443.06s
                               ETA: 631 mins 24.5 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 121289 steps/s (collection: 0.689s, learning 0.121s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.09
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1060
       Mean episode rew_ang_vel_xy: -0.0506
          Mean episode rew_dof_acc: -0.2195
   Mean episode rew_dof_pos_limits: -0.0158
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1213
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6257
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0633
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.81s
                        Total time: 443.87s
                               ETA: 631 mins 27.5 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 116863 steps/s (collection: 0.716s, learning 0.126s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.09
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0741
       Mean episode rew_ang_vel_xy: -0.0511
          Mean episode rew_dof_acc: -0.2137
   Mean episode rew_dof_pos_limits: -0.0148
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1190
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.6069
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0606
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.84s
                        Total time: 444.71s
                               ETA: 631 mins 33.1 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 122470 steps/s (collection: 0.678s, learning 0.125s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0932
       Mean episode rew_ang_vel_xy: -0.0511
          Mean episode rew_dof_acc: -0.2210
   Mean episode rew_dof_pos_limits: -0.0148
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1240
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.6193
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0593
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.80s
                        Total time: 445.51s
                               ETA: 631 mins 35.4 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 120884 steps/s (collection: 0.679s, learning 0.134s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0699
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.2170
   Mean episode rew_dof_pos_limits: -0.0137
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1207
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.6003
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0602
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.81s
                        Total time: 446.33s
                               ETA: 631 mins 38.6 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 108642 steps/s (collection: 0.782s, learning 0.123s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1094
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.2238
   Mean episode rew_dof_pos_limits: -0.0157
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1253
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.6254
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0624
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.90s
                        Total time: 447.23s
                               ETA: 631 mins 49.5 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 119535 steps/s (collection: 0.699s, learning 0.124s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1184
       Mean episode rew_ang_vel_xy: -0.0515
          Mean episode rew_dof_acc: -0.2211
   Mean episode rew_dof_pos_limits: -0.0182
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1205
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6354
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0608
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.82s
                        Total time: 448.05s
                               ETA: 631 mins 53.4 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 118761 steps/s (collection: 0.705s, learning 0.123s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.11
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1832
       Mean episode rew_ang_vel_xy: -0.0521
          Mean episode rew_dof_acc: -0.2247
   Mean episode rew_dof_pos_limits: -0.0190
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1194
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.6751
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0507
        Mean episode terrain_level: 0.0574
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.83s
                        Total time: 448.88s
                               ETA: 631 mins 57.7 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 115700 steps/s (collection: 0.722s, learning 0.128s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.11
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1483
       Mean episode rew_ang_vel_xy: -0.0523
          Mean episode rew_dof_acc: -0.2213
   Mean episode rew_dof_pos_limits: -0.0180
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1185
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6514
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0559
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.85s
                        Total time: 449.73s
                               ETA: 632 mins 3.9 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 111778 steps/s (collection: 0.738s, learning 0.142s)
               Value function loss: 0.0865
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.11
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1548
       Mean episode rew_ang_vel_xy: -0.0539
          Mean episode rew_dof_acc: -0.2232
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1241
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.6588
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0548
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.88s
                        Total time: 450.61s
                               ETA: 632 mins 12.6 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 108572 steps/s (collection: 0.769s, learning 0.137s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.11
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1594
       Mean episode rew_ang_vel_xy: -0.0534
          Mean episode rew_dof_acc: -0.2352
   Mean episode rew_dof_pos_limits: -0.0159
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1262
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6582
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0563
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.91s
                        Total time: 451.51s
                               ETA: 632 mins 23.4 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 109624 steps/s (collection: 0.757s, learning 0.140s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1270
       Mean episode rew_ang_vel_xy: -0.0527
          Mean episode rew_dof_acc: -0.2193
   Mean episode rew_dof_pos_limits: -0.0177
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6388
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0454
        Mean episode terrain_level: 0.0557
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.90s
                        Total time: 452.41s
                               ETA: 632 mins 33.4 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 119854 steps/s (collection: 0.696s, learning 0.124s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1261
       Mean episode rew_ang_vel_xy: -0.0517
          Mean episode rew_dof_acc: -0.2183
   Mean episode rew_dof_pos_limits: -0.0176
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1231
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6406
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0479
        Mean episode terrain_level: 0.0573
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.82s
                        Total time: 453.23s
                               ETA: 632 mins 37.0 s

################################################################################
                      Learning iteration 590/50000                      

                       Computation: 113545 steps/s (collection: 0.743s, learning 0.123s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1571
       Mean episode rew_ang_vel_xy: -0.0544
          Mean episode rew_dof_acc: -0.2245
   Mean episode rew_dof_pos_limits: -0.0168
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1243
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6560
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0428
        Mean episode terrain_level: 0.0555
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.87s
                        Total time: 454.10s
                               ETA: 632 mins 44.4 s

################################################################################
                      Learning iteration 591/50000                      

                       Computation: 120399 steps/s (collection: 0.693s, learning 0.123s)
               Value function loss: 0.0865
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1132
       Mean episode rew_ang_vel_xy: -0.0500
          Mean episode rew_dof_acc: -0.2162
   Mean episode rew_dof_pos_limits: -0.0154
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1264
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.6221
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0560
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.82s
                        Total time: 454.91s
                               ETA: 632 mins 47.7 s

################################################################################
                      Learning iteration 592/50000                      

                       Computation: 111850 steps/s (collection: 0.755s, learning 0.124s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.13
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2280
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2285
   Mean episode rew_dof_pos_limits: -0.0184
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1258
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.6950
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0563
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.88s
                        Total time: 455.79s
                               ETA: 632 mins 56.1 s

################################################################################
                      Learning iteration 593/50000                      

                       Computation: 116037 steps/s (collection: 0.723s, learning 0.124s)
               Value function loss: 0.0880
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.13
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1479
       Mean episode rew_ang_vel_xy: -0.0531
          Mean episode rew_dof_acc: -0.2192
   Mean episode rew_dof_pos_limits: -0.0174
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1249
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6488
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0570
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.85s
                        Total time: 456.64s
                               ETA: 633 mins 1.9 s

################################################################################
                      Learning iteration 594/50000                      

                       Computation: 117521 steps/s (collection: 0.713s, learning 0.124s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.13
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2037
       Mean episode rew_ang_vel_xy: -0.0550
          Mean episode rew_dof_acc: -0.2353
   Mean episode rew_dof_pos_limits: -0.0168
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1311
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.6743
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0419
        Mean episode terrain_level: 0.0613
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.84s
                        Total time: 457.48s
                               ETA: 633 mins 6.7 s

################################################################################
                      Learning iteration 595/50000                      

                       Computation: 112381 steps/s (collection: 0.752s, learning 0.122s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.13
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1443
       Mean episode rew_ang_vel_xy: -0.0507
          Mean episode rew_dof_acc: -0.2177
   Mean episode rew_dof_pos_limits: -0.0149
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.6397
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0612
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.87s
                        Total time: 458.35s
                               ETA: 633 mins 14.7 s

################################################################################
                      Learning iteration 596/50000                      

                       Computation: 103526 steps/s (collection: 0.815s, learning 0.135s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1499
       Mean episode rew_ang_vel_xy: -0.0513
          Mean episode rew_dof_acc: -0.2218
   Mean episode rew_dof_pos_limits: -0.0163
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1257
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6403
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0482
        Mean episode terrain_level: 0.0608
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.95s
                        Total time: 459.30s
                               ETA: 633 mins 28.9 s

################################################################################
                      Learning iteration 597/50000                      

                       Computation: 103843 steps/s (collection: 0.822s, learning 0.125s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2074
       Mean episode rew_ang_vel_xy: -0.0532
          Mean episode rew_dof_acc: -0.2261
   Mean episode rew_dof_pos_limits: -0.0160
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1267
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6824
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0646
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.95s
                        Total time: 460.25s
                               ETA: 633 mins 42.8 s

################################################################################
                      Learning iteration 598/50000                      

                       Computation: 122299 steps/s (collection: 0.673s, learning 0.131s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1815
       Mean episode rew_ang_vel_xy: -0.0535
          Mean episode rew_dof_acc: -0.2241
   Mean episode rew_dof_pos_limits: -0.0197
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1293
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.6677
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0665
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.80s
                        Total time: 461.05s
                               ETA: 633 mins 44.8 s

################################################################################
                      Learning iteration 599/50000                      

                       Computation: 114949 steps/s (collection: 0.733s, learning 0.122s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2009
       Mean episode rew_ang_vel_xy: -0.0543
          Mean episode rew_dof_acc: -0.2299
   Mean episode rew_dof_pos_limits: -0.0154
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1227
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6766
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0683
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.86s
                        Total time: 461.91s
                               ETA: 633 mins 51.1 s

################################################################################
                      Learning iteration 600/50000                      

                       Computation: 123914 steps/s (collection: 0.669s, learning 0.125s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.15
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1591
       Mean episode rew_ang_vel_xy: -0.0519
          Mean episode rew_dof_acc: -0.2238
   Mean episode rew_dof_pos_limits: -0.0155
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1273
           Mean episode rew_no_fly: 0.0087
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6501
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0704
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.79s
                        Total time: 462.70s
                               ETA: 633 mins 52.2 s

################################################################################
                      Learning iteration 601/50000                      

                       Computation: 118027 steps/s (collection: 0.705s, learning 0.127s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2003
       Mean episode rew_ang_vel_xy: -0.0533
          Mean episode rew_dof_acc: -0.2276
   Mean episode rew_dof_pos_limits: -0.0154
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1297
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6781
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0687
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.83s
                        Total time: 463.53s
                               ETA: 633 mins 56.6 s

################################################################################
                      Learning iteration 602/50000                      

                       Computation: 117739 steps/s (collection: 0.686s, learning 0.149s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1909
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.2235
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1234
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6677
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0654
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.83s
                        Total time: 464.37s
                               ETA: 634 mins 1.2 s

################################################################################
                      Learning iteration 603/50000                      

                       Computation: 115199 steps/s (collection: 0.729s, learning 0.125s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2507
       Mean episode rew_ang_vel_xy: -0.0537
          Mean episode rew_dof_acc: -0.2358
   Mean episode rew_dof_pos_limits: -0.0189
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1207
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.7036
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.85s
                        Total time: 465.22s
                               ETA: 634 mins 7.2 s

################################################################################
                      Learning iteration 604/50000                      

                       Computation: 110945 steps/s (collection: 0.762s, learning 0.124s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1959
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2204
   Mean episode rew_dof_pos_limits: -0.0175
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.6742
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0492
        Mean episode terrain_level: 0.0700
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.89s
                        Total time: 466.11s
                               ETA: 634 mins 15.9 s

################################################################################
                      Learning iteration 605/50000                      

                       Computation: 114462 steps/s (collection: 0.717s, learning 0.142s)
               Value function loss: 0.0895
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1822
       Mean episode rew_ang_vel_xy: -0.0523
          Mean episode rew_dof_acc: -0.2208
   Mean episode rew_dof_pos_limits: -0.0160
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1255
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6670
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0424
        Mean episode terrain_level: 0.0696
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.86s
                        Total time: 466.97s
                               ETA: 634 mins 22.3 s

################################################################################
                      Learning iteration 606/50000                      

                       Computation: 112586 steps/s (collection: 0.735s, learning 0.139s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1995
       Mean episode rew_ang_vel_xy: -0.0512
          Mean episode rew_dof_acc: -0.2228
   Mean episode rew_dof_pos_limits: -0.0168
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1206
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.6733
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0669
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.87s
                        Total time: 467.84s
                               ETA: 634 mins 29.9 s

################################################################################
                      Learning iteration 607/50000                      

                       Computation: 109708 steps/s (collection: 0.734s, learning 0.162s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2801
       Mean episode rew_ang_vel_xy: -0.0536
          Mean episode rew_dof_acc: -0.2321
   Mean episode rew_dof_pos_limits: -0.0190
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1209
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7280
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0620
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.90s
                        Total time: 468.74s
                               ETA: 634 mins 39.3 s

################################################################################
                      Learning iteration 608/50000                      

                       Computation: 118869 steps/s (collection: 0.705s, learning 0.122s)
               Value function loss: 0.0911
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2459
       Mean episode rew_ang_vel_xy: -0.0528
          Mean episode rew_dof_acc: -0.2221
   Mean episode rew_dof_pos_limits: -0.0180
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1228
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7077
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0636
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.83s
                        Total time: 469.56s
                               ETA: 634 mins 43.1 s

################################################################################
                      Learning iteration 609/50000                      

                       Computation: 118725 steps/s (collection: 0.699s, learning 0.129s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2361
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2292
   Mean episode rew_dof_pos_limits: -0.0168
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.6896
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0618
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.83s
                        Total time: 470.39s
                               ETA: 634 mins 46.9 s

################################################################################
                      Learning iteration 610/50000                      

                       Computation: 127061 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2299
       Mean episode rew_ang_vel_xy: -0.0532
          Mean episode rew_dof_acc: -0.2248
   Mean episode rew_dof_pos_limits: -0.0196
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1264
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.6947
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0478
        Mean episode terrain_level: 0.0622
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.77s
                        Total time: 471.16s
                               ETA: 634 mins 46.4 s

################################################################################
                      Learning iteration 611/50000                      

                       Computation: 117920 steps/s (collection: 0.711s, learning 0.123s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2186
       Mean episode rew_ang_vel_xy: -0.0533
          Mean episode rew_dof_acc: -0.2208
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1199
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6811
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0612
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.83s
                        Total time: 472.00s
                               ETA: 634 mins 50.7 s

################################################################################
                      Learning iteration 612/50000                      

                       Computation: 125642 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2642
       Mean episode rew_ang_vel_xy: -0.0541
          Mean episode rew_dof_acc: -0.2311
   Mean episode rew_dof_pos_limits: -0.0176
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1242
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7070
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0609
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.78s
                        Total time: 472.78s
                               ETA: 634 mins 50.8 s

################################################################################
                      Learning iteration 613/50000                      

                       Computation: 125253 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2584
       Mean episode rew_ang_vel_xy: -0.0534
          Mean episode rew_dof_acc: -0.2342
   Mean episode rew_dof_pos_limits: -0.0154
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.7115
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0601
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.78s
                        Total time: 473.56s
                               ETA: 634 mins 51.1 s

################################################################################
                      Learning iteration 614/50000                      

                       Computation: 124538 steps/s (collection: 0.668s, learning 0.122s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2328
       Mean episode rew_ang_vel_xy: -0.0518
          Mean episode rew_dof_acc: -0.2210
   Mean episode rew_dof_pos_limits: -0.0171
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1256
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.6955
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0608
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.79s
                        Total time: 474.35s
                               ETA: 634 mins 51.8 s

################################################################################
                      Learning iteration 615/50000                      

                       Computation: 110664 steps/s (collection: 0.764s, learning 0.124s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2144
       Mean episode rew_ang_vel_xy: -0.0529
          Mean episode rew_dof_acc: -0.2176
   Mean episode rew_dof_pos_limits: -0.0160
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1236
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6800
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0606
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.89s
                        Total time: 475.24s
                               ETA: 635 mins 0.4 s

################################################################################
                      Learning iteration 616/50000                      

                       Computation: 121620 steps/s (collection: 0.663s, learning 0.145s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3325
       Mean episode rew_ang_vel_xy: -0.0549
          Mean episode rew_dof_acc: -0.2306
   Mean episode rew_dof_pos_limits: -0.0206
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7562
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0614
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.81s
                        Total time: 476.05s
                               ETA: 635 mins 2.6 s

################################################################################
                      Learning iteration 617/50000                      

                       Computation: 121551 steps/s (collection: 0.686s, learning 0.123s)
               Value function loss: 0.0907
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2251
       Mean episode rew_ang_vel_xy: -0.0528
          Mean episode rew_dof_acc: -0.2278
   Mean episode rew_dof_pos_limits: -0.0172
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.6864
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0641
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.81s
                        Total time: 476.86s
                               ETA: 635 mins 4.8 s

################################################################################
                      Learning iteration 618/50000                      

                       Computation: 122017 steps/s (collection: 0.683s, learning 0.122s)
               Value function loss: 0.0910
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2466
       Mean episode rew_ang_vel_xy: -0.0529
          Mean episode rew_dof_acc: -0.2247
   Mean episode rew_dof_pos_limits: -0.0175
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1186
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.7013
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0456
        Mean episode terrain_level: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.81s
                        Total time: 477.67s
                               ETA: 635 mins 6.7 s

################################################################################
                      Learning iteration 619/50000                      

                       Computation: 122418 steps/s (collection: 0.678s, learning 0.125s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2043
       Mean episode rew_ang_vel_xy: -0.0521
          Mean episode rew_dof_acc: -0.2233
   Mean episode rew_dof_pos_limits: -0.0151
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1235
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.6795
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0666
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.80s
                        Total time: 478.47s
                               ETA: 635 mins 8.4 s

################################################################################
                      Learning iteration 620/50000                      

                       Computation: 119013 steps/s (collection: 0.686s, learning 0.140s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2553
       Mean episode rew_ang_vel_xy: -0.0535
          Mean episode rew_dof_acc: -0.2295
   Mean episode rew_dof_pos_limits: -0.0150
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1268
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.7072
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0676
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.83s
                        Total time: 479.29s
                               ETA: 635 mins 12.0 s

################################################################################
                      Learning iteration 621/50000                      

                       Computation: 107693 steps/s (collection: 0.771s, learning 0.142s)
               Value function loss: 0.0929
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2167
       Mean episode rew_ang_vel_xy: -0.0513
          Mean episode rew_dof_acc: -0.2158
   Mean episode rew_dof_pos_limits: -0.0163
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.6877
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0657
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.91s
                        Total time: 480.21s
                               ETA: 635 mins 22.4 s

################################################################################
                      Learning iteration 622/50000                      

                       Computation: 110957 steps/s (collection: 0.757s, learning 0.129s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2811
       Mean episode rew_ang_vel_xy: -0.0555
          Mean episode rew_dof_acc: -0.2336
   Mean episode rew_dof_pos_limits: -0.0165
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1340
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.7126
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0668
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.89s
                        Total time: 481.09s
                               ETA: 635 mins 30.7 s

################################################################################
                      Learning iteration 623/50000                      

                       Computation: 122394 steps/s (collection: 0.679s, learning 0.124s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2559
       Mean episode rew_ang_vel_xy: -0.0535
          Mean episode rew_dof_acc: -0.2239
   Mean episode rew_dof_pos_limits: -0.0175
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7114
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0459
        Mean episode terrain_level: 0.0662
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.80s
                        Total time: 481.90s
                               ETA: 635 mins 32.3 s

################################################################################
                      Learning iteration 624/50000                      

                       Computation: 115403 steps/s (collection: 0.727s, learning 0.125s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2475
       Mean episode rew_ang_vel_xy: -0.0530
          Mean episode rew_dof_acc: -0.2287
   Mean episode rew_dof_pos_limits: -0.0163
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1306
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.7082
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0663
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.85s
                        Total time: 482.75s
                               ETA: 635 mins 37.8 s

################################################################################
                      Learning iteration 625/50000                      

                       Computation: 113998 steps/s (collection: 0.713s, learning 0.149s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2844
       Mean episode rew_ang_vel_xy: -0.0536
          Mean episode rew_dof_acc: -0.2290
   Mean episode rew_dof_pos_limits: -0.0169
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1220
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.7292
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0655
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.86s
                        Total time: 483.61s
                               ETA: 635 mins 44.2 s

################################################################################
                      Learning iteration 626/50000                      

                       Computation: 117421 steps/s (collection: 0.701s, learning 0.137s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2952
       Mean episode rew_ang_vel_xy: -0.0553
          Mean episode rew_dof_acc: -0.2341
   Mean episode rew_dof_pos_limits: -0.0170
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1272
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7353
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0663
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.84s
                        Total time: 484.45s
                               ETA: 635 mins 48.5 s

################################################################################
                      Learning iteration 627/50000                      

                       Computation: 116916 steps/s (collection: 0.699s, learning 0.141s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2365
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.2221
   Mean episode rew_dof_pos_limits: -0.0163
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1212
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.6895
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0428
        Mean episode terrain_level: 0.0681
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.84s
                        Total time: 485.29s
                               ETA: 635 mins 53.1 s

################################################################################
                      Learning iteration 628/50000                      

                       Computation: 117630 steps/s (collection: 0.695s, learning 0.140s)
               Value function loss: 0.0943
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2596
       Mean episode rew_ang_vel_xy: -0.0526
          Mean episode rew_dof_acc: -0.2294
   Mean episode rew_dof_pos_limits: -0.0157
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1269
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.7129
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0698
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.84s
                        Total time: 486.12s
                               ETA: 635 mins 57.2 s

################################################################################
                      Learning iteration 629/50000                      

                       Computation: 102886 steps/s (collection: 0.823s, learning 0.132s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2771
       Mean episode rew_ang_vel_xy: -0.0533
          Mean episode rew_dof_acc: -0.2228
   Mean episode rew_dof_pos_limits: -0.0182
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1256
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.7206
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0685
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.96s
                        Total time: 487.08s
                               ETA: 636 mins 10.8 s

################################################################################
                      Learning iteration 630/50000                      

                       Computation: 123288 steps/s (collection: 0.675s, learning 0.122s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2935
       Mean episode rew_ang_vel_xy: -0.0541
          Mean episode rew_dof_acc: -0.2326
   Mean episode rew_dof_pos_limits: -0.0166
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1275
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.7353
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0446
        Mean episode terrain_level: 0.0681
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.80s
                        Total time: 487.88s
                               ETA: 636 mins 11.9 s

################################################################################
                      Learning iteration 631/50000                      

                       Computation: 126807 steps/s (collection: 0.651s, learning 0.124s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.21
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3342
       Mean episode rew_ang_vel_xy: -0.0545
          Mean episode rew_dof_acc: -0.2331
   Mean episode rew_dof_pos_limits: -0.0198
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.7512
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0699
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.78s
                        Total time: 488.65s
                               ETA: 636 mins 11.3 s

################################################################################
                      Learning iteration 632/50000                      

                       Computation: 117965 steps/s (collection: 0.702s, learning 0.131s)
               Value function loss: 0.1004
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.21
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3569
       Mean episode rew_ang_vel_xy: -0.0555
          Mean episode rew_dof_acc: -0.2343
   Mean episode rew_dof_pos_limits: -0.0186
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1265
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.7683
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0478
        Mean episode terrain_level: 0.0689
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.83s
                        Total time: 489.49s
                               ETA: 636 mins 15.2 s

################################################################################
                      Learning iteration 633/50000                      

                       Computation: 122630 steps/s (collection: 0.659s, learning 0.142s)
               Value function loss: 0.0985
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.21
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3711
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.2398
   Mean episode rew_dof_pos_limits: -0.0194
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1258
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7831
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0541
        Mean episode terrain_level: 0.0713
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.80s
                        Total time: 490.29s
                               ETA: 636 mins 16.6 s

################################################################################
                      Learning iteration 634/50000                      

                       Computation: 120566 steps/s (collection: 0.679s, learning 0.137s)
               Value function loss: 0.1000
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.21
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3051
       Mean episode rew_ang_vel_xy: -0.0536
          Mean episode rew_dof_acc: -0.2315
   Mean episode rew_dof_pos_limits: -0.0174
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1266
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7459
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1990
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0721
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.82s
                        Total time: 491.10s
                               ETA: 636 mins 19.1 s

################################################################################
                      Learning iteration 635/50000                      

                       Computation: 124725 steps/s (collection: 0.667s, learning 0.122s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3211
       Mean episode rew_ang_vel_xy: -0.0536
          Mean episode rew_dof_acc: -0.2295
   Mean episode rew_dof_pos_limits: -0.0174
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7470
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0699
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.79s
                        Total time: 491.89s
                               ETA: 636 mins 19.5 s

################################################################################
                      Learning iteration 636/50000                      

                       Computation: 124854 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.1019
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2712
       Mean episode rew_ang_vel_xy: -0.0547
          Mean episode rew_dof_acc: -0.2262
   Mean episode rew_dof_pos_limits: -0.0171
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1261
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7172
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0459
        Mean episode terrain_level: 0.0680
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.79s
                        Total time: 492.68s
                               ETA: 636 mins 19.8 s

################################################################################
                      Learning iteration 637/50000                      

                       Computation: 125955 steps/s (collection: 0.657s, learning 0.124s)
               Value function loss: 0.0959
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3112
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2286
   Mean episode rew_dof_pos_limits: -0.0170
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1211
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7385
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0687
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.78s
                        Total time: 493.46s
                               ETA: 636 mins 19.6 s

################################################################################
                      Learning iteration 638/50000                      

                       Computation: 115789 steps/s (collection: 0.707s, learning 0.142s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3509
       Mean episode rew_ang_vel_xy: -0.0559
          Mean episode rew_dof_acc: -0.2376
   Mean episode rew_dof_pos_limits: -0.0163
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7611
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0692
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.85s
                        Total time: 494.31s
                               ETA: 636 mins 24.6 s

################################################################################
                      Learning iteration 639/50000                      

                       Computation: 126293 steps/s (collection: 0.654s, learning 0.125s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3181
       Mean episode rew_ang_vel_xy: -0.0537
          Mean episode rew_dof_acc: -0.2253
   Mean episode rew_dof_pos_limits: -0.0182
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7490
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0703
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.78s
                        Total time: 495.09s
                               ETA: 636 mins 24.2 s

################################################################################
                      Learning iteration 640/50000                      

                       Computation: 114814 steps/s (collection: 0.726s, learning 0.130s)
               Value function loss: 0.0975
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3143
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2276
   Mean episode rew_dof_pos_limits: -0.0176
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1272
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7442
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0688
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.86s
                        Total time: 495.94s
                               ETA: 636 mins 29.8 s

################################################################################
                      Learning iteration 641/50000                      

                       Computation: 108882 steps/s (collection: 0.774s, learning 0.129s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2990
       Mean episode rew_ang_vel_xy: -0.0527
          Mean episode rew_dof_acc: -0.2220
   Mean episode rew_dof_pos_limits: -0.0189
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1213
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7357
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0697
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.90s
                        Total time: 496.84s
                               ETA: 636 mins 39.0 s

################################################################################
                      Learning iteration 642/50000                      

                       Computation: 119556 steps/s (collection: 0.690s, learning 0.133s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3451
       Mean episode rew_ang_vel_xy: -0.0542
          Mean episode rew_dof_acc: -0.2275
   Mean episode rew_dof_pos_limits: -0.0183
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1273
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7607
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0697
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.82s
                        Total time: 497.67s
                               ETA: 636 mins 41.9 s

################################################################################
                      Learning iteration 643/50000                      

                       Computation: 107053 steps/s (collection: 0.777s, learning 0.141s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3674
       Mean episode rew_ang_vel_xy: -0.0548
          Mean episode rew_dof_acc: -0.2365
   Mean episode rew_dof_pos_limits: -0.0195
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7710
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0701
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.92s
                        Total time: 498.59s
                               ETA: 636 mins 52.2 s

################################################################################
                      Learning iteration 644/50000                      

                       Computation: 118150 steps/s (collection: 0.692s, learning 0.140s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.23
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3485
       Mean episode rew_ang_vel_xy: -0.0548
          Mean episode rew_dof_acc: -0.2301
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1241
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7538
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0674
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.83s
                        Total time: 499.42s
                               ETA: 636 mins 55.9 s

################################################################################
                      Learning iteration 645/50000                      

                       Computation: 123794 steps/s (collection: 0.659s, learning 0.135s)
               Value function loss: 0.0956
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3311
       Mean episode rew_ang_vel_xy: -0.0543
          Mean episode rew_dof_acc: -0.2270
   Mean episode rew_dof_pos_limits: -0.0181
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1208
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7563
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0501
        Mean episode terrain_level: 0.0655
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.79s
                        Total time: 500.21s
                               ETA: 636 mins 56.6 s

################################################################################
                      Learning iteration 646/50000                      

                       Computation: 122018 steps/s (collection: 0.683s, learning 0.123s)
               Value function loss: 0.0996
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.24
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3501
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.2232
   Mean episode rew_dof_pos_limits: -0.0236
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1225
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.7623
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0575
        Mean episode terrain_level: 0.0646
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.81s
                        Total time: 501.02s
                               ETA: 636 mins 58.2 s

################################################################################
                      Learning iteration 647/50000                      

                       Computation: 118412 steps/s (collection: 0.699s, learning 0.131s)
               Value function loss: 0.1006
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3975
       Mean episode rew_ang_vel_xy: -0.0557
          Mean episode rew_dof_acc: -0.2312
   Mean episode rew_dof_pos_limits: -0.0221
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1246
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.7892
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0630
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.83s
                        Total time: 501.85s
                               ETA: 637 mins 1.7 s

################################################################################
                      Learning iteration 648/50000                      

                       Computation: 105013 steps/s (collection: 0.812s, learning 0.124s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3651
       Mean episode rew_ang_vel_xy: -0.0551
          Mean episode rew_dof_acc: -0.2305
   Mean episode rew_dof_pos_limits: -0.0182
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1317
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.7645
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0639
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.94s
                        Total time: 502.78s
                               ETA: 637 mins 13.2 s

################################################################################
                      Learning iteration 649/50000                      

                       Computation: 116882 steps/s (collection: 0.719s, learning 0.122s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3748
       Mean episode rew_ang_vel_xy: -0.0552
          Mean episode rew_dof_acc: -0.2341
   Mean episode rew_dof_pos_limits: -0.0189
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1219
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.7747
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0470
        Mean episode terrain_level: 0.0639
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.84s
                        Total time: 503.62s
                               ETA: 637 mins 17.5 s

################################################################################
                      Learning iteration 650/50000                      

                       Computation: 121166 steps/s (collection: 0.674s, learning 0.137s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4239
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.2374
   Mean episode rew_dof_pos_limits: -0.0179
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1255
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.8030
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0643
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.81s
                        Total time: 504.44s
                               ETA: 637 mins 19.5 s

################################################################################
                      Learning iteration 651/50000                      

                       Computation: 110041 steps/s (collection: 0.770s, learning 0.123s)
               Value function loss: 0.0932
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4232
       Mean episode rew_ang_vel_xy: -0.0543
          Mean episode rew_dof_acc: -0.2372
   Mean episode rew_dof_pos_limits: -0.0215
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1257
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.8037
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0521
        Mean episode terrain_level: 0.0677
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.89s
                        Total time: 505.33s
                               ETA: 637 mins 27.7 s

################################################################################
                      Learning iteration 652/50000                      

                       Computation: 124686 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4032
       Mean episode rew_ang_vel_xy: -0.0542
          Mean episode rew_dof_acc: -0.2329
   Mean episode rew_dof_pos_limits: -0.0192
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1240
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.7932
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0701
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.79s
                        Total time: 506.12s
                               ETA: 637 mins 27.9 s

################################################################################
                      Learning iteration 653/50000                      

                       Computation: 112510 steps/s (collection: 0.744s, learning 0.129s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2860
       Mean episode rew_ang_vel_xy: -0.0539
          Mean episode rew_dof_acc: -0.2201
   Mean episode rew_dof_pos_limits: -0.0171
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.7185
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0691
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.87s
                        Total time: 506.99s
                               ETA: 637 mins 34.6 s

################################################################################
                      Learning iteration 654/50000                      

                       Computation: 123050 steps/s (collection: 0.676s, learning 0.123s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3740
       Mean episode rew_ang_vel_xy: -0.0543
          Mean episode rew_dof_acc: -0.2278
   Mean episode rew_dof_pos_limits: -0.0189
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1214
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.7663
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0687
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.80s
                        Total time: 507.79s
                               ETA: 637 mins 35.6 s

################################################################################
                      Learning iteration 655/50000                      

                       Computation: 119613 steps/s (collection: 0.699s, learning 0.123s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3908
       Mean episode rew_ang_vel_xy: -0.0562
          Mean episode rew_dof_acc: -0.2242
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1266
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.7741
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0691
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.82s
                        Total time: 508.61s
                               ETA: 637 mins 38.3 s

################################################################################
                      Learning iteration 656/50000                      

                       Computation: 126559 steps/s (collection: 0.653s, learning 0.124s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3989
       Mean episode rew_ang_vel_xy: -0.0556
          Mean episode rew_dof_acc: -0.2365
   Mean episode rew_dof_pos_limits: -0.0184
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1266
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.7762
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0670
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.78s
                        Total time: 509.39s
                               ETA: 637 mins 37.6 s

################################################################################
                      Learning iteration 657/50000                      

                       Computation: 113288 steps/s (collection: 0.731s, learning 0.137s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4285
       Mean episode rew_ang_vel_xy: -0.0574
          Mean episode rew_dof_acc: -0.2375
   Mean episode rew_dof_pos_limits: -0.0176
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1273
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.7995
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0665
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.87s
                        Total time: 510.26s
                               ETA: 637 mins 43.8 s

################################################################################
                      Learning iteration 658/50000                      

                       Computation: 117814 steps/s (collection: 0.707s, learning 0.127s)
               Value function loss: 0.0957
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4084
       Mean episode rew_ang_vel_xy: -0.0573
          Mean episode rew_dof_acc: -0.2401
   Mean episode rew_dof_pos_limits: -0.0186
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1334
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.7943
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0651
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.83s
                        Total time: 511.09s
                               ETA: 637 mins 47.4 s

################################################################################
                      Learning iteration 659/50000                      

                       Computation: 118940 steps/s (collection: 0.695s, learning 0.132s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4725
       Mean episode rew_ang_vel_xy: -0.0568
          Mean episode rew_dof_acc: -0.2431
   Mean episode rew_dof_pos_limits: -0.0198
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.8292
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0555
        Mean episode terrain_level: 0.0635
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.83s
                        Total time: 511.92s
                               ETA: 637 mins 50.5 s

################################################################################
                      Learning iteration 660/50000                      

                       Computation: 110277 steps/s (collection: 0.747s, learning 0.144s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4454
       Mean episode rew_ang_vel_xy: -0.0570
          Mean episode rew_dof_acc: -0.2445
   Mean episode rew_dof_pos_limits: -0.0200
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1244
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8196
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0647
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.89s
                        Total time: 512.81s
                               ETA: 637 mins 58.3 s

################################################################################
                      Learning iteration 661/50000                      

                       Computation: 121515 steps/s (collection: 0.686s, learning 0.123s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4892
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2442
   Mean episode rew_dof_pos_limits: -0.0203
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1267
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8382
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0656
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.81s
                        Total time: 513.62s
                               ETA: 638 mins 0.0 s

################################################################################
                      Learning iteration 662/50000                      

                       Computation: 108840 steps/s (collection: 0.775s, learning 0.128s)
               Value function loss: 0.0971
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4182
       Mean episode rew_ang_vel_xy: -0.0552
          Mean episode rew_dof_acc: -0.2277
   Mean episode rew_dof_pos_limits: -0.0215
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1238
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.8025
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0541
        Mean episode terrain_level: 0.0663
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.90s
                        Total time: 514.52s
                               ETA: 638 mins 8.7 s

################################################################################
                      Learning iteration 663/50000                      

                       Computation: 113483 steps/s (collection: 0.731s, learning 0.135s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3973
       Mean episode rew_ang_vel_xy: -0.0558
          Mean episode rew_dof_acc: -0.2334
   Mean episode rew_dof_pos_limits: -0.0184
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1292
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.7840
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0671
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.87s
                        Total time: 515.39s
                               ETA: 638 mins 14.6 s

################################################################################
                      Learning iteration 664/50000                      

                       Computation: 123361 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.0990
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4577
       Mean episode rew_ang_vel_xy: -0.0560
          Mean episode rew_dof_acc: -0.2333
   Mean episode rew_dof_pos_limits: -0.0209
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1258
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8182
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0570
        Mean episode terrain_level: 0.0640
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.80s
                        Total time: 516.18s
                               ETA: 638 mins 15.4 s

################################################################################
                      Learning iteration 665/50000                      

                       Computation: 119490 steps/s (collection: 0.701s, learning 0.122s)
               Value function loss: 0.1009
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4492
       Mean episode rew_ang_vel_xy: -0.0563
          Mean episode rew_dof_acc: -0.2356
   Mean episode rew_dof_pos_limits: -0.0217
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1266
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8097
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0556
        Mean episode terrain_level: 0.0650
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.82s
                        Total time: 517.01s
                               ETA: 638 mins 18.1 s

################################################################################
                      Learning iteration 666/50000                      

                       Computation: 122226 steps/s (collection: 0.666s, learning 0.138s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.28
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4107
       Mean episode rew_ang_vel_xy: -0.0555
          Mean episode rew_dof_acc: -0.2321
   Mean episode rew_dof_pos_limits: -0.0193
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1234
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7836
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0676
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 0.80s
                        Total time: 517.81s
                               ETA: 638 mins 19.4 s

################################################################################
                      Learning iteration 667/50000                      

                       Computation: 121677 steps/s (collection: 0.666s, learning 0.142s)
               Value function loss: 0.0957
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.28
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5069
       Mean episode rew_ang_vel_xy: -0.0564
          Mean episode rew_dof_acc: -0.2393
   Mean episode rew_dof_pos_limits: -0.0185
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.8450
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0687
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 0.81s
                        Total time: 518.62s
                               ETA: 638 mins 20.9 s

################################################################################
                      Learning iteration 668/50000                      

                       Computation: 121352 steps/s (collection: 0.687s, learning 0.123s)
               Value function loss: 0.0953
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4504
       Mean episode rew_ang_vel_xy: -0.0562
          Mean episode rew_dof_acc: -0.2418
   Mean episode rew_dof_pos_limits: -0.0194
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1276
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.8109
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0517
        Mean episode terrain_level: 0.0693
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 0.81s
                        Total time: 519.43s
                               ETA: 638 mins 22.6 s

################################################################################
                      Learning iteration 669/50000                      

                       Computation: 115291 steps/s (collection: 0.728s, learning 0.124s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4187
       Mean episode rew_ang_vel_xy: -0.0549
          Mean episode rew_dof_acc: -0.2299
   Mean episode rew_dof_pos_limits: -0.0185
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1254
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.7914
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0715
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 0.85s
                        Total time: 520.28s
                               ETA: 638 mins 27.5 s

################################################################################
                      Learning iteration 670/50000                      

                       Computation: 106119 steps/s (collection: 0.785s, learning 0.141s)
               Value function loss: 0.0944
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5103
       Mean episode rew_ang_vel_xy: -0.0580
          Mean episode rew_dof_acc: -0.2483
   Mean episode rew_dof_pos_limits: -0.0178
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1259
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.8428
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0716
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 0.93s
                        Total time: 521.21s
                               ETA: 638 mins 37.7 s

################################################################################
                      Learning iteration 671/50000                      

                       Computation: 117916 steps/s (collection: 0.712s, learning 0.121s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5188
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2462
   Mean episode rew_dof_pos_limits: -0.0211
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1283
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8485
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0541
        Mean episode terrain_level: 0.0743
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 0.83s
                        Total time: 522.04s
                               ETA: 638 mins 41.1 s

################################################################################
                      Learning iteration 672/50000                      

                       Computation: 106394 steps/s (collection: 0.794s, learning 0.130s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5010
       Mean episode rew_ang_vel_xy: -0.0562
          Mean episode rew_dof_acc: -0.2369
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1300
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.8426
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0721
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 0.92s
                        Total time: 522.97s
                               ETA: 638 mins 51.1 s

################################################################################
                      Learning iteration 673/50000                      

                       Computation: 115629 steps/s (collection: 0.726s, learning 0.124s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5578
       Mean episode rew_ang_vel_xy: -0.0577
          Mean episode rew_dof_acc: -0.2396
   Mean episode rew_dof_pos_limits: -0.0225
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1318
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.8799
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0606
        Mean episode terrain_level: 0.0708
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 0.85s
                        Total time: 523.82s
                               ETA: 638 mins 55.7 s

################################################################################
                      Learning iteration 674/50000                      

                       Computation: 123073 steps/s (collection: 0.663s, learning 0.136s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5125
       Mean episode rew_ang_vel_xy: -0.0568
          Mean episode rew_dof_acc: -0.2472
   Mean episode rew_dof_pos_limits: -0.0172
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1245
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.8418
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0707
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 0.80s
                        Total time: 524.61s
                               ETA: 638 mins 56.5 s

################################################################################
                      Learning iteration 675/50000                      

                       Computation: 115367 steps/s (collection: 0.730s, learning 0.123s)
               Value function loss: 0.0976
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.30
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4079
       Mean episode rew_ang_vel_xy: -0.0554
          Mean episode rew_dof_acc: -0.2273
   Mean episode rew_dof_pos_limits: -0.0198
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1278
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.7886
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0739
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 0.85s
                        Total time: 525.47s
                               ETA: 639 mins 1.2 s

################################################################################
                      Learning iteration 676/50000                      

                       Computation: 121845 steps/s (collection: 0.684s, learning 0.123s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.30
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5571
       Mean episode rew_ang_vel_xy: -0.0582
          Mean episode rew_dof_acc: -0.2462
   Mean episode rew_dof_pos_limits: -0.0239
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1306
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.8738
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0758
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.81s
                        Total time: 526.27s
                               ETA: 639 mins 2.5 s

################################################################################
                      Learning iteration 677/50000                      

                       Computation: 114552 steps/s (collection: 0.730s, learning 0.129s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5242
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2451
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1299
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8543
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0761
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.86s
                        Total time: 527.13s
                               ETA: 639 mins 7.6 s

################################################################################
                      Learning iteration 678/50000                      

                       Computation: 127087 steps/s (collection: 0.651s, learning 0.122s)
               Value function loss: 0.0964
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5240
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.2376
   Mean episode rew_dof_pos_limits: -0.0172
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1278
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.8497
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1988
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0756
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.77s
                        Total time: 527.90s
                               ETA: 639 mins 6.6 s

################################################################################
                      Learning iteration 679/50000                      

                       Computation: 112819 steps/s (collection: 0.748s, learning 0.123s)
               Value function loss: 0.0991
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.31
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5656
       Mean episode rew_ang_vel_xy: -0.0572
          Mean episode rew_dof_acc: -0.2482
   Mean episode rew_dof_pos_limits: -0.0231
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1302
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.8788
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0751
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.87s
                        Total time: 528.78s
                               ETA: 639 mins 12.6 s

################################################################################
                      Learning iteration 680/50000                      

                       Computation: 115110 steps/s (collection: 0.721s, learning 0.133s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.31
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6168
       Mean episode rew_ang_vel_xy: -0.0591
          Mean episode rew_dof_acc: -0.2516
   Mean episode rew_dof_pos_limits: -0.0206
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1272
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.9071
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0588
        Mean episode terrain_level: 0.0732
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.85s
                        Total time: 529.63s
                               ETA: 639 mins 17.4 s

################################################################################
                      Learning iteration 681/50000                      

                       Computation: 125412 steps/s (collection: 0.662s, learning 0.122s)
               Value function loss: 0.1003
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5816
       Mean episode rew_ang_vel_xy: -0.0582
          Mean episode rew_dof_acc: -0.2461
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1274
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.8869
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0749
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.78s
                        Total time: 530.41s
                               ETA: 639 mins 17.0 s

################################################################################
                      Learning iteration 682/50000                      

                       Computation: 111324 steps/s (collection: 0.742s, learning 0.141s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5156
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.2370
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1255
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.8369
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0542
        Mean episode terrain_level: 0.0727
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.88s
                        Total time: 531.30s
                               ETA: 639 mins 23.8 s

################################################################################
                      Learning iteration 683/50000                      

                       Computation: 112432 steps/s (collection: 0.735s, learning 0.140s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5628
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.2431
   Mean episode rew_dof_pos_limits: -0.0216
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1294
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.8754
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0564
        Mean episode terrain_level: 0.0730
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.87s
                        Total time: 532.17s
                               ETA: 639 mins 30.0 s

################################################################################
                      Learning iteration 684/50000                      

                       Computation: 108180 steps/s (collection: 0.764s, learning 0.145s)
               Value function loss: 0.0907
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5245
       Mean episode rew_ang_vel_xy: -0.0574
          Mean episode rew_dof_acc: -0.2346
   Mean episode rew_dof_pos_limits: -0.0211
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1269
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.8549
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0584
        Mean episode terrain_level: 0.0725
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.91s
                        Total time: 533.08s
                               ETA: 639 mins 38.7 s

################################################################################
                      Learning iteration 685/50000                      

                       Computation: 121530 steps/s (collection: 0.687s, learning 0.122s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6214
       Mean episode rew_ang_vel_xy: -0.0584
          Mean episode rew_dof_acc: -0.2517
   Mean episode rew_dof_pos_limits: -0.0214
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1247
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9038
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0542
        Mean episode terrain_level: 0.0719
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.81s
                        Total time: 533.89s
                               ETA: 639 mins 40.1 s

################################################################################
                      Learning iteration 686/50000                      

                       Computation: 116549 steps/s (collection: 0.721s, learning 0.122s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6147
       Mean episode rew_ang_vel_xy: -0.0584
          Mean episode rew_dof_acc: -0.2480
   Mean episode rew_dof_pos_limits: -0.0222
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1326
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9022
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0580
        Mean episode terrain_level: 0.0747
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.84s
                        Total time: 534.73s
                               ETA: 639 mins 44.0 s

################################################################################
                      Learning iteration 687/50000                      

                       Computation: 124765 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0964
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6054
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2403
   Mean episode rew_dof_pos_limits: -0.0221
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1301
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.8909
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0555
        Mean episode terrain_level: 0.0766
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.79s
                        Total time: 535.52s
                               ETA: 639 mins 43.9 s

################################################################################
                      Learning iteration 688/50000                      

                       Computation: 114287 steps/s (collection: 0.739s, learning 0.121s)
               Value function loss: 0.0970
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5408
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2433
   Mean episode rew_dof_pos_limits: -0.0214
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1282
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8617
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0781
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.86s
                        Total time: 536.38s
                               ETA: 639 mins 49.0 s

################################################################################
                      Learning iteration 689/50000                      

                       Computation: 114685 steps/s (collection: 0.726s, learning 0.131s)
               Value function loss: 0.0992
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6656
       Mean episode rew_ang_vel_xy: -0.0595
          Mean episode rew_dof_acc: -0.2490
   Mean episode rew_dof_pos_limits: -0.0229
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1288
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9352
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0619
        Mean episode terrain_level: 0.0791
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.86s
                        Total time: 537.24s
                               ETA: 639 mins 53.8 s

################################################################################
                      Learning iteration 690/50000                      

                       Computation: 117337 steps/s (collection: 0.710s, learning 0.128s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4913
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.2287
   Mean episode rew_dof_pos_limits: -0.0193
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1230
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.8281
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0756
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.84s
                        Total time: 538.08s
                               ETA: 639 mins 57.2 s

################################################################################
                      Learning iteration 691/50000                      

                       Computation: 126518 steps/s (collection: 0.656s, learning 0.121s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6483
       Mean episode rew_ang_vel_xy: -0.0584
          Mean episode rew_dof_acc: -0.2543
   Mean episode rew_dof_pos_limits: -0.0209
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1305
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9252
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0594
        Mean episode terrain_level: 0.0719
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.78s
                        Total time: 538.85s
                               ETA: 639 mins 56.3 s

################################################################################
                      Learning iteration 692/50000                      

                       Computation: 110036 steps/s (collection: 0.751s, learning 0.143s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6212
       Mean episode rew_ang_vel_xy: -0.0591
          Mean episode rew_dof_acc: -0.2458
   Mean episode rew_dof_pos_limits: -0.0242
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1317
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9006
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0672
        Mean episode terrain_level: 0.0729
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.89s
                        Total time: 539.75s
                               ETA: 640 mins 3.7 s

################################################################################
                      Learning iteration 693/50000                      

                       Computation: 109286 steps/s (collection: 0.758s, learning 0.142s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6296
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2376
   Mean episode rew_dof_pos_limits: -0.0240
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1247
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9056
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0706
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.90s
                        Total time: 540.65s
                               ETA: 640 mins 11.5 s

################################################################################
                      Learning iteration 694/50000                      

                       Computation: 123790 steps/s (collection: 0.657s, learning 0.137s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5999
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2464
   Mean episode rew_dof_pos_limits: -0.0224
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1247
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.8876
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0710
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.79s
                        Total time: 541.44s
                               ETA: 640 mins 11.8 s

################################################################################
                      Learning iteration 695/50000                      

                       Computation: 108697 steps/s (collection: 0.770s, learning 0.135s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5481
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.2356
   Mean episode rew_dof_pos_limits: -0.0182
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1278
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.8611
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0480
        Mean episode terrain_level: 0.0712
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.90s
                        Total time: 542.34s
                               ETA: 640 mins 19.9 s

################################################################################
                      Learning iteration 696/50000                      

                       Computation: 119428 steps/s (collection: 0.681s, learning 0.142s)
               Value function loss: 0.1050
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.35
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6814
       Mean episode rew_ang_vel_xy: -0.0601
          Mean episode rew_dof_acc: -0.2522
   Mean episode rew_dof_pos_limits: -0.0228
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1301
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9314
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0578
        Mean episode terrain_level: 0.0723
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.82s
                        Total time: 543.17s
                               ETA: 640 mins 22.2 s

################################################################################
                      Learning iteration 697/50000                      

                       Computation: 120119 steps/s (collection: 0.677s, learning 0.142s)
               Value function loss: 0.1056
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.35
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6331
       Mean episode rew_ang_vel_xy: -0.0590
          Mean episode rew_dof_acc: -0.2503
   Mean episode rew_dof_pos_limits: -0.0196
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1356
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.9043
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0534
        Mean episode terrain_level: 0.0735
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.82s
                        Total time: 543.99s
                               ETA: 640 mins 24.2 s

################################################################################
                      Learning iteration 698/50000                      

                       Computation: 117903 steps/s (collection: 0.693s, learning 0.141s)
               Value function loss: 0.1090
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6461
       Mean episode rew_ang_vel_xy: -0.0577
          Mean episode rew_dof_acc: -0.2466
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1270
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9133
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0755
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.83s
                        Total time: 544.82s
                               ETA: 640 mins 27.3 s

################################################################################
                      Learning iteration 699/50000                      

                       Computation: 116073 steps/s (collection: 0.706s, learning 0.141s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.35
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6008
       Mean episode rew_ang_vel_xy: -0.0566
          Mean episode rew_dof_acc: -0.2393
   Mean episode rew_dof_pos_limits: -0.0230
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1207
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.8847
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0763
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.85s
                        Total time: 545.67s
                               ETA: 640 mins 31.3 s

################################################################################
                      Learning iteration 700/50000                      

                       Computation: 114770 steps/s (collection: 0.717s, learning 0.140s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.36
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6308
       Mean episode rew_ang_vel_xy: -0.0588
          Mean episode rew_dof_acc: -0.2419
   Mean episode rew_dof_pos_limits: -0.0227
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1361
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9045
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0766
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.86s
                        Total time: 546.52s
                               ETA: 640 mins 35.9 s

################################################################################
                      Learning iteration 701/50000                      

                       Computation: 114095 steps/s (collection: 0.721s, learning 0.140s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7040
       Mean episode rew_ang_vel_xy: -0.0602
          Mean episode rew_dof_acc: -0.2598
   Mean episode rew_dof_pos_limits: -0.0199
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1335
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9474
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0580
        Mean episode terrain_level: 0.0801
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.86s
                        Total time: 547.38s
                               ETA: 640 mins 40.9 s

################################################################################
                      Learning iteration 702/50000                      

                       Computation: 119470 steps/s (collection: 0.693s, learning 0.130s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6769
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.2424
   Mean episode rew_dof_pos_limits: -0.0238
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1333
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9373
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0811
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.82s
                        Total time: 548.21s
                               ETA: 640 mins 43.1 s

################################################################################
                      Learning iteration 703/50000                      

                       Computation: 125048 steps/s (collection: 0.662s, learning 0.124s)
               Value function loss: 0.1060
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6025
       Mean episode rew_ang_vel_xy: -0.0568
          Mean episode rew_dof_acc: -0.2428
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1304
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.8975
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0528
        Mean episode terrain_level: 0.0796
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.79s
                        Total time: 548.99s
                               ETA: 640 mins 42.8 s

################################################################################
                      Learning iteration 704/50000                      

                       Computation: 116886 steps/s (collection: 0.718s, learning 0.123s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6981
       Mean episode rew_ang_vel_xy: -0.0594
          Mean episode rew_dof_acc: -0.2587
   Mean episode rew_dof_pos_limits: -0.0209
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1332
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9495
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0545
        Mean episode terrain_level: 0.0787
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.84s
                        Total time: 549.83s
                               ETA: 640 mins 46.3 s

################################################################################
                      Learning iteration 705/50000                      

                       Computation: 118495 steps/s (collection: 0.694s, learning 0.136s)
               Value function loss: 0.1104
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6834
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2526
   Mean episode rew_dof_pos_limits: -0.0189
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1322
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.9368
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0526
        Mean episode terrain_level: 0.0797
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.83s
                        Total time: 550.66s
                               ETA: 640 mins 49.0 s

################################################################################
                      Learning iteration 706/50000                      

                       Computation: 121780 steps/s (collection: 0.680s, learning 0.127s)
               Value function loss: 0.1029
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6310
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2453
   Mean episode rew_dof_pos_limits: -0.0211
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1310
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.9136
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0558
        Mean episode terrain_level: 0.0786
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.81s
                        Total time: 551.47s
                               ETA: 640 mins 50.1 s

################################################################################
                      Learning iteration 707/50000                      

                       Computation: 118620 steps/s (collection: 0.699s, learning 0.129s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7196
       Mean episode rew_ang_vel_xy: -0.0602
          Mean episode rew_dof_acc: -0.2555
   Mean episode rew_dof_pos_limits: -0.0216
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1375
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9544
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0797
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.83s
                        Total time: 552.30s
                               ETA: 640 mins 52.7 s

################################################################################
                      Learning iteration 708/50000                      

                       Computation: 122165 steps/s (collection: 0.681s, learning 0.124s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7343
       Mean episode rew_ang_vel_xy: -0.0593
          Mean episode rew_dof_acc: -0.2486
   Mean episode rew_dof_pos_limits: -0.0231
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1322
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9670
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0781
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.80s
                        Total time: 553.10s
                               ETA: 640 mins 53.6 s

################################################################################
                      Learning iteration 709/50000                      

                       Computation: 123482 steps/s (collection: 0.672s, learning 0.124s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7410
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2502
   Mean episode rew_dof_pos_limits: -0.0235
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1316
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.9776
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0592
        Mean episode terrain_level: 0.0767
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.80s
                        Total time: 553.90s
                               ETA: 640 mins 54.0 s

################################################################################
                      Learning iteration 710/50000                      

                       Computation: 116063 steps/s (collection: 0.724s, learning 0.123s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7788
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2621
   Mean episode rew_dof_pos_limits: -0.0217
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.9868
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0765
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.85s
                        Total time: 554.75s
                               ETA: 640 mins 57.8 s

################################################################################
                      Learning iteration 711/50000                      

                       Computation: 119135 steps/s (collection: 0.702s, learning 0.123s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6978
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.2426
   Mean episode rew_dof_pos_limits: -0.0202
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1304
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.9396
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0528
        Mean episode terrain_level: 0.0767
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.83s
                        Total time: 555.57s
                               ETA: 641 mins 0.1 s

################################################################################
                      Learning iteration 712/50000                      

                       Computation: 121089 steps/s (collection: 0.682s, learning 0.130s)
               Value function loss: 0.1053
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7638
       Mean episode rew_ang_vel_xy: -0.0607
          Mean episode rew_dof_acc: -0.2601
   Mean episode rew_dof_pos_limits: -0.0223
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1332
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.9819
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0575
        Mean episode terrain_level: 0.0798
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.81s
                        Total time: 556.38s
                               ETA: 641 mins 1.5 s

################################################################################
                      Learning iteration 713/50000                      

                       Computation: 119516 steps/s (collection: 0.699s, learning 0.123s)
               Value function loss: 0.1043
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6261
       Mean episode rew_ang_vel_xy: -0.0579
          Mean episode rew_dof_acc: -0.2405
   Mean episode rew_dof_pos_limits: -0.0187
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1325
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.9026
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0772
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.82s
                        Total time: 557.21s
                               ETA: 641 mins 3.7 s

################################################################################
                      Learning iteration 714/50000                      

                       Computation: 117166 steps/s (collection: 0.713s, learning 0.126s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7286
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2459
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1360
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9575
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0574
        Mean episode terrain_level: 0.0764
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.84s
                        Total time: 558.05s
                               ETA: 641 mins 6.9 s

################################################################################
                      Learning iteration 715/50000                      

                       Computation: 123138 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7923
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.2509
   Mean episode rew_dof_pos_limits: -0.0249
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1339
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -0.9920
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0672
        Mean episode terrain_level: 0.0778
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.80s
                        Total time: 558.84s
                               ETA: 641 mins 7.4 s

################################################################################
                      Learning iteration 716/50000                      

                       Computation: 118027 steps/s (collection: 0.704s, learning 0.129s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7518
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.2560
   Mean episode rew_dof_pos_limits: -0.0217
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9763
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0578
        Mean episode terrain_level: 0.0800
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.83s
                        Total time: 559.68s
                               ETA: 641 mins 10.2 s

################################################################################
                      Learning iteration 717/50000                      

                       Computation: 119893 steps/s (collection: 0.689s, learning 0.131s)
               Value function loss: 0.1039
                    Surrogate loss: -0.0169
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6774
       Mean episode rew_ang_vel_xy: -0.0574
          Mean episode rew_dof_acc: -0.2427
   Mean episode rew_dof_pos_limits: -0.0219
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1353
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9295
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0822
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.82s
                        Total time: 560.50s
                               ETA: 641 mins 12.1 s

################################################################################
                      Learning iteration 718/50000                      

                       Computation: 103447 steps/s (collection: 0.810s, learning 0.140s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.40
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7442
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.2554
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1343
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.9713
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0836
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.95s
                        Total time: 561.45s
                               ETA: 641 mins 23.0 s

################################################################################
                      Learning iteration 719/50000                      

                       Computation: 126936 steps/s (collection: 0.653s, learning 0.122s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.40
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8430
       Mean episode rew_ang_vel_xy: -0.0617
          Mean episode rew_dof_acc: -0.2560
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1353
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.0250
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0691
        Mean episode terrain_level: 0.0812
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.77s
                        Total time: 562.22s
                               ETA: 641 mins 21.7 s

################################################################################
                      Learning iteration 720/50000                      

                       Computation: 125844 steps/s (collection: 0.656s, learning 0.125s)
               Value function loss: 0.1003
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.41
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8303
       Mean episode rew_ang_vel_xy: -0.0591
          Mean episode rew_dof_acc: -0.2545
   Mean episode rew_dof_pos_limits: -0.0247
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1275
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.0208
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0688
        Mean episode terrain_level: 0.0795
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.78s
                        Total time: 563.00s
                               ETA: 641 mins 21.0 s

################################################################################
                      Learning iteration 721/50000                      

                       Computation: 124997 steps/s (collection: 0.665s, learning 0.122s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.41
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7741
       Mean episode rew_ang_vel_xy: -0.0602
          Mean episode rew_dof_acc: -0.2560
   Mean episode rew_dof_pos_limits: -0.0224
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1343
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9819
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0549
        Mean episode terrain_level: 0.0797
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.79s
                        Total time: 563.79s
                               ETA: 641 mins 20.6 s

################################################################################
                      Learning iteration 722/50000                      

                       Computation: 106396 steps/s (collection: 0.797s, learning 0.127s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6989
       Mean episode rew_ang_vel_xy: -0.0581
          Mean episode rew_dof_acc: -0.2448
   Mean episode rew_dof_pos_limits: -0.0209
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1302
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.9441
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0542
        Mean episode terrain_level: 0.0797
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.92s
                        Total time: 564.71s
                               ETA: 641 mins 29.5 s

################################################################################
                      Learning iteration 723/50000                      

                       Computation: 124509 steps/s (collection: 0.665s, learning 0.124s)
               Value function loss: 0.0983
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7459
       Mean episode rew_ang_vel_xy: -0.0585
          Mean episode rew_dof_acc: -0.2466
   Mean episode rew_dof_pos_limits: -0.0197
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1274
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0051
       Mean episode rew_smoothness: -0.9673
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0775
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.79s
                        Total time: 565.50s
                               ETA: 641 mins 29.3 s

################################################################################
                      Learning iteration 724/50000                      

                       Computation: 119615 steps/s (collection: 0.688s, learning 0.134s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7319
       Mean episode rew_ang_vel_xy: -0.0577
          Mean episode rew_dof_acc: -0.2493
   Mean episode rew_dof_pos_limits: -0.0221
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1313
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.9643
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0753
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.82s
                        Total time: 566.32s
                               ETA: 641 mins 31.3 s

################################################################################
                      Learning iteration 725/50000                      

                       Computation: 113756 steps/s (collection: 0.734s, learning 0.131s)
               Value function loss: 0.1005
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7755
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.2423
   Mean episode rew_dof_pos_limits: -0.0250
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1249
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.9837
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0757
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.86s
                        Total time: 567.19s
                               ETA: 641 mins 36.2 s

################################################################################
                      Learning iteration 726/50000                      

                       Computation: 119431 steps/s (collection: 0.700s, learning 0.123s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8459
       Mean episode rew_ang_vel_xy: -0.0597
          Mean episode rew_dof_acc: -0.2604
   Mean episode rew_dof_pos_limits: -0.0220
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1319
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.0248
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0767
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.82s
                        Total time: 568.01s
                               ETA: 641 mins 38.2 s

################################################################################
                      Learning iteration 727/50000                      

                       Computation: 114738 steps/s (collection: 0.721s, learning 0.136s)
               Value function loss: 0.1028
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7195
       Mean episode rew_ang_vel_xy: -0.0578
          Mean episode rew_dof_acc: -0.2424
   Mean episode rew_dof_pos_limits: -0.0203
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1295
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.9453
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0778
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.86s
                        Total time: 568.87s
                               ETA: 641 mins 42.6 s

################################################################################
                      Learning iteration 728/50000                      

                       Computation: 112581 steps/s (collection: 0.745s, learning 0.128s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8015
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2494
   Mean episode rew_dof_pos_limits: -0.0212
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1260
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.9882
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0594
        Mean episode terrain_level: 0.0793
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.87s
                        Total time: 569.74s
                               ETA: 641 mins 48.0 s

################################################################################
                      Learning iteration 729/50000                      

                       Computation: 114134 steps/s (collection: 0.723s, learning 0.138s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8733
       Mean episode rew_ang_vel_xy: -0.0594
          Mean episode rew_dof_acc: -0.2531
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1258
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -1.0281
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0564
        Mean episode terrain_level: 0.0806
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.86s
                        Total time: 570.60s
                               ETA: 641 mins 52.6 s

################################################################################
                      Learning iteration 730/50000                      

                       Computation: 113905 steps/s (collection: 0.712s, learning 0.151s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8558
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2526
   Mean episode rew_dof_pos_limits: -0.0232
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1367
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -1.0247
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0795
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.86s
                        Total time: 571.47s
                               ETA: 641 mins 57.3 s

################################################################################
                      Learning iteration 731/50000                      

                       Computation: 115466 steps/s (collection: 0.709s, learning 0.143s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8502
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2575
   Mean episode rew_dof_pos_limits: -0.0225
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1380
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.0139
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0605
        Mean episode terrain_level: 0.0806
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.85s
                        Total time: 572.32s
                               ETA: 642 mins 1.2 s

################################################################################
                      Learning iteration 732/50000                      

                       Computation: 104141 steps/s (collection: 0.797s, learning 0.147s)
               Value function loss: 0.0966
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7973
       Mean episode rew_ang_vel_xy: -0.0566
          Mean episode rew_dof_acc: -0.2454
   Mean episode rew_dof_pos_limits: -0.0250
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1215
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -0.9869
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.0827
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.94s
                        Total time: 573.26s
                               ETA: 642 mins 11.3 s

################################################################################
                      Learning iteration 733/50000                      

                       Computation: 123005 steps/s (collection: 0.659s, learning 0.140s)
               Value function loss: 0.0990
                    Surrogate loss: -0.0172
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8398
       Mean episode rew_ang_vel_xy: -0.0604
          Mean episode rew_dof_acc: -0.2579
   Mean episode rew_dof_pos_limits: -0.0203
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1297
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -1.0051
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0545
        Mean episode terrain_level: 0.0804
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.80s
                        Total time: 574.06s
                               ETA: 642 mins 11.7 s

################################################################################
                      Learning iteration 734/50000                      

                       Computation: 111459 steps/s (collection: 0.748s, learning 0.133s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9716
       Mean episode rew_ang_vel_xy: -0.0606
          Mean episode rew_dof_acc: -0.2606
   Mean episode rew_dof_pos_limits: -0.0291
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1279
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.0952
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0803
        Mean episode terrain_level: 0.0788
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.88s
                        Total time: 574.94s
                               ETA: 642 mins 17.6 s

################################################################################
                      Learning iteration 735/50000                      

                       Computation: 120440 steps/s (collection: 0.689s, learning 0.127s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7428
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2377
   Mean episode rew_dof_pos_limits: -0.0199
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1284
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.9456
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0780
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.82s
                        Total time: 575.76s
                               ETA: 642 mins 19.1 s

################################################################################
                      Learning iteration 736/50000                      

                       Computation: 113675 steps/s (collection: 0.736s, learning 0.129s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8631
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.2523
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1274
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0243
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0791
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.86s
                        Total time: 576.62s
                               ETA: 642 mins 23.8 s

################################################################################
                      Learning iteration 737/50000                      

                       Computation: 118351 steps/s (collection: 0.703s, learning 0.128s)
               Value function loss: 0.0985
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8621
       Mean episode rew_ang_vel_xy: -0.0586
          Mean episode rew_dof_acc: -0.2440
   Mean episode rew_dof_pos_limits: -0.0230
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1249
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0118
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0822
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.83s
                        Total time: 577.45s
                               ETA: 642 mins 26.2 s

################################################################################
                      Learning iteration 738/50000                      

                       Computation: 116959 steps/s (collection: 0.701s, learning 0.140s)
               Value function loss: 0.1005
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9066
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2418
   Mean episode rew_dof_pos_limits: -0.0265
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1239
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.0559
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0670
        Mean episode terrain_level: 0.0829
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.84s
                        Total time: 578.29s
                               ETA: 642 mins 29.3 s

################################################################################
                      Learning iteration 739/50000                      

                       Computation: 106910 steps/s (collection: 0.797s, learning 0.122s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9283
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2564
   Mean episode rew_dof_pos_limits: -0.0222
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1293
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0560
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0862
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.92s
                        Total time: 579.21s
                               ETA: 642 mins 37.7 s

################################################################################
                      Learning iteration 740/50000                      

                       Computation: 118710 steps/s (collection: 0.687s, learning 0.142s)
               Value function loss: 0.1044
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9580
       Mean episode rew_ang_vel_xy: -0.0612
          Mean episode rew_dof_acc: -0.2611
   Mean episode rew_dof_pos_limits: -0.0223
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.0794
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0854
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.83s
                        Total time: 580.04s
                               ETA: 642 mins 39.9 s

################################################################################
                      Learning iteration 741/50000                      

                       Computation: 114579 steps/s (collection: 0.722s, learning 0.136s)
               Value function loss: 0.0951
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8067
       Mean episode rew_ang_vel_xy: -0.0579
          Mean episode rew_dof_acc: -0.2431
   Mean episode rew_dof_pos_limits: -0.0224
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1242
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.9906
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0580
        Mean episode terrain_level: 0.0846
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.86s
                        Total time: 580.90s
                               ETA: 642 mins 44.1 s

################################################################################
                      Learning iteration 742/50000                      

                       Computation: 112596 steps/s (collection: 0.748s, learning 0.125s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9180
       Mean episode rew_ang_vel_xy: -0.0607
          Mean episode rew_dof_acc: -0.2605
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1358
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0537
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0627
        Mean episode terrain_level: 0.0845
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.87s
                        Total time: 581.77s
                               ETA: 642 mins 49.3 s

################################################################################
                      Learning iteration 743/50000                      

                       Computation: 116829 steps/s (collection: 0.717s, learning 0.125s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8624
       Mean episode rew_ang_vel_xy: -0.0593
          Mean episode rew_dof_acc: -0.2400
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1280
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0242
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0834
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.84s
                        Total time: 582.61s
                               ETA: 642 mins 52.4 s

################################################################################
                      Learning iteration 744/50000                      

                       Computation: 122797 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9334
       Mean episode rew_ang_vel_xy: -0.0608
          Mean episode rew_dof_acc: -0.2611
   Mean episode rew_dof_pos_limits: -0.0214
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1295
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0487
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0828
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.80s
                        Total time: 583.42s
                               ETA: 642 mins 52.8 s

################################################################################
                      Learning iteration 745/50000                      

                       Computation: 120194 steps/s (collection: 0.693s, learning 0.125s)
               Value function loss: 0.0964
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9218
       Mean episode rew_ang_vel_xy: -0.0595
          Mean episode rew_dof_acc: -0.2521
   Mean episode rew_dof_pos_limits: -0.0230
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1283
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0609
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0842
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.82s
                        Total time: 584.23s
                               ETA: 642 mins 54.3 s

################################################################################
                      Learning iteration 746/50000                      

                       Computation: 115376 steps/s (collection: 0.705s, learning 0.147s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7968
       Mean episode rew_ang_vel_xy: -0.0575
          Mean episode rew_dof_acc: -0.2499
   Mean episode rew_dof_pos_limits: -0.0200
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1286
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.9837
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0828
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.85s
                        Total time: 585.09s
                               ETA: 642 mins 58.0 s

################################################################################
                      Learning iteration 747/50000                      

                       Computation: 125158 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.1060
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8547
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2498
   Mean episode rew_dof_pos_limits: -0.0203
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1357
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -1.0108
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0503
        Mean episode terrain_level: 0.0825
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.79s
                        Total time: 585.87s
                               ETA: 642 mins 57.4 s

################################################################################
                      Learning iteration 748/50000                      

                       Computation: 120988 steps/s (collection: 0.689s, learning 0.123s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.47
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0113
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.2646
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1321
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.1022
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0755
        Mean episode terrain_level: 0.0830
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.81s
                        Total time: 586.68s
                               ETA: 642 mins 58.5 s

################################################################################
                      Learning iteration 749/50000                      

                       Computation: 124229 steps/s (collection: 0.667s, learning 0.124s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.47
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8556
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2510
   Mean episode rew_dof_pos_limits: -0.0223
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1302
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.0067
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0542
        Mean episode terrain_level: 0.0813
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.79s
                        Total time: 587.47s
                               ETA: 642 mins 58.3 s

################################################################################
                      Learning iteration 750/50000                      

                       Computation: 106225 steps/s (collection: 0.790s, learning 0.135s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.47
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9587
       Mean episode rew_ang_vel_xy: -0.0589
          Mean episode rew_dof_acc: -0.2539
   Mean episode rew_dof_pos_limits: -0.0250
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1338
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.0812
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0624
        Mean episode terrain_level: 0.0825
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.93s
                        Total time: 588.40s
                               ETA: 643 mins 6.8 s

################################################################################
                      Learning iteration 751/50000                      

                       Computation: 120787 steps/s (collection: 0.690s, learning 0.123s)
               Value function loss: 0.1052
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9079
       Mean episode rew_ang_vel_xy: -0.0591
          Mean episode rew_dof_acc: -0.2515
   Mean episode rew_dof_pos_limits: -0.0233
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1284
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0458
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0615
        Mean episode terrain_level: 0.0860
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.81s
                        Total time: 589.21s
                               ETA: 643 mins 8.0 s

################################################################################
                      Learning iteration 752/50000                      

                       Computation: 121836 steps/s (collection: 0.684s, learning 0.123s)
               Value function loss: 0.1032
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9548
       Mean episode rew_ang_vel_xy: -0.0590
          Mean episode rew_dof_acc: -0.2393
   Mean episode rew_dof_pos_limits: -0.0251
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1300
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0669
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0670
        Mean episode terrain_level: 0.0872
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.81s
                        Total time: 590.02s
                               ETA: 643 mins 8.8 s

################################################################################
                      Learning iteration 753/50000                      

                       Computation: 109752 steps/s (collection: 0.736s, learning 0.159s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8933
       Mean episode rew_ang_vel_xy: -0.0603
          Mean episode rew_dof_acc: -0.2537
   Mean episode rew_dof_pos_limits: -0.0196
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1289
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -1.0306
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0891
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.90s
                        Total time: 590.92s
                               ETA: 643 mins 15.3 s

################################################################################
                      Learning iteration 754/50000                      

                       Computation: 120107 steps/s (collection: 0.696s, learning 0.123s)
               Value function loss: 0.1088
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9555
       Mean episode rew_ang_vel_xy: -0.0613
          Mean episode rew_dof_acc: -0.2606
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.0633
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0612
        Mean episode terrain_level: 0.0884
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.82s
                        Total time: 591.73s
                               ETA: 643 mins 16.8 s

################################################################################
                      Learning iteration 755/50000                      

                       Computation: 126585 steps/s (collection: 0.650s, learning 0.126s)
               Value function loss: 0.1062
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.49
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9320
       Mean episode rew_ang_vel_xy: -0.0593
          Mean episode rew_dof_acc: -0.2511
   Mean episode rew_dof_pos_limits: -0.0211
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1302
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0505
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0592
        Mean episode terrain_level: 0.0896
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.78s
                        Total time: 592.51s
                               ETA: 643 mins 15.5 s

################################################################################
                      Learning iteration 756/50000                      

                       Computation: 114945 steps/s (collection: 0.732s, learning 0.123s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.49
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9857
       Mean episode rew_ang_vel_xy: -0.0598
          Mean episode rew_dof_acc: -0.2572
   Mean episode rew_dof_pos_limits: -0.0238
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1343
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0810
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0954
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.86s
                        Total time: 593.37s
                               ETA: 643 mins 19.4 s

################################################################################
                      Learning iteration 757/50000                      

                       Computation: 120924 steps/s (collection: 0.681s, learning 0.132s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.49
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0470
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.2659
   Mean episode rew_dof_pos_limits: -0.0243
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1204
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0972
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.81s
                        Total time: 594.18s
                               ETA: 643 mins 20.5 s

################################################################################
                      Learning iteration 758/50000                      

                       Computation: 116449 steps/s (collection: 0.716s, learning 0.128s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.49
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0943
       Mean episode rew_ang_vel_xy: -0.0603
          Mean episode rew_dof_acc: -0.2687
   Mean episode rew_dof_pos_limits: -0.0263
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.1568
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.0984
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.84s
                        Total time: 595.02s
                               ETA: 643 mins 23.6 s

################################################################################
                      Learning iteration 759/50000                      

                       Computation: 122651 steps/s (collection: 0.680s, learning 0.121s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.50
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8596
       Mean episode rew_ang_vel_xy: -0.0590
          Mean episode rew_dof_acc: -0.2444
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1325
           Mean episode rew_no_fly: 0.0101
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -1.0058
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0504
        Mean episode terrain_level: 0.0957
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.80s
                        Total time: 595.83s
                               ETA: 643 mins 24.0 s

################################################################################
                      Learning iteration 760/50000                      

                       Computation: 124627 steps/s (collection: 0.662s, learning 0.127s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9825
       Mean episode rew_ang_vel_xy: -0.0613
          Mean episode rew_dof_acc: -0.2557
   Mean episode rew_dof_pos_limits: -0.0224
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1396
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -1.0824
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0923
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.79s
                        Total time: 596.61s
                               ETA: 643 mins 23.5 s

################################################################################
                      Learning iteration 761/50000                      

                       Computation: 115782 steps/s (collection: 0.723s, learning 0.126s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.50
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9311
       Mean episode rew_ang_vel_xy: -0.0576
          Mean episode rew_dof_acc: -0.2443
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1281
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.0657
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0955
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.85s
                        Total time: 597.46s
                               ETA: 643 mins 26.9 s

################################################################################
                      Learning iteration 762/50000                      

                       Computation: 120048 steps/s (collection: 0.694s, learning 0.125s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9880
       Mean episode rew_ang_vel_xy: -0.0584
          Mean episode rew_dof_acc: -0.2594
   Mean episode rew_dof_pos_limits: -0.0251
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1405
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.0883
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0950
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.82s
                        Total time: 598.28s
                               ETA: 643 mins 28.4 s

################################################################################
                      Learning iteration 763/50000                      

                       Computation: 126416 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0317
       Mean episode rew_ang_vel_xy: -0.0602
          Mean episode rew_dof_acc: -0.2578
   Mean episode rew_dof_pos_limits: -0.0256
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1383
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.1095
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0562
        Mean episode terrain_level: 0.0920
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.78s
                        Total time: 599.06s
                               ETA: 643 mins 27.2 s

################################################################################
                      Learning iteration 764/50000                      

                       Computation: 113851 steps/s (collection: 0.708s, learning 0.155s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9429
       Mean episode rew_ang_vel_xy: -0.0588
          Mean episode rew_dof_acc: -0.2506
   Mean episode rew_dof_pos_limits: -0.0240
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1339
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.0570
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0570
        Mean episode terrain_level: 0.0892
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.86s
                        Total time: 599.92s
                               ETA: 643 mins 31.5 s

################################################################################
                      Learning iteration 765/50000                      

                       Computation: 115095 steps/s (collection: 0.728s, learning 0.126s)
               Value function loss: 0.1070
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0106
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.2539
   Mean episode rew_dof_pos_limits: -0.0246
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1345
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.0906
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0603
        Mean episode terrain_level: 0.0930
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.85s
                        Total time: 600.78s
                               ETA: 643 mins 35.2 s

################################################################################
                      Learning iteration 766/50000                      

                       Computation: 118009 steps/s (collection: 0.698s, learning 0.135s)
               Value function loss: 0.1015
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0654
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2608
   Mean episode rew_dof_pos_limits: -0.0238
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1292
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.1291
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.0953
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.83s
                        Total time: 601.61s
                               ETA: 643 mins 37.6 s

################################################################################
                      Learning iteration 767/50000                      

                       Computation: 118442 steps/s (collection: 0.706s, learning 0.124s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0247
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.2766
   Mean episode rew_dof_pos_limits: -0.0176
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -1.0950
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1994
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0948
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.83s
                        Total time: 602.44s
                               ETA: 643 mins 39.7 s

################################################################################
                      Learning iteration 768/50000                      

                       Computation: 112918 steps/s (collection: 0.731s, learning 0.140s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0977
       Mean episode rew_ang_vel_xy: -0.0616
          Mean episode rew_dof_acc: -0.2623
   Mean episode rew_dof_pos_limits: -0.0244
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1363
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.1459
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.0946
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.87s
                        Total time: 603.31s
                               ETA: 643 mins 44.4 s

################################################################################
                      Learning iteration 769/50000                      

                       Computation: 106340 steps/s (collection: 0.781s, learning 0.144s)
               Value function loss: 0.0973
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0908
       Mean episode rew_ang_vel_xy: -0.0604
          Mean episode rew_dof_acc: -0.2700
   Mean episode rew_dof_pos_limits: -0.0227
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1331
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1443
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0633
        Mean episode terrain_level: 0.0949
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.92s
                        Total time: 604.24s
                               ETA: 643 mins 52.6 s

################################################################################
                      Learning iteration 770/50000                      

                       Computation: 118292 steps/s (collection: 0.709s, learning 0.122s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.53
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0283
       Mean episode rew_ang_vel_xy: -0.0622
          Mean episode rew_dof_acc: -0.2526
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1382
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -1.1024
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0996
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.83s
                        Total time: 605.07s
                               ETA: 643 mins 54.8 s

################################################################################
                      Learning iteration 771/50000                      

                       Computation: 112318 steps/s (collection: 0.753s, learning 0.122s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.53
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1014
       Mean episode rew_ang_vel_xy: -0.0617
          Mean episode rew_dof_acc: -0.2631
   Mean episode rew_dof_pos_limits: -0.0227
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1347
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.1475
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0955
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.88s
                        Total time: 605.94s
                               ETA: 643 mins 59.7 s

################################################################################
                      Learning iteration 772/50000                      

                       Computation: 114764 steps/s (collection: 0.731s, learning 0.126s)
               Value function loss: 0.0986
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.53
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0838
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2618
   Mean episode rew_dof_pos_limits: -0.0238
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1357
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1375
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0952
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.86s
                        Total time: 606.80s
                               ETA: 644 mins 3.5 s

################################################################################
                      Learning iteration 773/50000                      

                       Computation: 109508 steps/s (collection: 0.761s, learning 0.137s)
               Value function loss: 0.0991
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9641
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.2533
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1341
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.0725
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0963
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.90s
                        Total time: 607.70s
                               ETA: 644 mins 9.9 s

################################################################################
                      Learning iteration 774/50000                      

                       Computation: 105650 steps/s (collection: 0.796s, learning 0.135s)
               Value function loss: 0.1007
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2088
       Mean episode rew_ang_vel_xy: -0.0631
          Mean episode rew_dof_acc: -0.2664
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1354
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2108
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0970
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.93s
                        Total time: 608.63s
                               ETA: 644 mins 18.4 s

################################################################################
                      Learning iteration 775/50000                      

                       Computation: 107874 steps/s (collection: 0.781s, learning 0.130s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0448
       Mean episode rew_ang_vel_xy: -0.0595
          Mean episode rew_dof_acc: -0.2601
   Mean episode rew_dof_pos_limits: -0.0250
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1362
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1032
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0665
        Mean episode terrain_level: 0.0979
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.91s
                        Total time: 609.54s
                               ETA: 644 mins 25.6 s

################################################################################
                      Learning iteration 776/50000                      

                       Computation: 116816 steps/s (collection: 0.717s, learning 0.125s)
               Value function loss: 0.1028
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0647
       Mean episode rew_ang_vel_xy: -0.0611
          Mean episode rew_dof_acc: -0.2573
   Mean episode rew_dof_pos_limits: -0.0216
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -1.1203
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0564
        Mean episode terrain_level: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.84s
                        Total time: 610.38s
                               ETA: 644 mins 28.3 s

################################################################################
                      Learning iteration 777/50000                      

                       Computation: 120057 steps/s (collection: 0.685s, learning 0.134s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2876
       Mean episode rew_ang_vel_xy: -0.0641
          Mean episode rew_dof_acc: -0.2672
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1388
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2547
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.1005
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.82s
                        Total time: 611.20s
                               ETA: 644 mins 29.6 s

################################################################################
                      Learning iteration 778/50000                      

                       Computation: 120377 steps/s (collection: 0.676s, learning 0.141s)
               Value function loss: 0.0987
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.55
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1465
       Mean episode rew_ang_vel_xy: -0.0616
          Mean episode rew_dof_acc: -0.2689
   Mean episode rew_dof_pos_limits: -0.0239
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1330
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1639
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0619
        Mean episode terrain_level: 0.1024
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.82s
                        Total time: 612.01s
                               ETA: 644 mins 30.8 s

################################################################################
                      Learning iteration 779/50000                      

                       Computation: 117102 steps/s (collection: 0.717s, learning 0.123s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2139
       Mean episode rew_ang_vel_xy: -0.0629
          Mean episode rew_dof_acc: -0.2651
   Mean episode rew_dof_pos_limits: -0.0275
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1411
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.1964
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0685
        Mean episode terrain_level: 0.1030
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.84s
                        Total time: 612.85s
                               ETA: 644 mins 33.4 s

################################################################################
                      Learning iteration 780/50000                      

                       Computation: 120727 steps/s (collection: 0.691s, learning 0.123s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.55
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1638
       Mean episode rew_ang_vel_xy: -0.0630
          Mean episode rew_dof_acc: -0.2681
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1353
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.1717
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.1035
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.81s
                        Total time: 613.67s
                               ETA: 644 mins 34.4 s

################################################################################
                      Learning iteration 781/50000                      

                       Computation: 120938 steps/s (collection: 0.687s, learning 0.125s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1699
       Mean episode rew_ang_vel_xy: -0.0634
          Mean episode rew_dof_acc: -0.2673
   Mean episode rew_dof_pos_limits: -0.0239
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1351
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1743
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.1021
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.81s
                        Total time: 614.48s
                               ETA: 644 mins 35.4 s

################################################################################
                      Learning iteration 782/50000                      

                       Computation: 110614 steps/s (collection: 0.765s, learning 0.123s)
               Value function loss: 0.1013
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0686
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2576
   Mean episode rew_dof_pos_limits: -0.0229
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.1111
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0989
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.89s
                        Total time: 615.37s
                               ETA: 644 mins 41.0 s

################################################################################
                      Learning iteration 783/50000                      

                       Computation: 121741 steps/s (collection: 0.684s, learning 0.123s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0514
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.2527
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1317
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.1029
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0966
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.81s
                        Total time: 616.18s
                               ETA: 644 mins 41.6 s

################################################################################
                      Learning iteration 784/50000                      

                       Computation: 127789 steps/s (collection: 0.645s, learning 0.124s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.56
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1086
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.2566
   Mean episode rew_dof_pos_limits: -0.0235
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1391
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0585
        Mean episode terrain_level: 0.0962
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.77s
                        Total time: 616.95s
                               ETA: 644 mins 39.8 s

################################################################################
                      Learning iteration 785/50000                      

                       Computation: 113149 steps/s (collection: 0.717s, learning 0.152s)
               Value function loss: 0.1044
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1579
       Mean episode rew_ang_vel_xy: -0.0609
          Mean episode rew_dof_acc: -0.2524
   Mean episode rew_dof_pos_limits: -0.0245
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1291
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1625
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0574
        Mean episode terrain_level: 0.0965
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.87s
                        Total time: 617.82s
                               ETA: 644 mins 44.2 s

################################################################################
                      Learning iteration 786/50000                      

                       Computation: 123471 steps/s (collection: 0.663s, learning 0.133s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2337
       Mean episode rew_ang_vel_xy: -0.0626
          Mean episode rew_dof_acc: -0.2763
   Mean episode rew_dof_pos_limits: -0.0230
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1378
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1914
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0615
        Mean episode terrain_level: 0.0999
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.80s
                        Total time: 618.61s
                               ETA: 644 mins 44.0 s

################################################################################
                      Learning iteration 787/50000                      

                       Computation: 125825 steps/s (collection: 0.658s, learning 0.123s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1551
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2657
   Mean episode rew_dof_pos_limits: -0.0253
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1674
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.1017
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.78s
                        Total time: 619.39s
                               ETA: 644 mins 42.9 s

################################################################################
                      Learning iteration 788/50000                      

                       Computation: 123067 steps/s (collection: 0.676s, learning 0.122s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1425
       Mean episode rew_ang_vel_xy: -0.0593
          Mean episode rew_dof_acc: -0.2554
   Mean episode rew_dof_pos_limits: -0.0263
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1349
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -1.1481
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0670
        Mean episode terrain_level: 0.1040
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.80s
                        Total time: 620.19s
                               ETA: 644 mins 43.0 s

################################################################################
                      Learning iteration 789/50000                      

                       Computation: 106887 steps/s (collection: 0.771s, learning 0.148s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2070
       Mean episode rew_ang_vel_xy: -0.0622
          Mean episode rew_dof_acc: -0.2650
   Mean episode rew_dof_pos_limits: -0.0253
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1380
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1935
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0665
        Mean episode terrain_level: 0.1063
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.92s
                        Total time: 621.11s
                               ETA: 644 mins 50.5 s

################################################################################
                      Learning iteration 790/50000                      

                       Computation: 109992 steps/s (collection: 0.768s, learning 0.126s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.58
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1382
       Mean episode rew_ang_vel_xy: -0.0620
          Mean episode rew_dof_acc: -0.2561
   Mean episode rew_dof_pos_limits: -0.0253
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1323
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.1460
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.1074
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.89s
                        Total time: 622.00s
                               ETA: 644 mins 56.4 s

################################################################################
                      Learning iteration 791/50000                      

                       Computation: 114932 steps/s (collection: 0.706s, learning 0.149s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1467
       Mean episode rew_ang_vel_xy: -0.0616
          Mean episode rew_dof_acc: -0.2638
   Mean episode rew_dof_pos_limits: -0.0239
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1316
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1574
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0578
        Mean episode terrain_level: 0.1075
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.86s
                        Total time: 622.86s
                               ETA: 644 mins 59.9 s

################################################################################
                      Learning iteration 792/50000                      

                       Computation: 114390 steps/s (collection: 0.695s, learning 0.164s)
               Value function loss: 0.1065
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.58
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0795
       Mean episode rew_ang_vel_xy: -0.0590
          Mean episode rew_dof_acc: -0.2459
   Mean episode rew_dof_pos_limits: -0.0247
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1317
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.1189
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0606
        Mean episode terrain_level: 0.1076
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.86s
                        Total time: 623.72s
                               ETA: 645 mins 3.6 s

################################################################################
                      Learning iteration 793/50000                      

                       Computation: 115567 steps/s (collection: 0.704s, learning 0.146s)
               Value function loss: 0.1129
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2528
       Mean episode rew_ang_vel_xy: -0.0626
          Mean episode rew_dof_acc: -0.2646
   Mean episode rew_dof_pos_limits: -0.0263
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1328
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2231
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0711
        Mean episode terrain_level: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.85s
                        Total time: 624.57s
                               ETA: 645 mins 6.8 s

################################################################################
                      Learning iteration 794/50000                      

                       Computation: 123496 steps/s (collection: 0.673s, learning 0.123s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2073
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2513
   Mean episode rew_dof_pos_limits: -0.0260
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1314
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.1861
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0993
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.80s
                        Total time: 625.37s
                               ETA: 645 mins 6.6 s

################################################################################
                      Learning iteration 795/50000                      

                       Computation: 127206 steps/s (collection: 0.649s, learning 0.124s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2524
       Mean episode rew_ang_vel_xy: -0.0615
          Mean episode rew_dof_acc: -0.2658
   Mean episode rew_dof_pos_limits: -0.0241
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1324
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2248
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.77s
                        Total time: 626.14s
                               ETA: 645 mins 5.0 s

################################################################################
                      Learning iteration 796/50000                      

                       Computation: 116828 steps/s (collection: 0.717s, learning 0.124s)
               Value function loss: 0.1135
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3274
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.2726
   Mean episode rew_dof_pos_limits: -0.0274
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1331
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.2729
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0705
        Mean episode terrain_level: 0.0976
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.84s
                        Total time: 626.98s
                               ETA: 645 mins 7.6 s

################################################################################
                      Learning iteration 797/50000                      

                       Computation: 127219 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.1108
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.60
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2199
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2599
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1392
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2020
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0954
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.77s
                        Total time: 627.75s
                               ETA: 645 mins 5.9 s

################################################################################
                      Learning iteration 798/50000                      

                       Computation: 115515 steps/s (collection: 0.719s, learning 0.132s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2230
       Mean episode rew_ang_vel_xy: -0.0621
          Mean episode rew_dof_acc: -0.2617
   Mean episode rew_dof_pos_limits: -0.0228
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1353
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1897
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0618
        Mean episode terrain_level: 0.0920
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.85s
                        Total time: 628.60s
                               ETA: 645 mins 9.1 s

################################################################################
                      Learning iteration 799/50000                      

                       Computation: 122864 steps/s (collection: 0.677s, learning 0.123s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2388
       Mean episode rew_ang_vel_xy: -0.0611
          Mean episode rew_dof_acc: -0.2658
   Mean episode rew_dof_pos_limits: -0.0263
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1329
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.2157
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0919
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.80s
                        Total time: 629.40s
                               ETA: 645 mins 9.1 s

################################################################################
                      Learning iteration 800/50000                      

                       Computation: 118434 steps/s (collection: 0.706s, learning 0.124s)
               Value function loss: 0.1143
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2619
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.2694
   Mean episode rew_dof_pos_limits: -0.0250
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1393
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2240
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0945
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.83s
                        Total time: 630.23s
                               ETA: 645 mins 11.0 s

################################################################################
                      Learning iteration 801/50000                      

                       Computation: 122590 steps/s (collection: 0.672s, learning 0.130s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4207
       Mean episode rew_ang_vel_xy: -0.0647
          Mean episode rew_dof_acc: -0.2724
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1312
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3246
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0710
        Mean episode terrain_level: 0.0964
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.80s
                        Total time: 631.04s
                               ETA: 645 mins 11.1 s

################################################################################
                      Learning iteration 802/50000                      

                       Computation: 119541 steps/s (collection: 0.696s, learning 0.126s)
               Value function loss: 0.1093
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2912
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.2687
   Mean episode rew_dof_pos_limits: -0.0232
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1354
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2393
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0977
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.82s
                        Total time: 631.86s
                               ETA: 645 mins 12.5 s

################################################################################
                      Learning iteration 803/50000                      

                       Computation: 115474 steps/s (collection: 0.706s, learning 0.145s)
               Value function loss: 0.1082
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1619
       Mean episode rew_ang_vel_xy: -0.0616
          Mean episode rew_dof_acc: -0.2530
   Mean episode rew_dof_pos_limits: -0.0254
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1344
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1732
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0644
        Mean episode terrain_level: 0.0980
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.85s
                        Total time: 632.71s
                               ETA: 645 mins 15.7 s

################################################################################
                      Learning iteration 804/50000                      

                       Computation: 124359 steps/s (collection: 0.663s, learning 0.128s)
               Value function loss: 0.1104
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.62
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3748
       Mean episode rew_ang_vel_xy: -0.0627
          Mean episode rew_dof_acc: -0.2638
   Mean episode rew_dof_pos_limits: -0.0268
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1311
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.2813
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0677
        Mean episode terrain_level: 0.0995
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.79s
                        Total time: 633.50s
                               ETA: 645 mins 15.1 s

################################################################################
                      Learning iteration 805/50000                      

                       Computation: 113508 steps/s (collection: 0.742s, learning 0.124s)
               Value function loss: 0.1093
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.62
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2501
       Mean episode rew_ang_vel_xy: -0.0621
          Mean episode rew_dof_acc: -0.2621
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1356
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2221
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0701
        Mean episode terrain_level: 0.1024
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.87s
                        Total time: 634.37s
                               ETA: 645 mins 19.2 s

################################################################################
                      Learning iteration 806/50000                      

                       Computation: 117929 steps/s (collection: 0.696s, learning 0.137s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.62
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2018
       Mean episode rew_ang_vel_xy: -0.0636
          Mean episode rew_dof_acc: -0.2589
   Mean episode rew_dof_pos_limits: -0.0227
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.1780
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.1015
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.83s
                        Total time: 635.20s
                               ETA: 645 mins 21.2 s

################################################################################
                      Learning iteration 807/50000                      

                       Computation: 106533 steps/s (collection: 0.784s, learning 0.139s)
               Value function loss: 0.1118
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1687
       Mean episode rew_ang_vel_xy: -0.0628
          Mean episode rew_dof_acc: -0.2580
   Mean episode rew_dof_pos_limits: -0.0227
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1415
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.1708
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0992
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.92s
                        Total time: 636.12s
                               ETA: 645 mins 28.7 s

################################################################################
                      Learning iteration 808/50000                      

                       Computation: 119580 steps/s (collection: 0.698s, learning 0.124s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2132
       Mean episode rew_ang_vel_xy: -0.0628
          Mean episode rew_dof_acc: -0.2671
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1324
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.1983
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1980
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.1008
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.82s
                        Total time: 636.94s
                               ETA: 645 mins 30.0 s

################################################################################
                      Learning iteration 809/50000                      

                       Computation: 114649 steps/s (collection: 0.714s, learning 0.144s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2632
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.2644
   Mean episode rew_dof_pos_limits: -0.0253
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1304
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2203
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.1007
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.86s
                        Total time: 637.80s
                               ETA: 645 mins 33.5 s

################################################################################
                      Learning iteration 810/50000                      

                       Computation: 107112 steps/s (collection: 0.762s, learning 0.156s)
               Value function loss: 0.1140
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4177
       Mean episode rew_ang_vel_xy: -0.0635
          Mean episode rew_dof_acc: -0.2672
   Mean episode rew_dof_pos_limits: -0.0284
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1324
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.3249
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0786
        Mean episode terrain_level: 0.1013
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.92s
                        Total time: 638.72s
                               ETA: 645 mins 40.6 s

################################################################################
                      Learning iteration 811/50000                      

                       Computation: 111081 steps/s (collection: 0.742s, learning 0.143s)
               Value function loss: 0.1100
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2903
       Mean episode rew_ang_vel_xy: -0.0615
          Mean episode rew_dof_acc: -0.2531
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1302
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.2562
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0643
        Mean episode terrain_level: 0.0990
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.88s
                        Total time: 639.60s
                               ETA: 645 mins 45.7 s

################################################################################
                      Learning iteration 812/50000                      

                       Computation: 124539 steps/s (collection: 0.657s, learning 0.132s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3343
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.2674
   Mean episode rew_dof_pos_limits: -0.0259
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1326
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2761
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0695
        Mean episode terrain_level: 0.0983
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.79s
                        Total time: 640.39s
                               ETA: 645 mins 45.0 s

################################################################################
                      Learning iteration 813/50000                      

                       Computation: 116981 steps/s (collection: 0.716s, learning 0.124s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.64
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2470
       Mean episode rew_ang_vel_xy: -0.0624
          Mean episode rew_dof_acc: -0.2603
   Mean episode rew_dof_pos_limits: -0.0257
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2296
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0979
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.84s
                        Total time: 641.23s
                               ETA: 645 mins 47.4 s

################################################################################
                      Learning iteration 814/50000                      

                       Computation: 122368 steps/s (collection: 0.677s, learning 0.127s)
               Value function loss: 0.1043
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.64
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3122
       Mean episode rew_ang_vel_xy: -0.0626
          Mean episode rew_dof_acc: -0.2613
   Mean episode rew_dof_pos_limits: -0.0246
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1315
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -1.2594
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.1002
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.80s
                        Total time: 642.04s
                               ETA: 645 mins 47.6 s

################################################################################
                      Learning iteration 815/50000                      

                       Computation: 115525 steps/s (collection: 0.716s, learning 0.135s)
               Value function loss: 0.1005
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.64
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3526
       Mean episode rew_ang_vel_xy: -0.0640
          Mean episode rew_dof_acc: -0.2736
   Mean episode rew_dof_pos_limits: -0.0269
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.2751
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0678
        Mean episode terrain_level: 0.1048
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.85s
                        Total time: 642.89s
                               ETA: 645 mins 50.6 s

################################################################################
                      Learning iteration 816/50000                      

                       Computation: 125774 steps/s (collection: 0.659s, learning 0.122s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.64
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4002
       Mean episode rew_ang_vel_xy: -0.0625
          Mean episode rew_dof_acc: -0.2667
   Mean episode rew_dof_pos_limits: -0.0254
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1277
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.3044
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0626
        Mean episode terrain_level: 0.1060
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.78s
                        Total time: 643.67s
                               ETA: 645 mins 49.4 s

################################################################################
                      Learning iteration 817/50000                      

                       Computation: 121231 steps/s (collection: 0.684s, learning 0.127s)
               Value function loss: 0.1039
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2580
       Mean episode rew_ang_vel_xy: -0.0628
          Mean episode rew_dof_acc: -0.2586
   Mean episode rew_dof_pos_limits: -0.0260
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1387
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2280
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.1029
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.81s
                        Total time: 644.48s
                               ETA: 645 mins 50.0 s

################################################################################
                      Learning iteration 818/50000                      

                       Computation: 113388 steps/s (collection: 0.729s, learning 0.138s)
               Value function loss: 0.1028
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3218
       Mean episode rew_ang_vel_xy: -0.0630
          Mean episode rew_dof_acc: -0.2647
   Mean episode rew_dof_pos_limits: -0.0237
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1402
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2607
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0591
        Mean episode terrain_level: 0.1025
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.87s
                        Total time: 645.35s
                               ETA: 645 mins 54.0 s

################################################################################
                      Learning iteration 819/50000                      

                       Computation: 117887 steps/s (collection: 0.710s, learning 0.124s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.65
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3642
       Mean episode rew_ang_vel_xy: -0.0641
          Mean episode rew_dof_acc: -0.2766
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2855
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.83s
                        Total time: 646.18s
                               ETA: 645 mins 55.9 s

################################################################################
                      Learning iteration 820/50000                      

                       Computation: 125802 steps/s (collection: 0.656s, learning 0.125s)
               Value function loss: 0.0984
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.66
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3328
       Mean episode rew_ang_vel_xy: -0.0635
          Mean episode rew_dof_acc: -0.2595
   Mean episode rew_dof_pos_limits: -0.0262
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1436
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2648
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0680
        Mean episode terrain_level: 0.1054
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.78s
                        Total time: 646.96s
                               ETA: 645 mins 54.8 s

################################################################################
                      Learning iteration 821/50000                      

                       Computation: 113987 steps/s (collection: 0.735s, learning 0.127s)
               Value function loss: 0.1032
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.66
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2245
       Mean episode rew_ang_vel_xy: -0.0608
          Mean episode rew_dof_acc: -0.2592
   Mean episode rew_dof_pos_limits: -0.0248
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2021
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.1041
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.86s
                        Total time: 647.83s
                               ETA: 645 mins 58.4 s

################################################################################
                      Learning iteration 822/50000                      

                       Computation: 125645 steps/s (collection: 0.659s, learning 0.124s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3717
       Mean episode rew_ang_vel_xy: -0.0612
          Mean episode rew_dof_acc: -0.2677
   Mean episode rew_dof_pos_limits: -0.0277
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1363
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.2963
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0655
        Mean episode terrain_level: 0.0997
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.78s
                        Total time: 648.61s
                               ETA: 645 mins 57.3 s

################################################################################
                      Learning iteration 823/50000                      

                       Computation: 121212 steps/s (collection: 0.670s, learning 0.141s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3089
       Mean episode rew_ang_vel_xy: -0.0642
          Mean episode rew_dof_acc: -0.2576
   Mean episode rew_dof_pos_limits: -0.0245
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1322
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -1.2372
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0989
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.81s
                        Total time: 649.42s
                               ETA: 645 mins 57.9 s

################################################################################
                      Learning iteration 824/50000                      

                       Computation: 116309 steps/s (collection: 0.722s, learning 0.123s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4305
       Mean episode rew_ang_vel_xy: -0.0637
          Mean episode rew_dof_acc: -0.2726
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1364
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3241
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0695
        Mean episode terrain_level: 0.0978
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.85s
                        Total time: 650.26s
                               ETA: 646 mins 0.5 s

################################################################################
                      Learning iteration 825/50000                      

                       Computation: 109377 steps/s (collection: 0.775s, learning 0.123s)
               Value function loss: 0.1036
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2817
       Mean episode rew_ang_vel_xy: -0.0624
          Mean episode rew_dof_acc: -0.2470
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1406
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2366
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0655
        Mean episode terrain_level: 0.0971
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.90s
                        Total time: 651.16s
                               ETA: 646 mins 6.3 s

################################################################################
                      Learning iteration 826/50000                      

                       Computation: 105900 steps/s (collection: 0.798s, learning 0.131s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2674
       Mean episode rew_ang_vel_xy: -0.0615
          Mean episode rew_dof_acc: -0.2595
   Mean episode rew_dof_pos_limits: -0.0240
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1335
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2181
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0610
        Mean episode terrain_level: 0.0936
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.93s
                        Total time: 652.09s
                               ETA: 646 mins 13.8 s

################################################################################
                      Learning iteration 827/50000                      

                       Computation: 123756 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3938
       Mean episode rew_ang_vel_xy: -0.0629
          Mean episode rew_dof_acc: -0.2629
   Mean episode rew_dof_pos_limits: -0.0247
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1304
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -1.2777
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0907
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.79s
                        Total time: 652.89s
                               ETA: 646 mins 13.4 s

################################################################################
                      Learning iteration 828/50000                      

                       Computation: 120683 steps/s (collection: 0.692s, learning 0.123s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4235
       Mean episode rew_ang_vel_xy: -0.0631
          Mean episode rew_dof_acc: -0.2697
   Mean episode rew_dof_pos_limits: -0.0293
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1387
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -1.3059
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0645
        Mean episode terrain_level: 0.0926
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.81s
                        Total time: 653.70s
                               ETA: 646 mins 14.1 s

################################################################################
                      Learning iteration 829/50000                      

                       Computation: 128119 steps/s (collection: 0.643s, learning 0.125s)
               Value function loss: 0.1085
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4260
       Mean episode rew_ang_vel_xy: -0.0626
          Mean episode rew_dof_acc: -0.2622
   Mean episode rew_dof_pos_limits: -0.0254
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1357
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.3107
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0654
        Mean episode terrain_level: 0.0929
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.77s
                        Total time: 654.47s
                               ETA: 646 mins 12.1 s

################################################################################
                      Learning iteration 830/50000                      

                       Computation: 121783 steps/s (collection: 0.684s, learning 0.124s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2614
       Mean episode rew_ang_vel_xy: -0.0617
          Mean episode rew_dof_acc: -0.2542
   Mean episode rew_dof_pos_limits: -0.0248
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1398
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.2207
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0595
        Mean episode terrain_level: 0.0947
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.81s
                        Total time: 655.27s
                               ETA: 646 mins 12.4 s

################################################################################
                      Learning iteration 831/50000                      

                       Computation: 110043 steps/s (collection: 0.772s, learning 0.122s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3591
       Mean episode rew_ang_vel_xy: -0.0633
          Mean episode rew_dof_acc: -0.2668
   Mean episode rew_dof_pos_limits: -0.0268
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1391
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -1.2738
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0679
        Mean episode terrain_level: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.89s
                        Total time: 656.17s
                               ETA: 646 mins 17.8 s

################################################################################
                      Learning iteration 832/50000                      

                       Computation: 125765 steps/s (collection: 0.658s, learning 0.124s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4748
       Mean episode rew_ang_vel_xy: -0.0632
          Mean episode rew_dof_acc: -0.2685
   Mean episode rew_dof_pos_limits: -0.0266
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1353
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.3293
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0671
        Mean episode terrain_level: 0.0994
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.78s
                        Total time: 656.95s
                               ETA: 646 mins 16.6 s

################################################################################
                      Learning iteration 833/50000                      

                       Computation: 120275 steps/s (collection: 0.685s, learning 0.132s)
               Value function loss: 0.1074
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3583
       Mean episode rew_ang_vel_xy: -0.0621
          Mean episode rew_dof_acc: -0.2554
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1325
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.2674
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0684
        Mean episode terrain_level: 0.0980
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.82s
                        Total time: 657.77s
                               ETA: 646 mins 17.5 s

################################################################################
                      Learning iteration 834/50000                      

                       Computation: 117661 steps/s (collection: 0.712s, learning 0.124s)
               Value function loss: 0.1120
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3545
       Mean episode rew_ang_vel_xy: -0.0632
          Mean episode rew_dof_acc: -0.2607
   Mean episode rew_dof_pos_limits: -0.0256
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1389
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.2622
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0589
        Mean episode terrain_level: 0.0984
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.84s
                        Total time: 658.60s
                               ETA: 646 mins 19.5 s

################################################################################
                      Learning iteration 835/50000                      

                       Computation: 102629 steps/s (collection: 0.813s, learning 0.145s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2254
       Mean episode rew_ang_vel_xy: -0.0606
          Mean episode rew_dof_acc: -0.2423
   Mean episode rew_dof_pos_limits: -0.0240
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1277
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -1.1800
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0981
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.96s
                        Total time: 659.56s
                               ETA: 646 mins 28.6 s

################################################################################
                      Learning iteration 836/50000                      

                       Computation: 115944 steps/s (collection: 0.723s, learning 0.125s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4831
       Mean episode rew_ang_vel_xy: -0.0649
          Mean episode rew_dof_acc: -0.2708
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1403
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3350
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0702
        Mean episode terrain_level: 0.0988
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.85s
                        Total time: 660.41s
                               ETA: 646 mins 31.3 s

################################################################################
                      Learning iteration 837/50000                      

                       Computation: 105128 steps/s (collection: 0.806s, learning 0.129s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4921
       Mean episode rew_ang_vel_xy: -0.0645
          Mean episode rew_dof_acc: -0.2674
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1411
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -1.3228
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0688
        Mean episode terrain_level: 0.0988
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.94s
                        Total time: 661.34s
                               ETA: 646 mins 39.1 s

################################################################################
                      Learning iteration 838/50000                      

                       Computation: 103072 steps/s (collection: 0.818s, learning 0.135s)
               Value function loss: 0.1084
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2969
       Mean episode rew_ang_vel_xy: -0.0620
          Mean episode rew_dof_acc: -0.2542
   Mean episode rew_dof_pos_limits: -0.0229
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1305
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -1.2128
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0986
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.95s
                        Total time: 662.30s
                               ETA: 646 mins 47.9 s

################################################################################
                      Learning iteration 839/50000                      

                       Computation: 108539 steps/s (collection: 0.779s, learning 0.127s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2925
       Mean episode rew_ang_vel_xy: -0.0607
          Mean episode rew_dof_acc: -0.2544
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0056
       Mean episode rew_smoothness: -1.2089
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0957
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.91s
                        Total time: 663.20s
                               ETA: 646 mins 53.9 s

################################################################################
                      Learning iteration 840/50000                      

                       Computation: 102263 steps/s (collection: 0.815s, learning 0.146s)
               Value function loss: 0.1152
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5691
       Mean episode rew_ang_vel_xy: -0.0658
          Mean episode rew_dof_acc: -0.2759
   Mean episode rew_dof_pos_limits: -0.0270
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1432
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3670
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.0963
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.96s
                        Total time: 664.16s
                               ETA: 647 mins 3.2 s

################################################################################
                      Learning iteration 841/50000                      

                       Computation: 122497 steps/s (collection: 0.670s, learning 0.133s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7021
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.2805
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1423
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.4566
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0824
        Mean episode terrain_level: 0.0935
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.80s
                        Total time: 664.97s
                               ETA: 647 mins 3.2 s

################################################################################
                      Learning iteration 842/50000                      

                       Computation: 118556 steps/s (collection: 0.704s, learning 0.126s)
               Value function loss: 0.1083
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4936
       Mean episode rew_ang_vel_xy: -0.0640
          Mean episode rew_dof_acc: -0.2676
   Mean episode rew_dof_pos_limits: -0.0264
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1380
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.3222
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0678
        Mean episode terrain_level: 0.0966
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.83s
                        Total time: 665.80s
                               ETA: 647 mins 4.7 s

################################################################################
                      Learning iteration 843/50000                      

                       Computation: 122256 steps/s (collection: 0.675s, learning 0.129s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3527
       Mean episode rew_ang_vel_xy: -0.0622
          Mean episode rew_dof_acc: -0.2534
   Mean episode rew_dof_pos_limits: -0.0239
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1345
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -1.2450
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0977
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.80s
                        Total time: 666.60s
                               ETA: 647 mins 4.7 s

################################################################################
                      Learning iteration 844/50000                      

                       Computation: 117118 steps/s (collection: 0.716s, learning 0.124s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4215
       Mean episode rew_ang_vel_xy: -0.0627
          Mean episode rew_dof_acc: -0.2497
   Mean episode rew_dof_pos_limits: -0.0294
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1330
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.2763
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0710
        Mean episode terrain_level: 0.0989
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.84s
                        Total time: 667.44s
                               ETA: 647 mins 6.8 s

################################################################################
                      Learning iteration 845/50000                      

                       Computation: 116985 steps/s (collection: 0.710s, learning 0.130s)
               Value function loss: 0.1047
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4277
       Mean episode rew_ang_vel_xy: -0.0620
          Mean episode rew_dof_acc: -0.2426
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1329
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0061
       Mean episode rew_smoothness: -1.2793
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0650
        Mean episode terrain_level: 0.0979
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.84s
                        Total time: 668.28s
                               ETA: 647 mins 8.9 s

################################################################################
                      Learning iteration 846/50000                      

                       Computation: 117250 steps/s (collection: 0.706s, learning 0.132s)
               Value function loss: 0.1109
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6635
       Mean episode rew_ang_vel_xy: -0.0655
          Mean episode rew_dof_acc: -0.2685
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1400
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.4194
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0773
        Mean episode terrain_level: 0.0962
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.84s
                        Total time: 669.12s
                               ETA: 647 mins 11.0 s

################################################################################
                      Learning iteration 847/50000                      

                       Computation: 114406 steps/s (collection: 0.727s, learning 0.132s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4076
       Mean episode rew_ang_vel_xy: -0.0639
          Mean episode rew_dof_acc: -0.2666
   Mean episode rew_dof_pos_limits: -0.0256
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1376
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -1.2841
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0966
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.86s
                        Total time: 669.98s
                               ETA: 647 mins 14.2 s

################################################################################
                      Learning iteration 848/50000                      

                       Computation: 116690 steps/s (collection: 0.717s, learning 0.126s)
               Value function loss: 0.1028
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5779
       Mean episode rew_ang_vel_xy: -0.0642
          Mean episode rew_dof_acc: -0.2708
   Mean episode rew_dof_pos_limits: -0.0305
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1357
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.3709
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0761
        Mean episode terrain_level: 0.0975
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.84s
                        Total time: 670.82s
                               ETA: 647 mins 16.4 s

################################################################################
                      Learning iteration 849/50000                      

                       Computation: 125659 steps/s (collection: 0.658s, learning 0.124s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4246
       Mean episode rew_ang_vel_xy: -0.0630
          Mean episode rew_dof_acc: -0.2610
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1276
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2926
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.0952
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.78s
                        Total time: 671.60s
                               ETA: 647 mins 15.2 s

################################################################################
                      Learning iteration 850/50000                      

                       Computation: 124003 steps/s (collection: 0.667s, learning 0.126s)
               Value function loss: 0.1103
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.72
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4261
       Mean episode rew_ang_vel_xy: -0.0631
          Mean episode rew_dof_acc: -0.2597
   Mean episode rew_dof_pos_limits: -0.0273
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1378
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2825
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.0943
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.79s
                        Total time: 672.39s
                               ETA: 647 mins 14.5 s

################################################################################
                      Learning iteration 851/50000                      

                       Computation: 106465 steps/s (collection: 0.791s, learning 0.132s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5801
       Mean episode rew_ang_vel_xy: -0.0635
          Mean episode rew_dof_acc: -0.2678
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1332
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -1.3705
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0673
        Mean episode terrain_level: 0.0966
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.92s
                        Total time: 673.32s
                               ETA: 647 mins 21.4 s

################################################################################
                      Learning iteration 852/50000                      

                       Computation: 126527 steps/s (collection: 0.653s, learning 0.124s)
               Value function loss: 0.1071
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4950
       Mean episode rew_ang_vel_xy: -0.0631
          Mean episode rew_dof_acc: -0.2617
   Mean episode rew_dof_pos_limits: -0.0275
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1341
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -1.3196
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.0955
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.78s
                        Total time: 674.09s
                               ETA: 647 mins 19.9 s

################################################################################
                      Learning iteration 853/50000                      

                       Computation: 116652 steps/s (collection: 0.720s, learning 0.123s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5294
       Mean episode rew_ang_vel_xy: -0.0626
          Mean episode rew_dof_acc: -0.2551
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1360
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.3416
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0823
        Mean episode terrain_level: 0.0927
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.84s
                        Total time: 674.94s
                               ETA: 647 mins 22.1 s

################################################################################
                      Learning iteration 854/50000                      

                       Computation: 125198 steps/s (collection: 0.661s, learning 0.125s)
               Value function loss: 0.1054
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6131
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.2730
   Mean episode rew_dof_pos_limits: -0.0268
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1364
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.3985
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0617
        Mean episode terrain_level: 0.0960
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.79s
                        Total time: 675.72s
                               ETA: 647 mins 21.0 s

################################################################################
                      Learning iteration 855/50000                      

                       Computation: 120959 steps/s (collection: 0.689s, learning 0.123s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4196
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2522
   Mean episode rew_dof_pos_limits: -0.0249
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1365
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2844
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0981
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.81s
                        Total time: 676.54s
                               ETA: 647 mins 21.5 s

################################################################################
                      Learning iteration 856/50000                      

                       Computation: 119177 steps/s (collection: 0.670s, learning 0.155s)
               Value function loss: 0.1019
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6396
       Mean episode rew_ang_vel_xy: -0.0667
          Mean episode rew_dof_acc: -0.2639
   Mean episode rew_dof_pos_limits: -0.0287
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1370
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.3927
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0687
        Mean episode terrain_level: 0.1007
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.82s
                        Total time: 677.36s
                               ETA: 647 mins 22.7 s

################################################################################
                      Learning iteration 857/50000                      

                       Computation: 118106 steps/s (collection: 0.688s, learning 0.144s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5668
       Mean episode rew_ang_vel_xy: -0.0649
          Mean episode rew_dof_acc: -0.2677
   Mean episode rew_dof_pos_limits: -0.0260
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1351
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3723
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0995
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.83s
                        Total time: 678.19s
                               ETA: 647 mins 24.3 s

################################################################################
                      Learning iteration 858/50000                      

                       Computation: 123050 steps/s (collection: 0.659s, learning 0.139s)
               Value function loss: 0.1095
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7288
       Mean episode rew_ang_vel_xy: -0.0660
          Mean episode rew_dof_acc: -0.2820
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1350
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.4515
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0728
        Mean episode terrain_level: 0.0986
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.80s
                        Total time: 678.99s
                               ETA: 647 mins 24.0 s

################################################################################
                      Learning iteration 859/50000                      

                       Computation: 121795 steps/s (collection: 0.676s, learning 0.131s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.74
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4657
       Mean episode rew_ang_vel_xy: -0.0614
          Mean episode rew_dof_acc: -0.2553
   Mean episode rew_dof_pos_limits: -0.0245
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1269
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.2995
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0611
        Mean episode terrain_level: 0.0988
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.81s
                        Total time: 679.80s
                               ETA: 647 mins 24.2 s

################################################################################
                      Learning iteration 860/50000                      

                       Computation: 123177 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.74
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4901
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.2511
   Mean episode rew_dof_pos_limits: -0.0274
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1342
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.3228
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0993
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.80s
                        Total time: 680.60s
                               ETA: 647 mins 23.8 s

################################################################################
                      Learning iteration 861/50000                      

                       Computation: 124104 steps/s (collection: 0.668s, learning 0.124s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.74
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5395
       Mean episode rew_ang_vel_xy: -0.0648
          Mean episode rew_dof_acc: -0.2664
   Mean episode rew_dof_pos_limits: -0.0226
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1306
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.3436
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0585
        Mean episode terrain_level: 0.1004
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.79s
                        Total time: 681.39s
                               ETA: 647 mins 23.1 s

################################################################################
                      Learning iteration 862/50000                      

                       Computation: 126239 steps/s (collection: 0.656s, learning 0.122s)
               Value function loss: 0.1032
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6240
       Mean episode rew_ang_vel_xy: -0.0648
          Mean episode rew_dof_acc: -0.2715
   Mean episode rew_dof_pos_limits: -0.0278
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1406
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4026
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0663
        Mean episode terrain_level: 0.0996
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.78s
                        Total time: 682.17s
                               ETA: 647 mins 21.7 s

################################################################################
                      Learning iteration 863/50000                      

                       Computation: 127426 steps/s (collection: 0.650s, learning 0.122s)
               Value function loss: 0.1114
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6990
       Mean episode rew_ang_vel_xy: -0.0657
          Mean episode rew_dof_acc: -0.2668
   Mean episode rew_dof_pos_limits: -0.0285
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1350
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.4444
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0731
        Mean episode terrain_level: 0.0994
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.77s
                        Total time: 682.94s
                               ETA: 647 mins 19.8 s

################################################################################
                      Learning iteration 864/50000                      

                       Computation: 125556 steps/s (collection: 0.658s, learning 0.125s)
               Value function loss: 0.1125
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5620
       Mean episode rew_ang_vel_xy: -0.0644
          Mean episode rew_dof_acc: -0.2648
   Mean episode rew_dof_pos_limits: -0.0281
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1410
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.3648
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0700
        Mean episode terrain_level: 0.0989
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.78s
                        Total time: 683.72s
                               ETA: 647 mins 18.6 s

################################################################################
                      Learning iteration 865/50000                      

                       Computation: 126550 steps/s (collection: 0.654s, learning 0.123s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5394
       Mean episode rew_ang_vel_xy: -0.0627
          Mean episode rew_dof_acc: -0.2573
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1325
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.3672
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0720
        Mean episode terrain_level: 0.0992
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.78s
                        Total time: 684.50s
                               ETA: 647 mins 17.0 s

################################################################################
                      Learning iteration 866/50000                      

                       Computation: 125038 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.1055
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.76
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5693
       Mean episode rew_ang_vel_xy: -0.0617
          Mean episode rew_dof_acc: -0.2609
   Mean episode rew_dof_pos_limits: -0.0259
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1300
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.3894
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0679
        Mean episode terrain_level: 0.0954
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.79s
                        Total time: 685.29s
                               ETA: 647 mins 16.0 s

################################################################################
                      Learning iteration 867/50000                      

                       Computation: 125022 steps/s (collection: 0.653s, learning 0.133s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.76
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7192
       Mean episode rew_ang_vel_xy: -0.0682
          Mean episode rew_dof_acc: -0.2665
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1331
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.4492
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0700
        Mean episode terrain_level: 0.0941
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.79s
                        Total time: 686.07s
                               ETA: 647 mins 15.0 s

################################################################################
                      Learning iteration 868/50000                      

                       Computation: 121825 steps/s (collection: 0.666s, learning 0.141s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.76
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7118
       Mean episode rew_ang_vel_xy: -0.0668
          Mean episode rew_dof_acc: -0.2694
   Mean episode rew_dof_pos_limits: -0.0294
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1381
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.4471
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0724
        Mean episode terrain_level: 0.0963
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.81s
                        Total time: 686.88s
                               ETA: 647 mins 15.1 s

################################################################################
                      Learning iteration 869/50000                      

                       Computation: 123450 steps/s (collection: 0.655s, learning 0.142s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.76
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7297
       Mean episode rew_ang_vel_xy: -0.0665
          Mean episode rew_dof_acc: -0.2765
   Mean episode rew_dof_pos_limits: -0.0268
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1378
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.4646
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0676
        Mean episode terrain_level: 0.0986
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.80s
                        Total time: 687.67s
                               ETA: 647 mins 14.6 s

################################################################################
                      Learning iteration 870/50000                      

                       Computation: 123177 steps/s (collection: 0.658s, learning 0.140s)
               Value function loss: 0.1067
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5537
       Mean episode rew_ang_vel_xy: -0.0646
          Mean episode rew_dof_acc: -0.2622
   Mean episode rew_dof_pos_limits: -0.0265
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1367
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.3646
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0682
        Mean episode terrain_level: 0.0995
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.80s
                        Total time: 688.47s
                               ETA: 647 mins 14.3 s

################################################################################
                      Learning iteration 871/50000                      

                       Computation: 119084 steps/s (collection: 0.659s, learning 0.167s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6131
       Mean episode rew_ang_vel_xy: -0.0629
          Mean episode rew_dof_acc: -0.2581
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1314
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.3996
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.0991
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.83s
                        Total time: 689.30s
                               ETA: 647 mins 15.5 s

################################################################################
                      Learning iteration 872/50000                      

                       Computation: 120464 steps/s (collection: 0.691s, learning 0.125s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6394
       Mean episode rew_ang_vel_xy: -0.0645
          Mean episode rew_dof_acc: -0.2678
   Mean episode rew_dof_pos_limits: -0.0281
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1394
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4209
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0688
        Mean episode terrain_level: 0.0993
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.82s
                        Total time: 690.11s
                               ETA: 647 mins 16.1 s

################################################################################
                      Learning iteration 873/50000                      

                       Computation: 114438 steps/s (collection: 0.729s, learning 0.130s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7505
       Mean episode rew_ang_vel_xy: -0.0660
          Mean episode rew_dof_acc: -0.2638
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1360
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.4717
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0813
        Mean episode terrain_level: 0.1008
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.86s
                        Total time: 690.97s
                               ETA: 647 mins 19.2 s

################################################################################
                      Learning iteration 874/50000                      

                       Computation: 110967 steps/s (collection: 0.762s, learning 0.124s)
               Value function loss: 0.1054
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5892
       Mean episode rew_ang_vel_xy: -0.0633
          Mean episode rew_dof_acc: -0.2563
   Mean episode rew_dof_pos_limits: -0.0291
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1344
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.3855
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.0989
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.89s
                        Total time: 691.86s
                               ETA: 647 mins 23.7 s

################################################################################
                      Learning iteration 875/50000                      

                       Computation: 115826 steps/s (collection: 0.726s, learning 0.123s)
               Value function loss: 0.1102
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.78
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6742
       Mean episode rew_ang_vel_xy: -0.0641
          Mean episode rew_dof_acc: -0.2662
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1378
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.4318
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0715
        Mean episode terrain_level: 0.0992
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.85s
                        Total time: 692.71s
                               ETA: 647 mins 26.2 s

################################################################################
                      Learning iteration 876/50000                      

                       Computation: 116362 steps/s (collection: 0.713s, learning 0.132s)
               Value function loss: 0.1057
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.78
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6350
       Mean episode rew_ang_vel_xy: -0.0632
          Mean episode rew_dof_acc: -0.2622
   Mean episode rew_dof_pos_limits: -0.0256
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1334
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.4177
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.1019
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.84s
                        Total time: 693.55s
                               ETA: 647 mins 28.4 s

################################################################################
                      Learning iteration 877/50000                      

                       Computation: 115838 steps/s (collection: 0.718s, learning 0.130s)
               Value function loss: 0.1082
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.78
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6946
       Mean episode rew_ang_vel_xy: -0.0654
          Mean episode rew_dof_acc: -0.2690
   Mean episode rew_dof_pos_limits: -0.0271
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1384
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4487
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.1035
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.85s
                        Total time: 694.40s
                               ETA: 647 mins 30.9 s

################################################################################
                      Learning iteration 878/50000                      

                       Computation: 120433 steps/s (collection: 0.691s, learning 0.125s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.79
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9105
       Mean episode rew_ang_vel_xy: -0.0680
          Mean episode rew_dof_acc: -0.2840
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1355
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.5615
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0783
        Mean episode terrain_level: 0.1041
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.82s
                        Total time: 695.22s
                               ETA: 647 mins 31.5 s

################################################################################
                      Learning iteration 879/50000                      

                       Computation: 122186 steps/s (collection: 0.666s, learning 0.139s)
               Value function loss: 0.1117
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.79
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7050
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.2694
   Mean episode rew_dof_pos_limits: -0.0261
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1361
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4320
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0667
        Mean episode terrain_level: 0.1072
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.80s
                        Total time: 696.02s
                               ETA: 647 mins 31.5 s

################################################################################
                      Learning iteration 880/50000                      

                       Computation: 121657 steps/s (collection: 0.683s, learning 0.125s)
               Value function loss: 0.1106
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.79
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5218
       Mean episode rew_ang_vel_xy: -0.0621
          Mean episode rew_dof_acc: -0.2527
   Mean episode rew_dof_pos_limits: -0.0315
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1349
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.3407
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0768
        Mean episode terrain_level: 0.1112
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.81s
                        Total time: 696.83s
                               ETA: 647 mins 31.6 s

################################################################################
                      Learning iteration 881/50000                      

                       Computation: 121448 steps/s (collection: 0.672s, learning 0.138s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.80
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7278
       Mean episode rew_ang_vel_xy: -0.0650
          Mean episode rew_dof_acc: -0.2665
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1339
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.4459
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0775
        Mean episode terrain_level: 0.1111
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.81s
                        Total time: 697.64s
                               ETA: 647 mins 31.9 s

################################################################################
                      Learning iteration 882/50000                      

                       Computation: 120596 steps/s (collection: 0.678s, learning 0.137s)
               Value function loss: 0.1103
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.80
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7196
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.2600
   Mean episode rew_dof_pos_limits: -0.0270
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1334
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4473
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0670
        Mean episode terrain_level: 0.1097
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.82s
                        Total time: 698.45s
                               ETA: 647 mins 32.4 s

################################################################################
                      Learning iteration 883/50000                      

                       Computation: 119477 steps/s (collection: 0.700s, learning 0.123s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.80
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7832
       Mean episode rew_ang_vel_xy: -0.0672
          Mean episode rew_dof_acc: -0.2661
   Mean episode rew_dof_pos_limits: -0.0322
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1410
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.4739
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0728
        Mean episode terrain_level: 0.1113
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.82s
                        Total time: 699.28s
                               ETA: 647 mins 33.4 s

################################################################################
                      Learning iteration 884/50000                      

                       Computation: 120229 steps/s (collection: 0.687s, learning 0.131s)
               Value function loss: 0.1058
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.80
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7515
       Mean episode rew_ang_vel_xy: -0.0652
          Mean episode rew_dof_acc: -0.2622
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1366
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.4564
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0776
        Mean episode terrain_level: 0.1112
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.82s
                        Total time: 700.10s
                               ETA: 647 mins 34.1 s

################################################################################
                      Learning iteration 885/50000                      

                       Computation: 121449 steps/s (collection: 0.685s, learning 0.124s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7148
       Mean episode rew_ang_vel_xy: -0.0664
          Mean episode rew_dof_acc: -0.2609
   Mean episode rew_dof_pos_limits: -0.0287
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1370
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4358
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0708
        Mean episode terrain_level: 0.1085
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.81s
                        Total time: 700.90s
                               ETA: 647 mins 34.3 s

################################################################################
                      Learning iteration 886/50000                      

                       Computation: 120235 steps/s (collection: 0.690s, learning 0.128s)
               Value function loss: 0.1111
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7841
       Mean episode rew_ang_vel_xy: -0.0673
          Mean episode rew_dof_acc: -0.2632
   Mean episode rew_dof_pos_limits: -0.0292
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.4750
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0722
        Mean episode terrain_level: 0.1052
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.82s
                        Total time: 701.72s
                               ETA: 647 mins 35.0 s

################################################################################
                      Learning iteration 887/50000                      

                       Computation: 122486 steps/s (collection: 0.679s, learning 0.123s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7508
       Mean episode rew_ang_vel_xy: -0.0670
          Mean episode rew_dof_acc: -0.2736
   Mean episode rew_dof_pos_limits: -0.0224
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1403
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.4320
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.1090
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.80s
                        Total time: 702.52s
                               ETA: 647 mins 34.8 s

################################################################################
                      Learning iteration 888/50000                      

                       Computation: 122691 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.1074
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8734
       Mean episode rew_ang_vel_xy: -0.0676
          Mean episode rew_dof_acc: -0.2807
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1371
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.5116
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0763
        Mean episode terrain_level: 0.1115
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.80s
                        Total time: 703.33s
                               ETA: 647 mins 34.6 s

################################################################################
                      Learning iteration 889/50000                      

                       Computation: 102326 steps/s (collection: 0.810s, learning 0.151s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.82
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6782
       Mean episode rew_ang_vel_xy: -0.0650
          Mean episode rew_dof_acc: -0.2688
   Mean episode rew_dof_pos_limits: -0.0267
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1442
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4264
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.1104
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.96s
                        Total time: 704.29s
                               ETA: 647 mins 43.2 s

################################################################################
                      Learning iteration 890/50000                      

                       Computation: 105278 steps/s (collection: 0.799s, learning 0.135s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.82
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8241
       Mean episode rew_ang_vel_xy: -0.0666
          Mean episode rew_dof_acc: -0.2684
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1400
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.4884
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0730
        Mean episode terrain_level: 0.1058
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.93s
                        Total time: 705.22s
                               ETA: 647 mins 50.2 s

################################################################################
                      Learning iteration 891/50000                      

                       Computation: 118411 steps/s (collection: 0.694s, learning 0.136s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.82
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7945
       Mean episode rew_ang_vel_xy: -0.0634
          Mean episode rew_dof_acc: -0.2610
   Mean episode rew_dof_pos_limits: -0.0324
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1356
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.4740
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0829
        Mean episode terrain_level: 0.1019
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.83s
                        Total time: 706.05s
                               ETA: 647 mins 51.6 s

################################################################################
                      Learning iteration 892/50000                      

                       Computation: 113890 steps/s (collection: 0.731s, learning 0.132s)
               Value function loss: 0.1043
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.82
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7306
       Mean episode rew_ang_vel_xy: -0.0645
          Mean episode rew_dof_acc: -0.2717
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1358
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.4600
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0668
        Mean episode terrain_level: 0.1014
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.86s
                        Total time: 706.91s
                               ETA: 647 mins 54.7 s

################################################################################
                      Learning iteration 893/50000                      

                       Computation: 116307 steps/s (collection: 0.721s, learning 0.124s)
               Value function loss: 0.1026
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.83
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7164
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.2627
   Mean episode rew_dof_pos_limits: -0.0270
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1373
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4267
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.1034
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.85s
                        Total time: 707.76s
                               ETA: 647 mins 56.9 s

################################################################################
                      Learning iteration 894/50000                      

                       Computation: 112024 steps/s (collection: 0.749s, learning 0.129s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.83
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5848
       Mean episode rew_ang_vel_xy: -0.0635
          Mean episode rew_dof_acc: -0.2529
   Mean episode rew_dof_pos_limits: -0.0275
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1393
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.3789
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.1055
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.88s
                        Total time: 708.64s
                               ETA: 648 mins 0.8 s

################################################################################
                      Learning iteration 895/50000                      

                       Computation: 123520 steps/s (collection: 0.670s, learning 0.126s)
               Value function loss: 0.1062
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.83
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8457
       Mean episode rew_ang_vel_xy: -0.0685
          Mean episode rew_dof_acc: -0.2769
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1317
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.5044
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.1051
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.80s
                        Total time: 709.43s
                               ETA: 648 mins 0.2 s

################################################################################
                      Learning iteration 896/50000                      

                       Computation: 118692 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.1113
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6776
       Mean episode rew_ang_vel_xy: -0.0657
          Mean episode rew_dof_acc: -0.2558
   Mean episode rew_dof_pos_limits: -0.0283
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4116
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0689
        Mean episode terrain_level: 0.1042
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.83s
                        Total time: 710.26s
                               ETA: 648 mins 1.4 s

################################################################################
                      Learning iteration 897/50000                      

                       Computation: 125237 steps/s (collection: 0.661s, learning 0.124s)
               Value function loss: 0.1084
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8211
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.2655
   Mean episode rew_dof_pos_limits: -0.0291
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1346
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.5003
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.1023
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.78s
                        Total time: 711.05s
                               ETA: 648 mins 0.2 s

################################################################################
                      Learning iteration 898/50000                      

                       Computation: 115692 steps/s (collection: 0.727s, learning 0.123s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9267
       Mean episode rew_ang_vel_xy: -0.0663
          Mean episode rew_dof_acc: -0.2789
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1424
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.5574
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0755
        Mean episode terrain_level: 0.0995
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.85s
                        Total time: 711.90s
                               ETA: 648 mins 2.6 s

################################################################################
                      Learning iteration 899/50000                      

                       Computation: 120510 steps/s (collection: 0.674s, learning 0.142s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9225
       Mean episode rew_ang_vel_xy: -0.0682
          Mean episode rew_dof_acc: -0.2702
   Mean episode rew_dof_pos_limits: -0.0261
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1361
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.5427
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.1010
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.82s
                        Total time: 712.71s
                               ETA: 648 mins 3.1 s

################################################################################
                      Learning iteration 900/50000                      

                       Computation: 104736 steps/s (collection: 0.802s, learning 0.137s)
               Value function loss: 0.1090
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8969
       Mean episode rew_ang_vel_xy: -0.0687
          Mean episode rew_dof_acc: -0.2752
   Mean episode rew_dof_pos_limits: -0.0259
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1370
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.5334
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.1023
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.94s
                        Total time: 713.65s
                               ETA: 648 mins 10.3 s

################################################################################
                      Learning iteration 901/50000                      

                       Computation: 118507 steps/s (collection: 0.692s, learning 0.138s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8290
       Mean episode rew_ang_vel_xy: -0.0660
          Mean episode rew_dof_acc: -0.2707
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1354
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.4857
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0584
        Mean episode terrain_level: 0.1017
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.83s
                        Total time: 714.48s
                               ETA: 648 mins 11.6 s

################################################################################
                      Learning iteration 902/50000                      

                       Computation: 108620 steps/s (collection: 0.762s, learning 0.143s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9411
       Mean episode rew_ang_vel_xy: -0.0687
          Mean episode rew_dof_acc: -0.2853
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1387
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.5708
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0702
        Mean episode terrain_level: 0.1048
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.91s
                        Total time: 715.38s
                               ETA: 648 mins 16.9 s

################################################################################
                      Learning iteration 903/50000                      

                       Computation: 121453 steps/s (collection: 0.685s, learning 0.125s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8233
       Mean episode rew_ang_vel_xy: -0.0646
          Mean episode rew_dof_acc: -0.2616
   Mean episode rew_dof_pos_limits: -0.0267
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1341
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4955
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0695
        Mean episode terrain_level: 0.1041
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.81s
                        Total time: 716.19s
                               ETA: 648 mins 17.1 s

################################################################################
                      Learning iteration 904/50000                      

                       Computation: 119123 steps/s (collection: 0.704s, learning 0.121s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9811
       Mean episode rew_ang_vel_xy: -0.0683
          Mean episode rew_dof_acc: -0.2828
   Mean episode rew_dof_pos_limits: -0.0285
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.5851
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0688
        Mean episode terrain_level: 0.1027
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.83s
                        Total time: 717.02s
                               ETA: 648 mins 18.1 s

################################################################################
                      Learning iteration 905/50000                      

                       Computation: 111913 steps/s (collection: 0.736s, learning 0.143s)
               Value function loss: 0.1087
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8277
       Mean episode rew_ang_vel_xy: -0.0648
          Mean episode rew_dof_acc: -0.2552
   Mean episode rew_dof_pos_limits: -0.0273
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1358
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4872
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0667
        Mean episode terrain_level: 0.1053
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.88s
                        Total time: 717.90s
                               ETA: 648 mins 21.9 s

################################################################################
                      Learning iteration 906/50000                      

                       Computation: 117628 steps/s (collection: 0.693s, learning 0.143s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7743
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2650
   Mean episode rew_dof_pos_limits: -0.0259
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1386
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.4604
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0570
        Mean episode terrain_level: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.84s
                        Total time: 718.73s
                               ETA: 648 mins 23.5 s

################################################################################
                      Learning iteration 907/50000                      

                       Computation: 112887 steps/s (collection: 0.735s, learning 0.136s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8478
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2675
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1402
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.4998
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.1037
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.87s
                        Total time: 719.60s
                               ETA: 648 mins 26.9 s

################################################################################
                      Learning iteration 908/50000                      

                       Computation: 122425 steps/s (collection: 0.668s, learning 0.135s)
               Value function loss: 0.1147
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8332
       Mean episode rew_ang_vel_xy: -0.0662
          Mean episode rew_dof_acc: -0.2676
   Mean episode rew_dof_pos_limits: -0.0253
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1369
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.4899
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0662
        Mean episode terrain_level: 0.1024
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.80s
                        Total time: 720.41s
                               ETA: 648 mins 26.7 s

################################################################################
                      Learning iteration 909/50000                      

                       Computation: 124754 steps/s (collection: 0.663s, learning 0.125s)
               Value function loss: 0.1050
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1075
       Mean episode rew_ang_vel_xy: -0.0680
          Mean episode rew_dof_acc: -0.2746
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1385
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.6445
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0834
        Mean episode terrain_level: 0.0992
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.79s
                        Total time: 721.19s
                               ETA: 648 mins 25.7 s

################################################################################
                      Learning iteration 910/50000                      

                       Computation: 115941 steps/s (collection: 0.724s, learning 0.124s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8305
       Mean episode rew_ang_vel_xy: -0.0642
          Mean episode rew_dof_acc: -0.2563
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1315
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4932
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0725
        Mean episode terrain_level: 0.0978
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.85s
                        Total time: 722.04s
                               ETA: 648 mins 27.9 s

################################################################################
                      Learning iteration 911/50000                      

                       Computation: 109024 steps/s (collection: 0.770s, learning 0.131s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9158
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2746
   Mean episode rew_dof_pos_limits: -0.0277
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1351
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.5395
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0677
        Mean episode terrain_level: 0.0974
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.90s
                        Total time: 722.94s
                               ETA: 648 mins 32.9 s

################################################################################
                      Learning iteration 912/50000                      

                       Computation: 118914 steps/s (collection: 0.704s, learning 0.122s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.87
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9237
       Mean episode rew_ang_vel_xy: -0.0671
          Mean episode rew_dof_acc: -0.2726
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1379
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.5636
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0724
        Mean episode terrain_level: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.83s
                        Total time: 723.77s
                               ETA: 648 mins 34.0 s

################################################################################
                      Learning iteration 913/50000                      

                       Computation: 115765 steps/s (collection: 0.714s, learning 0.135s)
               Value function loss: 0.1109
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.87
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9443
       Mean episode rew_ang_vel_xy: -0.0651
          Mean episode rew_dof_acc: -0.2820
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.5731
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0772
        Mean episode terrain_level: 0.0954
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.85s
                        Total time: 724.62s
                               ETA: 648 mins 36.2 s

################################################################################
                      Learning iteration 914/50000                      

                       Computation: 114877 steps/s (collection: 0.735s, learning 0.121s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.87
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9138
       Mean episode rew_ang_vel_xy: -0.0641
          Mean episode rew_dof_acc: -0.2550
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1319
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.5503
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0734
        Mean episode terrain_level: 0.0950
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.86s
                        Total time: 725.48s
                               ETA: 648 mins 38.8 s

################################################################################
                      Learning iteration 915/50000                      

                       Computation: 121262 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.1125
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.87
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7801
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2687
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1336
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.4652
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0946
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.81s
                        Total time: 726.29s
                               ETA: 648 mins 39.0 s

################################################################################
                      Learning iteration 916/50000                      

                       Computation: 114925 steps/s (collection: 0.730s, learning 0.126s)
               Value function loss: 0.1145
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9139
       Mean episode rew_ang_vel_xy: -0.0679
          Mean episode rew_dof_acc: -0.2802
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.5409
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0942
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.86s
                        Total time: 727.14s
                               ETA: 648 mins 41.5 s

################################################################################
                      Learning iteration 917/50000                      

                       Computation: 123689 steps/s (collection: 0.654s, learning 0.141s)
               Value function loss: 0.1071
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0681
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.2689
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1327
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.6165
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0812
        Mean episode terrain_level: 0.0948
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.79s
                        Total time: 727.94s
                               ETA: 648 mins 40.8 s

################################################################################
                      Learning iteration 918/50000                      

                       Computation: 119240 steps/s (collection: 0.702s, learning 0.122s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9135
       Mean episode rew_ang_vel_xy: -0.0654
          Mean episode rew_dof_acc: -0.2582
   Mean episode rew_dof_pos_limits: -0.0333
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1360
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.5486
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0768
        Mean episode terrain_level: 0.0929
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.82s
                        Total time: 728.76s
                               ETA: 648 mins 41.7 s

################################################################################
                      Learning iteration 919/50000                      

                       Computation: 125343 steps/s (collection: 0.661s, learning 0.123s)
               Value function loss: 0.1146
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9642
       Mean episode rew_ang_vel_xy: -0.0667
          Mean episode rew_dof_acc: -0.2754
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1418
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.5595
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0706
        Mean episode terrain_level: 0.0919
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.78s
                        Total time: 729.55s
                               ETA: 648 mins 40.4 s

################################################################################
                      Learning iteration 920/50000                      

                       Computation: 116878 steps/s (collection: 0.709s, learning 0.132s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9988
       Mean episode rew_ang_vel_xy: -0.0686
          Mean episode rew_dof_acc: -0.2818
   Mean episode rew_dof_pos_limits: -0.0264
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1437
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.5732
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.0939
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.84s
                        Total time: 730.39s
                               ETA: 648 mins 42.2 s

################################################################################
                      Learning iteration 921/50000                      

                       Computation: 108766 steps/s (collection: 0.779s, learning 0.125s)
               Value function loss: 0.1070
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9678
       Mean episode rew_ang_vel_xy: -0.0662
          Mean episode rew_dof_acc: -0.2712
   Mean episode rew_dof_pos_limits: -0.0262
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1403
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.5569
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0627
        Mean episode terrain_level: 0.0935
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.90s
                        Total time: 731.29s
                               ETA: 648 mins 47.3 s

################################################################################
                      Learning iteration 922/50000                      

                       Computation: 111291 steps/s (collection: 0.744s, learning 0.139s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0203
       Mean episode rew_ang_vel_xy: -0.0671
          Mean episode rew_dof_acc: -0.2739
   Mean episode rew_dof_pos_limits: -0.0266
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1374
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.5832
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0626
        Mean episode terrain_level: 0.0928
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.88s
                        Total time: 732.17s
                               ETA: 648 mins 51.3 s

################################################################################
                      Learning iteration 923/50000                      

                       Computation: 122880 steps/s (collection: 0.677s, learning 0.123s)
               Value function loss: 0.1060
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9607
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.2841
   Mean episode rew_dof_pos_limits: -0.0260
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1369
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.5660
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0620
        Mean episode terrain_level: 0.0932
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.80s
                        Total time: 732.97s
                               ETA: 648 mins 50.9 s

################################################################################
                      Learning iteration 924/50000                      

                       Computation: 117263 steps/s (collection: 0.707s, learning 0.131s)
               Value function loss: 0.1122
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0386
       Mean episode rew_ang_vel_xy: -0.0643
          Mean episode rew_dof_acc: -0.2665
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1321
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.6100
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0757
        Mean episode terrain_level: 0.0935
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.84s
                        Total time: 733.81s
                               ETA: 648 mins 52.5 s

################################################################################
                      Learning iteration 925/50000                      

                       Computation: 112025 steps/s (collection: 0.741s, learning 0.137s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1312
       Mean episode rew_ang_vel_xy: -0.0692
          Mean episode rew_dof_acc: -0.2806
   Mean episode rew_dof_pos_limits: -0.0299
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1472
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.6588
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0691
        Mean episode terrain_level: 0.0968
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.88s
                        Total time: 734.69s
                               ETA: 648 mins 56.1 s

################################################################################
                      Learning iteration 926/50000                      

                       Computation: 112940 steps/s (collection: 0.736s, learning 0.134s)
               Value function loss: 0.1026
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9508
       Mean episode rew_ang_vel_xy: -0.0658
          Mean episode rew_dof_acc: -0.2759
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1385
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.5466
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0663
        Mean episode terrain_level: 0.1006
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.87s
                        Total time: 735.56s
                               ETA: 648 mins 59.4 s

################################################################################
                      Learning iteration 927/50000                      

                       Computation: 124416 steps/s (collection: 0.666s, learning 0.124s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0422
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2800
   Mean episode rew_dof_pos_limits: -0.0332
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.6310
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0858
        Mean episode terrain_level: 0.0998
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.79s
                        Total time: 736.35s
                               ETA: 648 mins 58.5 s

################################################################################
                      Learning iteration 928/50000                      

                       Computation: 111832 steps/s (collection: 0.740s, learning 0.139s)
               Value function loss: 0.1100
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8356
       Mean episode rew_ang_vel_xy: -0.0668
          Mean episode rew_dof_acc: -0.2727
   Mean episode rew_dof_pos_limits: -0.0235
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.4896
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1985
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0987
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.88s
                        Total time: 737.23s
                               ETA: 649 mins 2.2 s

################################################################################
                      Learning iteration 929/50000                      

                       Computation: 115718 steps/s (collection: 0.726s, learning 0.124s)
               Value function loss: 0.1047
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0809
       Mean episode rew_ang_vel_xy: -0.0665
          Mean episode rew_dof_acc: -0.2921
   Mean episode rew_dof_pos_limits: -0.0275
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1338
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.6292
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.0988
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.85s
                        Total time: 738.08s
                               ETA: 649 mins 4.3 s

################################################################################
                      Learning iteration 930/50000                      

                       Computation: 119200 steps/s (collection: 0.698s, learning 0.126s)
               Value function loss: 0.1084
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9810
       Mean episode rew_ang_vel_xy: -0.0654
          Mean episode rew_dof_acc: -0.2709
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1371
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.5959
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0769
        Mean episode terrain_level: 0.0984
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.82s
                        Total time: 738.90s
                               ETA: 649 mins 5.2 s

################################################################################
                      Learning iteration 931/50000                      

                       Computation: 113888 steps/s (collection: 0.737s, learning 0.126s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9817
       Mean episode rew_ang_vel_xy: -0.0658
          Mean episode rew_dof_acc: -0.2667
   Mean episode rew_dof_pos_limits: -0.0275
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1320
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.5760
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0679
        Mean episode terrain_level: 0.0969
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.86s
                        Total time: 739.77s
                               ETA: 649 mins 8.0 s

################################################################################
                      Learning iteration 932/50000                      

                       Computation: 109263 steps/s (collection: 0.778s, learning 0.122s)
               Value function loss: 0.1054
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0490
       Mean episode rew_ang_vel_xy: -0.0676
          Mean episode rew_dof_acc: -0.2757
   Mean episode rew_dof_pos_limits: -0.0306
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1427
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.6187
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0734
        Mean episode terrain_level: 0.0956
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.90s
                        Total time: 740.67s
                               ETA: 649 mins 12.8 s

################################################################################
                      Learning iteration 933/50000                      

                       Computation: 120108 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0714
       Mean episode rew_ang_vel_xy: -0.0686
          Mean episode rew_dof_acc: -0.2712
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1400
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.6348
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0721
        Mean episode terrain_level: 0.0946
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.82s
                        Total time: 741.48s
                               ETA: 649 mins 13.3 s

################################################################################
                      Learning iteration 934/50000                      

                       Computation: 126175 steps/s (collection: 0.657s, learning 0.122s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8706
       Mean episode rew_ang_vel_xy: -0.0654
          Mean episode rew_dof_acc: -0.2714
   Mean episode rew_dof_pos_limits: -0.0231
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1345
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.5189
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0940
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.78s
                        Total time: 742.26s
                               ETA: 649 mins 11.8 s

################################################################################
                      Learning iteration 935/50000                      

                       Computation: 110048 steps/s (collection: 0.768s, learning 0.126s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.92
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0809
       Mean episode rew_ang_vel_xy: -0.0666
          Mean episode rew_dof_acc: -0.2747
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1324
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.6386
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0696
        Mean episode terrain_level: 0.0982
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.89s
                        Total time: 743.16s
                               ETA: 649 mins 16.2 s

################################################################################
                      Learning iteration 936/50000                      

                       Computation: 114208 steps/s (collection: 0.734s, learning 0.127s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.92
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0316
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.2716
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1367
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.6184
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0641
        Mean episode terrain_level: 0.0980
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.86s
                        Total time: 744.02s
                               ETA: 649 mins 18.9 s

################################################################################
                      Learning iteration 937/50000                      

                       Computation: 125614 steps/s (collection: 0.659s, learning 0.124s)
               Value function loss: 0.1110
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.92
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2224
       Mean episode rew_ang_vel_xy: -0.0690
          Mean episode rew_dof_acc: -0.2864
   Mean episode rew_dof_pos_limits: -0.0352
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1378
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.7189
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0788
        Mean episode terrain_level: 0.0998
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.78s
                        Total time: 744.80s
                               ETA: 649 mins 17.5 s

################################################################################
                      Learning iteration 938/50000                      

                       Computation: 117042 steps/s (collection: 0.718s, learning 0.122s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0143
       Mean episode rew_ang_vel_xy: -0.0673
          Mean episode rew_dof_acc: -0.2837
   Mean episode rew_dof_pos_limits: -0.0287
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.6068
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.1021
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.84s
                        Total time: 745.64s
                               ETA: 649 mins 19.1 s

################################################################################
                      Learning iteration 939/50000                      

                       Computation: 108539 steps/s (collection: 0.761s, learning 0.145s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.93
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1326
       Mean episode rew_ang_vel_xy: -0.0681
          Mean episode rew_dof_acc: -0.2733
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1406
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.6727
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0720
        Mean episode terrain_level: 0.1012
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.91s
                        Total time: 746.55s
                               ETA: 649 mins 24.1 s

################################################################################
                      Learning iteration 940/50000                      

                       Computation: 116522 steps/s (collection: 0.694s, learning 0.150s)
               Value function loss: 0.1106
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.93
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1861
       Mean episode rew_ang_vel_xy: -0.0680
          Mean episode rew_dof_acc: -0.2788
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1450
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.6973
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0769
        Mean episode terrain_level: 0.1055
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.84s
                        Total time: 747.39s
                               ETA: 649 mins 25.9 s

################################################################################
                      Learning iteration 941/50000                      

                       Computation: 121226 steps/s (collection: 0.685s, learning 0.126s)
               Value function loss: 0.1094
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.94
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1367
       Mean episode rew_ang_vel_xy: -0.0667
          Mean episode rew_dof_acc: -0.2837
   Mean episode rew_dof_pos_limits: -0.0266
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1373
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.6655
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0681
        Mean episode terrain_level: 0.1066
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.81s
                        Total time: 748.20s
                               ETA: 649 mins 26.0 s

################################################################################
                      Learning iteration 942/50000                      

                       Computation: 119867 steps/s (collection: 0.682s, learning 0.138s)
               Value function loss: 0.1142
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.94
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2132
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.2845
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.7339
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0843
        Mean episode terrain_level: 0.1064
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.82s
                        Total time: 749.02s
                               ETA: 649 mins 26.5 s

################################################################################
                      Learning iteration 943/50000                      

                       Computation: 125106 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.94
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2956
       Mean episode rew_ang_vel_xy: -0.0701
          Mean episode rew_dof_acc: -0.2969
   Mean episode rew_dof_pos_limits: -0.0296
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.7616
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0749
        Mean episode terrain_level: 0.1064
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.79s
                        Total time: 749.81s
                               ETA: 649 mins 25.3 s

################################################################################
                      Learning iteration 944/50000                      

                       Computation: 111560 steps/s (collection: 0.744s, learning 0.137s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.94
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9788
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.2692
   Mean episode rew_dof_pos_limits: -0.0264
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1375
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -1.5906
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0610
        Mean episode terrain_level: 0.1085
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.88s
                        Total time: 750.69s
                               ETA: 649 mins 29.0 s

################################################################################
                      Learning iteration 945/50000                      

                       Computation: 119927 steps/s (collection: 0.687s, learning 0.133s)
               Value function loss: 0.1035
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.95
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1309
       Mean episode rew_ang_vel_xy: -0.0683
          Mean episode rew_dof_acc: -0.2833
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1446
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.6717
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0697
        Mean episode terrain_level: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.82s
                        Total time: 751.51s
                               ETA: 649 mins 29.5 s

################################################################################
                      Learning iteration 946/50000                      

                       Computation: 108093 steps/s (collection: 0.772s, learning 0.138s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.95
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2641
       Mean episode rew_ang_vel_xy: -0.0685
          Mean episode rew_dof_acc: -0.2855
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.7408
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0699
        Mean episode terrain_level: 0.1129
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.91s
                        Total time: 752.42s
                               ETA: 649 mins 34.7 s

################################################################################
                      Learning iteration 947/50000                      

                       Computation: 122498 steps/s (collection: 0.678s, learning 0.125s)
               Value function loss: 0.1103
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.95
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3372
       Mean episode rew_ang_vel_xy: -0.0692
          Mean episode rew_dof_acc: -0.2849
   Mean episode rew_dof_pos_limits: -0.0363
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1360
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8026
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0866
        Mean episode terrain_level: 0.1108
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.80s
                        Total time: 753.22s
                               ETA: 649 mins 34.3 s

################################################################################
                      Learning iteration 948/50000                      

                       Computation: 119135 steps/s (collection: 0.703s, learning 0.123s)
               Value function loss: 0.1122
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.96
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2126
       Mean episode rew_ang_vel_xy: -0.0666
          Mean episode rew_dof_acc: -0.2798
   Mean episode rew_dof_pos_limits: -0.0331
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1370
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.7369
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0806
        Mean episode terrain_level: 0.1121
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.83s
                        Total time: 754.04s
                               ETA: 649 mins 35.1 s

################################################################################
                      Learning iteration 949/50000                      

                       Computation: 116518 steps/s (collection: 0.701s, learning 0.142s)
               Value function loss: 0.1071
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.96
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2163
       Mean episode rew_ang_vel_xy: -0.0674
          Mean episode rew_dof_acc: -0.2837
   Mean episode rew_dof_pos_limits: -0.0322
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1469
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.7179
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0773
        Mean episode terrain_level: 0.1076
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.84s
                        Total time: 754.89s
                               ETA: 649 mins 36.8 s

################################################################################
                      Learning iteration 950/50000                      

                       Computation: 112259 steps/s (collection: 0.736s, learning 0.140s)
               Value function loss: 0.1199
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.96
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3473
       Mean episode rew_ang_vel_xy: -0.0681
          Mean episode rew_dof_acc: -0.2853
   Mean episode rew_dof_pos_limits: -0.0331
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1334
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.7916
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.1061
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.88s
                        Total time: 755.76s
                               ETA: 649 mins 40.2 s

################################################################################
                      Learning iteration 951/50000                      

                       Computation: 118290 steps/s (collection: 0.674s, learning 0.157s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9367
       Mean episode rew_ang_vel_xy: -0.0657
          Mean episode rew_dof_acc: -0.2673
   Mean episode rew_dof_pos_limits: -0.0263
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1355
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.5550
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.1044
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.83s
                        Total time: 756.59s
                               ETA: 649 mins 41.3 s

################################################################################
                      Learning iteration 952/50000                      

                       Computation: 116402 steps/s (collection: 0.703s, learning 0.141s)
               Value function loss: 0.1151
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2223
       Mean episode rew_ang_vel_xy: -0.0688
          Mean episode rew_dof_acc: -0.2765
   Mean episode rew_dof_pos_limits: -0.0293
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1413
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.7215
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0657
        Mean episode terrain_level: 0.1051
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.84s
                        Total time: 757.44s
                               ETA: 649 mins 43.1 s

################################################################################
                      Learning iteration 953/50000                      

                       Computation: 111083 steps/s (collection: 0.760s, learning 0.125s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3067
       Mean episode rew_ang_vel_xy: -0.0684
          Mean episode rew_dof_acc: -0.2811
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1367
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.7731
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0779
        Mean episode terrain_level: 0.1113
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.88s
                        Total time: 758.32s
                               ETA: 649 mins 46.9 s

################################################################################
                      Learning iteration 954/50000                      

                       Computation: 119964 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2119
       Mean episode rew_ang_vel_xy: -0.0673
          Mean episode rew_dof_acc: -0.2780
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1459
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.7078
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.1137
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.82s
                        Total time: 759.14s
                               ETA: 649 mins 47.4 s

################################################################################
                      Learning iteration 955/50000                      

                       Computation: 121295 steps/s (collection: 0.664s, learning 0.147s)
               Value function loss: 0.1207
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1802
       Mean episode rew_ang_vel_xy: -0.0672
          Mean episode rew_dof_acc: -0.2700
   Mean episode rew_dof_pos_limits: -0.0315
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1407
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.6909
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0733
        Mean episode terrain_level: 0.1150
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.81s
                        Total time: 759.95s
                               ETA: 649 mins 47.4 s

################################################################################
                      Learning iteration 956/50000                      

                       Computation: 122179 steps/s (collection: 0.677s, learning 0.127s)
               Value function loss: 0.1180
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2349
       Mean episode rew_ang_vel_xy: -0.0671
          Mean episode rew_dof_acc: -0.2763
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1400
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.7457
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0742
        Mean episode terrain_level: 0.1124
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.80s
                        Total time: 760.76s
                               ETA: 649 mins 47.1 s

################################################################################
                      Learning iteration 957/50000                      

                       Computation: 113287 steps/s (collection: 0.743s, learning 0.125s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3517
       Mean episode rew_ang_vel_xy: -0.0708
          Mean episode rew_dof_acc: -0.2911
   Mean episode rew_dof_pos_limits: -0.0329
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.7731
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0765
        Mean episode terrain_level: 0.1131
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.87s
                        Total time: 761.63s
                               ETA: 649 mins 50.0 s

################################################################################
                      Learning iteration 958/50000                      

                       Computation: 122570 steps/s (collection: 0.678s, learning 0.124s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2631
       Mean episode rew_ang_vel_xy: -0.0688
          Mean episode rew_dof_acc: -0.2749
   Mean episode rew_dof_pos_limits: -0.0317
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1416
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.7356
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.1106
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.80s
                        Total time: 762.43s
                               ETA: 649 mins 49.6 s

################################################################################
                      Learning iteration 959/50000                      

                       Computation: 125123 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.1154
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1555
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.2819
   Mean episode rew_dof_pos_limits: -0.0287
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1400
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.6771
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0685
        Mean episode terrain_level: 0.1108
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.79s
                        Total time: 763.21s
                               ETA: 649 mins 48.3 s

################################################################################
                      Learning iteration 960/50000                      

                       Computation: 113941 steps/s (collection: 0.735s, learning 0.128s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4254
       Mean episode rew_ang_vel_xy: -0.0701
          Mean episode rew_dof_acc: -0.2944
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1423
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.8124
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0753
        Mean episode terrain_level: 0.1094
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.86s
                        Total time: 764.08s
                               ETA: 649 mins 51.0 s

################################################################################
                      Learning iteration 961/50000                      

                       Computation: 111200 steps/s (collection: 0.760s, learning 0.124s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2430
       Mean episode rew_ang_vel_xy: -0.0695
          Mean episode rew_dof_acc: -0.2855
   Mean episode rew_dof_pos_limits: -0.0291
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.7192
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0749
        Mean episode terrain_level: 0.1087
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.88s
                        Total time: 764.96s
                               ETA: 649 mins 54.7 s

################################################################################
                      Learning iteration 962/50000                      

                       Computation: 123970 steps/s (collection: 0.660s, learning 0.133s)
               Value function loss: 0.1162
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3448
       Mean episode rew_ang_vel_xy: -0.0699
          Mean episode rew_dof_acc: -0.2968
   Mean episode rew_dof_pos_limits: -0.0287
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1429
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.7817
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0759
        Mean episode terrain_level: 0.1136
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.79s
                        Total time: 765.75s
                               ETA: 649 mins 53.8 s

################################################################################
                      Learning iteration 963/50000                      

                       Computation: 125282 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0625
       Mean episode rew_ang_vel_xy: -0.0660
          Mean episode rew_dof_acc: -0.2761
   Mean episode rew_dof_pos_limits: -0.0228
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1416
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.6287
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.1138
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.78s
                        Total time: 766.54s
                               ETA: 649 mins 52.5 s

################################################################################
                      Learning iteration 964/50000                      

                       Computation: 115355 steps/s (collection: 0.730s, learning 0.123s)
               Value function loss: 0.1229
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5198
       Mean episode rew_ang_vel_xy: -0.0725
          Mean episode rew_dof_acc: -0.3164
   Mean episode rew_dof_pos_limits: -0.0321
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1417
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9108
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0822
        Mean episode terrain_level: 0.1140
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.85s
                        Total time: 767.39s
                               ETA: 649 mins 54.6 s

################################################################################
                      Learning iteration 965/50000                      

                       Computation: 120384 steps/s (collection: 0.695s, learning 0.122s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2600
       Mean episode rew_ang_vel_xy: -0.0696
          Mean episode rew_dof_acc: -0.2881
   Mean episode rew_dof_pos_limits: -0.0269
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.7266
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0646
        Mean episode terrain_level: 0.1143
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.82s
                        Total time: 768.21s
                               ETA: 649 mins 54.8 s

################################################################################
                      Learning iteration 966/50000                      

                       Computation: 117003 steps/s (collection: 0.708s, learning 0.133s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4385
       Mean episode rew_ang_vel_xy: -0.0701
          Mean episode rew_dof_acc: -0.2845
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8110
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0775
        Mean episode terrain_level: 0.1129
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.84s
                        Total time: 769.05s
                               ETA: 649 mins 56.3 s

################################################################################
                      Learning iteration 967/50000                      

                       Computation: 106819 steps/s (collection: 0.782s, learning 0.138s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4624
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2810
   Mean episode rew_dof_pos_limits: -0.0311
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1423
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.8439
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0807
        Mean episode terrain_level: 0.1126
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.92s
                        Total time: 769.97s
                               ETA: 650 mins 1.9 s

################################################################################
                      Learning iteration 968/50000                      

                       Computation: 116599 steps/s (collection: 0.713s, learning 0.130s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1582
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.2604
   Mean episode rew_dof_pos_limits: -0.0271
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1358
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -1.6791
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0624
        Mean episode terrain_level: 0.1137
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.84s
                        Total time: 770.81s
                               ETA: 650 mins 3.5 s

################################################################################
                      Learning iteration 969/50000                      

                       Computation: 115987 steps/s (collection: 0.714s, learning 0.134s)
               Value function loss: 0.1171
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3492
       Mean episode rew_ang_vel_xy: -0.0702
          Mean episode rew_dof_acc: -0.2830
   Mean episode rew_dof_pos_limits: -0.0256
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.7833
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0639
        Mean episode terrain_level: 0.1100
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.85s
                        Total time: 771.66s
                               ETA: 650 mins 5.3 s

################################################################################
                      Learning iteration 970/50000                      

                       Computation: 120544 steps/s (collection: 0.677s, learning 0.138s)
               Value function loss: 0.1185
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2426
       Mean episode rew_ang_vel_xy: -0.0675
          Mean episode rew_dof_acc: -0.2826
   Mean episode rew_dof_pos_limits: -0.0292
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1430
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.7244
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.1091
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.82s
                        Total time: 772.47s
                               ETA: 650 mins 5.5 s

################################################################################
                      Learning iteration 971/50000                      

                       Computation: 111031 steps/s (collection: 0.761s, learning 0.124s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4106
       Mean episode rew_ang_vel_xy: -0.0717
          Mean episode rew_dof_acc: -0.2869
   Mean episode rew_dof_pos_limits: -0.0274
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1436
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.8073
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.1115
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.89s
                        Total time: 773.36s
                               ETA: 650 mins 9.3 s

################################################################################
                      Learning iteration 972/50000                      

                       Computation: 120848 steps/s (collection: 0.689s, learning 0.125s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2825
       Mean episode rew_ang_vel_xy: -0.0681
          Mean episode rew_dof_acc: -0.2725
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1388
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.7609
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0754
        Mean episode terrain_level: 0.1107
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.81s
                        Total time: 774.17s
                               ETA: 650 mins 9.4 s

################################################################################
                      Learning iteration 973/50000                      

                       Computation: 117272 steps/s (collection: 0.714s, learning 0.125s)
               Value function loss: 0.1147
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4260
       Mean episode rew_ang_vel_xy: -0.0710
          Mean episode rew_dof_acc: -0.2964
   Mean episode rew_dof_pos_limits: -0.0294
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1514
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.8081
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0689
        Mean episode terrain_level: 0.1111
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.84s
                        Total time: 775.01s
                               ETA: 650 mins 10.7 s

################################################################################
                      Learning iteration 974/50000                      

                       Computation: 106691 steps/s (collection: 0.795s, learning 0.127s)
               Value function loss: 0.1141
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4604
       Mean episode rew_ang_vel_xy: -0.0704
          Mean episode rew_dof_acc: -0.2818
   Mean episode rew_dof_pos_limits: -0.0341
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1464
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.8417
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0824
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.92s
                        Total time: 775.93s
                               ETA: 650 mins 16.2 s

################################################################################
                      Learning iteration 975/50000                      

                       Computation: 122390 steps/s (collection: 0.679s, learning 0.124s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4719
       Mean episode rew_ang_vel_xy: -0.0712
          Mean episode rew_dof_acc: -0.2893
   Mean episode rew_dof_pos_limits: -0.0295
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1520
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.8430
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.80s
                        Total time: 776.74s
                               ETA: 650 mins 15.8 s

################################################################################
                      Learning iteration 976/50000                      

                       Computation: 116848 steps/s (collection: 0.712s, learning 0.130s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4213
       Mean episode rew_ang_vel_xy: -0.0691
          Mean episode rew_dof_acc: -0.2889
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1374
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.8201
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0772
        Mean episode terrain_level: 0.1149
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.84s
                        Total time: 777.58s
                               ETA: 650 mins 17.3 s

################################################################################
                      Learning iteration 977/50000                      

                       Computation: 117482 steps/s (collection: 0.715s, learning 0.122s)
               Value function loss: 0.1144
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3942
       Mean episode rew_ang_vel_xy: -0.0668
          Mean episode rew_dof_acc: -0.2822
   Mean episode rew_dof_pos_limits: -0.0284
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1426
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.8128
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.1123
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.84s
                        Total time: 778.41s
                               ETA: 650 mins 18.6 s

################################################################################
                      Learning iteration 978/50000                      

                       Computation: 113010 steps/s (collection: 0.749s, learning 0.121s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4351
       Mean episode rew_ang_vel_xy: -0.0705
          Mean episode rew_dof_acc: -0.2917
   Mean episode rew_dof_pos_limits: -0.0295
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1429
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.8344
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0714
        Mean episode terrain_level: 0.1123
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.87s
                        Total time: 779.28s
                               ETA: 650 mins 21.5 s

################################################################################
                      Learning iteration 979/50000                      

                       Computation: 112572 steps/s (collection: 0.726s, learning 0.147s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4846
       Mean episode rew_ang_vel_xy: -0.0714
          Mean episode rew_dof_acc: -0.2970
   Mean episode rew_dof_pos_limits: -0.0266
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1440
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.8266
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.87s
                        Total time: 780.16s
                               ETA: 650 mins 24.5 s

################################################################################
                      Learning iteration 980/50000                      

                       Computation: 118467 steps/s (collection: 0.706s, learning 0.124s)
               Value function loss: 0.1190
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5175
       Mean episode rew_ang_vel_xy: -0.0700
          Mean episode rew_dof_acc: -0.2873
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1456
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8644
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0763
        Mean episode terrain_level: 0.1163
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.83s
                        Total time: 780.99s
                               ETA: 650 mins 25.4 s

################################################################################
                      Learning iteration 981/50000                      

                       Computation: 118595 steps/s (collection: 0.686s, learning 0.143s)
               Value function loss: 0.1195
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5664
       Mean episode rew_ang_vel_xy: -0.0710
          Mean episode rew_dof_acc: -0.2871
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8888
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0738
        Mean episode terrain_level: 0.1124
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.83s
                        Total time: 781.81s
                               ETA: 650 mins 26.3 s

################################################################################
                      Learning iteration 982/50000                      

                       Computation: 114807 steps/s (collection: 0.723s, learning 0.133s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3965
       Mean episode rew_ang_vel_xy: -0.0703
          Mean episode rew_dof_acc: -0.2903
   Mean episode rew_dof_pos_limits: -0.0264
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.8009
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0683
        Mean episode terrain_level: 0.1099
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.86s
                        Total time: 782.67s
                               ETA: 650 mins 28.5 s

################################################################################
                      Learning iteration 983/50000                      

                       Computation: 116736 steps/s (collection: 0.717s, learning 0.125s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5933
       Mean episode rew_ang_vel_xy: -0.0695
          Mean episode rew_dof_acc: -0.2914
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1375
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9173
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0782
        Mean episode terrain_level: 0.1118
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.84s
                        Total time: 783.51s
                               ETA: 650 mins 29.9 s

################################################################################
                      Learning iteration 984/50000                      

                       Computation: 123270 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.1146
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3001
       Mean episode rew_ang_vel_xy: -0.0684
          Mean episode rew_dof_acc: -0.2742
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1413
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.7450
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.1117
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.80s
                        Total time: 784.31s
                               ETA: 650 mins 29.2 s

################################################################################
                      Learning iteration 985/50000                      

                       Computation: 106545 steps/s (collection: 0.799s, learning 0.123s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5434
       Mean episode rew_ang_vel_xy: -0.0696
          Mean episode rew_dof_acc: -0.2957
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1438
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.8891
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0732
        Mean episode terrain_level: 0.1101
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.92s
                        Total time: 785.23s
                               ETA: 650 mins 34.7 s

################################################################################
                      Learning iteration 986/50000                      

                       Computation: 120343 steps/s (collection: 0.686s, learning 0.130s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3139
       Mean episode rew_ang_vel_xy: -0.0677
          Mean episode rew_dof_acc: -0.2648
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1365
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.7650
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0699
        Mean episode terrain_level: 0.1105
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.82s
                        Total time: 786.05s
                               ETA: 650 mins 34.9 s

################################################################################
                      Learning iteration 987/50000                      

                       Computation: 114119 steps/s (collection: 0.728s, learning 0.133s)
               Value function loss: 0.1146
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3962
       Mean episode rew_ang_vel_xy: -0.0687
          Mean episode rew_dof_acc: -0.2842
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.8031
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.86s
                        Total time: 786.91s
                               ETA: 650 mins 37.3 s

################################################################################
                      Learning iteration 988/50000                      

                       Computation: 113657 steps/s (collection: 0.701s, learning 0.164s)
               Value function loss: 0.1118
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5146
       Mean episode rew_ang_vel_xy: -0.0694
          Mean episode rew_dof_acc: -0.2760
   Mean episode rew_dof_pos_limits: -0.0344
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1392
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8910
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0829
        Mean episode terrain_level: 0.1132
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.86s
                        Total time: 787.78s
                               ETA: 650 mins 39.9 s

################################################################################
                      Learning iteration 989/50000                      

                       Computation: 108771 steps/s (collection: 0.746s, learning 0.157s)
               Value function loss: 0.1205
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2759
       Mean episode rew_ang_vel_xy: -0.0668
          Mean episode rew_dof_acc: -0.2678
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.7409
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0687
        Mean episode terrain_level: 0.1142
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.90s
                        Total time: 788.68s
                               ETA: 650 mins 44.5 s

################################################################################
                      Learning iteration 990/50000                      

                       Computation: 116973 steps/s (collection: 0.716s, learning 0.125s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5675
       Mean episode rew_ang_vel_xy: -0.0698
          Mean episode rew_dof_acc: -0.2897
   Mean episode rew_dof_pos_limits: -0.0380
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1430
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -1.8983
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.1133
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.84s
                        Total time: 789.52s
                               ETA: 650 mins 45.8 s

################################################################################
                      Learning iteration 991/50000                      

                       Computation: 111401 steps/s (collection: 0.734s, learning 0.148s)
               Value function loss: 0.1148
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4328
       Mean episode rew_ang_vel_xy: -0.0672
          Mean episode rew_dof_acc: -0.2856
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.8337
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0761
        Mean episode terrain_level: 0.1155
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.88s
                        Total time: 790.40s
                               ETA: 650 mins 49.3 s

################################################################################
                      Learning iteration 992/50000                      

                       Computation: 109921 steps/s (collection: 0.772s, learning 0.123s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4716
       Mean episode rew_ang_vel_xy: -0.0674
          Mean episode rew_dof_acc: -0.2851
   Mean episode rew_dof_pos_limits: -0.0313
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1346
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.8592
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0758
        Mean episode terrain_level: 0.1141
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.89s
                        Total time: 791.30s
                               ETA: 650 mins 53.3 s

################################################################################
                      Learning iteration 993/50000                      

                       Computation: 118140 steps/s (collection: 0.709s, learning 0.123s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.06
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5220
       Mean episode rew_ang_vel_xy: -0.0707
          Mean episode rew_dof_acc: -0.2882
   Mean episode rew_dof_pos_limits: -0.0311
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.8788
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0793
        Mean episode terrain_level: 0.1129
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.83s
                        Total time: 792.13s
                               ETA: 650 mins 54.2 s

################################################################################
                      Learning iteration 994/50000                      

                       Computation: 112101 steps/s (collection: 0.747s, learning 0.130s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.06
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7816
       Mean episode rew_ang_vel_xy: -0.0708
          Mean episode rew_dof_acc: -0.3008
   Mean episode rew_dof_pos_limits: -0.0328
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.0059
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.1124
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.88s
                        Total time: 793.01s
                               ETA: 650 mins 57.4 s

################################################################################
                      Learning iteration 995/50000                      

                       Computation: 106190 steps/s (collection: 0.781s, learning 0.144s)
               Value function loss: 0.1161
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4076
       Mean episode rew_ang_vel_xy: -0.0678
          Mean episode rew_dof_acc: -0.2797
   Mean episode rew_dof_pos_limits: -0.0271
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1418
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.8007
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.1117
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.93s
                        Total time: 793.93s
                               ETA: 651 mins 2.9 s

################################################################################
                      Learning iteration 996/50000                      

                       Computation: 118231 steps/s (collection: 0.702s, learning 0.130s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.06
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6058
       Mean episode rew_ang_vel_xy: -0.0707
          Mean episode rew_dof_acc: -0.2934
   Mean episode rew_dof_pos_limits: -0.0329
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9221
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0776
        Mean episode terrain_level: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.83s
                        Total time: 794.76s
                               ETA: 651 mins 3.8 s

################################################################################
                      Learning iteration 997/50000                      

                       Computation: 118032 steps/s (collection: 0.706s, learning 0.127s)
               Value function loss: 0.1170
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8672
       Mean episode rew_ang_vel_xy: -0.0717
          Mean episode rew_dof_acc: -0.2947
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1436
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.0416
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1151
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.83s
                        Total time: 795.60s
                               ETA: 651 mins 4.7 s

################################################################################
                      Learning iteration 998/50000                      

                       Computation: 120263 steps/s (collection: 0.692s, learning 0.126s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3642
       Mean episode rew_ang_vel_xy: -0.0694
          Mean episode rew_dof_acc: -0.2841
   Mean episode rew_dof_pos_limits: -0.0249
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.7773
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.82s
                        Total time: 796.41s
                               ETA: 651 mins 4.9 s

################################################################################
                      Learning iteration 999/50000                      

                       Computation: 120772 steps/s (collection: 0.691s, learning 0.123s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7252
       Mean episode rew_ang_vel_xy: -0.0712
          Mean episode rew_dof_acc: -0.2915
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.9769
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0749
        Mean episode terrain_level: 0.1165
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.81s
                        Total time: 797.23s
                               ETA: 651 mins 5.0 s

################################################################################
                     Learning iteration 1000/50000                      

                       Computation: 116655 steps/s (collection: 0.720s, learning 0.123s)
               Value function loss: 0.1154
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6210
       Mean episode rew_ang_vel_xy: -0.0704
          Mean episode rew_dof_acc: -0.2956
   Mean episode rew_dof_pos_limits: -0.0294
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1442
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9147
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0747
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 0.84s
                        Total time: 798.07s
                               ETA: 651 mins 6.4 s

################################################################################
                     Learning iteration 1001/50000                      

                       Computation: 120821 steps/s (collection: 0.682s, learning 0.131s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.08
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6095
       Mean episode rew_ang_vel_xy: -0.0708
          Mean episode rew_dof_acc: -0.2959
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1438
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.9150
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0705
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 0.81s
                        Total time: 798.88s
                               ETA: 651 mins 6.4 s

################################################################################
                     Learning iteration 1002/50000                      

                       Computation: 111550 steps/s (collection: 0.740s, learning 0.141s)
               Value function loss: 0.1146
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.08
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5864
       Mean episode rew_ang_vel_xy: -0.0690
          Mean episode rew_dof_acc: -0.2747
   Mean episode rew_dof_pos_limits: -0.0344
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9063
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1186
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 0.88s
                        Total time: 799.77s
                               ETA: 651 mins 9.7 s

################################################################################
                     Learning iteration 1003/50000                      

                       Computation: 118918 steps/s (collection: 0.703s, learning 0.123s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.08
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6591
       Mean episode rew_ang_vel_xy: -0.0723
          Mean episode rew_dof_acc: -0.2969
   Mean episode rew_dof_pos_limits: -0.0302
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.9368
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0721
        Mean episode terrain_level: 0.1199
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 0.83s
                        Total time: 800.59s
                               ETA: 651 mins 10.3 s

################################################################################
                     Learning iteration 1004/50000                      

                       Computation: 118064 steps/s (collection: 0.706s, learning 0.127s)
               Value function loss: 0.1151
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.08
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4890
       Mean episode rew_ang_vel_xy: -0.0699
          Mean episode rew_dof_acc: -0.2717
   Mean episode rew_dof_pos_limits: -0.0282
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1516
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.8374
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0748
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 0.83s
                        Total time: 801.42s
                               ETA: 651 mins 11.2 s

################################################################################
                     Learning iteration 1005/50000                      

                       Computation: 124153 steps/s (collection: 0.669s, learning 0.122s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.09
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7880
       Mean episode rew_ang_vel_xy: -0.0717
          Mean episode rew_dof_acc: -0.2863
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.0183
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0912
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 0.79s
                        Total time: 802.22s
                               ETA: 651 mins 10.2 s

################################################################################
                     Learning iteration 1006/50000                      

                       Computation: 120088 steps/s (collection: 0.694s, learning 0.125s)
               Value function loss: 0.1189
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.09
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2650
       Mean episode rew_ang_vel_xy: -0.0682
          Mean episode rew_dof_acc: -0.2783
   Mean episode rew_dof_pos_limits: -0.0234
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1402
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.7017
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0546
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 0.82s
                        Total time: 803.04s
                               ETA: 651 mins 10.4 s

################################################################################
                     Learning iteration 1007/50000                      

                       Computation: 120231 steps/s (collection: 0.693s, learning 0.125s)
               Value function loss: 0.1139
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.09
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8308
       Mean episode rew_ang_vel_xy: -0.0743
          Mean episode rew_dof_acc: -0.3100
   Mean episode rew_dof_pos_limits: -0.0291
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1474
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.0357
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0753
        Mean episode terrain_level: 0.1157
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 0.82s
                        Total time: 803.85s
                               ETA: 651 mins 10.6 s

################################################################################
                     Learning iteration 1008/50000                      

                       Computation: 125745 steps/s (collection: 0.658s, learning 0.124s)
               Value function loss: 0.1197
                    Surrogate loss: -0.0170
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.10
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5205
       Mean episode rew_ang_vel_xy: -0.0700
          Mean episode rew_dof_acc: -0.2789
   Mean episode rew_dof_pos_limits: -0.0295
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1516
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.8688
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 0.78s
                        Total time: 804.63s
                               ETA: 651 mins 9.0 s

################################################################################
                     Learning iteration 1009/50000                      

                       Computation: 109137 steps/s (collection: 0.770s, learning 0.130s)
               Value function loss: 0.1245
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.10
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4488
       Mean episode rew_ang_vel_xy: -0.0698
          Mean episode rew_dof_acc: -0.2879
   Mean episode rew_dof_pos_limits: -0.0258
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.8453
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.1152
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.90s
                        Total time: 805.54s
                               ETA: 651 mins 13.2 s

################################################################################
                     Learning iteration 1010/50000                      

                       Computation: 116000 steps/s (collection: 0.706s, learning 0.141s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.10
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7931
       Mean episode rew_ang_vel_xy: -0.0711
          Mean episode rew_dof_acc: -0.2964
   Mean episode rew_dof_pos_limits: -0.0333
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.0359
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.85s
                        Total time: 806.38s
                               ETA: 651 mins 14.9 s

################################################################################
                     Learning iteration 1011/50000                      

                       Computation: 121368 steps/s (collection: 0.676s, learning 0.134s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.10
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7035
       Mean episode rew_ang_vel_xy: -0.0710
          Mean episode rew_dof_acc: -0.2940
   Mean episode rew_dof_pos_limits: -0.0282
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1399
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.9654
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0745
        Mean episode terrain_level: 0.1198
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.81s
                        Total time: 807.19s
                               ETA: 651 mins 14.7 s

################################################################################
                     Learning iteration 1012/50000                      

                       Computation: 123666 steps/s (collection: 0.673s, learning 0.122s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5544
       Mean episode rew_ang_vel_xy: -0.0682
          Mean episode rew_dof_acc: -0.2730
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1376
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.8918
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0680
        Mean episode terrain_level: 0.1201
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.79s
                        Total time: 807.99s
                               ETA: 651 mins 13.7 s

################################################################################
                     Learning iteration 1013/50000                      

                       Computation: 120634 steps/s (collection: 0.687s, learning 0.128s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6482
       Mean episode rew_ang_vel_xy: -0.0688
          Mean episode rew_dof_acc: -0.2900
   Mean episode rew_dof_pos_limits: -0.0351
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1430
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9692
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.1195
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.81s
                        Total time: 808.80s
                               ETA: 651 mins 13.8 s

################################################################################
                     Learning iteration 1014/50000                      

                       Computation: 122591 steps/s (collection: 0.661s, learning 0.141s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6946
       Mean episode rew_ang_vel_xy: -0.0693
          Mean episode rew_dof_acc: -0.2889
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1415
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9869
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0678
        Mean episode terrain_level: 0.1153
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.80s
                        Total time: 809.60s
                               ETA: 651 mins 13.2 s

################################################################################
                     Learning iteration 1015/50000                      

                       Computation: 110619 steps/s (collection: 0.753s, learning 0.136s)
               Value function loss: 0.1222
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8656
       Mean episode rew_ang_vel_xy: -0.0727
          Mean episode rew_dof_acc: -0.3153
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.0518
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0806
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.89s
                        Total time: 810.49s
                               ETA: 651 mins 16.8 s

################################################################################
                     Learning iteration 1016/50000                      

                       Computation: 116729 steps/s (collection: 0.699s, learning 0.143s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7490
       Mean episode rew_ang_vel_xy: -0.0721
          Mean episode rew_dof_acc: -0.2976
   Mean episode rew_dof_pos_limits: -0.0320
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.0083
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0831
        Mean episode terrain_level: 0.1144
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.84s
                        Total time: 811.34s
                               ETA: 651 mins 18.1 s

################################################################################
                     Learning iteration 1017/50000                      

                       Computation: 120237 steps/s (collection: 0.692s, learning 0.126s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5154
       Mean episode rew_ang_vel_xy: -0.0699
          Mean episode rew_dof_acc: -0.2763
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1452
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.8644
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0699
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.82s
                        Total time: 812.15s
                               ETA: 651 mins 18.3 s

################################################################################
                     Learning iteration 1018/50000                      

                       Computation: 121472 steps/s (collection: 0.685s, learning 0.124s)
               Value function loss: 0.1162
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8747
       Mean episode rew_ang_vel_xy: -0.0721
          Mean episode rew_dof_acc: -0.2928
   Mean episode rew_dof_pos_limits: -0.0326
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.0625
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0823
        Mean episode terrain_level: 0.1165
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.81s
                        Total time: 812.96s
                               ETA: 651 mins 18.0 s

################################################################################
                     Learning iteration 1019/50000                      

                       Computation: 109355 steps/s (collection: 0.774s, learning 0.125s)
               Value function loss: 0.1191
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6697
       Mean episode rew_ang_vel_xy: -0.0696
          Mean episode rew_dof_acc: -0.2861
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1454
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.9485
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0801
        Mean episode terrain_level: 0.1172
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.90s
                        Total time: 813.86s
                               ETA: 651 mins 22.1 s

################################################################################
                     Learning iteration 1020/50000                      

                       Computation: 111867 steps/s (collection: 0.757s, learning 0.122s)
               Value function loss: 0.1145
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8608
       Mean episode rew_ang_vel_xy: -0.0746
          Mean episode rew_dof_acc: -0.3026
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1456
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0361
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.1157
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.88s
                        Total time: 814.74s
                               ETA: 651 mins 25.2 s

################################################################################
                     Learning iteration 1021/50000                      

                       Computation: 113358 steps/s (collection: 0.734s, learning 0.134s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8291
       Mean episode rew_ang_vel_xy: -0.0754
          Mean episode rew_dof_acc: -0.3046
   Mean episode rew_dof_pos_limits: -0.0303
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1553
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.0214
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0741
        Mean episode terrain_level: 0.1155
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.87s
                        Total time: 815.61s
                               ETA: 651 mins 27.7 s

################################################################################
                     Learning iteration 1022/50000                      

                       Computation: 125582 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.1180
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8137
       Mean episode rew_ang_vel_xy: -0.0700
          Mean episode rew_dof_acc: -0.2844
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.0263
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0821
        Mean episode terrain_level: 0.1149
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.78s
                        Total time: 816.39s
                               ETA: 651 mins 26.2 s

################################################################################
                     Learning iteration 1023/50000                      

                       Computation: 125800 steps/s (collection: 0.658s, learning 0.123s)
               Value function loss: 0.1191
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.13
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8856
       Mean episode rew_ang_vel_xy: -0.0730
          Mean episode rew_dof_acc: -0.3123
   Mean episode rew_dof_pos_limits: -0.0299
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.0483
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0686
        Mean episode terrain_level: 0.1156
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.78s
                        Total time: 817.17s
                               ETA: 651 mins 24.6 s

################################################################################
                     Learning iteration 1024/50000                      

                       Computation: 110978 steps/s (collection: 0.764s, learning 0.122s)
               Value function loss: 0.1187
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.13
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5817
       Mean episode rew_ang_vel_xy: -0.0705
          Mean episode rew_dof_acc: -0.2860
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.8709
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.1200
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.89s
                        Total time: 818.06s
                               ETA: 651 mins 28.0 s

################################################################################
                     Learning iteration 1025/50000                      

                       Computation: 125607 steps/s (collection: 0.659s, learning 0.124s)
               Value function loss: 0.1187
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.13
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7458
       Mean episode rew_ang_vel_xy: -0.0715
          Mean episode rew_dof_acc: -0.2948
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1419
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9706
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0711
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.78s
                        Total time: 818.84s
                               ETA: 651 mins 26.4 s

################################################################################
                     Learning iteration 1026/50000                      

                       Computation: 123741 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7915
       Mean episode rew_ang_vel_xy: -0.0740
          Mean episode rew_dof_acc: -0.2939
   Mean episode rew_dof_pos_limits: -0.0314
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.9820
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0755
        Mean episode terrain_level: 0.1211
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.79s
                        Total time: 819.63s
                               ETA: 651 mins 25.4 s

################################################################################
                     Learning iteration 1027/50000                      

                       Computation: 114690 steps/s (collection: 0.715s, learning 0.143s)
               Value function loss: 0.1120
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9106
       Mean episode rew_ang_vel_xy: -0.0718
          Mean episode rew_dof_acc: -0.2878
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1445
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.0621
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0859
        Mean episode terrain_level: 0.1201
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.86s
                        Total time: 820.49s
                               ETA: 651 mins 27.5 s

################################################################################
                     Learning iteration 1028/50000                      

                       Computation: 110734 steps/s (collection: 0.752s, learning 0.136s)
               Value function loss: 0.1134
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8993
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.2889
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.0368
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0850
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.89s
                        Total time: 821.38s
                               ETA: 651 mins 30.9 s

################################################################################
                     Learning iteration 1029/50000                      

                       Computation: 117817 steps/s (collection: 0.697s, learning 0.137s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8373
       Mean episode rew_ang_vel_xy: -0.0711
          Mean episode rew_dof_acc: -0.2840
   Mean episode rew_dof_pos_limits: -0.0328
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1502
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0145
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0829
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.83s
                        Total time: 822.21s
                               ETA: 651 mins 31.8 s

################################################################################
                     Learning iteration 1030/50000                      

                       Computation: 121231 steps/s (collection: 0.687s, learning 0.124s)
               Value function loss: 0.1159
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7338
       Mean episode rew_ang_vel_xy: -0.0689
          Mean episode rew_dof_acc: -0.2750
   Mean episode rew_dof_pos_limits: -0.0345
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1466
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9493
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0769
        Mean episode terrain_level: 0.1229
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.81s
                        Total time: 823.02s
                               ETA: 651 mins 31.6 s

################################################################################
                     Learning iteration 1031/50000                      

                       Computation: 109506 steps/s (collection: 0.771s, learning 0.127s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7933
       Mean episode rew_ang_vel_xy: -0.0723
          Mean episode rew_dof_acc: -0.2911
   Mean episode rew_dof_pos_limits: -0.0281
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1427
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9696
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0702
        Mean episode terrain_level: 0.1240
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.90s
                        Total time: 823.92s
                               ETA: 651 mins 35.6 s

################################################################################
                     Learning iteration 1032/50000                      

                       Computation: 106328 steps/s (collection: 0.779s, learning 0.146s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6466
       Mean episode rew_ang_vel_xy: -0.0703
          Mean episode rew_dof_acc: -0.2904
   Mean episode rew_dof_pos_limits: -0.0242
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1452
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.8955
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1983
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0613
        Mean episode terrain_level: 0.1231
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.92s
                        Total time: 824.85s
                               ETA: 651 mins 40.7 s

################################################################################
                     Learning iteration 1033/50000                      

                       Computation: 123109 steps/s (collection: 0.671s, learning 0.127s)
               Value function loss: 0.1174
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7269
       Mean episode rew_ang_vel_xy: -0.0691
          Mean episode rew_dof_acc: -0.2706
   Mean episode rew_dof_pos_limits: -0.0355
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1446
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.9660
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0775
        Mean episode terrain_level: 0.1220
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.80s
                        Total time: 825.64s
                               ETA: 651 mins 40.0 s

################################################################################
                     Learning iteration 1034/50000                      

                       Computation: 106983 steps/s (collection: 0.778s, learning 0.141s)
               Value function loss: 0.1133
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8248
       Mean episode rew_ang_vel_xy: -0.0721
          Mean episode rew_dof_acc: -0.2799
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1484
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.0189
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0751
        Mean episode terrain_level: 0.1232
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.92s
                        Total time: 826.56s
                               ETA: 651 mins 44.8 s

################################################################################
                     Learning iteration 1035/50000                      

                       Computation: 118299 steps/s (collection: 0.698s, learning 0.133s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7244
       Mean episode rew_ang_vel_xy: -0.0702
          Mean episode rew_dof_acc: -0.2781
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.9459
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0743
        Mean episode terrain_level: 0.1247
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.83s
                        Total time: 827.39s
                               ETA: 651 mins 45.6 s

################################################################################
                     Learning iteration 1036/50000                      

                       Computation: 105206 steps/s (collection: 0.794s, learning 0.140s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8927
       Mean episode rew_ang_vel_xy: -0.0719
          Mean episode rew_dof_acc: -0.2884
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1407
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0480
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0712
        Mean episode terrain_level: 0.1217
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.93s
                        Total time: 828.33s
                               ETA: 651 mins 51.2 s

################################################################################
                     Learning iteration 1037/50000                      

                       Computation: 114686 steps/s (collection: 0.731s, learning 0.126s)
               Value function loss: 0.1164
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8805
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.2975
   Mean episode rew_dof_pos_limits: -0.0288
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.0411
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0757
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.86s
                        Total time: 829.19s
                               ETA: 651 mins 53.1 s

################################################################################
                     Learning iteration 1038/50000                      

                       Computation: 122228 steps/s (collection: 0.679s, learning 0.125s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9319
       Mean episode rew_ang_vel_xy: -0.0723
          Mean episode rew_dof_acc: -0.2824
   Mean episode rew_dof_pos_limits: -0.0306
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1418
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -2.0568
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0763
        Mean episode terrain_level: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.80s
                        Total time: 829.99s
                               ETA: 651 mins 52.6 s

################################################################################
                     Learning iteration 1039/50000                      

                       Computation: 115509 steps/s (collection: 0.726s, learning 0.125s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6199
       Mean episode rew_ang_vel_xy: -0.0706
          Mean episode rew_dof_acc: -0.2796
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1460
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.9078
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0771
        Mean episode terrain_level: 0.1215
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.85s
                        Total time: 830.84s
                               ETA: 651 mins 54.3 s

################################################################################
                     Learning iteration 1040/50000                      

                       Computation: 122111 steps/s (collection: 0.668s, learning 0.137s)
               Value function loss: 0.1151
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9786
       Mean episode rew_ang_vel_xy: -0.0728
          Mean episode rew_dof_acc: -0.2998
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.0790
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0754
        Mean episode terrain_level: 0.1220
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.81s
                        Total time: 831.65s
                               ETA: 651 mins 53.8 s

################################################################################
                     Learning iteration 1041/50000                      

                       Computation: 114809 steps/s (collection: 0.702s, learning 0.154s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7333
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2696
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.9420
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.1223
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.86s
                        Total time: 832.50s
                               ETA: 651 mins 55.6 s

################################################################################
                     Learning iteration 1042/50000                      

                       Computation: 110034 steps/s (collection: 0.753s, learning 0.140s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7267
       Mean episode rew_ang_vel_xy: -0.0699
          Mean episode rew_dof_acc: -0.2873
   Mean episode rew_dof_pos_limits: -0.0292
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1440
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.9409
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.1207
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.89s
                        Total time: 833.40s
                               ETA: 651 mins 59.3 s

################################################################################
                     Learning iteration 1043/50000                      

                       Computation: 101477 steps/s (collection: 0.824s, learning 0.145s)
               Value function loss: 0.1151
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9745
       Mean episode rew_ang_vel_xy: -0.0722
          Mean episode rew_dof_acc: -0.2905
   Mean episode rew_dof_pos_limits: -0.0349
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1433
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.0897
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0826
        Mean episode terrain_level: 0.1212
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.97s
                        Total time: 834.36s
                               ETA: 652 mins 6.4 s

################################################################################
                     Learning iteration 1044/50000                      

                       Computation: 109949 steps/s (collection: 0.738s, learning 0.157s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2181
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.3117
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.2168
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0856
        Mean episode terrain_level: 0.1227
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.89s
                        Total time: 835.26s
                               ETA: 652 mins 10.1 s

################################################################################
                     Learning iteration 1045/50000                      

                       Computation: 105765 steps/s (collection: 0.799s, learning 0.130s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.18
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0408
       Mean episode rew_ang_vel_xy: -0.0711
          Mean episode rew_dof_acc: -0.2887
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1460
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.1385
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0935
        Mean episode terrain_level: 0.1253
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.93s
                        Total time: 836.19s
                               ETA: 652 mins 15.4 s

################################################################################
                     Learning iteration 1046/50000                      

                       Computation: 106116 steps/s (collection: 0.778s, learning 0.149s)
               Value function loss: 0.1203
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.18
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9994
       Mean episode rew_ang_vel_xy: -0.0736
          Mean episode rew_dof_acc: -0.2977
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1453
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0863
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0804
        Mean episode terrain_level: 0.1253
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.93s
                        Total time: 837.11s
                               ETA: 652 mins 20.5 s

################################################################################
                     Learning iteration 1047/50000                      

                       Computation: 111320 steps/s (collection: 0.748s, learning 0.135s)
               Value function loss: 0.1228
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6304
       Mean episode rew_ang_vel_xy: -0.0715
          Mean episode rew_dof_acc: -0.2858
   Mean episode rew_dof_pos_limits: -0.0245
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1371
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.8823
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.1272
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.88s
                        Total time: 838.00s
                               ETA: 652 mins 23.6 s

################################################################################
                     Learning iteration 1048/50000                      

                       Computation: 110180 steps/s (collection: 0.753s, learning 0.139s)
               Value function loss: 0.1215
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8216
       Mean episode rew_ang_vel_xy: -0.0702
          Mean episode rew_dof_acc: -0.2828
   Mean episode rew_dof_pos_limits: -0.0299
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.9877
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0703
        Mean episode terrain_level: 0.1272
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.89s
                        Total time: 838.89s
                               ETA: 652 mins 27.1 s

################################################################################
                     Learning iteration 1049/50000                      

                       Computation: 115355 steps/s (collection: 0.722s, learning 0.131s)
               Value function loss: 0.1164
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9160
       Mean episode rew_ang_vel_xy: -0.0719
          Mean episode rew_dof_acc: -0.2845
   Mean episode rew_dof_pos_limits: -0.0344
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1461
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.0422
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.1264
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.85s
                        Total time: 839.74s
                               ETA: 652 mins 28.8 s

################################################################################
                     Learning iteration 1050/50000                      

                       Computation: 123756 steps/s (collection: 0.671s, learning 0.123s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7911
       Mean episode rew_ang_vel_xy: -0.0705
          Mean episode rew_dof_acc: -0.2768
   Mean episode rew_dof_pos_limits: -0.0305
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1399
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.9731
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0757
        Mean episode terrain_level: 0.1252
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.79s
                        Total time: 840.54s
                               ETA: 652 mins 27.7 s

################################################################################
                     Learning iteration 1051/50000                      

                       Computation: 124262 steps/s (collection: 0.665s, learning 0.126s)
               Value function loss: 0.1172
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9104
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2777
   Mean episode rew_dof_pos_limits: -0.0326
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1403
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.0313
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0838
        Mean episode terrain_level: 0.1219
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.79s
                        Total time: 841.33s
                               ETA: 652 mins 26.5 s

################################################################################
                     Learning iteration 1052/50000                      

                       Computation: 124455 steps/s (collection: 0.664s, learning 0.126s)
               Value function loss: 0.1153
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7605
       Mean episode rew_ang_vel_xy: -0.0713
          Mean episode rew_dof_acc: -0.2911
   Mean episode rew_dof_pos_limits: -0.0293
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.9672
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.1226
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.79s
                        Total time: 842.12s
                               ETA: 652 mins 25.3 s

################################################################################
                     Learning iteration 1053/50000                      

                       Computation: 124434 steps/s (collection: 0.665s, learning 0.125s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1153
       Mean episode rew_ang_vel_xy: -0.0710
          Mean episode rew_dof_acc: -0.2923
   Mean episode rew_dof_pos_limits: -0.0319
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1394
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.1448
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0810
        Mean episode terrain_level: 0.1211
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.79s
                        Total time: 842.91s
                               ETA: 652 mins 24.0 s

################################################################################
                     Learning iteration 1054/50000                      

                       Computation: 119625 steps/s (collection: 0.698s, learning 0.124s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9423
       Mean episode rew_ang_vel_xy: -0.0716
          Mean episode rew_dof_acc: -0.2860
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0493
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0741
        Mean episode terrain_level: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.82s
                        Total time: 843.73s
                               ETA: 652 mins 24.2 s

################################################################################
                     Learning iteration 1055/50000                      

                       Computation: 105057 steps/s (collection: 0.800s, learning 0.135s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8309
       Mean episode rew_ang_vel_xy: -0.0711
          Mean episode rew_dof_acc: -0.2810
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1433
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.9878
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0756
        Mean episode terrain_level: 0.1219
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.94s
                        Total time: 844.67s
                               ETA: 652 mins 29.7 s

################################################################################
                     Learning iteration 1056/50000                      

                       Computation: 123025 steps/s (collection: 0.676s, learning 0.123s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9811
       Mean episode rew_ang_vel_xy: -0.0692
          Mean episode rew_dof_acc: -0.2669
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1363
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.0791
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0884
        Mean episode terrain_level: 0.1240
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.80s
                        Total time: 845.46s
                               ETA: 652 mins 28.9 s

################################################################################
                     Learning iteration 1057/50000                      

                       Computation: 125208 steps/s (collection: 0.660s, learning 0.125s)
               Value function loss: 0.1175
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8324
       Mean episode rew_ang_vel_xy: -0.0725
          Mean episode rew_dof_acc: -0.2833
   Mean episode rew_dof_pos_limits: -0.0296
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.9727
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0723
        Mean episode terrain_level: 0.1252
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.79s
                        Total time: 846.25s
                               ETA: 652 mins 27.4 s

################################################################################
                     Learning iteration 1058/50000                      

                       Computation: 122666 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8757
       Mean episode rew_ang_vel_xy: -0.0692
          Mean episode rew_dof_acc: -0.2722
   Mean episode rew_dof_pos_limits: -0.0313
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1421
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0124
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0739
        Mean episode terrain_level: 0.1248
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.80s
                        Total time: 847.05s
                               ETA: 652 mins 26.7 s

################################################################################
                     Learning iteration 1059/50000                      

                       Computation: 109985 steps/s (collection: 0.745s, learning 0.149s)
               Value function loss: 0.1232
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0577
       Mean episode rew_ang_vel_xy: -0.0731
          Mean episode rew_dof_acc: -0.2938
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1487
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.1017
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0790
        Mean episode terrain_level: 0.1245
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.89s
                        Total time: 847.94s
                               ETA: 652 mins 30.2 s

################################################################################
                     Learning iteration 1060/50000                      

                       Computation: 123193 steps/s (collection: 0.660s, learning 0.138s)
               Value function loss: 0.1166
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1364
       Mean episode rew_ang_vel_xy: -0.0716
          Mean episode rew_dof_acc: -0.2947
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1472
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.1716
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0820
        Mean episode terrain_level: 0.1272
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.80s
                        Total time: 848.74s
                               ETA: 652 mins 29.3 s

################################################################################
                     Learning iteration 1061/50000                      

                       Computation: 102717 steps/s (collection: 0.800s, learning 0.157s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2829
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.3068
   Mean episode rew_dof_pos_limits: -0.0313
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.2084
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0907
        Mean episode terrain_level: 0.1265
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.96s
                        Total time: 849.70s
                               ETA: 652 mins 35.8 s

################################################################################
                     Learning iteration 1062/50000                      

                       Computation: 117569 steps/s (collection: 0.712s, learning 0.124s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9770
       Mean episode rew_ang_vel_xy: -0.0699
          Mean episode rew_dof_acc: -0.2793
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.0504
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0785
        Mean episode terrain_level: 0.1261
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.84s
                        Total time: 850.54s
                               ETA: 652 mins 36.6 s

################################################################################
                     Learning iteration 1063/50000                      

                       Computation: 105396 steps/s (collection: 0.797s, learning 0.136s)
               Value function loss: 0.1195
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1566
       Mean episode rew_ang_vel_xy: -0.0726
          Mean episode rew_dof_acc: -0.2833
   Mean episode rew_dof_pos_limits: -0.0360
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.1620
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0889
        Mean episode terrain_level: 0.1256
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.93s
                        Total time: 851.47s
                               ETA: 652 mins 41.9 s

################################################################################
                     Learning iteration 1064/50000                      

                       Computation: 110483 steps/s (collection: 0.748s, learning 0.142s)
               Value function loss: 0.1167
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9833
       Mean episode rew_ang_vel_xy: -0.0725
          Mean episode rew_dof_acc: -0.2974
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.0611
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0694
        Mean episode terrain_level: 0.1268
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.89s
                        Total time: 852.36s
                               ETA: 652 mins 45.3 s

################################################################################
                     Learning iteration 1065/50000                      

                       Computation: 102505 steps/s (collection: 0.783s, learning 0.177s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4556
       Mean episode rew_ang_vel_xy: -0.0740
          Mean episode rew_dof_acc: -0.3100
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.3609
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1928
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1269
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.96s
                        Total time: 853.32s
                               ETA: 652 mins 51.7 s

################################################################################
                     Learning iteration 1066/50000                      

                       Computation: 105110 steps/s (collection: 0.793s, learning 0.142s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0342
       Mean episode rew_ang_vel_xy: -0.0716
          Mean episode rew_dof_acc: -0.2772
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1417
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.0996
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0742
        Mean episode terrain_level: 0.1276
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.94s
                        Total time: 854.25s
                               ETA: 652 mins 57.1 s

################################################################################
                     Learning iteration 1067/50000                      

                       Computation: 119315 steps/s (collection: 0.700s, learning 0.124s)
               Value function loss: 0.1183
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2097
       Mean episode rew_ang_vel_xy: -0.0738
          Mean episode rew_dof_acc: -0.3044
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1500
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.1709
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.1281
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.82s
                        Total time: 855.08s
                               ETA: 652 mins 57.4 s

################################################################################
                     Learning iteration 1068/50000                      

                       Computation: 124366 steps/s (collection: 0.666s, learning 0.125s)
               Value function loss: 0.1116
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0609
       Mean episode rew_ang_vel_xy: -0.0723
          Mean episode rew_dof_acc: -0.3014
   Mean episode rew_dof_pos_limits: -0.0296
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1433
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.1006
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0739
        Mean episode terrain_level: 0.1298
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.79s
                        Total time: 855.87s
                               ETA: 652 mins 56.1 s

################################################################################
                     Learning iteration 1069/50000                      

                       Computation: 107857 steps/s (collection: 0.788s, learning 0.124s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1786
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.3181
   Mean episode rew_dof_pos_limits: -0.0320
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1511
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.1844
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0760
        Mean episode terrain_level: 0.1289
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.91s
                        Total time: 856.78s
                               ETA: 653 mins 0.4 s

################################################################################
                     Learning iteration 1070/50000                      

                       Computation: 111462 steps/s (collection: 0.759s, learning 0.123s)
               Value function loss: 0.1225
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0343
       Mean episode rew_ang_vel_xy: -0.0707
          Mean episode rew_dof_acc: -0.2814
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1431
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.1186
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0899
        Mean episode terrain_level: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.88s
                        Total time: 857.66s
                               ETA: 653 mins 3.3 s

################################################################################
                     Learning iteration 1071/50000                      

                       Computation: 120014 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0135
       Mean episode rew_ang_vel_xy: -0.0703
          Mean episode rew_dof_acc: -0.2707
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1364
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.0743
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0766
        Mean episode terrain_level: 0.1271
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.82s
                        Total time: 858.48s
                               ETA: 653 mins 3.3 s

################################################################################
                     Learning iteration 1072/50000                      

                       Computation: 118477 steps/s (collection: 0.689s, learning 0.141s)
               Value function loss: 0.1205
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.23
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0615
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2806
   Mean episode rew_dof_pos_limits: -0.0353
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1513
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.1175
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0917
        Mean episode terrain_level: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.83s
                        Total time: 859.31s
                               ETA: 653 mins 3.8 s

################################################################################
                     Learning iteration 1073/50000                      

                       Computation: 118436 steps/s (collection: 0.708s, learning 0.122s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8788
       Mean episode rew_ang_vel_xy: -0.0714
          Mean episode rew_dof_acc: -0.2883
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1449
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -2.0186
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0680
        Mean episode terrain_level: 0.1272
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.83s
                        Total time: 860.14s
                               ETA: 653 mins 4.4 s

################################################################################
                     Learning iteration 1074/50000                      

                       Computation: 110323 steps/s (collection: 0.760s, learning 0.131s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.23
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6389
       Mean episode rew_ang_vel_xy: -0.0775
          Mean episode rew_dof_acc: -0.3254
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.4169
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1264
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.89s
                        Total time: 861.03s
                               ETA: 653 mins 7.7 s

################################################################################
                     Learning iteration 1075/50000                      

                       Computation: 120773 steps/s (collection: 0.683s, learning 0.131s)
               Value function loss: 0.1166
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1558
       Mean episode rew_ang_vel_xy: -0.0749
          Mean episode rew_dof_acc: -0.3050
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.1496
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0756
        Mean episode terrain_level: 0.1259
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.81s
                        Total time: 861.84s
                               ETA: 653 mins 7.5 s

################################################################################
                     Learning iteration 1076/50000                      

                       Computation: 100868 steps/s (collection: 0.828s, learning 0.146s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5193
       Mean episode rew_ang_vel_xy: -0.0750
          Mean episode rew_dof_acc: -0.3080
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1412
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.3577
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.0950
        Mean episode terrain_level: 0.1262
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.97s
                        Total time: 862.82s
                               ETA: 653 mins 14.5 s

################################################################################
                     Learning iteration 1077/50000                      

                       Computation: 122961 steps/s (collection: 0.668s, learning 0.132s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1209
       Mean episode rew_ang_vel_xy: -0.0717
          Mean episode rew_dof_acc: -0.2810
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.1419
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.1237
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.80s
                        Total time: 863.62s
                               ETA: 653 mins 13.7 s

################################################################################
                     Learning iteration 1078/50000                      

                       Computation: 120251 steps/s (collection: 0.695s, learning 0.123s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3978
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.3138
   Mean episode rew_dof_pos_limits: -0.0355
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1552
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.2917
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0833
        Mean episode terrain_level: 0.1215
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.82s
                        Total time: 864.44s
                               ETA: 653 mins 13.6 s

################################################################################
                     Learning iteration 1079/50000                      

                       Computation: 121111 steps/s (collection: 0.663s, learning 0.148s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1141
       Mean episode rew_ang_vel_xy: -0.0738
          Mean episode rew_dof_acc: -0.2887
   Mean episode rew_dof_pos_limits: -0.0309
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1412
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.1306
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0721
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.81s
                        Total time: 865.25s
                               ETA: 653 mins 13.3 s

################################################################################
                     Learning iteration 1080/50000                      

                       Computation: 126083 steps/s (collection: 0.654s, learning 0.126s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1745
       Mean episode rew_ang_vel_xy: -0.0763
          Mean episode rew_dof_acc: -0.3011
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1470
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.1547
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1978
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0637
        Mean episode terrain_level: 0.1211
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.78s
                        Total time: 866.03s
                               ETA: 653 mins 11.5 s

################################################################################
                     Learning iteration 1081/50000                      

                       Computation: 125558 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0953
       Mean episode rew_ang_vel_xy: -0.0726
          Mean episode rew_dof_acc: -0.2936
   Mean episode rew_dof_pos_limits: -0.0308
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1573
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.1129
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0744
        Mean episode terrain_level: 0.1214
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.78s
                        Total time: 866.81s
                               ETA: 653 mins 9.9 s

################################################################################
                     Learning iteration 1082/50000                      

                       Computation: 115742 steps/s (collection: 0.723s, learning 0.127s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4305
       Mean episode rew_ang_vel_xy: -0.0750
          Mean episode rew_dof_acc: -0.3089
   Mean episode rew_dof_pos_limits: -0.0369
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.3082
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0877
        Mean episode terrain_level: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.85s
                        Total time: 867.66s
                               ETA: 653 mins 11.3 s

################################################################################
                     Learning iteration 1083/50000                      

                       Computation: 121181 steps/s (collection: 0.687s, learning 0.124s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1466
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2869
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1469
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.1576
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0830
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.81s
                        Total time: 868.47s
                               ETA: 653 mins 10.9 s

################################################################################
                     Learning iteration 1084/50000                      

                       Computation: 125109 steps/s (collection: 0.664s, learning 0.121s)
               Value function loss: 0.1203
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.0598
       Mean episode rew_ang_vel_xy: -0.0708
          Mean episode rew_dof_acc: -0.2782
   Mean episode rew_dof_pos_limits: -0.0317
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.0845
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0744
        Mean episode terrain_level: 0.1214
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.79s
                        Total time: 869.26s
                               ETA: 653 mins 9.4 s

################################################################################
                     Learning iteration 1085/50000                      

                       Computation: 111035 steps/s (collection: 0.762s, learning 0.124s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3323
       Mean episode rew_ang_vel_xy: -0.0743
          Mean episode rew_dof_acc: -0.3056
   Mean episode rew_dof_pos_limits: -0.0308
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.2511
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0762
        Mean episode terrain_level: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.89s
                        Total time: 870.14s
                               ETA: 653 mins 12.4 s

################################################################################
                     Learning iteration 1086/50000                      

                       Computation: 101786 steps/s (collection: 0.823s, learning 0.143s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2095
       Mean episode rew_ang_vel_xy: -0.0757
          Mean episode rew_dof_acc: -0.3044
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1544
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.1895
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.1230
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.97s
                        Total time: 871.11s
                               ETA: 653 mins 19.0 s

################################################################################
                     Learning iteration 1087/50000                      

                       Computation: 103294 steps/s (collection: 0.830s, learning 0.122s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3219
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.3070
   Mean episode rew_dof_pos_limits: -0.0320
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.2267
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.1203
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.95s
                        Total time: 872.06s
                               ETA: 653 mins 25.0 s

################################################################################
                     Learning iteration 1088/50000                      

                       Computation: 122323 steps/s (collection: 0.666s, learning 0.137s)
               Value function loss: 0.1211
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1054
       Mean episode rew_ang_vel_xy: -0.0722
          Mean episode rew_dof_acc: -0.2818
   Mean episode rew_dof_pos_limits: -0.0303
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1474
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.1226
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0751
        Mean episode terrain_level: 0.1195
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.80s
                        Total time: 872.86s
                               ETA: 653 mins 24.3 s

################################################################################
                     Learning iteration 1089/50000                      

                       Computation: 110645 steps/s (collection: 0.725s, learning 0.163s)
               Value function loss: 0.1164
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7033
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3232
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1562
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.4433
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0859
        Mean episode terrain_level: 0.1198
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.89s
                        Total time: 873.75s
                               ETA: 653 mins 27.4 s

################################################################################
                     Learning iteration 1090/50000                      

                       Computation: 99568 steps/s (collection: 0.823s, learning 0.165s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.27
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2570
       Mean episode rew_ang_vel_xy: -0.0739
          Mean episode rew_dof_acc: -0.2864
   Mean episode rew_dof_pos_limits: -0.0326
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1541
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.2173
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0778
        Mean episode terrain_level: 0.1188
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.99s
                        Total time: 874.74s
                               ETA: 653 mins 34.9 s

################################################################################
                     Learning iteration 1091/50000                      

                       Computation: 98436 steps/s (collection: 0.823s, learning 0.176s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2719
       Mean episode rew_ang_vel_xy: -0.0738
          Mean episode rew_dof_acc: -0.2977
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.1991
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0733
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.00s
                        Total time: 875.74s
                               ETA: 653 mins 42.9 s

################################################################################
                     Learning iteration 1092/50000                      

                       Computation: 99521 steps/s (collection: 0.813s, learning 0.175s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0168
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.27
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2963
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.2967
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.2211
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0822
        Mean episode terrain_level: 0.1206
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.99s
                        Total time: 876.72s
                               ETA: 653 mins 50.4 s

################################################################################
                     Learning iteration 1093/50000                      

                       Computation: 111584 steps/s (collection: 0.741s, learning 0.140s)
               Value function loss: 0.1176
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.28
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3361
       Mean episode rew_ang_vel_xy: -0.0752
          Mean episode rew_dof_acc: -0.2936
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.2471
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0868
        Mean episode terrain_level: 0.1223
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.88s
                        Total time: 877.61s
                               ETA: 653 mins 53.1 s

################################################################################
                     Learning iteration 1094/50000                      

                       Computation: 99663 steps/s (collection: 0.820s, learning 0.166s)
               Value function loss: 0.1214
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.28
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2912
       Mean episode rew_ang_vel_xy: -0.0730
          Mean episode rew_dof_acc: -0.2883
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1447
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.2211
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0832
        Mean episode terrain_level: 0.1206
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.99s
                        Total time: 878.59s
                               ETA: 654 mins 0.6 s

################################################################################
                     Learning iteration 1095/50000                      

                       Computation: 100174 steps/s (collection: 0.815s, learning 0.167s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.28
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1587
       Mean episode rew_ang_vel_xy: -0.0738
          Mean episode rew_dof_acc: -0.2956
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1520
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.1436
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0753
        Mean episode terrain_level: 0.1198
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.98s
                        Total time: 879.57s
                               ETA: 654 mins 7.7 s

################################################################################
                     Learning iteration 1096/50000                      

                       Computation: 97742 steps/s (collection: 0.831s, learning 0.175s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.29
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6081
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3050
   Mean episode rew_dof_pos_limits: -0.0376
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1414
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.3897
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0915
        Mean episode terrain_level: 0.1184
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.01s
                        Total time: 880.58s
                               ETA: 654 mins 16.0 s

################################################################################
                     Learning iteration 1097/50000                      

                       Computation: 107436 steps/s (collection: 0.742s, learning 0.173s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.29
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2838
       Mean episode rew_ang_vel_xy: -0.0719
          Mean episode rew_dof_acc: -0.2837
   Mean episode rew_dof_pos_limits: -0.0324
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.2046
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0780
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.91s
                        Total time: 881.49s
                               ETA: 654 mins 20.2 s

################################################################################
                     Learning iteration 1098/50000                      

                       Computation: 101744 steps/s (collection: 0.800s, learning 0.166s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.29
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4025
       Mean episode rew_ang_vel_xy: -0.0744
          Mean episode rew_dof_acc: -0.2985
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.2952
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0902
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.97s
                        Total time: 882.46s
                               ETA: 654 mins 26.7 s

################################################################################
                     Learning iteration 1099/50000                      

                       Computation: 102516 steps/s (collection: 0.818s, learning 0.141s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.29
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5588
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.3021
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1481
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3632
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0131
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0835
        Mean episode terrain_level: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.96s
                        Total time: 883.42s
                               ETA: 654 mins 32.8 s

################################################################################
                     Learning iteration 1100/50000                      

                       Computation: 109625 steps/s (collection: 0.729s, learning 0.168s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4667
       Mean episode rew_ang_vel_xy: -0.0747
          Mean episode rew_dof_acc: -0.2980
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1514
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.2932
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0754
        Mean episode terrain_level: 0.1241
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.90s
                        Total time: 884.32s
                               ETA: 654 mins 36.2 s

################################################################################
                     Learning iteration 1101/50000                      

                       Computation: 97100 steps/s (collection: 0.841s, learning 0.172s)
               Value function loss: 0.1170
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.30
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9168
       Mean episode rew_ang_vel_xy: -0.0790
          Mean episode rew_dof_acc: -0.3196
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1511
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.5707
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0858
        Mean episode terrain_level: 0.1262
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 1.01s
                        Total time: 885.33s
                               ETA: 654 mins 44.6 s

################################################################################
                     Learning iteration 1102/50000                      

                       Computation: 105005 steps/s (collection: 0.791s, learning 0.145s)
               Value function loss: 0.1199
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.30
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7049
       Mean episode rew_ang_vel_xy: -0.0746
          Mean episode rew_dof_acc: -0.3052
   Mean episode rew_dof_pos_limits: -0.0381
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.4486
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0944
        Mean episode terrain_level: 0.1269
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.94s
                        Total time: 886.26s
                               ETA: 654 mins 49.7 s

################################################################################
                     Learning iteration 1103/50000                      

                       Computation: 106144 steps/s (collection: 0.789s, learning 0.138s)
               Value function loss: 0.1185
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.30
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4045
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.3034
   Mean episode rew_dof_pos_limits: -0.0309
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.2820
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0768
        Mean episode terrain_level: 0.1253
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.93s
                        Total time: 887.19s
                               ETA: 654 mins 54.3 s

################################################################################
                     Learning iteration 1104/50000                      

                       Computation: 120159 steps/s (collection: 0.694s, learning 0.124s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.30
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8129
       Mean episode rew_ang_vel_xy: -0.0762
          Mean episode rew_dof_acc: -0.3184
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1511
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.5289
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1259
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.82s
                        Total time: 888.01s
                               ETA: 654 mins 54.2 s

################################################################################
                     Learning iteration 1105/50000                      

                       Computation: 117160 steps/s (collection: 0.715s, learning 0.124s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.31
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5829
       Mean episode rew_ang_vel_xy: -0.0749
          Mean episode rew_dof_acc: -0.3015
   Mean episode rew_dof_pos_limits: -0.0331
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1441
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.3702
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0790
        Mean episode terrain_level: 0.1244
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.84s
                        Total time: 888.85s
                               ETA: 654 mins 54.9 s

################################################################################
                     Learning iteration 1106/50000                      

                       Computation: 114087 steps/s (collection: 0.736s, learning 0.126s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.31
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6767
       Mean episode rew_ang_vel_xy: -0.0748
          Mean episode rew_dof_acc: -0.3055
   Mean episode rew_dof_pos_limits: -0.0369
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.4389
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0908
        Mean episode terrain_level: 0.1253
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.86s
                        Total time: 889.71s
                               ETA: 654 mins 56.7 s

################################################################################
                     Learning iteration 1107/50000                      

                       Computation: 113651 steps/s (collection: 0.739s, learning 0.126s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.31
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4845
       Mean episode rew_ang_vel_xy: -0.0753
          Mean episode rew_dof_acc: -0.3036
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1461
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3080
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0763
        Mean episode terrain_level: 0.1283
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.86s
                        Total time: 890.57s
                               ETA: 654 mins 58.6 s

################################################################################
                     Learning iteration 1108/50000                      

                       Computation: 122364 steps/s (collection: 0.663s, learning 0.140s)
               Value function loss: 0.1171
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4663
       Mean episode rew_ang_vel_xy: -0.0737
          Mean episode rew_dof_acc: -0.2968
   Mean episode rew_dof_pos_limits: -0.0293
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1433
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.2840
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0712
        Mean episode terrain_level: 0.1285
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.80s
                        Total time: 891.38s
                               ETA: 654 mins 57.8 s

################################################################################
                     Learning iteration 1109/50000                      

                       Computation: 113997 steps/s (collection: 0.715s, learning 0.148s)
               Value function loss: 0.1135
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.32
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7310
       Mean episode rew_ang_vel_xy: -0.0762
          Mean episode rew_dof_acc: -0.3132
   Mean episode rew_dof_pos_limits: -0.0329
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1519
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.4348
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0844
        Mean episode terrain_level: 0.1281
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.86s
                        Total time: 892.24s
                               ETA: 654 mins 59.6 s

################################################################################
                     Learning iteration 1110/50000                      

                       Computation: 102693 steps/s (collection: 0.803s, learning 0.154s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6609
       Mean episode rew_ang_vel_xy: -0.0778
          Mean episode rew_dof_acc: -0.3113
   Mean episode rew_dof_pos_limits: -0.0380
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1551
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.4151
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.1280
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.96s
                        Total time: 893.20s
                               ETA: 655 mins 5.5 s

################################################################################
                     Learning iteration 1111/50000                      

                       Computation: 108006 steps/s (collection: 0.761s, learning 0.149s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.32
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4287
       Mean episode rew_ang_vel_xy: -0.0730
          Mean episode rew_dof_acc: -0.2945
   Mean episode rew_dof_pos_limits: -0.0346
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1431
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.2783
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0840
        Mean episode terrain_level: 0.1292
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.91s
                        Total time: 894.11s
                               ETA: 655 mins 9.4 s

################################################################################
                     Learning iteration 1112/50000                      

                       Computation: 109997 steps/s (collection: 0.746s, learning 0.147s)
               Value function loss: 0.1153
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.33
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4958
       Mean episode rew_ang_vel_xy: -0.0754
          Mean episode rew_dof_acc: -0.3021
   Mean episode rew_dof_pos_limits: -0.0337
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1508
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3164
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0825
        Mean episode terrain_level: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.89s
                        Total time: 895.00s
                               ETA: 655 mins 12.5 s

################################################################################
                     Learning iteration 1113/50000                      

                       Computation: 107060 steps/s (collection: 0.771s, learning 0.147s)
               Value function loss: 0.1162
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3553
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2938
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -2.2302
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0679
        Mean episode terrain_level: 0.1268
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.92s
                        Total time: 895.92s
                               ETA: 655 mins 16.7 s

################################################################################
                     Learning iteration 1114/50000                      

                       Computation: 112635 steps/s (collection: 0.726s, learning 0.147s)
               Value function loss: 0.1163
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.33
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6458
       Mean episode rew_ang_vel_xy: -0.0766
          Mean episode rew_dof_acc: -0.3063
   Mean episode rew_dof_pos_limits: -0.0384
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1502
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.4003
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0890
        Mean episode terrain_level: 0.1298
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.87s
                        Total time: 896.79s
                               ETA: 655 mins 18.9 s

################################################################################
                     Learning iteration 1115/50000                      

                       Computation: 111034 steps/s (collection: 0.740s, learning 0.146s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.33
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7331
       Mean episode rew_ang_vel_xy: -0.0763
          Mean episode rew_dof_acc: -0.3067
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1539
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.4266
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0817
        Mean episode terrain_level: 0.1308
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.89s
                        Total time: 897.68s
                               ETA: 655 mins 21.7 s

################################################################################
                     Learning iteration 1116/50000                      

                       Computation: 113184 steps/s (collection: 0.741s, learning 0.128s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.34
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7139
       Mean episode rew_ang_vel_xy: -0.0725
          Mean episode rew_dof_acc: -0.2904
   Mean episode rew_dof_pos_limits: -0.0454
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1466
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.4772
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1911
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1150
        Mean episode terrain_level: 0.1317
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.87s
                        Total time: 898.55s
                               ETA: 655 mins 23.7 s

################################################################################
                     Learning iteration 1117/50000                      

                       Computation: 107358 steps/s (collection: 0.786s, learning 0.129s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.34
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5240
       Mean episode rew_ang_vel_xy: -0.0728
          Mean episode rew_dof_acc: -0.2926
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.3081
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0790
        Mean episode terrain_level: 0.1314
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.92s
                        Total time: 899.46s
                               ETA: 655 mins 27.7 s

################################################################################
                     Learning iteration 1118/50000                      

                       Computation: 110891 steps/s (collection: 0.752s, learning 0.134s)
               Value function loss: 0.1163
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.34
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4337
       Mean episode rew_ang_vel_xy: -0.0740
          Mean episode rew_dof_acc: -0.3005
   Mean episode rew_dof_pos_limits: -0.0306
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.2836
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0740
        Mean episode terrain_level: 0.1281
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.89s
                        Total time: 900.35s
                               ETA: 655 mins 30.5 s

################################################################################
                     Learning iteration 1119/50000                      

                       Computation: 106097 steps/s (collection: 0.771s, learning 0.155s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.34
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8062
       Mean episode rew_ang_vel_xy: -0.0803
          Mean episode rew_dof_acc: -0.3121
   Mean episode rew_dof_pos_limits: -0.0362
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1529
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.4717
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0933
        Mean episode terrain_level: 0.1284
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.93s
                        Total time: 901.27s
                               ETA: 655 mins 35.0 s

################################################################################
                     Learning iteration 1120/50000                      

                       Computation: 109993 steps/s (collection: 0.751s, learning 0.143s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.34
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3323
       Mean episode rew_ang_vel_xy: -0.0727
          Mean episode rew_dof_acc: -0.2750
   Mean episode rew_dof_pos_limits: -0.0344
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1437
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.2412
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0855
        Mean episode terrain_level: 0.1295
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.89s
                        Total time: 902.17s
                               ETA: 655 mins 38.1 s

################################################################################
                     Learning iteration 1121/50000                      

                       Computation: 111258 steps/s (collection: 0.728s, learning 0.156s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.35
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3841
       Mean episode rew_ang_vel_xy: -0.0737
          Mean episode rew_dof_acc: -0.2859
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1447
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.2529
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0730
        Mean episode terrain_level: 0.1283
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.88s
                        Total time: 903.05s
                               ETA: 655 mins 40.7 s

################################################################################
                     Learning iteration 1122/50000                      

                       Computation: 110767 steps/s (collection: 0.753s, learning 0.134s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6323
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.3087
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.3860
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0818
        Mean episode terrain_level: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.89s
                        Total time: 903.94s
                               ETA: 655 mins 43.5 s

################################################################################
                     Learning iteration 1123/50000                      

                       Computation: 103498 steps/s (collection: 0.820s, learning 0.130s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.35
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4495
       Mean episode rew_ang_vel_xy: -0.0756
          Mean episode rew_dof_acc: -0.2976
   Mean episode rew_dof_pos_limits: -0.0308
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.3068
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.1293
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.95s
                        Total time: 904.89s
                               ETA: 655 mins 49.0 s

################################################################################
                     Learning iteration 1124/50000                      

                       Computation: 106770 steps/s (collection: 0.789s, learning 0.131s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.35
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0838
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.3056
   Mean episode rew_dof_pos_limits: -0.0449
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1511
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -2.6412
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1914
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0263
 Mean episode rew_tracking_lin_vel: 0.1080
        Mean episode terrain_level: 0.1297
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.92s
                        Total time: 905.81s
                               ETA: 655 mins 53.2 s

################################################################################
                     Learning iteration 1125/50000                      

                       Computation: 111814 steps/s (collection: 0.749s, learning 0.130s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.35
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5499
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.2902
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1506
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.3518
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0779
        Mean episode terrain_level: 0.1300
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.88s
                        Total time: 906.69s
                               ETA: 655 mins 55.6 s

################################################################################
                     Learning iteration 1126/50000                      

                       Computation: 102749 steps/s (collection: 0.803s, learning 0.153s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.36
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7043
       Mean episode rew_ang_vel_xy: -0.0746
          Mean episode rew_dof_acc: -0.2928
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1440
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.4455
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0907
        Mean episode terrain_level: 0.1307
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.96s
                        Total time: 907.65s
                               ETA: 656 mins 1.4 s

################################################################################
                     Learning iteration 1127/50000                      

                       Computation: 110453 steps/s (collection: 0.750s, learning 0.140s)
               Value function loss: 0.1215
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.36
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6326
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.3057
   Mean episode rew_dof_pos_limits: -0.0369
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1541
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.4083
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0935
        Mean episode terrain_level: 0.1282
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.89s
                        Total time: 908.54s
                               ETA: 656 mins 4.3 s

################################################################################
                     Learning iteration 1128/50000                      

                       Computation: 112100 steps/s (collection: 0.754s, learning 0.123s)
               Value function loss: 0.1200
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.36
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5772
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.3017
   Mean episode rew_dof_pos_limits: -0.0367
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1477
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3912
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.1279
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.88s
                        Total time: 909.41s
                               ETA: 656 mins 6.5 s

################################################################################
                     Learning iteration 1129/50000                      

                       Computation: 115409 steps/s (collection: 0.728s, learning 0.124s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.36
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0445
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3111
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.6320
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0969
        Mean episode terrain_level: 0.1255
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.85s
                        Total time: 910.26s
                               ETA: 656 mins 7.7 s

################################################################################
                     Learning iteration 1130/50000                      

                       Computation: 118050 steps/s (collection: 0.703s, learning 0.130s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.37
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6627
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.2897
   Mean episode rew_dof_pos_limits: -0.0337
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3948
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.83s
                        Total time: 911.10s
                               ETA: 656 mins 8.1 s

################################################################################
                     Learning iteration 1131/50000                      

                       Computation: 114193 steps/s (collection: 0.705s, learning 0.156s)
               Value function loss: 0.1175
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.37
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6118
       Mean episode rew_ang_vel_xy: -0.0739
          Mean episode rew_dof_acc: -0.2986
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3871
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0131
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0733
        Mean episode terrain_level: 0.1217
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.86s
                        Total time: 911.96s
                               ETA: 656 mins 9.7 s

################################################################################
                     Learning iteration 1132/50000                      

                       Computation: 111999 steps/s (collection: 0.713s, learning 0.165s)
               Value function loss: 0.1195
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.37
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6568
       Mean episode rew_ang_vel_xy: -0.0733
          Mean episode rew_dof_acc: -0.2880
   Mean episode rew_dof_pos_limits: -0.0317
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1450
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.3820
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0779
        Mean episode terrain_level: 0.1184
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.88s
                        Total time: 912.84s
                               ETA: 656 mins 12.0 s

################################################################################
                     Learning iteration 1133/50000                      

                       Computation: 110608 steps/s (collection: 0.749s, learning 0.140s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.37
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8904
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.3029
   Mean episode rew_dof_pos_limits: -0.0351
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.5304
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0909
        Mean episode terrain_level: 0.1181
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.89s
                        Total time: 913.72s
                               ETA: 656 mins 14.8 s

################################################################################
                     Learning iteration 1134/50000                      

                       Computation: 109017 steps/s (collection: 0.762s, learning 0.139s)
               Value function loss: 0.1187
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.37
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7890
       Mean episode rew_ang_vel_xy: -0.0766
          Mean episode rew_dof_acc: -0.2939
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1467
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.4829
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0941
        Mean episode terrain_level: 0.1165
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.90s
                        Total time: 914.63s
                               ETA: 656 mins 18.1 s

################################################################################
                     Learning iteration 1135/50000                      

                       Computation: 112451 steps/s (collection: 0.751s, learning 0.123s)
               Value function loss: 0.1141
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.38
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8459
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.3000
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1442
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.4948
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0912
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.87s
                        Total time: 915.50s
                               ETA: 656 mins 20.2 s

################################################################################
                     Learning iteration 1136/50000                      

                       Computation: 116413 steps/s (collection: 0.720s, learning 0.124s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.38
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7193
       Mean episode rew_ang_vel_xy: -0.0767
          Mean episode rew_dof_acc: -0.2965
   Mean episode rew_dof_pos_limits: -0.0341
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1453
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.4317
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0860
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.84s
                        Total time: 916.35s
                               ETA: 656 mins 21.1 s

################################################################################
                     Learning iteration 1137/50000                      

                       Computation: 124659 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.38
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2783
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.2836
   Mean episode rew_dof_pos_limits: -0.0292
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0114
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -2.1848
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0671
        Mean episode terrain_level: 0.1185
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.79s
                        Total time: 917.13s
                               ETA: 656 mins 19.5 s

################################################################################
                     Learning iteration 1138/50000                      

                       Computation: 116227 steps/s (collection: 0.720s, learning 0.126s)
               Value function loss: 0.1166
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.38
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5739
       Mean episode rew_ang_vel_xy: -0.0731
          Mean episode rew_dof_acc: -0.2977
   Mean episode rew_dof_pos_limits: -0.0339
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1518
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3657
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0131
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0733
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.85s
                        Total time: 917.98s
                               ETA: 656 mins 20.4 s

################################################################################
                     Learning iteration 1139/50000                      

                       Computation: 124844 steps/s (collection: 0.663s, learning 0.124s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5584
       Mean episode rew_ang_vel_xy: -0.0745
          Mean episode rew_dof_acc: -0.2885
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.3595
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0849
        Mean episode terrain_level: 0.1218
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.79s
                        Total time: 918.77s
                               ETA: 656 mins 18.8 s

################################################################################
                     Learning iteration 1140/50000                      

                       Computation: 114283 steps/s (collection: 0.737s, learning 0.123s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.4243
       Mean episode rew_ang_vel_xy: -0.0750
          Mean episode rew_dof_acc: -0.2890
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1469
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.2544
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.86s
                        Total time: 919.63s
                               ETA: 656 mins 20.3 s

################################################################################
                     Learning iteration 1141/50000                      

                       Computation: 119863 steps/s (collection: 0.693s, learning 0.127s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.3890
       Mean episode rew_ang_vel_xy: -0.0732
          Mean episode rew_dof_acc: -0.2791
   Mean episode rew_dof_pos_limits: -0.0362
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.2810
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0827
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.82s
                        Total time: 920.45s
                               ETA: 656 mins 20.1 s

################################################################################
                     Learning iteration 1142/50000                      

                       Computation: 122241 steps/s (collection: 0.680s, learning 0.125s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.2868
       Mean episode rew_ang_vel_xy: -0.0727
          Mean episode rew_dof_acc: -0.2773
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -2.2111
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0759
        Mean episode terrain_level: 0.1208
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.80s
                        Total time: 921.25s
                               ETA: 656 mins 19.3 s

################################################################################
                     Learning iteration 1143/50000                      

                       Computation: 112397 steps/s (collection: 0.751s, learning 0.123s)
               Value function loss: 0.1205
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6725
       Mean episode rew_ang_vel_xy: -0.0748
          Mean episode rew_dof_acc: -0.2899
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1446
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.4278
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0934
        Mean episode terrain_level: 0.1183
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.87s
                        Total time: 922.13s
                               ETA: 656 mins 21.4 s

################################################################################
                     Learning iteration 1144/50000                      

                       Computation: 120420 steps/s (collection: 0.693s, learning 0.123s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.39
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5974
       Mean episode rew_ang_vel_xy: -0.0748
          Mean episode rew_dof_acc: -0.2926
   Mean episode rew_dof_pos_limits: -0.0321
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3377
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0791
        Mean episode terrain_level: 0.1187
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.82s
                        Total time: 922.94s
                               ETA: 656 mins 21.0 s

################################################################################
                     Learning iteration 1145/50000                      

                       Computation: 110446 steps/s (collection: 0.750s, learning 0.140s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.40
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7792
       Mean episode rew_ang_vel_xy: -0.0755
          Mean episode rew_dof_acc: -0.2929
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.4624
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0878
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.89s
                        Total time: 923.83s
                               ETA: 656 mins 23.8 s

################################################################################
                     Learning iteration 1146/50000                      

                       Computation: 113158 steps/s (collection: 0.743s, learning 0.125s)
               Value function loss: 0.1205
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.40
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8657
       Mean episode rew_ang_vel_xy: -0.0767
          Mean episode rew_dof_acc: -0.2981
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1414
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.5402
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.1212
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.87s
                        Total time: 924.70s
                               ETA: 656 mins 25.7 s

################################################################################
                     Learning iteration 1147/50000                      

                       Computation: 115021 steps/s (collection: 0.731s, learning 0.123s)
               Value function loss: 0.1216
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5072
       Mean episode rew_ang_vel_xy: -0.0709
          Mean episode rew_dof_acc: -0.2835
   Mean episode rew_dof_pos_limits: -0.0345
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1413
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3121
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0887
        Mean episode terrain_level: 0.1191
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.85s
                        Total time: 925.56s
                               ETA: 656 mins 26.9 s

################################################################################
                     Learning iteration 1148/50000                      

                       Computation: 118678 steps/s (collection: 0.683s, learning 0.146s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.40
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8513
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.3088
   Mean episode rew_dof_pos_limits: -0.0328
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1450
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.5049
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0837
        Mean episode terrain_level: 0.1169
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.83s
                        Total time: 926.38s
                               ETA: 656 mins 27.0 s

################################################################################
                     Learning iteration 1149/50000                      

                       Computation: 121640 steps/s (collection: 0.685s, learning 0.123s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.5598
       Mean episode rew_ang_vel_xy: -0.0742
          Mean episode rew_dof_acc: -0.2847
   Mean episode rew_dof_pos_limits: -0.0315
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1435
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.3568
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0821
        Mean episode terrain_level: 0.1175
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.81s
                        Total time: 927.19s
                               ETA: 656 mins 26.3 s

################################################################################
                     Learning iteration 1150/50000                      

                       Computation: 120132 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8155
       Mean episode rew_ang_vel_xy: -0.0751
          Mean episode rew_dof_acc: -0.2936
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1428
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.4986
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0904
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.82s
                        Total time: 928.01s
                               ETA: 656 mins 26.0 s

################################################################################
                     Learning iteration 1151/50000                      

                       Computation: 121975 steps/s (collection: 0.679s, learning 0.127s)
               Value function loss: 0.1250
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8755
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.3027
   Mean episode rew_dof_pos_limits: -0.0367
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.5157
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0851
        Mean episode terrain_level: 0.1190
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.81s
                        Total time: 928.82s
                               ETA: 656 mins 25.2 s

################################################################################
                     Learning iteration 1152/50000                      

                       Computation: 111541 steps/s (collection: 0.758s, learning 0.124s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9843
       Mean episode rew_ang_vel_xy: -0.0762
          Mean episode rew_dof_acc: -0.2980
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.5661
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0988
        Mean episode terrain_level: 0.1201
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.88s
                        Total time: 929.70s
                               ETA: 656 mins 27.6 s

################################################################################
                     Learning iteration 1153/50000                      

                       Computation: 114000 steps/s (collection: 0.727s, learning 0.135s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3418
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3134
   Mean episode rew_dof_pos_limits: -0.0464
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -2.7809
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1908
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1154
        Mean episode terrain_level: 0.1202
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.86s
                        Total time: 930.56s
                               ETA: 656 mins 29.1 s

################################################################################
                     Learning iteration 1154/50000                      

                       Computation: 109426 steps/s (collection: 0.776s, learning 0.123s)
               Value function loss: 0.1154
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.41
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9162
       Mean episode rew_ang_vel_xy: -0.0765
          Mean episode rew_dof_acc: -0.3104
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1435
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.5272
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0866
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.90s
                        Total time: 931.46s
                               ETA: 656 mins 32.2 s

################################################################################
                     Learning iteration 1155/50000                      

                       Computation: 115988 steps/s (collection: 0.721s, learning 0.126s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0024
       Mean episode rew_ang_vel_xy: -0.0790
          Mean episode rew_dof_acc: -0.2984
   Mean episode rew_dof_pos_limits: -0.0330
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.5775
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0824
        Mean episode terrain_level: 0.1202
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.85s
                        Total time: 932.31s
                               ETA: 656 mins 33.2 s

################################################################################
                     Learning iteration 1156/50000                      

                       Computation: 124692 steps/s (collection: 0.665s, learning 0.124s)
               Value function loss: 0.1203
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0274
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.3147
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1465
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.6041
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0949
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.79s
                        Total time: 933.09s
                               ETA: 656 mins 31.6 s

################################################################################
                     Learning iteration 1157/50000                      

                       Computation: 114161 steps/s (collection: 0.731s, learning 0.130s)
               Value function loss: 0.1204
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1026
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.3257
   Mean episode rew_dof_pos_limits: -0.0335
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.6400
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1199
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.86s
                        Total time: 933.96s
                               ETA: 656 mins 33.1 s

################################################################################
                     Learning iteration 1158/50000                      

                       Computation: 110651 steps/s (collection: 0.751s, learning 0.138s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6499
       Mean episode rew_ang_vel_xy: -0.0744
          Mean episode rew_dof_acc: -0.2873
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1500
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.3987
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0791
        Mean episode terrain_level: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.89s
                        Total time: 934.84s
                               ETA: 656 mins 35.7 s

################################################################################
                     Learning iteration 1159/50000                      

                       Computation: 117694 steps/s (collection: 0.685s, learning 0.150s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8655
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2996
   Mean episode rew_dof_pos_limits: -0.0368
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.5183
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0874
        Mean episode terrain_level: 0.1230
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.84s
                        Total time: 935.68s
                               ETA: 656 mins 36.1 s

################################################################################
                     Learning iteration 1160/50000                      

                       Computation: 103688 steps/s (collection: 0.798s, learning 0.151s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.42
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0899
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.3070
   Mean episode rew_dof_pos_limits: -0.0365
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.6340
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0906
        Mean episode terrain_level: 0.1224
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.95s
                        Total time: 936.63s
                               ETA: 656 mins 41.3 s

################################################################################
                     Learning iteration 1161/50000                      

                       Computation: 107723 steps/s (collection: 0.766s, learning 0.147s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.43
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8815
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2985
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1519
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.4887
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0767
        Mean episode terrain_level: 0.1222
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.91s
                        Total time: 937.54s
                               ETA: 656 mins 44.9 s

################################################################################
                     Learning iteration 1162/50000                      

                       Computation: 115026 steps/s (collection: 0.710s, learning 0.145s)
               Value function loss: 0.1171
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.43
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6167
       Mean episode rew_ang_vel_xy: -0.0752
          Mean episode rew_dof_acc: -0.2891
   Mean episode rew_dof_pos_limits: -0.0306
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1422
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.3584
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.1203
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.85s
                        Total time: 938.39s
                               ETA: 656 mins 46.1 s

################################################################################
                     Learning iteration 1163/50000                      

                       Computation: 115739 steps/s (collection: 0.724s, learning 0.125s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.43
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9069
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.2955
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.5110
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0837
        Mean episode terrain_level: 0.1164
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.85s
                        Total time: 939.24s
                               ETA: 656 mins 47.1 s

################################################################################
                     Learning iteration 1164/50000                      

                       Computation: 117823 steps/s (collection: 0.711s, learning 0.123s)
               Value function loss: 0.1199
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.43
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1374
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.3058
   Mean episode rew_dof_pos_limits: -0.0413
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.6647
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1003
        Mean episode terrain_level: 0.1152
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.83s
                        Total time: 940.08s
                               ETA: 656 mins 47.4 s

################################################################################
                     Learning iteration 1165/50000                      

                       Computation: 115625 steps/s (collection: 0.726s, learning 0.124s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.44
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8923
       Mean episode rew_ang_vel_xy: -0.0751
          Mean episode rew_dof_acc: -0.2924
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1436
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.5194
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0811
        Mean episode terrain_level: 0.1137
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.85s
                        Total time: 940.93s
                               ETA: 656 mins 48.4 s

################################################################################
                     Learning iteration 1166/50000                      

                       Computation: 115888 steps/s (collection: 0.707s, learning 0.142s)
               Value function loss: 0.1222
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6900
       Mean episode rew_ang_vel_xy: -0.0737
          Mean episode rew_dof_acc: -0.2939
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.4127
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0728
        Mean episode terrain_level: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.85s
                        Total time: 941.78s
                               ETA: 656 mins 49.3 s

################################################################################
                     Learning iteration 1167/50000                      

                       Computation: 120684 steps/s (collection: 0.689s, learning 0.125s)
               Value function loss: 0.1230
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.44
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0101
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.3151
   Mean episode rew_dof_pos_limits: -0.0286
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.5752
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1972
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0704
        Mean episode terrain_level: 0.1131
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.81s
                        Total time: 942.59s
                               ETA: 656 mins 48.9 s

################################################################################
                     Learning iteration 1168/50000                      

                       Computation: 123356 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.44
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2137
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.3057
   Mean episode rew_dof_pos_limits: -0.0365
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.6776
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1136
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.80s
                        Total time: 943.39s
                               ETA: 656 mins 47.6 s

################################################################################
                     Learning iteration 1169/50000                      

                       Computation: 125972 steps/s (collection: 0.658s, learning 0.123s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6718
       Mean episode rew_ang_vel_xy: -0.0733
          Mean episode rew_dof_acc: -0.2872
   Mean episode rew_dof_pos_limits: -0.0273
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1408
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.3670
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0594
        Mean episode terrain_level: 0.1148
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.78s
                        Total time: 944.17s
                               ETA: 656 mins 45.7 s

################################################################################
                     Learning iteration 1170/50000                      

                       Computation: 118510 steps/s (collection: 0.707s, learning 0.122s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1819
       Mean episode rew_ang_vel_xy: -0.0762
          Mean episode rew_dof_acc: -0.3029
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1441
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.6591
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0835
        Mean episode terrain_level: 0.1153
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.83s
                        Total time: 945.00s
                               ETA: 656 mins 45.8 s

################################################################################
                     Learning iteration 1171/50000                      

                       Computation: 112151 steps/s (collection: 0.753s, learning 0.123s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.45
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5235
       Mean episode rew_ang_vel_xy: -0.0806
          Mean episode rew_dof_acc: -0.3315
   Mean episode rew_dof_pos_limits: -0.0391
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -2.8649
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0852
        Mean episode terrain_level: 0.1181
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.88s
                        Total time: 945.87s
                               ETA: 656 mins 47.9 s

################################################################################
                     Learning iteration 1172/50000                      

                       Computation: 118551 steps/s (collection: 0.702s, learning 0.127s)
               Value function loss: 0.1232
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.45
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1087
       Mean episode rew_ang_vel_xy: -0.0757
          Mean episode rew_dof_acc: -0.2930
   Mean episode rew_dof_pos_limits: -0.0400
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1475
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.6309
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0937
        Mean episode terrain_level: 0.1164
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.83s
                        Total time: 946.70s
                               ETA: 656 mins 48.0 s

################################################################################
                     Learning iteration 1173/50000                      

                       Computation: 119274 steps/s (collection: 0.696s, learning 0.128s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0999
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.2991
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.6248
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0815
        Mean episode terrain_level: 0.1149
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.82s
                        Total time: 947.53s
                               ETA: 656 mins 47.9 s

################################################################################
                     Learning iteration 1174/50000                      

                       Computation: 121097 steps/s (collection: 0.690s, learning 0.122s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8084
       Mean episode rew_ang_vel_xy: -0.0755
          Mean episode rew_dof_acc: -0.2949
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1473
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.4538
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0878
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.81s
                        Total time: 948.34s
                               ETA: 656 mins 47.3 s

################################################################################
                     Learning iteration 1175/50000                      

                       Computation: 108721 steps/s (collection: 0.777s, learning 0.127s)
               Value function loss: 0.1216
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2586
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3105
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.7321
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0961
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.90s
                        Total time: 949.24s
                               ETA: 656 mins 50.6 s

################################################################################
                     Learning iteration 1176/50000                      

                       Computation: 118006 steps/s (collection: 0.709s, learning 0.124s)
               Value function loss: 0.1270
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0174
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.2990
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.5705
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0795
        Mean episode terrain_level: 0.1185
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.83s
                        Total time: 950.08s
                               ETA: 656 mins 50.8 s

################################################################################
                     Learning iteration 1177/50000                      

                       Computation: 125777 steps/s (collection: 0.658s, learning 0.124s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9575
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.3037
   Mean episode rew_dof_pos_limits: -0.0296
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1459
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.5337
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0731
        Mean episode terrain_level: 0.1173
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.78s
                        Total time: 950.86s
                               ETA: 656 mins 49.0 s

################################################################################
                     Learning iteration 1178/50000                      

                       Computation: 112548 steps/s (collection: 0.720s, learning 0.153s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7761
       Mean episode rew_ang_vel_xy: -0.0750
          Mean episode rew_dof_acc: -0.2815
   Mean episode rew_dof_pos_limits: -0.0290
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.4264
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0736
        Mean episode terrain_level: 0.1154
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.87s
                        Total time: 951.73s
                               ETA: 656 mins 50.9 s

################################################################################
                     Learning iteration 1179/50000                      

                       Computation: 114904 steps/s (collection: 0.721s, learning 0.134s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.46
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2123
       Mean episode rew_ang_vel_xy: -0.0767
          Mean episode rew_dof_acc: -0.2978
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1442
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.6832
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.0920
        Mean episode terrain_level: 0.1126
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.86s
                        Total time: 952.59s
                               ETA: 656 mins 52.1 s

################################################################################
                     Learning iteration 1180/50000                      

                       Computation: 114442 steps/s (collection: 0.733s, learning 0.126s)
               Value function loss: 0.1187
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.47
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2487
       Mean episode rew_ang_vel_xy: -0.0759
          Mean episode rew_dof_acc: -0.2965
   Mean episode rew_dof_pos_limits: -0.0359
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1465
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.6864
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.1127
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.86s
                        Total time: 953.45s
                               ETA: 656 mins 53.4 s

################################################################################
                     Learning iteration 1181/50000                      

                       Computation: 118748 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.1161
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.47
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1238
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.3173
   Mean episode rew_dof_pos_limits: -0.0341
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1512
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.6400
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0857
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.83s
                        Total time: 954.27s
                               ETA: 656 mins 53.5 s

################################################################################
                     Learning iteration 1182/50000                      

                       Computation: 104186 steps/s (collection: 0.804s, learning 0.139s)
               Value function loss: 0.1157
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.47
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1747
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.3027
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.6443
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0756
        Mean episode terrain_level: 0.1091
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.94s
                        Total time: 955.22s
                               ETA: 656 mins 58.3 s

################################################################################
                     Learning iteration 1183/50000                      

                       Computation: 117299 steps/s (collection: 0.708s, learning 0.131s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.47
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7997
       Mean episode rew_ang_vel_xy: -0.0747
          Mean episode rew_dof_acc: -0.2891
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.4442
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0741
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.84s
                        Total time: 956.06s
                               ETA: 656 mins 58.7 s

################################################################################
                     Learning iteration 1184/50000                      

                       Computation: 122022 steps/s (collection: 0.673s, learning 0.133s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.48
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0241
       Mean episode rew_ang_vel_xy: -0.0764
          Mean episode rew_dof_acc: -0.2990
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1426
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.5743
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0759
        Mean episode terrain_level: 0.1101
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.81s
                        Total time: 956.86s
                               ETA: 656 mins 57.8 s

################################################################################
                     Learning iteration 1185/50000                      

                       Computation: 117820 steps/s (collection: 0.711s, learning 0.124s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.48
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0744
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.3023
   Mean episode rew_dof_pos_limits: -0.0322
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.6074
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0774
        Mean episode terrain_level: 0.1125
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.83s
                        Total time: 957.70s
                               ETA: 656 mins 58.1 s

################################################################################
                     Learning iteration 1186/50000                      

                       Computation: 122675 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.1165
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.48
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.8054
       Mean episode rew_ang_vel_xy: -0.0742
          Mean episode rew_dof_acc: -0.2803
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.4348
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0747
        Mean episode terrain_level: 0.1148
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.80s
                        Total time: 958.50s
                               ETA: 656 mins 57.1 s

################################################################################
                     Learning iteration 1187/50000                      

                       Computation: 117502 steps/s (collection: 0.698s, learning 0.138s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.48
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.7000
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.2834
   Mean episode rew_dof_pos_limits: -0.0295
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.3763
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0719
        Mean episode terrain_level: 0.1143
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.84s
                        Total time: 959.33s
                               ETA: 656 mins 57.5 s

################################################################################
                     Learning iteration 1188/50000                      

                       Computation: 124700 steps/s (collection: 0.663s, learning 0.125s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.49
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1839
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.2987
   Mean episode rew_dof_pos_limits: -0.0331
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1455
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.6376
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0807
        Mean episode terrain_level: 0.1139
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.79s
                        Total time: 960.12s
                               ETA: 656 mins 55.9 s

################################################################################
                     Learning iteration 1189/50000                      

                       Computation: 113636 steps/s (collection: 0.733s, learning 0.132s)
               Value function loss: 0.1218
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.49
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1916
       Mean episode rew_ang_vel_xy: -0.0761
          Mean episode rew_dof_acc: -0.3151
   Mean episode rew_dof_pos_limits: -0.0334
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1432
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.6792
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0750
        Mean episode terrain_level: 0.1143
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.87s
                        Total time: 960.99s
                               ETA: 656 mins 57.4 s

################################################################################
                     Learning iteration 1190/50000                      

                       Computation: 115529 steps/s (collection: 0.720s, learning 0.130s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.49
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2381
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3096
   Mean episode rew_dof_pos_limits: -0.0368
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.7051
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0772
        Mean episode terrain_level: 0.1159
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.85s
                        Total time: 961.84s
                               ETA: 656 mins 58.4 s

################################################################################
                     Learning iteration 1191/50000                      

                       Computation: 106587 steps/s (collection: 0.777s, learning 0.145s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.49
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9647
       Mean episode rew_ang_vel_xy: -0.0749
          Mean episode rew_dof_acc: -0.2988
   Mean episode rew_dof_pos_limits: -0.0312
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.5307
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0705
        Mean episode terrain_level: 0.1183
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.92s
                        Total time: 962.76s
                               ETA: 657 mins 2.3 s

################################################################################
                     Learning iteration 1192/50000                      

                       Computation: 116970 steps/s (collection: 0.699s, learning 0.142s)
               Value function loss: 0.1245
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.49
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2427
       Mean episode rew_ang_vel_xy: -0.0756
          Mean episode rew_dof_acc: -0.2974
   Mean episode rew_dof_pos_limits: -0.0391
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.6955
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0907
        Mean episode terrain_level: 0.1211
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.84s
                        Total time: 963.60s
                               ETA: 657 mins 2.8 s

################################################################################
                     Learning iteration 1193/50000                      

                       Computation: 111055 steps/s (collection: 0.758s, learning 0.127s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4760
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.3109
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.8063
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0887
        Mean episode terrain_level: 0.1191
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.89s
                        Total time: 964.49s
                               ETA: 657 mins 5.2 s

################################################################################
                     Learning iteration 1194/50000                      

                       Computation: 118157 steps/s (collection: 0.707s, learning 0.125s)
               Value function loss: 0.1211
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3290
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.3076
   Mean episode rew_dof_pos_limits: -0.0351
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1518
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.7335
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0861
        Mean episode terrain_level: 0.1187
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.83s
                        Total time: 965.32s
                               ETA: 657 mins 5.4 s

################################################################################
                     Learning iteration 1195/50000                      

                       Computation: 122687 steps/s (collection: 0.673s, learning 0.128s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2561
       Mean episode rew_ang_vel_xy: -0.0759
          Mean episode rew_dof_acc: -0.3062
   Mean episode rew_dof_pos_limits: -0.0382
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.6974
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.80s
                        Total time: 966.12s
                               ETA: 657 mins 4.3 s

################################################################################
                     Learning iteration 1196/50000                      

                       Computation: 109687 steps/s (collection: 0.775s, learning 0.121s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5495
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.3207
   Mean episode rew_dof_pos_limits: -0.0376
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.8571
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0907
        Mean episode terrain_level: 0.1200
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.90s
                        Total time: 967.02s
                               ETA: 657 mins 7.1 s

################################################################################
                     Learning iteration 1197/50000                      

                       Computation: 105042 steps/s (collection: 0.803s, learning 0.133s)
               Value function loss: 0.1249
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9178
       Mean episode rew_ang_vel_xy: -0.0729
          Mean episode rew_dof_acc: -0.2875
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1412
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.5180
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0791
        Mean episode terrain_level: 0.1184
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.94s
                        Total time: 967.95s
                               ETA: 657 mins 11.5 s

################################################################################
                     Learning iteration 1198/50000                      

                       Computation: 115931 steps/s (collection: 0.726s, learning 0.122s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2815
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.3052
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.7258
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0902
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.85s
                        Total time: 968.80s
                               ETA: 657 mins 12.3 s

################################################################################
                     Learning iteration 1199/50000                      

                       Computation: 16425 steps/s (collection: 5.836s, learning 0.148s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.50
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9034
       Mean episode rew_ang_vel_xy: -0.0747
          Mean episode rew_dof_acc: -0.2892
   Mean episode rew_dof_pos_limits: -0.0278
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1470
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.4914
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1975
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.1163
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 5.98s
                        Total time: 974.78s
                               ETA: 660 mins 42.0 s

################################################################################
                     Learning iteration 1200/50000                      

                       Computation: 12345 steps/s (collection: 7.839s, learning 0.123s)
               Value function loss: 0.1214
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.51
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0005
       Mean episode rew_ang_vel_xy: -0.0740
          Mean episode rew_dof_acc: -0.2874
   Mean episode rew_dof_pos_limits: -0.0299
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1437
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -2.5579
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0683
        Mean episode terrain_level: 0.1175
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 7.96s
                        Total time: 982.75s
                               ETA: 665 mins 31.7 s

################################################################################
                     Learning iteration 1201/50000                      

                       Computation: 108939 steps/s (collection: 0.769s, learning 0.133s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.51
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1330
       Mean episode rew_ang_vel_xy: -0.0747
          Mean episode rew_dof_acc: -0.2953
   Mean episode rew_dof_pos_limits: -0.0356
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1465
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.6752
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0858
        Mean episode terrain_level: 0.1200
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.90s
                        Total time: 983.65s
                               ETA: 665 mins 34.3 s

################################################################################
                     Learning iteration 1202/50000                      

                       Computation: 111273 steps/s (collection: 0.734s, learning 0.150s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.51
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5009
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.3143
   Mean episode rew_dof_pos_limits: -0.0426
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1414
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.8647
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1010
        Mean episode terrain_level: 0.1215
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.88s
                        Total time: 984.53s
                               ETA: 665 mins 36.2 s

################################################################################
                     Learning iteration 1203/50000                      

                       Computation: 119037 steps/s (collection: 0.690s, learning 0.136s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3177
       Mean episode rew_ang_vel_xy: -0.0775
          Mean episode rew_dof_acc: -0.3091
   Mean episode rew_dof_pos_limits: -0.0393
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1529
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.7587
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0929
        Mean episode terrain_level: 0.1215
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.83s
                        Total time: 985.36s
                               ETA: 665 mins 35.6 s

################################################################################
                     Learning iteration 1204/50000                      

                       Computation: 122107 steps/s (collection: 0.662s, learning 0.144s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.51
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.6710
       Mean episode rew_ang_vel_xy: -0.0726
          Mean episode rew_dof_acc: -0.2824
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1464
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.4069
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0122
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0786
        Mean episode terrain_level: 0.1174
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.81s
                        Total time: 986.16s
                               ETA: 665 mins 34.3 s

################################################################################
                     Learning iteration 1205/50000                      

                       Computation: 120547 steps/s (collection: 0.671s, learning 0.145s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.52
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0702
       Mean episode rew_ang_vel_xy: -0.0735
          Mean episode rew_dof_acc: -0.2877
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1426
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.6169
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0804
        Mean episode terrain_level: 0.1186
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.82s
                        Total time: 986.98s
                               ETA: 665 mins 33.3 s

################################################################################
                     Learning iteration 1206/50000                      

                       Computation: 104060 steps/s (collection: 0.794s, learning 0.151s)
               Value function loss: 0.1214
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.52
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5811
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.3094
   Mean episode rew_dof_pos_limits: -0.0385
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.9042
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0948
        Mean episode terrain_level: 0.1183
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.94s
                        Total time: 987.92s
                               ETA: 665 mins 37.6 s

################################################################################
                     Learning iteration 1207/50000                      

                       Computation: 124421 steps/s (collection: 0.666s, learning 0.124s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.52
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3392
       Mean episode rew_ang_vel_xy: -0.0765
          Mean episode rew_dof_acc: -0.3055
   Mean episode rew_dof_pos_limits: -0.0375
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1420
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.7633
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.1169
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.79s
                        Total time: 988.71s
                               ETA: 665 mins 35.7 s

################################################################################
                     Learning iteration 1208/50000                      

                       Computation: 113772 steps/s (collection: 0.734s, learning 0.130s)
               Value function loss: 0.1216
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.52
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5006
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.3061
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1467
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.8363
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0986
        Mean episode terrain_level: 0.1173
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.86s
                        Total time: 989.58s
                               ETA: 665 mins 36.7 s

################################################################################
                     Learning iteration 1209/50000                      

                       Computation: 110957 steps/s (collection: 0.759s, learning 0.127s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.52
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3670
       Mean episode rew_ang_vel_xy: -0.0741
          Mean episode rew_dof_acc: -0.2987
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1438
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.7537
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0921
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.89s
                        Total time: 990.46s
                               ETA: 665 mins 38.6 s

################################################################################
                     Learning iteration 1210/50000                      

                       Computation: 114565 steps/s (collection: 0.718s, learning 0.140s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.53
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2355
       Mean episode rew_ang_vel_xy: -0.0727
          Mean episode rew_dof_acc: -0.2957
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.7069
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.1134
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.86s
                        Total time: 991.32s
                               ETA: 665 mins 39.4 s

################################################################################
                     Learning iteration 1211/50000                      

                       Computation: 125358 steps/s (collection: 0.655s, learning 0.130s)
               Value function loss: 0.1292
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.53
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4616
       Mean episode rew_ang_vel_xy: -0.0754
          Mean episode rew_dof_acc: -0.3009
   Mean episode rew_dof_pos_limits: -0.0411
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1414
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.8189
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0961
        Mean episode terrain_level: 0.1133
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.78s
                        Total time: 992.11s
                               ETA: 665 mins 37.2 s

################################################################################
                     Learning iteration 1212/50000                      

                       Computation: 123808 steps/s (collection: 0.667s, learning 0.127s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.53
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3302
       Mean episode rew_ang_vel_xy: -0.0737
          Mean episode rew_dof_acc: -0.3048
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.7330
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0863
        Mean episode terrain_level: 0.1144
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.79s
                        Total time: 992.90s
                               ETA: 665 mins 35.3 s

################################################################################
                     Learning iteration 1213/50000                      

                       Computation: 116286 steps/s (collection: 0.723s, learning 0.122s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.9795
       Mean episode rew_ang_vel_xy: -0.0718
          Mean episode rew_dof_acc: -0.2839
   Mean episode rew_dof_pos_limits: -0.0329
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1453
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -2.5503
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0787
        Mean episode terrain_level: 0.1156
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.85s
                        Total time: 993.74s
                               ETA: 665 mins 35.6 s

################################################################################
                     Learning iteration 1214/50000                      

                       Computation: 122690 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.54
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5808
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3199
   Mean episode rew_dof_pos_limits: -0.0375
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8699
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.80s
                        Total time: 994.55s
                               ETA: 665 mins 34.1 s

################################################################################
                     Learning iteration 1215/50000                      

                       Computation: 117032 steps/s (collection: 0.708s, learning 0.132s)
               Value function loss: 0.1264
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.54
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5526
       Mean episode rew_ang_vel_xy: -0.0754
          Mean episode rew_dof_acc: -0.3154
   Mean episode rew_dof_pos_limits: -0.0349
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1445
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.8411
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0869
        Mean episode terrain_level: 0.1159
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.84s
                        Total time: 995.39s
                               ETA: 665 mins 34.1 s

################################################################################
                     Learning iteration 1216/50000                      

                       Computation: 111954 steps/s (collection: 0.753s, learning 0.125s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.54
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2746
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.3000
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1550
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.7022
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0901
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.88s
                        Total time: 996.26s
                               ETA: 665 mins 35.7 s

################################################################################
                     Learning iteration 1217/50000                      

                       Computation: 103126 steps/s (collection: 0.811s, learning 0.142s)
               Value function loss: 0.1319
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3116
       Mean episode rew_ang_vel_xy: -0.0754
          Mean episode rew_dof_acc: -0.3038
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.7185
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0855
        Mean episode terrain_level: 0.1157
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.95s
                        Total time: 997.22s
                               ETA: 665 mins 40.3 s

################################################################################
                     Learning iteration 1218/50000                      

                       Computation: 110901 steps/s (collection: 0.764s, learning 0.123s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.8828
       Mean episode rew_ang_vel_xy: -0.0797
          Mean episode rew_dof_acc: -0.3254
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1460
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.0306
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0929
        Mean episode terrain_level: 0.1167
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.89s
                        Total time: 998.10s
                               ETA: 665 mins 42.2 s

################################################################################
                     Learning iteration 1219/50000                      

                       Computation: 116932 steps/s (collection: 0.704s, learning 0.137s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5127
       Mean episode rew_ang_vel_xy: -0.0792
          Mean episode rew_dof_acc: -0.3257
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.8296
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0752
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.84s
                        Total time: 998.94s
                               ETA: 665 mins 42.2 s

################################################################################
                     Learning iteration 1220/50000                      

                       Computation: 116205 steps/s (collection: 0.723s, learning 0.123s)
               Value function loss: 0.1326
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6376
       Mean episode rew_ang_vel_xy: -0.0811
          Mean episode rew_dof_acc: -0.3349
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1573
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8557
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0844
        Mean episode terrain_level: 0.1173
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.85s
                        Total time: 999.79s
                               ETA: 665 mins 42.5 s

################################################################################
                     Learning iteration 1221/50000                      

                       Computation: 122446 steps/s (collection: 0.662s, learning 0.141s)
               Value function loss: 0.1329
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6107
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3201
   Mean episode rew_dof_pos_limits: -0.0358
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.8846
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0857
        Mean episode terrain_level: 0.1184
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.80s
                        Total time: 1000.59s
                               ETA: 665 mins 41.0 s

################################################################################
                     Learning iteration 1222/50000                      

                       Computation: 109610 steps/s (collection: 0.742s, learning 0.155s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.0311
       Mean episode rew_ang_vel_xy: -0.0738
          Mean episode rew_dof_acc: -0.2953
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.5399
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0664
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.90s
                        Total time: 1001.49s
                               ETA: 665 mins 43.3 s

################################################################################
                     Learning iteration 1223/50000                      

                       Computation: 123980 steps/s (collection: 0.651s, learning 0.141s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4726
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.3179
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1445
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.7823
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0859
        Mean episode terrain_level: 0.1232
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.79s
                        Total time: 1002.28s
                               ETA: 665 mins 41.5 s

################################################################################
                     Learning iteration 1224/50000                      

                       Computation: 105354 steps/s (collection: 0.781s, learning 0.152s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.1856
       Mean episode rew_ang_vel_xy: -0.0757
          Mean episode rew_dof_acc: -0.3003
   Mean episode rew_dof_pos_limits: -0.0349
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1439
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.6667
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0739
        Mean episode terrain_level: 0.1224
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.93s
                        Total time: 1003.22s
                               ETA: 665 mins 45.2 s

################################################################################
                     Learning iteration 1225/50000                      

                       Computation: 122990 steps/s (collection: 0.659s, learning 0.141s)
               Value function loss: 0.1283
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7431
       Mean episode rew_ang_vel_xy: -0.0780
          Mean episode rew_dof_acc: -0.3161
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.9328
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0889
        Mean episode terrain_level: 0.1185
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.80s
                        Total time: 1004.02s
                               ETA: 665 mins 43.6 s

################################################################################
                     Learning iteration 1226/50000                      

                       Computation: 109452 steps/s (collection: 0.743s, learning 0.155s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6207
       Mean episode rew_ang_vel_xy: -0.0785
          Mean episode rew_dof_acc: -0.3070
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1543
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8621
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0927
        Mean episode terrain_level: 0.1193
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.90s
                        Total time: 1004.91s
                               ETA: 665 mins 45.9 s

################################################################################
                     Learning iteration 1227/50000                      

                       Computation: 113788 steps/s (collection: 0.739s, learning 0.125s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9050
       Mean episode rew_ang_vel_xy: -0.0809
          Mean episode rew_dof_acc: -0.3174
   Mean episode rew_dof_pos_limits: -0.0379
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1502
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.0086
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.0940
        Mean episode terrain_level: 0.1202
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.86s
                        Total time: 1005.78s
                               ETA: 665 mins 46.9 s

################################################################################
                     Learning iteration 1228/50000                      

                       Computation: 121237 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5862
       Mean episode rew_ang_vel_xy: -0.0790
          Mean episode rew_dof_acc: -0.3200
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.8497
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0735
        Mean episode terrain_level: 0.1197
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.81s
                        Total time: 1006.59s
                               ETA: 665 mins 45.7 s

################################################################################
                     Learning iteration 1229/50000                      

                       Computation: 125320 steps/s (collection: 0.661s, learning 0.123s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5816
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.3126
   Mean episode rew_dof_pos_limits: -0.0328
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1555
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.8295
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0805
        Mean episode terrain_level: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.78s
                        Total time: 1007.37s
                               ETA: 665 mins 43.6 s

################################################################################
                     Learning iteration 1230/50000                      

                       Computation: 124043 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.57
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4534
       Mean episode rew_ang_vel_xy: -0.0751
          Mean episode rew_dof_acc: -0.3058
   Mean episode rew_dof_pos_limits: -0.0373
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1416
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.8018
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1237
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.79s
                        Total time: 1008.17s
                               ETA: 665 mins 41.7 s

################################################################################
                     Learning iteration 1231/50000                      

                       Computation: 120018 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1283
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.58
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6918
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3109
   Mean episode rew_dof_pos_limits: -0.0373
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -2.9222
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0873
        Mean episode terrain_level: 0.1252
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.82s
                        Total time: 1008.98s
                               ETA: 665 mins 40.9 s

################################################################################
                     Learning iteration 1232/50000                      

                       Computation: 123672 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.58
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4605
       Mean episode rew_ang_vel_xy: -0.0786
          Mean episode rew_dof_acc: -0.3021
   Mean episode rew_dof_pos_limits: -0.0378
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -2.7763
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.1245
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.79s
                        Total time: 1009.78s
                               ETA: 665 mins 39.1 s

################################################################################
                     Learning iteration 1233/50000                      

                       Computation: 124084 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.1244
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.58
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0737
       Mean episode rew_ang_vel_xy: -0.0808
          Mean episode rew_dof_acc: -0.3425
   Mean episode rew_dof_pos_limits: -0.0356
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1554
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.0756
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0869
        Mean episode terrain_level: 0.1240
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.79s
                        Total time: 1010.57s
                               ETA: 665 mins 37.2 s

################################################################################
                     Learning iteration 1234/50000                      

                       Computation: 109631 steps/s (collection: 0.750s, learning 0.146s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.58
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.3663
       Mean episode rew_ang_vel_xy: -0.0749
          Mean episode rew_dof_acc: -0.3026
   Mean episode rew_dof_pos_limits: -0.0328
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1431
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.7211
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0745
        Mean episode terrain_level: 0.1240
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.90s
                        Total time: 1011.47s
                               ETA: 665 mins 39.5 s

################################################################################
                     Learning iteration 1235/50000                      

                       Computation: 116211 steps/s (collection: 0.710s, learning 0.136s)
               Value function loss: 0.1264
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.59
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9835
       Mean episode rew_ang_vel_xy: -0.0761
          Mean episode rew_dof_acc: -0.3169
   Mean episode rew_dof_pos_limits: -0.0447
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1458
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -3.0596
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1101
        Mean episode terrain_level: 0.1252
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.85s
                        Total time: 1012.31s
                               ETA: 665 mins 39.7 s

################################################################################
                     Learning iteration 1236/50000                      

                       Computation: 124608 steps/s (collection: 0.666s, learning 0.122s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.59
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6642
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.3032
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1452
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8884
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.1254
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.79s
                        Total time: 1013.10s
                               ETA: 665 mins 37.7 s

################################################################################
                     Learning iteration 1237/50000                      

                       Computation: 118040 steps/s (collection: 0.709s, learning 0.123s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.59
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.8149
       Mean episode rew_ang_vel_xy: -0.0796
          Mean episode rew_dof_acc: -0.3184
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -2.9787
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.1011
        Mean episode terrain_level: 0.1246
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.83s
                        Total time: 1013.94s
                               ETA: 665 mins 37.4 s

################################################################################
                     Learning iteration 1238/50000                      

                       Computation: 117780 steps/s (collection: 0.713s, learning 0.121s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.60
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.2659
       Mean episode rew_ang_vel_xy: -0.0757
          Mean episode rew_dof_acc: -0.2890
   Mean episode rew_dof_pos_limits: -0.0318
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.6451
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0131
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0767
        Mean episode terrain_level: 0.1249
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.83s
                        Total time: 1014.77s
                               ETA: 665 mins 37.2 s

################################################################################
                     Learning iteration 1239/50000                      

                       Computation: 121771 steps/s (collection: 0.686s, learning 0.121s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.60
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5689
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.2925
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1531
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.8252
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.0963
        Mean episode terrain_level: 0.1220
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.81s
                        Total time: 1015.58s
                               ETA: 665 mins 35.9 s

################################################################################
                     Learning iteration 1240/50000                      

                       Computation: 121530 steps/s (collection: 0.669s, learning 0.140s)
               Value function loss: 0.1255
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.60
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7876
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3036
   Mean episode rew_dof_pos_limits: -0.0451
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1484
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -2.9509
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.1109
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.81s
                        Total time: 1016.39s
                               ETA: 665 mins 34.7 s

################################################################################
                     Learning iteration 1241/50000                      

                       Computation: 118752 steps/s (collection: 0.701s, learning 0.126s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.60
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9067
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3161
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1460
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.0174
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0942
        Mean episode terrain_level: 0.1202
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.83s
                        Total time: 1017.21s
                               ETA: 665 mins 34.3 s

################################################################################
                     Learning iteration 1242/50000                      

                       Computation: 126025 steps/s (collection: 0.657s, learning 0.123s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.60
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6722
       Mean episode rew_ang_vel_xy: -0.0793
          Mean episode rew_dof_acc: -0.3129
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.8690
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0877
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.78s
                        Total time: 1017.99s
                               ETA: 665 mins 31.9 s

################################################################################
                     Learning iteration 1243/50000                      

                       Computation: 118655 steps/s (collection: 0.705s, learning 0.123s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.61
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6292
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3128
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8734
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0884
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.83s
                        Total time: 1018.82s
                               ETA: 665 mins 31.5 s

################################################################################
                     Learning iteration 1244/50000                      

                       Computation: 114925 steps/s (collection: 0.720s, learning 0.135s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.61
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6743
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.3014
   Mean episode rew_dof_pos_limits: -0.0378
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.8733
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.1198
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.86s
                        Total time: 1019.68s
                               ETA: 665 mins 32.1 s

################################################################################
                     Learning iteration 1245/50000                      

                       Computation: 112540 steps/s (collection: 0.750s, learning 0.123s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.61
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1659
       Mean episode rew_ang_vel_xy: -0.0824
          Mean episode rew_dof_acc: -0.3292
   Mean episode rew_dof_pos_limits: -0.0345
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.1205
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0883
        Mean episode terrain_level: 0.1207
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.87s
                        Total time: 1020.55s
                               ETA: 665 mins 33.4 s

################################################################################
                     Learning iteration 1246/50000                      

                       Computation: 124148 steps/s (collection: 0.661s, learning 0.131s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.62
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1158
       Mean episode rew_ang_vel_xy: -0.0833
          Mean episode rew_dof_acc: -0.3262
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1561
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.1274
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.0921
        Mean episode terrain_level: 0.1214
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.79s
                        Total time: 1021.34s
                               ETA: 665 mins 31.5 s

################################################################################
                     Learning iteration 1247/50000                      

                       Computation: 126264 steps/s (collection: 0.656s, learning 0.123s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.62
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1124
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.3242
   Mean episode rew_dof_pos_limits: -0.0358
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.0939
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0864
        Mean episode terrain_level: 0.1238
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.78s
                        Total time: 1022.12s
                               ETA: 665 mins 29.1 s

################################################################################
                     Learning iteration 1248/50000                      

                       Computation: 112347 steps/s (collection: 0.752s, learning 0.123s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.62
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2183
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3301
   Mean episode rew_dof_pos_limits: -0.0429
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -3.1626
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1088
        Mean episode terrain_level: 0.1237
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.88s
                        Total time: 1023.00s
                               ETA: 665 mins 30.5 s

################################################################################
                     Learning iteration 1249/50000                      

                       Computation: 109843 steps/s (collection: 0.771s, learning 0.124s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.62
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7516
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.2973
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1460
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.9432
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.1249
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.89s
                        Total time: 1023.89s
                               ETA: 665 mins 32.6 s

################################################################################
                     Learning iteration 1250/50000                      

                       Computation: 125052 steps/s (collection: 0.665s, learning 0.121s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0689
       Mean episode rew_ang_vel_xy: -0.0817
          Mean episode rew_dof_acc: -0.3491
   Mean episode rew_dof_pos_limits: -0.0379
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1616
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.0749
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0948
        Mean episode terrain_level: 0.1275
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.79s
                        Total time: 1024.68s
                               ETA: 665 mins 30.5 s

################################################################################
                     Learning iteration 1251/50000                      

                       Computation: 119092 steps/s (collection: 0.689s, learning 0.136s)
               Value function loss: 0.1196
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7983
       Mean episode rew_ang_vel_xy: -0.0785
          Mean episode rew_dof_acc: -0.3209
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1568
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.9313
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0804
        Mean episode terrain_level: 0.1325
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.83s
                        Total time: 1025.50s
                               ETA: 665 mins 29.9 s

################################################################################
                     Learning iteration 1252/50000                      

                       Computation: 107548 steps/s (collection: 0.771s, learning 0.143s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.63
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4443
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.3007
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.7544
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0789
        Mean episode terrain_level: 0.1319
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.91s
                        Total time: 1026.42s
                               ETA: 665 mins 32.8 s

################################################################################
                     Learning iteration 1253/50000                      

                       Computation: 124138 steps/s (collection: 0.661s, learning 0.131s)
               Value function loss: 0.1189
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.63
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6029
       Mean episode rew_ang_vel_xy: -0.0780
          Mean episode rew_dof_acc: -0.3050
   Mean episode rew_dof_pos_limits: -0.0332
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.8118
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.1318
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.79s
                        Total time: 1027.21s
                               ETA: 665 mins 30.9 s

################################################################################
                     Learning iteration 1254/50000                      

                       Computation: 123103 steps/s (collection: 0.674s, learning 0.125s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.64
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0001
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.3126
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.0002
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0855
        Mean episode terrain_level: 0.1281
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.80s
                        Total time: 1028.01s
                               ETA: 665 mins 29.3 s

################################################################################
                     Learning iteration 1255/50000                      

                       Computation: 126640 steps/s (collection: 0.653s, learning 0.123s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.64
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.8382
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.3049
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -2.9496
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0900
        Mean episode terrain_level: 0.1293
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.78s
                        Total time: 1028.78s
                               ETA: 665 mins 26.8 s

################################################################################
                     Learning iteration 1256/50000                      

                       Computation: 121183 steps/s (collection: 0.689s, learning 0.123s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.64
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3204
       Mean episode rew_ang_vel_xy: -0.0803
          Mean episode rew_dof_acc: -0.3206
   Mean episode rew_dof_pos_limits: -0.0456
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1519
           Mean episode rew_no_fly: 0.0161
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -3.1870
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1918
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0257
 Mean episode rew_tracking_lin_vel: 0.1093
        Mean episode terrain_level: 0.1309
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.81s
                        Total time: 1029.60s
                               ETA: 665 mins 25.7 s

################################################################################
                     Learning iteration 1257/50000                      

                       Computation: 122139 steps/s (collection: 0.682s, learning 0.123s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.64
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4044
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3352
   Mean episode rew_dof_pos_limits: -0.0434
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1544
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -3.2246
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1074
        Mean episode terrain_level: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.80s
                        Total time: 1030.40s
                               ETA: 665 mins 24.3 s

################################################################################
                     Learning iteration 1258/50000                      

                       Computation: 121786 steps/s (collection: 0.683s, learning 0.124s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.65
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1389
       Mean episode rew_ang_vel_xy: -0.0796
          Mean episode rew_dof_acc: -0.2966
   Mean episode rew_dof_pos_limits: -0.0425
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1474
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.0744
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1001
        Mean episode terrain_level: 0.1288
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.81s
                        Total time: 1031.21s
                               ETA: 665 mins 23.0 s

################################################################################
                     Learning iteration 1259/50000                      

                       Computation: 127641 steps/s (collection: 0.645s, learning 0.126s)
               Value function loss: 0.1207
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.65
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2160
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.3097
   Mean episode rew_dof_pos_limits: -0.0414
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.1414
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1026
        Mean episode terrain_level: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.77s
                        Total time: 1031.98s
                               ETA: 665 mins 20.3 s

################################################################################
                     Learning iteration 1260/50000                      

                       Computation: 119858 steps/s (collection: 0.697s, learning 0.123s)
               Value function loss: 0.1288
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.65
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3805
       Mean episode rew_ang_vel_xy: -0.0822
          Mean episode rew_dof_acc: -0.3303
   Mean episode rew_dof_pos_limits: -0.0387
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1548
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.1874
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0973
        Mean episode terrain_level: 0.1270
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.82s
                        Total time: 1032.80s
                               ETA: 665 mins 19.6 s

################################################################################
                     Learning iteration 1261/50000                      

                       Computation: 118785 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.1239
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.65
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.8383
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.2910
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1473
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.9294
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.1027
        Mean episode terrain_level: 0.1244
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.83s
                        Total time: 1033.63s
                               ETA: 665 mins 19.1 s

################################################################################
                     Learning iteration 1262/50000                      

                       Computation: 121221 steps/s (collection: 0.688s, learning 0.123s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.66
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0577
       Mean episode rew_ang_vel_xy: -0.0789
          Mean episode rew_dof_acc: -0.3071
   Mean episode rew_dof_pos_limits: -0.0413
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1487
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.0516
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0980
        Mean episode terrain_level: 0.1226
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.81s
                        Total time: 1034.44s
                               ETA: 665 mins 17.9 s

################################################################################
                     Learning iteration 1263/50000                      

                       Computation: 113278 steps/s (collection: 0.723s, learning 0.145s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.66
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0398
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.3063
   Mean episode rew_dof_pos_limits: -0.0405
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.0363
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.1074
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.87s
                        Total time: 1035.30s
                               ETA: 665 mins 19.0 s

################################################################################
                     Learning iteration 1264/50000                      

                       Computation: 125653 steps/s (collection: 0.660s, learning 0.122s)
               Value function loss: 0.1307
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.66
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9903
       Mean episode rew_ang_vel_xy: -0.0789
          Mean episode rew_dof_acc: -0.3160
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1513
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.0022
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0872
        Mean episode terrain_level: 0.1212
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.78s
                        Total time: 1036.09s
                               ETA: 665 mins 16.8 s

################################################################################
                     Learning iteration 1265/50000                      

                       Computation: 119272 steps/s (collection: 0.688s, learning 0.136s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.66
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9783
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3033
   Mean episode rew_dof_pos_limits: -0.0344
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1552
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.0016
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0865
        Mean episode terrain_level: 0.1211
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.82s
                        Total time: 1036.91s
                               ETA: 665 mins 16.1 s

################################################################################
                     Learning iteration 1266/50000                      

                       Computation: 119544 steps/s (collection: 0.693s, learning 0.130s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.66
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7140
       Mean episode rew_ang_vel_xy: -0.0756
          Mean episode rew_dof_acc: -0.2940
   Mean episode rew_dof_pos_limits: -0.0380
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1481
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8745
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0870
        Mean episode terrain_level: 0.1186
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.82s
                        Total time: 1037.73s
                               ETA: 665 mins 15.5 s

################################################################################
                     Learning iteration 1267/50000                      

                       Computation: 111735 steps/s (collection: 0.744s, learning 0.136s)
               Value function loss: 0.1339
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.67
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.5230
       Mean episode rew_ang_vel_xy: -0.0759
          Mean episode rew_dof_acc: -0.3067
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.7836
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0743
        Mean episode terrain_level: 0.1163
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.88s
                        Total time: 1038.61s
                               ETA: 665 mins 17.0 s

################################################################################
                     Learning iteration 1268/50000                      

                       Computation: 109963 steps/s (collection: 0.758s, learning 0.136s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3240
       Mean episode rew_ang_vel_xy: -0.0796
          Mean episode rew_dof_acc: -0.3249
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1531
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -3.1756
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.1046
        Mean episode terrain_level: 0.1172
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.89s
                        Total time: 1039.51s
                               ETA: 665 mins 19.0 s

################################################################################
                     Learning iteration 1269/50000                      

                       Computation: 119090 steps/s (collection: 0.690s, learning 0.136s)
               Value function loss: 0.1283
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.67
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4231
       Mean episode rew_ang_vel_xy: -0.0824
          Mean episode rew_dof_acc: -0.3232
   Mean episode rew_dof_pos_limits: -0.0439
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -3.2462
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1915
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.1127
        Mean episode terrain_level: 0.1194
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.83s
                        Total time: 1040.33s
                               ETA: 665 mins 18.4 s

################################################################################
                     Learning iteration 1270/50000                      

                       Computation: 112850 steps/s (collection: 0.747s, learning 0.124s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.68
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4893
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.3319
   Mean episode rew_dof_pos_limits: -0.0438
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1585
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -3.2760
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1088
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.87s
                        Total time: 1041.20s
                               ETA: 665 mins 19.6 s

################################################################################
                     Learning iteration 1271/50000                      

                       Computation: 120644 steps/s (collection: 0.692s, learning 0.123s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.68
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.6052
       Mean episode rew_ang_vel_xy: -0.0743
          Mean episode rew_dof_acc: -0.2942
   Mean episode rew_dof_pos_limits: -0.0400
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.8202
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0904
        Mean episode terrain_level: 0.1173
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.81s
                        Total time: 1042.02s
                               ETA: 665 mins 18.6 s

################################################################################
                     Learning iteration 1272/50000                      

                       Computation: 115806 steps/s (collection: 0.705s, learning 0.144s)
               Value function loss: 0.1288
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.68
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2567
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3076
   Mean episode rew_dof_pos_limits: -0.0404
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1481
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.1575
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1918
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.1012
        Mean episode terrain_level: 0.1183
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.85s
                        Total time: 1042.87s
                               ETA: 665 mins 18.9 s

################################################################################
                     Learning iteration 1273/50000                      

                       Computation: 110884 steps/s (collection: 0.749s, learning 0.137s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2780
       Mean episode rew_ang_vel_xy: -0.0811
          Mean episode rew_dof_acc: -0.3221
   Mean episode rew_dof_pos_limits: -0.0414
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1537
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.1611
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.0974
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.89s
                        Total time: 1043.75s
                               ETA: 665 mins 20.7 s

################################################################################
                     Learning iteration 1274/50000                      

                       Computation: 123961 steps/s (collection: 0.668s, learning 0.125s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.69
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0064
       Mean episode rew_ang_vel_xy: -0.0804
          Mean episode rew_dof_acc: -0.3106
   Mean episode rew_dof_pos_limits: -0.0368
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1537
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.0013
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0878
        Mean episode terrain_level: 0.1162
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.79s
                        Total time: 1044.55s
                               ETA: 665 mins 18.9 s

################################################################################
                     Learning iteration 1275/50000                      

                       Computation: 117305 steps/s (collection: 0.714s, learning 0.124s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.69
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0109
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.3107
   Mean episode rew_dof_pos_limits: -0.0335
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.9706
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0794
        Mean episode terrain_level: 0.1169
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.84s
                        Total time: 1045.38s
                               ETA: 665 mins 18.8 s

################################################################################
                     Learning iteration 1276/50000                      

                       Computation: 112727 steps/s (collection: 0.735s, learning 0.137s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.69
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4949
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3372
   Mean episode rew_dof_pos_limits: -0.0391
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.2370
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0995
        Mean episode terrain_level: 0.1169
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.87s
                        Total time: 1046.26s
                               ETA: 665 mins 20.0 s

################################################################################
                     Learning iteration 1277/50000                      

                       Computation: 120583 steps/s (collection: 0.657s, learning 0.159s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.69
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9246
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3016
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.9443
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0983
        Mean episode terrain_level: 0.1185
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.82s
                        Total time: 1047.07s
                               ETA: 665 mins 19.0 s

################################################################################
                     Learning iteration 1278/50000                      

                       Computation: 110097 steps/s (collection: 0.747s, learning 0.146s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.69
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9532
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2995
   Mean episode rew_dof_pos_limits: -0.0346
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -2.9261
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0861
        Mean episode terrain_level: 0.1208
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.89s
                        Total time: 1047.96s
                               ETA: 665 mins 21.0 s

################################################################################
                     Learning iteration 1279/50000                      

                       Computation: 127693 steps/s (collection: 0.649s, learning 0.121s)
               Value function loss: 0.1223
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.70
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4879
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.3217
   Mean episode rew_dof_pos_limits: -0.0426
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -3.2384
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1023
        Mean episode terrain_level: 0.1217
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.77s
                        Total time: 1048.73s
                               ETA: 665 mins 18.3 s

################################################################################
                     Learning iteration 1280/50000                      

                       Computation: 119714 steps/s (collection: 0.699s, learning 0.122s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.70
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.4600
       Mean episode rew_ang_vel_xy: -0.0757
          Mean episode rew_dof_acc: -0.2933
   Mean episode rew_dof_pos_limits: -0.0321
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1456
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.7137
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0757
        Mean episode terrain_level: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.82s
                        Total time: 1049.56s
                               ETA: 665 mins 17.5 s

################################################################################
                     Learning iteration 1281/50000                      

                       Computation: 116946 steps/s (collection: 0.719s, learning 0.122s)
               Value function loss: 0.1331
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.70
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9737
       Mean episode rew_ang_vel_xy: -0.0790
          Mean episode rew_dof_acc: -0.3100
   Mean episode rew_dof_pos_limits: -0.0352
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1530
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.9800
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0851
        Mean episode terrain_level: 0.1214
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.84s
                        Total time: 1050.40s
                               ETA: 665 mins 17.5 s

################################################################################
                     Learning iteration 1282/50000                      

                       Computation: 117294 steps/s (collection: 0.702s, learning 0.136s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.70
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3655
       Mean episode rew_ang_vel_xy: -0.0812
          Mean episode rew_dof_acc: -0.3149
   Mean episode rew_dof_pos_limits: -0.0334
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1545
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.1655
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0909
        Mean episode terrain_level: 0.1247
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.84s
                        Total time: 1051.23s
                               ETA: 665 mins 17.4 s

################################################################################
                     Learning iteration 1283/50000                      

                       Computation: 120169 steps/s (collection: 0.689s, learning 0.129s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.71
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.8475
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.3077
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1510
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.9024
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.1273
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.82s
                        Total time: 1052.05s
                               ETA: 665 mins 16.5 s

################################################################################
                     Learning iteration 1284/50000                      

                       Computation: 125370 steps/s (collection: 0.663s, learning 0.121s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.71
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1376
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3192
   Mean episode rew_dof_pos_limits: -0.0385
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1514
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.0855
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0937
        Mean episode terrain_level: 0.1255
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.78s
                        Total time: 1052.84s
                               ETA: 665 mins 14.4 s

################################################################################
                     Learning iteration 1285/50000                      

                       Computation: 119513 steps/s (collection: 0.700s, learning 0.122s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.71
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1977
       Mean episode rew_ang_vel_xy: -0.0787
          Mean episode rew_dof_acc: -0.3111
   Mean episode rew_dof_pos_limits: -0.0415
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1522
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.1110
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1020
        Mean episode terrain_level: 0.1242
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.82s
                        Total time: 1053.66s
                               ETA: 665 mins 13.7 s

################################################################################
                     Learning iteration 1286/50000                      

                       Computation: 121262 steps/s (collection: 0.686s, learning 0.125s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.71
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4367
       Mean episode rew_ang_vel_xy: -0.0836
          Mean episode rew_dof_acc: -0.3355
   Mean episode rew_dof_pos_limits: -0.0356
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1547
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.2303
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0898
        Mean episode terrain_level: 0.1228
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.81s
                        Total time: 1054.47s
                               ETA: 665 mins 12.5 s

################################################################################
                     Learning iteration 1287/50000                      

                       Computation: 124176 steps/s (collection: 0.668s, learning 0.123s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.72
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7632
       Mean episode rew_ang_vel_xy: -0.0764
          Mean episode rew_dof_acc: -0.2974
   Mean episode rew_dof_pos_limits: -0.0352
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.8588
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0872
        Mean episode terrain_level: 0.1234
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.79s
                        Total time: 1055.26s
                               ETA: 665 mins 10.7 s

################################################################################
                     Learning iteration 1288/50000                      

                       Computation: 122146 steps/s (collection: 0.670s, learning 0.134s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.72
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9441
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.3076
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -2.9819
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0743
        Mean episode terrain_level: 0.1265
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.80s
                        Total time: 1056.07s
                               ETA: 665 mins 9.3 s

################################################################################
                     Learning iteration 1289/50000                      

                       Computation: 118581 steps/s (collection: 0.684s, learning 0.145s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1579
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3009
   Mean episode rew_dof_pos_limits: -0.0370
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1528
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.0681
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0936
        Mean episode terrain_level: 0.1312
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.83s
                        Total time: 1056.90s
                               ETA: 665 mins 8.9 s

################################################################################
                     Learning iteration 1290/50000                      

                       Computation: 115937 steps/s (collection: 0.723s, learning 0.125s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7661
       Mean episode rew_ang_vel_xy: -0.0759
          Mean episode rew_dof_acc: -0.2934
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1510
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.8936
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0853
        Mean episode terrain_level: 0.1304
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.85s
                        Total time: 1057.74s
                               ETA: 665 mins 9.1 s

################################################################################
                     Learning iteration 1291/50000                      

                       Computation: 126380 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.1259
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.72
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5178
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.3214
   Mean episode rew_dof_pos_limits: -0.0406
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1570
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.2875
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.1050
        Mean episode terrain_level: 0.1296
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.78s
                        Total time: 1058.52s
                               ETA: 665 mins 6.7 s

################################################################################
                     Learning iteration 1292/50000                      

                       Computation: 116407 steps/s (collection: 0.696s, learning 0.149s)
               Value function loss: 0.1279
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.73
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0788
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.3032
   Mean episode rew_dof_pos_limits: -0.0345
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.0469
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0886
        Mean episode terrain_level: 0.1295
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.84s
                        Total time: 1059.37s
                               ETA: 665 mins 6.9 s

################################################################################
                     Learning iteration 1293/50000                      

                       Computation: 124479 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.1342
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.73
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2738
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3273
   Mean episode rew_dof_pos_limits: -0.0359
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.1726
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0886
        Mean episode terrain_level: 0.1301
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.79s
                        Total time: 1060.16s
                               ETA: 665 mins 4.9 s

################################################################################
                     Learning iteration 1294/50000                      

                       Computation: 118625 steps/s (collection: 0.705s, learning 0.124s)
               Value function loss: 0.1290
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.73
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3226
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3207
   Mean episode rew_dof_pos_limits: -0.0351
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1516
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.1699
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0865
        Mean episode terrain_level: 0.1299
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.83s
                        Total time: 1060.98s
                               ETA: 665 mins 4.5 s

################################################################################
                     Learning iteration 1295/50000                      

                       Computation: 121684 steps/s (collection: 0.668s, learning 0.139s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.73
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0727
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3075
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1540
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.0471
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0863
        Mean episode terrain_level: 0.1299
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.81s
                        Total time: 1061.79s
                               ETA: 665 mins 3.2 s

################################################################################
                     Learning iteration 1296/50000                      

                       Computation: 118607 steps/s (collection: 0.689s, learning 0.140s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.74
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9076
       Mean episode rew_ang_vel_xy: -0.0864
          Mean episode rew_dof_acc: -0.3397
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -3.4436
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.1030
        Mean episode terrain_level: 0.1277
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.83s
                        Total time: 1062.62s
                               ETA: 665 mins 2.8 s

################################################################################
                     Learning iteration 1297/50000                      

                       Computation: 115864 steps/s (collection: 0.697s, learning 0.151s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.74
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2578
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.3121
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.1509
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.1296
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.85s
                        Total time: 1063.47s
                               ETA: 665 mins 3.0 s

################################################################################
                     Learning iteration 1298/50000                      

                       Computation: 116681 steps/s (collection: 0.694s, learning 0.149s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.74
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4335
       Mean episode rew_ang_vel_xy: -0.0800
          Mean episode rew_dof_acc: -0.3212
   Mean episode rew_dof_pos_limits: -0.0373
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.2203
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0917
        Mean episode terrain_level: 0.1285
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.84s
                        Total time: 1064.31s
                               ETA: 665 mins 3.1 s

################################################################################
                     Learning iteration 1299/50000                      

                       Computation: 123151 steps/s (collection: 0.660s, learning 0.139s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.75
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0627
       Mean episode rew_ang_vel_xy: -0.0763
          Mean episode rew_dof_acc: -0.3028
   Mean episode rew_dof_pos_limits: -0.0341
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.0476
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0788
        Mean episode terrain_level: 0.1265
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.80s
                        Total time: 1065.11s
                               ETA: 665 mins 1.5 s

################################################################################
                     Learning iteration 1300/50000                      

                       Computation: 102752 steps/s (collection: 0.813s, learning 0.144s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.75
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.7268
       Mean episode rew_ang_vel_xy: -0.0753
          Mean episode rew_dof_acc: -0.2913
   Mean episode rew_dof_pos_limits: -0.0315
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1449
           Mean episode rew_no_fly: 0.0125
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.8443
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0770
        Mean episode terrain_level: 0.1263
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.96s
                        Total time: 1066.07s
                               ETA: 665 mins 5.8 s

################################################################################
                     Learning iteration 1301/50000                      

                       Computation: 123119 steps/s (collection: 0.670s, learning 0.128s)
               Value function loss: 0.1306
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.75
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7696
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3478
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1550
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -3.4180
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0970
        Mean episode terrain_level: 0.1259
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.80s
                        Total time: 1066.86s
                               ETA: 665 mins 4.2 s

################################################################################
                     Learning iteration 1302/50000                      

                       Computation: 128712 steps/s (collection: 0.643s, learning 0.121s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.75
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3711
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3104
   Mean episode rew_dof_pos_limits: -0.0411
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.2046
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0988
        Mean episode terrain_level: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.76s
                        Total time: 1067.63s
                               ETA: 665 mins 1.3 s

################################################################################
                     Learning iteration 1303/50000                      

                       Computation: 123512 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.1307
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.76
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2163
       Mean episode rew_ang_vel_xy: -0.0777
          Mean episode rew_dof_acc: -0.3139
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1423
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.1288
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0835
        Mean episode terrain_level: 0.1264
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.80s
                        Total time: 1068.42s
                               ETA: 664 mins 59.6 s

################################################################################
                     Learning iteration 1304/50000                      

                       Computation: 118739 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.1217
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.76
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4289
       Mean episode rew_ang_vel_xy: -0.0806
          Mean episode rew_dof_acc: -0.3163
   Mean episode rew_dof_pos_limits: -0.0375
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.2344
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0852
        Mean episode terrain_level: 0.1251
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.83s
                        Total time: 1069.25s
                               ETA: 664 mins 59.1 s

################################################################################
                     Learning iteration 1305/50000                      

                       Computation: 116324 steps/s (collection: 0.722s, learning 0.123s)
               Value function loss: 0.1212
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.76
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7719
       Mean episode rew_ang_vel_xy: -0.0832
          Mean episode rew_dof_acc: -0.3354
   Mean episode rew_dof_pos_limits: -0.0420
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -3.3927
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1017
        Mean episode terrain_level: 0.1249
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.85s
                        Total time: 1070.10s
                               ETA: 664 mins 59.2 s

################################################################################
                     Learning iteration 1306/50000                      

                       Computation: 127360 steps/s (collection: 0.649s, learning 0.123s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.77
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5454
       Mean episode rew_ang_vel_xy: -0.0818
          Mean episode rew_dof_acc: -0.3299
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.2607
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.1244
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.77s
                        Total time: 1070.87s
                               ETA: 664 mins 56.6 s

################################################################################
                     Learning iteration 1307/50000                      

                       Computation: 104944 steps/s (collection: 0.813s, learning 0.123s)
               Value function loss: 0.1235
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.77
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1592
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.3081
   Mean episode rew_dof_pos_limits: -0.0360
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1518
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.1046
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0847
        Mean episode terrain_level: 0.1222
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.94s
                        Total time: 1071.81s
                               ETA: 665 mins 0.2 s

################################################################################
                     Learning iteration 1308/50000                      

                       Computation: 109655 steps/s (collection: 0.767s, learning 0.130s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.77
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6854
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3419
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1621
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.3408
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0891
        Mean episode terrain_level: 0.1218
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.90s
                        Total time: 1072.70s
                               ETA: 665 mins 2.2 s

################################################################################
                     Learning iteration 1309/50000                      

                       Computation: 123890 steps/s (collection: 0.671s, learning 0.122s)
               Value function loss: 0.1250
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.77
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1253
       Mean episode rew_ang_vel_xy: -0.0747
          Mean episode rew_dof_acc: -0.2977
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.0509
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0905
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.79s
                        Total time: 1073.50s
                               ETA: 665 mins 0.5 s

################################################################################
                     Learning iteration 1310/50000                      

                       Computation: 125661 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1738
       Mean episode rew_ang_vel_xy: -0.0765
          Mean episode rew_dof_acc: -0.2962
   Mean episode rew_dof_pos_limits: -0.0385
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.1077
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0859
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.78s
                        Total time: 1074.28s
                               ETA: 664 mins 58.3 s

################################################################################
                     Learning iteration 1311/50000                      

                       Computation: 115091 steps/s (collection: 0.722s, learning 0.132s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6650
       Mean episode rew_ang_vel_xy: -0.0789
          Mean episode rew_dof_acc: -0.3221
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1474
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.3627
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1000
        Mean episode terrain_level: 0.1219
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.85s
                        Total time: 1075.13s
                               ETA: 664 mins 58.7 s

################################################################################
                     Learning iteration 1312/50000                      

                       Computation: 124208 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9355
       Mean episode rew_ang_vel_xy: -0.0830
          Mean episode rew_dof_acc: -0.3519
   Mean episode rew_dof_pos_limits: -0.0423
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1599
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -3.5490
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1033
        Mean episode terrain_level: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.79s
                        Total time: 1075.92s
                               ETA: 664 mins 56.9 s

################################################################################
                     Learning iteration 1313/50000                      

                       Computation: 127919 steps/s (collection: 0.646s, learning 0.123s)
               Value function loss: 0.1225
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1544
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.2989
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.0979
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0823
        Mean episode terrain_level: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.77s
                        Total time: 1076.69s
                               ETA: 664 mins 54.2 s

################################################################################
                     Learning iteration 1314/50000                      

                       Computation: 105202 steps/s (collection: 0.811s, learning 0.124s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5078
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3259
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1484
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.2775
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0818
        Mean episode terrain_level: 0.1197
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.93s
                        Total time: 1077.63s
                               ETA: 664 mins 57.6 s

################################################################################
                     Learning iteration 1315/50000                      

                       Computation: 116243 steps/s (collection: 0.720s, learning 0.126s)
               Value function loss: 0.1222
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.78
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7766
       Mean episode rew_ang_vel_xy: -0.0833
          Mean episode rew_dof_acc: -0.3246
   Mean episode rew_dof_pos_limits: -0.0414
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -3.4105
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.0927
        Mean episode terrain_level: 0.1176
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.85s
                        Total time: 1078.47s
                               ETA: 664 mins 57.7 s

################################################################################
                     Learning iteration 1316/50000                      

                       Computation: 114324 steps/s (collection: 0.736s, learning 0.124s)
               Value function loss: 0.1295
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.79
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.2686
       Mean episode rew_ang_vel_xy: -0.0792
          Mean episode rew_dof_acc: -0.3051
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1592
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.1618
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0824
        Mean episode terrain_level: 0.1207
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.86s
                        Total time: 1079.33s
                               ETA: 664 mins 58.4 s

################################################################################
                     Learning iteration 1317/50000                      

                       Computation: 109617 steps/s (collection: 0.754s, learning 0.143s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.79
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6708
       Mean episode rew_ang_vel_xy: -0.0775
          Mean episode rew_dof_acc: -0.3175
   Mean episode rew_dof_pos_limits: -0.0406
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.4177
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0953
        Mean episode terrain_level: 0.1196
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.90s
                        Total time: 1080.23s
                               ETA: 665 mins 0.5 s

################################################################################
                     Learning iteration 1318/50000                      

                       Computation: 124901 steps/s (collection: 0.660s, learning 0.127s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.79
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7450
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.3365
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1561
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.4548
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0962
        Mean episode terrain_level: 0.1217
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.79s
                        Total time: 1081.02s
                               ETA: 664 mins 58.4 s

################################################################################
                     Learning iteration 1319/50000                      

                       Computation: 121176 steps/s (collection: 0.688s, learning 0.123s)
               Value function loss: 0.1238
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.79
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9987
       Mean episode rew_ang_vel_xy: -0.0753
          Mean episode rew_dof_acc: -0.2789
   Mean episode rew_dof_pos_limits: -0.0370
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1444
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.0279
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0854
        Mean episode terrain_level: 0.1232
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.81s
                        Total time: 1081.83s
                               ETA: 664 mins 57.3 s

################################################################################
                     Learning iteration 1320/50000                      

                       Computation: 122509 steps/s (collection: 0.681s, learning 0.121s)
               Value function loss: 0.1230
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.80
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6413
       Mean episode rew_ang_vel_xy: -0.0792
          Mean episode rew_dof_acc: -0.3172
   Mean episode rew_dof_pos_limits: -0.0406
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.4034
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0954
        Mean episode terrain_level: 0.1201
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.80s
                        Total time: 1082.63s
                               ETA: 664 mins 55.9 s

################################################################################
                     Learning iteration 1321/50000                      

                       Computation: 121371 steps/s (collection: 0.675s, learning 0.135s)
               Value function loss: 0.1213
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.80
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0964
       Mean episode rew_ang_vel_xy: -0.0751
          Mean episode rew_dof_acc: -0.2907
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1437
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.0972
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0903
        Mean episode terrain_level: 0.1181
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.81s
                        Total time: 1083.44s
                               ETA: 664 mins 54.7 s

################################################################################
                     Learning iteration 1322/50000                      

                       Computation: 116862 steps/s (collection: 0.718s, learning 0.123s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.80
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3367
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.3004
   Mean episode rew_dof_pos_limits: -0.0360
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.2366
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1166
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.84s
                        Total time: 1084.28s
                               ETA: 664 mins 54.7 s

################################################################################
                     Learning iteration 1323/50000                      

                       Computation: 121417 steps/s (collection: 0.674s, learning 0.136s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.80
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -5.9803
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.3029
   Mean episode rew_dof_pos_limits: -0.0309
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -3.0599
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1969
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0724
        Mean episode terrain_level: 0.1182
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.81s
                        Total time: 1085.09s
                               ETA: 664 mins 53.5 s

################################################################################
                     Learning iteration 1324/50000                      

                       Computation: 112192 steps/s (collection: 0.753s, learning 0.123s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.81
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1168
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3424
   Mean episode rew_dof_pos_limits: -0.0409
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -3.6528
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.1084
        Mean episode terrain_level: 0.1188
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.88s
                        Total time: 1085.97s
                               ETA: 664 mins 54.7 s

################################################################################
                     Learning iteration 1325/50000                      

                       Computation: 117396 steps/s (collection: 0.714s, learning 0.124s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.81
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7602
       Mean episode rew_ang_vel_xy: -0.0810
          Mean episode rew_dof_acc: -0.3362
   Mean episode rew_dof_pos_limits: -0.0373
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.4416
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0876
        Mean episode terrain_level: 0.1172
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.84s
                        Total time: 1086.80s
                               ETA: 664 mins 54.6 s

################################################################################
                     Learning iteration 1326/50000                      

                       Computation: 115226 steps/s (collection: 0.720s, learning 0.133s)
               Value function loss: 0.1295
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.81
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6818
       Mean episode rew_ang_vel_xy: -0.0778
          Mean episode rew_dof_acc: -0.3214
   Mean episode rew_dof_pos_limits: -0.0370
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.4160
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0866
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.85s
                        Total time: 1087.66s
                               ETA: 664 mins 55.0 s

################################################################################
                     Learning iteration 1327/50000                      

                       Computation: 122679 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.1258
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.81
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1889
       Mean episode rew_ang_vel_xy: -0.0748
          Mean episode rew_dof_acc: -0.2994
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1405
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -3.1599
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0812
        Mean episode terrain_level: 0.1160
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.80s
                        Total time: 1088.46s
                               ETA: 664 mins 53.5 s

################################################################################
                     Learning iteration 1328/50000                      

                       Computation: 120160 steps/s (collection: 0.681s, learning 0.137s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5876
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.3197
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.3455
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0885
        Mean episode terrain_level: 0.1125
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.82s
                        Total time: 1089.28s
                               ETA: 664 mins 52.6 s

################################################################################
                     Learning iteration 1329/50000                      

                       Computation: 110914 steps/s (collection: 0.756s, learning 0.130s)
               Value function loss: 0.1322
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5316
       Mean episode rew_ang_vel_xy: -0.0766
          Mean episode rew_dof_acc: -0.3172
   Mean episode rew_dof_pos_limits: -0.0420
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.3586
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1003
        Mean episode terrain_level: 0.1128
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.89s
                        Total time: 1090.16s
                               ETA: 664 mins 54.2 s

################################################################################
                     Learning iteration 1330/50000                      

                       Computation: 123522 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.1303
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0246
       Mean episode rew_ang_vel_xy: -0.0755
          Mean episode rew_dof_acc: -0.2876
   Mean episode rew_dof_pos_limits: -0.0330
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1430
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.0723
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0728
        Mean episode terrain_level: 0.1131
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.80s
                        Total time: 1090.96s
                               ETA: 664 mins 52.5 s

################################################################################
                     Learning iteration 1331/50000                      

                       Computation: 111448 steps/s (collection: 0.760s, learning 0.122s)
               Value function loss: 0.1290
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5680
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.3190
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.3539
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0904
        Mean episode terrain_level: 0.1125
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.88s
                        Total time: 1091.84s
                               ETA: 664 mins 54.0 s

################################################################################
                     Learning iteration 1332/50000                      

                       Computation: 109791 steps/s (collection: 0.745s, learning 0.150s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.0943
       Mean episode rew_ang_vel_xy: -0.0763
          Mean episode rew_dof_acc: -0.2914
   Mean episode rew_dof_pos_limits: -0.0342
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.0717
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0821
        Mean episode terrain_level: 0.1132
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.90s
                        Total time: 1092.74s
                               ETA: 664 mins 55.9 s

################################################################################
                     Learning iteration 1333/50000                      

                       Computation: 125805 steps/s (collection: 0.657s, learning 0.124s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.82
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6438
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3151
   Mean episode rew_dof_pos_limits: -0.0400
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.3797
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1123
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 0.78s
                        Total time: 1093.52s
                               ETA: 664 mins 53.7 s

################################################################################
                     Learning iteration 1334/50000                      

                       Computation: 121818 steps/s (collection: 0.685s, learning 0.121s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.83
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.3104
       Mean episode rew_ang_vel_xy: -0.0761
          Mean episode rew_dof_acc: -0.3056
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.1824
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0853
        Mean episode terrain_level: 0.1147
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 0.81s
                        Total time: 1094.32s
                               ETA: 664 mins 52.4 s

################################################################################
                     Learning iteration 1335/50000                      

                       Computation: 115431 steps/s (collection: 0.729s, learning 0.122s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.83
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0376
       Mean episode rew_ang_vel_xy: -0.0823
          Mean episode rew_dof_acc: -0.3433
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.5857
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0946
        Mean episode terrain_level: 0.1133
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 0.85s
                        Total time: 1095.18s
                               ETA: 664 mins 52.8 s

################################################################################
                     Learning iteration 1336/50000                      

                       Computation: 126217 steps/s (collection: 0.656s, learning 0.123s)
               Value function loss: 0.1335
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.83
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4296
       Mean episode rew_ang_vel_xy: -0.0832
          Mean episode rew_dof_acc: -0.3362
   Mean episode rew_dof_pos_limits: -0.0436
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0110
       Mean episode rew_smoothness: -3.7710
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1917
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.1099
        Mean episode terrain_level: 0.1125
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 0.78s
                        Total time: 1095.96s
                               ETA: 664 mins 50.5 s

################################################################################
                     Learning iteration 1337/50000                      

                       Computation: 117319 steps/s (collection: 0.699s, learning 0.139s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.83
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4378
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.2999
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.2664
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0896
        Mean episode terrain_level: 0.1113
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 0.84s
                        Total time: 1096.79s
                               ETA: 664 mins 50.3 s

################################################################################
                     Learning iteration 1338/50000                      

                       Computation: 117841 steps/s (collection: 0.688s, learning 0.146s)
               Value function loss: 0.1245
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.83
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5682
       Mean episode rew_ang_vel_xy: -0.0799
          Mean episode rew_dof_acc: -0.3091
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.3405
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.1132
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 0.83s
                        Total time: 1097.63s
                               ETA: 664 mins 50.0 s

################################################################################
                     Learning iteration 1339/50000                      

                       Computation: 114904 steps/s (collection: 0.699s, learning 0.157s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.84
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1826
       Mean episode rew_ang_vel_xy: -0.0744
          Mean episode rew_dof_acc: -0.2962
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.1120
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0847
        Mean episode terrain_level: 0.1145
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 0.86s
                        Total time: 1098.48s
                               ETA: 664 mins 50.5 s

################################################################################
                     Learning iteration 1340/50000                      

                       Computation: 120904 steps/s (collection: 0.675s, learning 0.138s)
               Value function loss: 0.1291
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.84
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4329
       Mean episode rew_ang_vel_xy: -0.0766
          Mean episode rew_dof_acc: -0.3040
   Mean episode rew_dof_pos_limits: -0.0308
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -3.1912
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1973
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0781
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 0.81s
                        Total time: 1099.30s
                               ETA: 664 mins 49.4 s

################################################################################
                     Learning iteration 1341/50000                      

                       Computation: 127558 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.1229
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.84
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0812
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.3282
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1559
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.5616
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1189
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 0.77s
                        Total time: 1100.07s
                               ETA: 664 mins 46.8 s

################################################################################
                     Learning iteration 1342/50000                      

                       Computation: 107524 steps/s (collection: 0.791s, learning 0.124s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.84
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6771
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.3183
   Mean episode rew_dof_pos_limits: -0.0390
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1508
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.3761
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0880
        Mean episode terrain_level: 0.1162
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.91s
                        Total time: 1100.98s
                               ETA: 664 mins 49.4 s

################################################################################
                     Learning iteration 1343/50000                      

                       Computation: 113458 steps/s (collection: 0.743s, learning 0.124s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.84
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5285
       Mean episode rew_ang_vel_xy: -0.0864
          Mean episode rew_dof_acc: -0.3551
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1564
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -3.8256
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.0958
        Mean episode terrain_level: 0.1136
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.87s
                        Total time: 1101.85s
                               ETA: 664 mins 50.3 s

################################################################################
                     Learning iteration 1344/50000                      

                       Computation: 120702 steps/s (collection: 0.693s, learning 0.121s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.85
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0929
       Mean episode rew_ang_vel_xy: -0.0808
          Mean episode rew_dof_acc: -0.3135
   Mean episode rew_dof_pos_limits: -0.0432
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1530
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.6240
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1046
        Mean episode terrain_level: 0.1142
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.81s
                        Total time: 1102.66s
                               ETA: 664 mins 49.3 s

################################################################################
                     Learning iteration 1345/50000                      

                       Computation: 123569 steps/s (collection: 0.673s, learning 0.122s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.85
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1254
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.3239
   Mean episode rew_dof_pos_limits: -0.0410
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1470
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.5884
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0999
        Mean episode terrain_level: 0.1137
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.80s
                        Total time: 1103.46s
                               ETA: 664 mins 47.6 s

################################################################################
                     Learning iteration 1346/50000                      

                       Computation: 114044 steps/s (collection: 0.724s, learning 0.138s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.85
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.1247
       Mean episode rew_ang_vel_xy: -0.0739
          Mean episode rew_dof_acc: -0.2949
   Mean episode rew_dof_pos_limits: -0.0309
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -3.0293
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0728
        Mean episode terrain_level: 0.1142
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.86s
                        Total time: 1104.32s
                               ETA: 664 mins 48.3 s

################################################################################
                     Learning iteration 1347/50000                      

                       Computation: 127082 steps/s (collection: 0.651s, learning 0.123s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.86
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5271
       Mean episode rew_ang_vel_xy: -0.0797
          Mean episode rew_dof_acc: -0.3133
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1570
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.2273
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0738
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.77s
                        Total time: 1105.09s
                               ETA: 664 mins 45.8 s

################################################################################
                     Learning iteration 1348/50000                      

                       Computation: 115113 steps/s (collection: 0.731s, learning 0.123s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.86
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5831
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3118
   Mean episode rew_dof_pos_limits: -0.0329
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1536
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.2514
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0780
        Mean episode terrain_level: 0.1213
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.85s
                        Total time: 1105.95s
                               ETA: 664 mins 46.2 s

################################################################################
                     Learning iteration 1349/50000                      

                       Computation: 108009 steps/s (collection: 0.789s, learning 0.121s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.86
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5134
       Mean episode rew_ang_vel_xy: -0.0778
          Mean episode rew_dof_acc: -0.3001
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.2224
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.1199
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.91s
                        Total time: 1106.86s
                               ETA: 664 mins 48.7 s

################################################################################
                     Learning iteration 1350/50000                      

                       Computation: 118305 steps/s (collection: 0.707s, learning 0.124s)
               Value function loss: 0.1226
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.86
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8078
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3098
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1466
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.4039
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0864
        Mean episode terrain_level: 0.1189
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.83s
                        Total time: 1107.69s
                               ETA: 664 mins 48.2 s

################################################################################
                     Learning iteration 1351/50000                      

                       Computation: 119422 steps/s (collection: 0.701s, learning 0.122s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.86
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8715
       Mean episode rew_ang_vel_xy: -0.0808
          Mean episode rew_dof_acc: -0.3114
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.4405
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0847
        Mean episode terrain_level: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.82s
                        Total time: 1108.51s
                               ETA: 664 mins 47.5 s

################################################################################
                     Learning iteration 1352/50000                      

                       Computation: 117899 steps/s (collection: 0.698s, learning 0.136s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.87
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8032
       Mean episode rew_ang_vel_xy: -0.0804
          Mean episode rew_dof_acc: -0.3201
   Mean episode rew_dof_pos_limits: -0.0391
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1559
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.4335
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0902
        Mean episode terrain_level: 0.1192
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.83s
                        Total time: 1109.34s
                               ETA: 664 mins 47.2 s

################################################################################
                     Learning iteration 1353/50000                      

                       Computation: 118756 steps/s (collection: 0.706s, learning 0.122s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.87
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9362
       Mean episode rew_ang_vel_xy: -0.0782
          Mean episode rew_dof_acc: -0.3088
   Mean episode rew_dof_pos_limits: -0.0375
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.4162
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1205
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.83s
                        Total time: 1110.17s
                               ETA: 664 mins 46.7 s

################################################################################
                     Learning iteration 1354/50000                      

                       Computation: 104710 steps/s (collection: 0.817s, learning 0.122s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.87
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8292
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3142
   Mean episode rew_dof_pos_limits: -0.0359
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.4129
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0888
        Mean episode terrain_level: 0.1204
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.94s
                        Total time: 1111.11s
                               ETA: 664 mins 50.1 s

################################################################################
                     Learning iteration 1355/50000                      

                       Computation: 128019 steps/s (collection: 0.646s, learning 0.122s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.87
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7088
       Mean episode rew_ang_vel_xy: -0.0751
          Mean episode rew_dof_acc: -0.3138
   Mean episode rew_dof_pos_limits: -0.0354
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1487
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.3548
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0911
        Mean episode terrain_level: 0.1195
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.77s
                        Total time: 1111.88s
                               ETA: 664 mins 47.4 s

################################################################################
                     Learning iteration 1356/50000                      

                       Computation: 119830 steps/s (collection: 0.697s, learning 0.123s)
               Value function loss: 0.1329
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9532
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.3242
   Mean episode rew_dof_pos_limits: -0.0380
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1527
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.4505
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0883
        Mean episode terrain_level: 0.1190
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.82s
                        Total time: 1112.70s
                               ETA: 664 mins 46.6 s

################################################################################
                     Learning iteration 1357/50000                      

                       Computation: 118536 steps/s (collection: 0.705s, learning 0.124s)
               Value function loss: 0.1325
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.5512
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.3003
   Mean episode rew_dof_pos_limits: -0.0331
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -3.2458
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0774
        Mean episode terrain_level: 0.1243
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.83s
                        Total time: 1113.53s
                               ETA: 664 mins 46.1 s

################################################################################
                     Learning iteration 1358/50000                      

                       Computation: 116823 steps/s (collection: 0.718s, learning 0.123s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6323
       Mean episode rew_ang_vel_xy: -0.0774
          Mean episode rew_dof_acc: -0.2987
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.3034
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0902
        Mean episode terrain_level: 0.1231
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.84s
                        Total time: 1114.37s
                               ETA: 664 mins 46.1 s

################################################################################
                     Learning iteration 1359/50000                      

                       Computation: 126824 steps/s (collection: 0.651s, learning 0.124s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0990
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3172
   Mean episode rew_dof_pos_limits: -0.0383
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.5471
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0912
        Mean episode terrain_level: 0.1221
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.78s
                        Total time: 1115.15s
                               ETA: 664 mins 43.7 s

################################################################################
                     Learning iteration 1360/50000                      

                       Computation: 127921 steps/s (collection: 0.647s, learning 0.122s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2206
       Mean episode rew_ang_vel_xy: -0.0858
          Mean episode rew_dof_acc: -0.3431
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.5963
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0839
        Mean episode terrain_level: 0.1252
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.77s
                        Total time: 1115.91s
                               ETA: 664 mins 41.0 s

################################################################################
                     Learning iteration 1361/50000                      

                       Computation: 113689 steps/s (collection: 0.741s, learning 0.124s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.88
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6524
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.3386
   Mean episode rew_dof_pos_limits: -0.0382
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.8361
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.1258
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.86s
                        Total time: 1116.78s
                               ETA: 664 mins 41.8 s

################################################################################
                     Learning iteration 1362/50000                      

                       Computation: 122322 steps/s (collection: 0.681s, learning 0.122s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.89
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9457
       Mean episode rew_ang_vel_xy: -0.0787
          Mean episode rew_dof_acc: -0.3236
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1502
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.4332
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0840
        Mean episode terrain_level: 0.1273
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.80s
                        Total time: 1117.58s
                               ETA: 664 mins 40.4 s

################################################################################
                     Learning iteration 1363/50000                      

                       Computation: 124569 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.89
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1388
       Mean episode rew_ang_vel_xy: -0.0821
          Mean episode rew_dof_acc: -0.3343
   Mean episode rew_dof_pos_limits: -0.0379
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1577
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.5507
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0900
        Mean episode terrain_level: 0.1257
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.79s
                        Total time: 1118.37s
                               ETA: 664 mins 38.5 s

################################################################################
                     Learning iteration 1364/50000                      

                       Computation: 128603 steps/s (collection: 0.643s, learning 0.121s)
               Value function loss: 0.1309
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.89
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1691
       Mean episode rew_ang_vel_xy: -0.0796
          Mean episode rew_dof_acc: -0.3214
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1484
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.5835
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.0914
        Mean episode terrain_level: 0.1251
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.76s
                        Total time: 1119.14s
                               ETA: 664 mins 35.7 s

################################################################################
                     Learning iteration 1365/50000                      

                       Computation: 110960 steps/s (collection: 0.763s, learning 0.123s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.90
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2883
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3246
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1532
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.6192
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0917
        Mean episode terrain_level: 0.1234
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.89s
                        Total time: 1120.02s
                               ETA: 664 mins 37.2 s

################################################################################
                     Learning iteration 1366/50000                      

                       Computation: 126907 steps/s (collection: 0.654s, learning 0.121s)
               Value function loss: 0.1321
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.90
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1162
       Mean episode rew_ang_vel_xy: -0.0809
          Mean episode rew_dof_acc: -0.3231
   Mean episode rew_dof_pos_limits: -0.0363
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1546
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.5048
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0930
        Mean episode terrain_level: 0.1246
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.77s
                        Total time: 1120.80s
                               ETA: 664 mins 34.8 s

################################################################################
                     Learning iteration 1367/50000                      

                       Computation: 114452 steps/s (collection: 0.738s, learning 0.121s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.90
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4184
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3343
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1544
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.6951
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0961
        Mean episode terrain_level: 0.1260
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.86s
                        Total time: 1121.66s
                               ETA: 664 mins 35.3 s

################################################################################
                     Learning iteration 1368/50000                      

                       Computation: 116047 steps/s (collection: 0.684s, learning 0.164s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.91
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.4341
       Mean episode rew_ang_vel_xy: -0.0752
          Mean episode rew_dof_acc: -0.2948
   Mean episode rew_dof_pos_limits: -0.0327
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -3.1805
      Mean episode rew_stand_still: -0.0031
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0137
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0790
        Mean episode terrain_level: 0.1277
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.85s
                        Total time: 1122.50s
                               ETA: 664 mins 35.5 s

################################################################################
                     Learning iteration 1369/50000                      

                       Computation: 122293 steps/s (collection: 0.666s, learning 0.138s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.91
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6276
       Mean episode rew_ang_vel_xy: -0.0882
          Mean episode rew_dof_acc: -0.3507
   Mean episode rew_dof_pos_limits: -0.0379
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1573
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.7813
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.1291
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.80s
                        Total time: 1123.31s
                               ETA: 664 mins 34.1 s

################################################################################
                     Learning iteration 1370/50000                      

                       Computation: 122677 steps/s (collection: 0.669s, learning 0.132s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.91
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2384
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3137
   Mean episode rew_dof_pos_limits: -0.0382
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1461
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.5717
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0913
        Mean episode terrain_level: 0.1277
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.80s
                        Total time: 1124.11s
                               ETA: 664 mins 32.6 s

################################################################################
                     Learning iteration 1371/50000                      

                       Computation: 123353 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.92
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.6055
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.2912
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -3.2415
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0789
        Mean episode terrain_level: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.80s
                        Total time: 1124.90s
                               ETA: 664 mins 31.0 s

################################################################################
                     Learning iteration 1372/50000                      

                       Computation: 114110 steps/s (collection: 0.721s, learning 0.141s)
               Value function loss: 0.1255
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.92
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5328
       Mean episode rew_ang_vel_xy: -0.0830
          Mean episode rew_dof_acc: -0.3376
   Mean episode rew_dof_pos_limits: -0.0415
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1546
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.7467
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.1056
        Mean episode terrain_level: 0.1264
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.86s
                        Total time: 1125.77s
                               ETA: 664 mins 31.6 s

################################################################################
                     Learning iteration 1373/50000                      

                       Computation: 118940 steps/s (collection: 0.700s, learning 0.126s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.92
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1786
       Mean episode rew_ang_vel_xy: -0.0778
          Mean episode rew_dof_acc: -0.3036
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.5198
      Mean episode rew_stand_still: -0.0059
      Mean episode rew_termination: -0.1923
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0247
 Mean episode rew_tracking_lin_vel: 0.1050
        Mean episode terrain_level: 0.1266
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.83s
                        Total time: 1126.59s
                               ETA: 664 mins 31.0 s

################################################################################
                     Learning iteration 1374/50000                      

                       Computation: 127041 steps/s (collection: 0.651s, learning 0.123s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.92
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0671
       Mean episode rew_ang_vel_xy: -0.0786
          Mean episode rew_dof_acc: -0.3060
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.4752
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.0893
        Mean episode terrain_level: 0.1258
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.77s
                        Total time: 1127.37s
                               ETA: 664 mins 28.6 s

################################################################################
                     Learning iteration 1375/50000                      

                       Computation: 115163 steps/s (collection: 0.720s, learning 0.133s)
               Value function loss: 0.1270
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.93
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 197.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7874
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3401
   Mean episode rew_dof_pos_limits: -0.0439
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1533
           Mean episode rew_no_fly: 0.0160
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -3.8866
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1920
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1082
        Mean episode terrain_level: 0.1225
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.85s
                        Total time: 1128.22s
                               ETA: 664 mins 29.0 s

################################################################################
                     Learning iteration 1376/50000                      

                       Computation: 121247 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.93
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5024
       Mean episode rew_ang_vel_xy: -0.0822
          Mean episode rew_dof_acc: -0.3227
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1461
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.7054
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.0960
        Mean episode terrain_level: 0.1179
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.81s
                        Total time: 1129.03s
                               ETA: 664 mins 27.8 s

################################################################################
                     Learning iteration 1377/50000                      

                       Computation: 119514 steps/s (collection: 0.700s, learning 0.123s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.93
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8508
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.2951
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1381
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.3797
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0954
        Mean episode terrain_level: 0.1172
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.82s
                        Total time: 1129.85s
                               ETA: 664 mins 27.1 s

################################################################################
                     Learning iteration 1378/50000                      

                       Computation: 126406 steps/s (collection: 0.656s, learning 0.122s)
               Value function loss: 0.1244
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1649
       Mean episode rew_ang_vel_xy: -0.0804
          Mean episode rew_dof_acc: -0.3119
   Mean episode rew_dof_pos_limits: -0.0346
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.5029
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0837
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.78s
                        Total time: 1130.63s
                               ETA: 664 mins 24.8 s

################################################################################
                     Learning iteration 1379/50000                      

                       Computation: 119215 steps/s (collection: 0.695s, learning 0.129s)
               Value function loss: 0.1196
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1364
       Mean episode rew_ang_vel_xy: -0.0813
          Mean episode rew_dof_acc: -0.3147
   Mean episode rew_dof_pos_limits: -0.0310
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -3.4471
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1964
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0776
        Mean episode terrain_level: 0.1165
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.82s
                        Total time: 1131.46s
                               ETA: 664 mins 24.1 s

################################################################################
                     Learning iteration 1380/50000                      

                       Computation: 115534 steps/s (collection: 0.728s, learning 0.123s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5441
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.3284
   Mean episode rew_dof_pos_limits: -0.0401
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.7283
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0938
        Mean episode terrain_level: 0.1191
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.85s
                        Total time: 1132.31s
                               ETA: 664 mins 24.4 s

################################################################################
                     Learning iteration 1381/50000                      

                       Computation: 119837 steps/s (collection: 0.691s, learning 0.130s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1885
       Mean episode rew_ang_vel_xy: -0.0803
          Mean episode rew_dof_acc: -0.3088
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.5022
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0916
        Mean episode terrain_level: 0.1209
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.82s
                        Total time: 1133.13s
                               ETA: 664 mins 23.6 s

################################################################################
                     Learning iteration 1382/50000                      

                       Computation: 124284 steps/s (collection: 0.667s, learning 0.124s)
               Value function loss: 0.1190
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8889
       Mean episode rew_ang_vel_xy: -0.0770
          Mean episode rew_dof_acc: -0.2982
   Mean episode rew_dof_pos_limits: -0.0355
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.3700
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0841
        Mean episode terrain_level: 0.1200
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.79s
                        Total time: 1133.92s
                               ETA: 664 mins 21.8 s

################################################################################
                     Learning iteration 1383/50000                      

                       Computation: 117016 steps/s (collection: 0.718s, learning 0.123s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.94
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1947
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.3108
   Mean episode rew_dof_pos_limits: -0.0424
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1470
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.5866
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1914
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1032
        Mean episode terrain_level: 0.1178
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.84s
                        Total time: 1134.76s
                               ETA: 664 mins 21.6 s

################################################################################
                     Learning iteration 1384/50000                      

                       Computation: 120956 steps/s (collection: 0.689s, learning 0.123s)
               Value function loss: 0.1338
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.95
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8843
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3328
   Mean episode rew_dof_pos_limits: -0.0476
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1514
           Mean episode rew_no_fly: 0.0165
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -3.9434
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1914
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0256
 Mean episode rew_tracking_lin_vel: 0.1153
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.81s
                        Total time: 1135.57s
                               ETA: 664 mins 20.6 s

################################################################################
                     Learning iteration 1385/50000                      

                       Computation: 123454 steps/s (collection: 0.673s, learning 0.123s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.95
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4278
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3237
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1432
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.6548
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0909
        Mean episode terrain_level: 0.1139
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.80s
                        Total time: 1136.37s
                               ETA: 664 mins 18.9 s

################################################################################
                     Learning iteration 1386/50000                      

                       Computation: 123672 steps/s (collection: 0.656s, learning 0.138s)
               Value function loss: 0.1270
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.95
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7146
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3183
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1477
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.7982
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0977
        Mean episode terrain_level: 0.1142
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.79s
                        Total time: 1137.16s
                               ETA: 664 mins 17.2 s

################################################################################
                     Learning iteration 1387/50000                      

                       Computation: 114147 steps/s (collection: 0.738s, learning 0.123s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.95
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4390
       Mean episode rew_ang_vel_xy: -0.0803
          Mean episode rew_dof_acc: -0.3149
   Mean episode rew_dof_pos_limits: -0.0427
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.6763
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1040
        Mean episode terrain_level: 0.1154
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.86s
                        Total time: 1138.02s
                               ETA: 664 mins 17.8 s

################################################################################
                     Learning iteration 1388/50000                      

                       Computation: 123174 steps/s (collection: 0.677s, learning 0.121s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.95
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5155
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3102
   Mean episode rew_dof_pos_limits: -0.0461
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1568
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.6958
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1084
        Mean episode terrain_level: 0.1168
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.80s
                        Total time: 1138.82s
                               ETA: 664 mins 16.3 s

################################################################################
                     Learning iteration 1389/50000                      

                       Computation: 119225 steps/s (collection: 0.701s, learning 0.123s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.96
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5319
       Mean episode rew_ang_vel_xy: -0.0830
          Mean episode rew_dof_acc: -0.3312
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.6920
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0227
 Mean episode rew_tracking_lin_vel: 0.0894
        Mean episode terrain_level: 0.1188
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.82s
                        Total time: 1139.65s
                               ETA: 664 mins 15.6 s

################################################################################
                     Learning iteration 1390/50000                      

                       Computation: 122118 steps/s (collection: 0.679s, learning 0.126s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.96
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2199
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3176
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1533
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.4913
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0783
        Mean episode terrain_level: 0.1177
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.80s
                        Total time: 1140.45s
                               ETA: 664 mins 14.3 s

################################################################################
                     Learning iteration 1391/50000                      

                       Computation: 119682 steps/s (collection: 0.681s, learning 0.141s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.96
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9236
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.3002
   Mean episode rew_dof_pos_limits: -0.0322
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1404
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -3.3916
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0795
        Mean episode terrain_level: 0.1170
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.82s
                        Total time: 1141.27s
                               ETA: 664 mins 13.5 s

################################################################################
                     Learning iteration 1392/50000                      

                       Computation: 127144 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.1328
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.96
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0747
       Mean episode rew_ang_vel_xy: -0.0786
          Mean episode rew_dof_acc: -0.3049
   Mean episode rew_dof_pos_limits: -0.0404
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.4709
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0968
        Mean episode terrain_level: 0.1170
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.77s
                        Total time: 1142.04s
                               ETA: 664 mins 11.1 s

################################################################################
                     Learning iteration 1393/50000                      

                       Computation: 123994 steps/s (collection: 0.670s, learning 0.123s)
               Value function loss: 0.1304
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.96
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4417
       Mean episode rew_ang_vel_xy: -0.0811
          Mean episode rew_dof_acc: -0.3127
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1489
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.6194
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0852
        Mean episode terrain_level: 0.1158
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.79s
                        Total time: 1142.84s
                               ETA: 664 mins 9.3 s

################################################################################
                     Learning iteration 1394/50000                      

                       Computation: 116868 steps/s (collection: 0.717s, learning 0.124s)
               Value function loss: 0.1239
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.97
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8853
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.3152
   Mean episode rew_dof_pos_limits: -0.0333
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1473
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -3.3638
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0791
        Mean episode terrain_level: 0.1141
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.84s
                        Total time: 1143.68s
                               ETA: 664 mins 9.2 s

################################################################################
                     Learning iteration 1395/50000                      

                       Computation: 125259 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.97
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0521
       Mean episode rew_ang_vel_xy: -0.0845
          Mean episode rew_dof_acc: -0.3438
   Mean episode rew_dof_pos_limits: -0.0427
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1510
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -3.9547
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1910
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1019
        Mean episode terrain_level: 0.1124
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.78s
                        Total time: 1144.46s
                               ETA: 664 mins 7.2 s

################################################################################
                     Learning iteration 1396/50000                      

                       Computation: 127337 steps/s (collection: 0.649s, learning 0.123s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.97
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2962
       Mean episode rew_ang_vel_xy: -0.0809
          Mean episode rew_dof_acc: -0.3197
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.5660
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0852
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.77s
                        Total time: 1145.24s
                               ETA: 664 mins 4.7 s

################################################################################
                     Learning iteration 1397/50000                      

                       Computation: 110363 steps/s (collection: 0.750s, learning 0.141s)
               Value function loss: 0.1282
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.98
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.3346
       Mean episode rew_ang_vel_xy: -0.0806
          Mean episode rew_dof_acc: -0.3078
   Mean episode rew_dof_pos_limits: -0.0393
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.5540
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.1010
        Mean episode terrain_level: 0.1127
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.89s
                        Total time: 1146.13s
                               ETA: 664 mins 6.3 s

################################################################################
                     Learning iteration 1398/50000                      

                       Computation: 109594 steps/s (collection: 0.774s, learning 0.123s)
               Value function loss: 0.1230
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.98
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.8159
       Mean episode rew_ang_vel_xy: -0.0780
          Mean episode rew_dof_acc: -0.2976
   Mean episode rew_dof_pos_limits: -0.0350
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -3.3043
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1961
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0804
        Mean episode terrain_level: 0.1109
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.90s
                        Total time: 1147.02s
                               ETA: 664 mins 8.2 s

################################################################################
                     Learning iteration 1399/50000                      

                       Computation: 127009 steps/s (collection: 0.648s, learning 0.126s)
               Value function loss: 0.1248
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.98
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4497
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.3001
   Mean episode rew_dof_pos_limits: -0.0375
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.6234
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1120
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.77s
                        Total time: 1147.80s
                               ETA: 664 mins 5.8 s

################################################################################
                     Learning iteration 1400/50000                      

                       Computation: 111691 steps/s (collection: 0.742s, learning 0.138s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.98
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9166
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3042
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -3.3705
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0839
        Mean episode terrain_level: 0.1128
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.88s
                        Total time: 1148.68s
                               ETA: 664 mins 7.1 s

################################################################################
                     Learning iteration 1401/50000                      

                       Computation: 113930 steps/s (collection: 0.700s, learning 0.163s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.99
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6188
       Mean episode rew_ang_vel_xy: -0.0818
          Mean episode rew_dof_acc: -0.3322
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1587
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.7315
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1928
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.0991
        Mean episode terrain_level: 0.1129
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.86s
                        Total time: 1149.54s
                               ETA: 664 mins 7.7 s

################################################################################
                     Learning iteration 1402/50000                      

                       Computation: 117496 steps/s (collection: 0.710s, learning 0.126s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.99
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3722
       Mean episode rew_ang_vel_xy: -0.0839
          Mean episode rew_dof_acc: -0.3296
   Mean episode rew_dof_pos_limits: -0.0433
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.0608
      Mean episode rew_stand_still: -0.0059
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1117
        Mean episode terrain_level: 0.1143
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.84s
                        Total time: 1150.38s
                               ETA: 664 mins 7.5 s

################################################################################
                     Learning iteration 1403/50000                      

                       Computation: 124960 steps/s (collection: 0.657s, learning 0.129s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.99
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6873
       Mean episode rew_ang_vel_xy: -0.0818
          Mean episode rew_dof_acc: -0.3155
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1454
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.7293
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1919
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0957
        Mean episode terrain_level: 0.1135
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.79s
                        Total time: 1151.16s
                               ETA: 664 mins 5.5 s

################################################################################
                     Learning iteration 1404/50000                      

                       Computation: 118705 steps/s (collection: 0.677s, learning 0.151s)
               Value function loss: 0.1319
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.00
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6130
       Mean episode rew_ang_vel_xy: -0.0801
          Mean episode rew_dof_acc: -0.3212
   Mean episode rew_dof_pos_limits: -0.0423
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1588
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.7377
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.1025
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.83s
                        Total time: 1151.99s
                               ETA: 664 mins 5.0 s

################################################################################
                     Learning iteration 1405/50000                      

                       Computation: 124769 steps/s (collection: 0.663s, learning 0.125s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.00
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5814
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3083
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.6901
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0991
        Mean episode terrain_level: 0.1101
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.79s
                        Total time: 1152.78s
                               ETA: 664 mins 3.0 s

################################################################################
                     Learning iteration 1406/50000                      

                       Computation: 126505 steps/s (collection: 0.654s, learning 0.123s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.00
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5117
       Mean episode rew_ang_vel_xy: -0.0810
          Mean episode rew_dof_acc: -0.3022
   Mean episode rew_dof_pos_limits: -0.0438
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.6556
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0999
        Mean episode terrain_level: 0.1107
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.78s
                        Total time: 1153.56s
                               ETA: 664 mins 0.7 s

################################################################################
                     Learning iteration 1407/50000                      

                       Computation: 123125 steps/s (collection: 0.677s, learning 0.121s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.00
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.7571
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2753
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.2897
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0896
        Mean episode terrain_level: 0.1110
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.80s
                        Total time: 1154.36s
                               ETA: 663 mins 59.2 s

################################################################################
                     Learning iteration 1408/50000                      

                       Computation: 103794 steps/s (collection: 0.790s, learning 0.157s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.01
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4485
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.3240
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1539
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.5872
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0806
        Mean episode terrain_level: 0.1111
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.95s
                        Total time: 1155.30s
                               ETA: 664 mins 2.8 s

################################################################################
                     Learning iteration 1409/50000                      

                       Computation: 124865 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.1354
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.01
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4609
       Mean episode rew_ang_vel_xy: -0.0809
          Mean episode rew_dof_acc: -0.3188
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.6320
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0965
        Mean episode terrain_level: 0.1134
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.79s
                        Total time: 1156.09s
                               ETA: 664 mins 0.8 s

################################################################################
                     Learning iteration 1410/50000                      

                       Computation: 126229 steps/s (collection: 0.657s, learning 0.122s)
               Value function loss: 0.1331
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.01
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8134
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3247
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1480
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.7477
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.1011
        Mean episode terrain_level: 0.1111
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.78s
                        Total time: 1156.87s
                               ETA: 663 mins 58.6 s

################################################################################
                     Learning iteration 1411/50000                      

                       Computation: 128645 steps/s (collection: 0.643s, learning 0.121s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.01
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5120
       Mean episode rew_ang_vel_xy: -0.0804
          Mean episode rew_dof_acc: -0.2991
   Mean episode rew_dof_pos_limits: -0.0402
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.6474
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.1095
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.76s
                        Total time: 1157.63s
                               ETA: 663 mins 55.8 s

################################################################################
                     Learning iteration 1412/50000                      

                       Computation: 113501 steps/s (collection: 0.743s, learning 0.123s)
               Value function loss: 0.1327
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.02
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.3623
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3082
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.5780
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.1098
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.87s
                        Total time: 1158.50s
                               ETA: 663 mins 56.6 s

################################################################################
                     Learning iteration 1413/50000                      

                       Computation: 111515 steps/s (collection: 0.748s, learning 0.133s)
               Value function loss: 0.1323
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.02
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5776
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.3177
   Mean episode rew_dof_pos_limits: -0.0361
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1473
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.6268
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0869
        Mean episode terrain_level: 0.1103
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.88s
                        Total time: 1159.38s
                               ETA: 663 mins 57.9 s

################################################################################
                     Learning iteration 1414/50000                      

                       Computation: 116642 steps/s (collection: 0.718s, learning 0.125s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.02
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1859
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3052
   Mean episode rew_dof_pos_limits: -0.0354
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1510
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.4571
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0820
        Mean episode terrain_level: 0.1109
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.84s
                        Total time: 1160.22s
                               ETA: 663 mins 57.9 s

################################################################################
                     Learning iteration 1415/50000                      

                       Computation: 119347 steps/s (collection: 0.700s, learning 0.123s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.02
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8547
       Mean episode rew_ang_vel_xy: -0.0797
          Mean episode rew_dof_acc: -0.3154
   Mean episode rew_dof_pos_limits: -0.0439
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1431
           Mean episode rew_no_fly: 0.0160
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.8253
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1923
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.0966
        Mean episode terrain_level: 0.1096
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.82s
                        Total time: 1161.05s
                               ETA: 663 mins 57.2 s

################################################################################
                     Learning iteration 1416/50000                      

                       Computation: 120852 steps/s (collection: 0.690s, learning 0.123s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.02
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0923
       Mean episode rew_ang_vel_xy: -0.0833
          Mean episode rew_dof_acc: -0.3234
   Mean episode rew_dof_pos_limits: -0.0390
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1556
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.9141
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0996
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.81s
                        Total time: 1161.86s
                               ETA: 663 mins 56.1 s

################################################################################
                     Learning iteration 1417/50000                      

                       Computation: 128388 steps/s (collection: 0.643s, learning 0.122s)
               Value function loss: 0.1365
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.03
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6364
       Mean episode rew_ang_vel_xy: -0.0826
          Mean episode rew_dof_acc: -0.3184
   Mean episode rew_dof_pos_limits: -0.0368
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1567
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.7074
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0889
        Mean episode terrain_level: 0.1095
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.77s
                        Total time: 1162.63s
                               ETA: 663 mins 53.5 s

################################################################################
                     Learning iteration 1418/50000                      

                       Computation: 116151 steps/s (collection: 0.725s, learning 0.121s)
               Value function loss: 0.1319
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.03
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4162
       Mean episode rew_ang_vel_xy: -0.0805
          Mean episode rew_dof_acc: -0.3112
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.5989
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0929
        Mean episode terrain_level: 0.1083
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.85s
                        Total time: 1163.47s
                               ETA: 663 mins 53.5 s

################################################################################
                     Learning iteration 1419/50000                      

                       Computation: 106554 steps/s (collection: 0.776s, learning 0.146s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.03
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4540
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.3042
   Mean episode rew_dof_pos_limits: -0.0345
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1467
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.5940
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0854
        Mean episode terrain_level: 0.1082
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.92s
                        Total time: 1164.39s
                               ETA: 663 mins 56.2 s

################################################################################
                     Learning iteration 1420/50000                      

                       Computation: 112753 steps/s (collection: 0.748s, learning 0.124s)
               Value function loss: 0.1317
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.04
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4628
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.3149
   Mean episode rew_dof_pos_limits: -0.0326
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1582
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -3.5665
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0808
        Mean episode terrain_level: 0.1092
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.87s
                        Total time: 1165.27s
                               ETA: 663 mins 57.2 s

################################################################################
                     Learning iteration 1421/50000                      

                       Computation: 122346 steps/s (collection: 0.680s, learning 0.124s)
               Value function loss: 0.1235
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.04
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5155
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.2982
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -3.6360
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.1104
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.80s
                        Total time: 1166.07s
                               ETA: 663 mins 55.8 s

################################################################################
                     Learning iteration 1422/50000                      

                       Computation: 126872 steps/s (collection: 0.652s, learning 0.123s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.04
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6935
       Mean episode rew_ang_vel_xy: -0.0821
          Mean episode rew_dof_acc: -0.3211
   Mean episode rew_dof_pos_limits: -0.0409
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.7318
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0976
        Mean episode terrain_level: 0.1116
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.77s
                        Total time: 1166.84s
                               ETA: 663 mins 53.4 s

################################################################################
                     Learning iteration 1423/50000                      

                       Computation: 123701 steps/s (collection: 0.670s, learning 0.124s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.04
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.3600
       Mean episode rew_ang_vel_xy: -0.0810
          Mean episode rew_dof_acc: -0.3003
   Mean episode rew_dof_pos_limits: -0.0341
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1563
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -3.5209
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0884
        Mean episode terrain_level: 0.1120
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.79s
                        Total time: 1167.64s
                               ETA: 663 mins 51.8 s

################################################################################
                     Learning iteration 1424/50000                      

                       Computation: 118913 steps/s (collection: 0.695s, learning 0.132s)
               Value function loss: 0.1264
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.04
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4485
       Mean episode rew_ang_vel_xy: -0.0785
          Mean episode rew_dof_acc: -0.2955
   Mean episode rew_dof_pos_limits: -0.0357
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.6059
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0979
        Mean episode terrain_level: 0.1111
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.83s
                        Total time: 1168.47s
                               ETA: 663 mins 51.2 s

################################################################################
                     Learning iteration 1425/50000                      

                       Computation: 111180 steps/s (collection: 0.749s, learning 0.135s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.05
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1437
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.2900
   Mean episode rew_dof_pos_limits: -0.0367
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1519
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -3.4352
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0877
        Mean episode terrain_level: 0.1101
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.88s
                        Total time: 1169.35s
                               ETA: 663 mins 52.5 s

################################################################################
                     Learning iteration 1426/50000                      

                       Computation: 116863 steps/s (collection: 0.692s, learning 0.150s)
               Value function loss: 0.1255
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.05
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -6.9179
       Mean episode rew_ang_vel_xy: -0.0764
          Mean episode rew_dof_acc: -0.2860
   Mean episode rew_dof_pos_limits: -0.0323
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1518
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -3.3229
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0798
        Mean episode terrain_level: 0.1121
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.84s
                        Total time: 1170.19s
                               ETA: 663 mins 52.4 s

################################################################################
                     Learning iteration 1427/50000                      

                       Computation: 119400 steps/s (collection: 0.701s, learning 0.123s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.05
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5765
       Mean episode rew_ang_vel_xy: -0.0794
          Mean episode rew_dof_acc: -0.2994
   Mean episode rew_dof_pos_limits: -0.0369
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.6773
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0925
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.82s
                        Total time: 1171.01s
                               ETA: 663 mins 51.7 s

################################################################################
                     Learning iteration 1428/50000                      

                       Computation: 122757 steps/s (collection: 0.675s, learning 0.126s)
               Value function loss: 0.1223
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.06
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5502
       Mean episode rew_ang_vel_xy: -0.0806
          Mean episode rew_dof_acc: -0.3059
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1474
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.6475
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.1169
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.80s
                        Total time: 1171.82s
                               ETA: 663 mins 50.2 s

################################################################################
                     Learning iteration 1429/50000                      

                       Computation: 114706 steps/s (collection: 0.725s, learning 0.132s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.06
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.3003
       Mean episode rew_ang_vel_xy: -0.0773
          Mean episode rew_dof_acc: -0.2963
   Mean episode rew_dof_pos_limits: -0.0376
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.5927
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.1147
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.86s
                        Total time: 1172.67s
                               ETA: 663 mins 50.7 s

################################################################################
                     Learning iteration 1430/50000                      

                       Computation: 124456 steps/s (collection: 0.667s, learning 0.122s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.06
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2066
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.3044
   Mean episode rew_dof_pos_limits: -0.0325
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -3.4885
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0144
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0787
        Mean episode terrain_level: 0.1145
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.79s
                        Total time: 1173.46s
                               ETA: 663 mins 48.8 s

################################################################################
                     Learning iteration 1431/50000                      

                       Computation: 127781 steps/s (collection: 0.647s, learning 0.122s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.06
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.9920
       Mean episode rew_ang_vel_xy: -0.0823
          Mean episode rew_dof_acc: -0.3232
   Mean episode rew_dof_pos_limits: -0.0430
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1543
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.9201
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1073
        Mean episode terrain_level: 0.1167
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.77s
                        Total time: 1174.23s
                               ETA: 663 mins 46.3 s

################################################################################
                     Learning iteration 1432/50000                      

                       Computation: 121190 steps/s (collection: 0.688s, learning 0.124s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5655
       Mean episode rew_ang_vel_xy: -0.0813
          Mean episode rew_dof_acc: -0.3131
   Mean episode rew_dof_pos_limits: -0.0363
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.6733
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0835
        Mean episode terrain_level: 0.1193
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.81s
                        Total time: 1175.04s
                               ETA: 663 mins 45.2 s

################################################################################
                     Learning iteration 1433/50000                      

                       Computation: 127137 steps/s (collection: 0.651s, learning 0.122s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.07
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7259
       Mean episode rew_ang_vel_xy: -0.0808
          Mean episode rew_dof_acc: -0.3198
   Mean episode rew_dof_pos_limits: -0.0446
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1568
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -3.8158
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1913
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0253
 Mean episode rew_tracking_lin_vel: 0.1137
        Mean episode terrain_level: 0.1220
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.77s
                        Total time: 1175.82s
                               ETA: 663 mins 42.8 s

################################################################################
                     Learning iteration 1434/50000                      

                       Computation: 109499 steps/s (collection: 0.763s, learning 0.134s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.07
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.1918
       Mean episode rew_ang_vel_xy: -0.0763
          Mean episode rew_dof_acc: -0.2923
   Mean episode rew_dof_pos_limits: -0.0356
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.4691
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0912
        Mean episode terrain_level: 0.1218
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.90s
                        Total time: 1176.71s
                               ETA: 663 mins 44.6 s

################################################################################
                     Learning iteration 1435/50000                      

                       Computation: 123939 steps/s (collection: 0.652s, learning 0.141s)
               Value function loss: 0.1237
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.07
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8100
       Mean episode rew_ang_vel_xy: -0.0782
          Mean episode rew_dof_acc: -0.3024
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1458
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.7917
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.0974
        Mean episode terrain_level: 0.1206
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.79s
                        Total time: 1177.51s
                               ETA: 663 mins 42.9 s

################################################################################
                     Learning iteration 1436/50000                      

                       Computation: 119492 steps/s (collection: 0.701s, learning 0.122s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.07
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7976
       Mean episode rew_ang_vel_xy: -0.0822
          Mean episode rew_dof_acc: -0.3110
   Mean episode rew_dof_pos_limits: -0.0369
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1528
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.7782
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1220
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.82s
                        Total time: 1178.33s
                               ETA: 663 mins 42.1 s

################################################################################
                     Learning iteration 1437/50000                      

                       Computation: 117732 steps/s (collection: 0.702s, learning 0.133s)
               Value function loss: 0.1333
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.07
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.9053
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3133
   Mean episode rew_dof_pos_limits: -0.0382
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1455
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.8474
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0977
        Mean episode terrain_level: 0.1190
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.83s
                        Total time: 1179.16s
                               ETA: 663 mins 41.8 s

################################################################################
                     Learning iteration 1438/50000                      

                       Computation: 114096 steps/s (collection: 0.735s, learning 0.127s)
               Value function loss: 0.1221
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.08
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5001
       Mean episode rew_ang_vel_xy: -0.0794
          Mean episode rew_dof_acc: -0.3145
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.6535
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1966
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0823
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.86s
                        Total time: 1180.03s
                               ETA: 663 mins 42.4 s

################################################################################
                     Learning iteration 1439/50000                      

                       Computation: 123642 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.1207
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.08
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.6812
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.2964
   Mean episode rew_dof_pos_limits: -0.0394
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1530
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.7152
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0988
        Mean episode terrain_level: 0.1153
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.80s
                        Total time: 1180.82s
                               ETA: 663 mins 40.7 s

################################################################################
                     Learning iteration 1440/50000                      

                       Computation: 111319 steps/s (collection: 0.749s, learning 0.134s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.08
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.0440
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.2792
   Mean episode rew_dof_pos_limits: -0.0338
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1473
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -3.4133
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0811
        Mean episode terrain_level: 0.1139
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.88s
                        Total time: 1181.70s
                               ETA: 663 mins 42.0 s

################################################################################
                     Learning iteration 1441/50000                      

                       Computation: 109811 steps/s (collection: 0.758s, learning 0.137s)
               Value function loss: 0.1244
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.08
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5424
       Mean episode rew_ang_vel_xy: -0.0805
          Mean episode rew_dof_acc: -0.3062
   Mean episode rew_dof_pos_limits: -0.0364
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.6705
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0877
        Mean episode terrain_level: 0.1137
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.90s
                        Total time: 1182.60s
                               ETA: 663 mins 43.8 s

################################################################################
                     Learning iteration 1442/50000                      

                       Computation: 116972 steps/s (collection: 0.714s, learning 0.126s)
               Value function loss: 0.1288
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.9986
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3119
   Mean episode rew_dof_pos_limits: -0.0377
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.8655
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0871
        Mean episode terrain_level: 0.1163
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.84s
                        Total time: 1183.44s
                               ETA: 663 mins 43.6 s

################################################################################
                     Learning iteration 1443/50000                      

                       Computation: 125871 steps/s (collection: 0.660s, learning 0.121s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1305
       Mean episode rew_ang_vel_xy: -0.0853
          Mean episode rew_dof_acc: -0.3327
   Mean episode rew_dof_pos_limits: -0.0438
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -4.0448
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.1103
        Mean episode terrain_level: 0.1146
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.78s
                        Total time: 1184.22s
                               ETA: 663 mins 41.5 s

################################################################################
                     Learning iteration 1444/50000                      

                       Computation: 112949 steps/s (collection: 0.723s, learning 0.147s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2252
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3017
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1559
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.4976
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0145
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0796
        Mean episode terrain_level: 0.1131
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.87s
                        Total time: 1185.09s
                               ETA: 663 mins 42.3 s

################################################################################
                     Learning iteration 1445/50000                      

                       Computation: 120357 steps/s (collection: 0.693s, learning 0.123s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.4531
       Mean episode rew_ang_vel_xy: -0.0801
          Mean episode rew_dof_acc: -0.3034
   Mean episode rew_dof_pos_limits: -0.0354
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -3.6353
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.1108
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.82s
                        Total time: 1185.91s
                               ETA: 663 mins 41.4 s

################################################################################
                     Learning iteration 1446/50000                      

                       Computation: 116268 steps/s (collection: 0.723s, learning 0.123s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8666
       Mean episode rew_ang_vel_xy: -0.0815
          Mean episode rew_dof_acc: -0.3085
   Mean episode rew_dof_pos_limits: -0.0397
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.8330
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0935
        Mean episode terrain_level: 0.1101
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.85s
                        Total time: 1186.75s
                               ETA: 663 mins 41.4 s

################################################################################
                     Learning iteration 1447/50000                      

                       Computation: 110175 steps/s (collection: 0.770s, learning 0.123s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7105
       Mean episode rew_ang_vel_xy: -0.0799
          Mean episode rew_dof_acc: -0.3148
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.7398
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0971
        Mean episode terrain_level: 0.1140
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.89s
                        Total time: 1187.65s
                               ETA: 663 mins 43.0 s

################################################################################
                     Learning iteration 1448/50000                      

                       Computation: 116456 steps/s (collection: 0.704s, learning 0.141s)
               Value function loss: 0.1333
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.09
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5861
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.2895
   Mean episode rew_dof_pos_limits: -0.0434
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.6926
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1921
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1051
        Mean episode terrain_level: 0.1126
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.84s
                        Total time: 1188.49s
                               ETA: 663 mins 43.0 s

################################################################################
                     Learning iteration 1449/50000                      

                       Computation: 110110 steps/s (collection: 0.754s, learning 0.139s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.10
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.2691
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.2851
   Mean episode rew_dof_pos_limits: -0.0333
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -3.4970
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1971
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.1106
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.89s
                        Total time: 1189.38s
                               ETA: 663 mins 44.6 s

################################################################################
                     Learning iteration 1450/50000                      

                       Computation: 114512 steps/s (collection: 0.725s, learning 0.134s)
               Value function loss: 0.1323
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.10
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3631
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.3301
   Mean episode rew_dof_pos_limits: -0.0400
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.0996
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0168
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.0974
        Mean episode terrain_level: 0.1130
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.86s
                        Total time: 1190.24s
                               ETA: 663 mins 45.1 s

################################################################################
                     Learning iteration 1451/50000                      

                       Computation: 116448 steps/s (collection: 0.720s, learning 0.125s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.10
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.9773
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3179
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1522
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.9231
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0215
 Mean episode rew_tracking_lin_vel: 0.0928
        Mean episode terrain_level: 0.1117
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.84s
                        Total time: 1191.09s
                               ETA: 663 mins 45.1 s

################################################################################
                     Learning iteration 1452/50000                      

                       Computation: 118746 steps/s (collection: 0.706s, learning 0.121s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.11
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1404
       Mean episode rew_ang_vel_xy: -0.0834
          Mean episode rew_dof_acc: -0.3171
   Mean episode rew_dof_pos_limits: -0.0392
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -3.9426
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0897
        Mean episode terrain_level: 0.1116
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.83s
                        Total time: 1191.91s
                               ETA: 663 mins 44.5 s

################################################################################
                     Learning iteration 1453/50000                      

                       Computation: 113274 steps/s (collection: 0.727s, learning 0.141s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.11
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0371
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.3227
   Mean episode rew_dof_pos_limits: -0.0365
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.9264
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0875
        Mean episode terrain_level: 0.1126
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.87s
                        Total time: 1192.78s
                               ETA: 663 mins 45.3 s

################################################################################
                     Learning iteration 1454/50000                      

                       Computation: 125703 steps/s (collection: 0.642s, learning 0.140s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.11
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7921
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3133
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1558
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -3.8567
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0961
        Mean episode terrain_level: 0.1136
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.78s
                        Total time: 1193.56s
                               ETA: 663 mins 43.2 s

################################################################################
                     Learning iteration 1455/50000                      

                       Computation: 110797 steps/s (collection: 0.748s, learning 0.139s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.11
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7882
       Mean episode rew_ang_vel_xy: -0.0852
          Mean episode rew_dof_acc: -0.3401
   Mean episode rew_dof_pos_limits: -0.0428
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1512
           Mean episode rew_no_fly: 0.0161
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -4.3562
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1032
        Mean episode terrain_level: 0.1125
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.89s
                        Total time: 1194.45s
                               ETA: 663 mins 44.6 s

################################################################################
                     Learning iteration 1456/50000                      

                       Computation: 124774 steps/s (collection: 0.649s, learning 0.138s)
               Value function loss: 0.1313
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.12
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0270
       Mean episode rew_ang_vel_xy: -0.0829
          Mean episode rew_dof_acc: -0.3160
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.9248
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0833
        Mean episode terrain_level: 0.1119
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.79s
                        Total time: 1195.24s
                               ETA: 663 mins 42.7 s

################################################################################
                     Learning iteration 1457/50000                      

                       Computation: 115820 steps/s (collection: 0.721s, learning 0.128s)
               Value function loss: 0.1236
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.12
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7506
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3309
   Mean episode rew_dof_pos_limits: -0.0491
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0168
      Mean episode rew_orientation: -0.0113
       Mean episode rew_smoothness: -4.3711
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1914
          Mean episode rew_torques: -0.0186
 Mean episode rew_tracking_ang_vel: 0.0278
 Mean episode rew_tracking_lin_vel: 0.1179
        Mean episode terrain_level: 0.1113
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.85s
                        Total time: 1196.09s
                               ETA: 663 mins 42.8 s

################################################################################
                     Learning iteration 1458/50000                      

                       Computation: 103394 steps/s (collection: 0.811s, learning 0.140s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.12
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.5055
       Mean episode rew_ang_vel_xy: -0.0862
          Mean episode rew_dof_acc: -0.3297
   Mean episode rew_dof_pos_limits: -0.0467
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1556
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.2201
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1923
          Mean episode rew_torques: -0.0176
 Mean episode rew_tracking_ang_vel: 0.0265
 Mean episode rew_tracking_lin_vel: 0.1025
        Mean episode terrain_level: 0.1124
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.95s
                        Total time: 1197.04s
                               ETA: 663 mins 46.3 s

################################################################################
                     Learning iteration 1459/50000                      

                       Computation: 126618 steps/s (collection: 0.653s, learning 0.123s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.12
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.5784
       Mean episode rew_ang_vel_xy: -0.0799
          Mean episode rew_dof_acc: -0.3036
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -3.6563
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0828
        Mean episode terrain_level: 0.1135
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.78s
                        Total time: 1197.81s
                               ETA: 663 mins 44.0 s

################################################################################
                     Learning iteration 1460/50000                      

                       Computation: 109404 steps/s (collection: 0.775s, learning 0.123s)
               Value function loss: 0.1246
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2224
       Mean episode rew_ang_vel_xy: -0.0811
          Mean episode rew_dof_acc: -0.3141
   Mean episode rew_dof_pos_limits: -0.0400
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.0248
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.1128
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.90s
                        Total time: 1198.71s
                               ETA: 663 mins 45.8 s

################################################################################
                     Learning iteration 1461/50000                      

                       Computation: 117771 steps/s (collection: 0.694s, learning 0.140s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0547
       Mean episode rew_ang_vel_xy: -0.0848
          Mean episode rew_dof_acc: -0.3174
   Mean episode rew_dof_pos_limits: -0.0381
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1583
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.9496
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0923
        Mean episode terrain_level: 0.1093
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.83s
                        Total time: 1199.55s
                               ETA: 663 mins 45.5 s

################################################################################
                     Learning iteration 1462/50000                      

                       Computation: 124177 steps/s (collection: 0.670s, learning 0.121s)
               Value function loss: 0.1246
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.5415
       Mean episode rew_ang_vel_xy: -0.0847
          Mean episode rew_dof_acc: -0.3229
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.1500
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0227
 Mean episode rew_tracking_lin_vel: 0.1047
        Mean episode terrain_level: 0.1099
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.79s
                        Total time: 1200.34s
                               ETA: 663 mins 43.7 s

################################################################################
                     Learning iteration 1463/50000                      

                       Computation: 112027 steps/s (collection: 0.754s, learning 0.123s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7217
       Mean episode rew_ang_vel_xy: -0.0796
          Mean episode rew_dof_acc: -0.2962
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1559
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.8038
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0159
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0945
        Mean episode terrain_level: 0.1083
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.88s
                        Total time: 1201.22s
                               ETA: 663 mins 44.8 s

################################################################################
                     Learning iteration 1464/50000                      

                       Computation: 114487 steps/s (collection: 0.724s, learning 0.135s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2985
       Mean episode rew_ang_vel_xy: -0.0827
          Mean episode rew_dof_acc: -0.3281
   Mean episode rew_dof_pos_limits: -0.0367
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1584
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -4.0210
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0952
        Mean episode terrain_level: 0.1061
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.86s
                        Total time: 1202.08s
                               ETA: 663 mins 45.2 s

################################################################################
                     Learning iteration 1465/50000                      

                       Computation: 119622 steps/s (collection: 0.678s, learning 0.143s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.13
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9844
       Mean episode rew_ang_vel_xy: -0.0841
          Mean episode rew_dof_acc: -0.3445
   Mean episode rew_dof_pos_limits: -0.0409
      Mean episode rew_joint_power: -0.0074
        Mean episode rew_lin_vel_z: -0.1529
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0112
       Mean episode rew_smoothness: -4.4141
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0178
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1041
        Mean episode terrain_level: 0.1088
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.82s
                        Total time: 1202.90s
                               ETA: 663 mins 44.4 s

################################################################################
                     Learning iteration 1466/50000                      

                       Computation: 111505 steps/s (collection: 0.742s, learning 0.140s)
               Value function loss: 0.1282
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7092
       Mean episode rew_ang_vel_xy: -0.0835
          Mean episode rew_dof_acc: -0.3380
   Mean episode rew_dof_pos_limits: -0.0433
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1565
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0110
       Mean episode rew_smoothness: -4.2718
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0176
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.0975
        Mean episode terrain_level: 0.1091
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.88s
                        Total time: 1203.78s
                               ETA: 663 mins 45.6 s

################################################################################
                     Learning iteration 1467/50000                      

                       Computation: 123095 steps/s (collection: 0.656s, learning 0.142s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2780
       Mean episode rew_ang_vel_xy: -0.0801
          Mean episode rew_dof_acc: -0.3122
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1545
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -4.0501
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.1020
        Mean episode terrain_level: 0.1092
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.80s
                        Total time: 1204.58s
                               ETA: 663 mins 44.1 s

################################################################################
                     Learning iteration 1468/50000                      

                       Computation: 119790 steps/s (collection: 0.683s, learning 0.137s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1662
       Mean episode rew_ang_vel_xy: -0.0838
          Mean episode rew_dof_acc: -0.3279
   Mean episode rew_dof_pos_limits: -0.0360
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.9786
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0877
        Mean episode terrain_level: 0.1090
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.82s
                        Total time: 1205.40s
                               ETA: 663 mins 43.3 s

################################################################################
                     Learning iteration 1469/50000                      

                       Computation: 120888 steps/s (collection: 0.674s, learning 0.139s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2314
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3129
   Mean episode rew_dof_pos_limits: -0.0353
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -3.9684
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1967
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0870
        Mean episode terrain_level: 0.1106
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.81s
                        Total time: 1206.21s
                               ETA: 663 mins 42.2 s

################################################################################
                     Learning iteration 1470/50000                      

                       Computation: 123647 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.1367
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8970
       Mean episode rew_ang_vel_xy: -0.0780
          Mean episode rew_dof_acc: -0.3046
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1390
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -3.8395
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0155
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0916
        Mean episode terrain_level: 0.1104
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.80s
                        Total time: 1207.01s
                               ETA: 663 mins 40.5 s

################################################################################
                     Learning iteration 1471/50000                      

                       Computation: 122931 steps/s (collection: 0.678s, learning 0.122s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.14
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8702
       Mean episode rew_ang_vel_xy: -0.0789
          Mean episode rew_dof_acc: -0.3037
   Mean episode rew_dof_pos_limits: -0.0392
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1481
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -3.8564
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.1097
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.80s
                        Total time: 1207.81s
                               ETA: 663 mins 39.0 s

################################################################################
                     Learning iteration 1472/50000                      

                       Computation: 121504 steps/s (collection: 0.688s, learning 0.121s)
               Value function loss: 0.1264
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.15
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1263
       Mean episode rew_ang_vel_xy: -0.0798
          Mean episode rew_dof_acc: -0.3025
   Mean episode rew_dof_pos_limits: -0.0393
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -3.9671
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0957
        Mean episode terrain_level: 0.1113
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.81s
                        Total time: 1208.61s
                               ETA: 663 mins 37.8 s

################################################################################
                     Learning iteration 1473/50000                      

                       Computation: 110115 steps/s (collection: 0.770s, learning 0.123s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.15
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2958
       Mean episode rew_ang_vel_xy: -0.0810
          Mean episode rew_dof_acc: -0.3124
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -4.0314
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0160
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0941
        Mean episode terrain_level: 0.1107
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.89s
                        Total time: 1209.51s
                               ETA: 663 mins 39.4 s

################################################################################
                     Learning iteration 1474/50000                      

                       Computation: 110908 steps/s (collection: 0.764s, learning 0.122s)
               Value function loss: 0.1327
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.15
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.7975
       Mean episode rew_ang_vel_xy: -0.0800
          Mean episode rew_dof_acc: -0.3133
   Mean episode rew_dof_pos_limits: -0.0343
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -3.8310
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0795
        Mean episode terrain_level: 0.1080
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.89s
                        Total time: 1210.39s
                               ETA: 663 mins 40.7 s

################################################################################
                     Learning iteration 1475/50000                      

                       Computation: 122472 steps/s (collection: 0.677s, learning 0.126s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.15
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.0948
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3115
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1490
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -3.9470
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.1083
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.80s
                        Total time: 1211.20s
                               ETA: 663 mins 39.3 s

################################################################################
                     Learning iteration 1476/50000                      

                       Computation: 104485 steps/s (collection: 0.809s, learning 0.132s)
               Value function loss: 0.1229
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.15
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.4730
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3229
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.1421
      Mean episode rew_stand_still: -0.0066
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0168
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.1112
        Mean episode terrain_level: 0.1099
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.94s
                        Total time: 1212.14s
                               ETA: 663 mins 42.4 s

################################################################################
                     Learning iteration 1477/50000                      

                       Computation: 121487 steps/s (collection: 0.687s, learning 0.122s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.16
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6793
       Mean episode rew_ang_vel_xy: -0.0842
          Mean episode rew_dof_acc: -0.3152
   Mean episode rew_dof_pos_limits: -0.0445
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1580
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0111
       Mean episode rew_smoothness: -4.2473
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1018
        Mean episode terrain_level: 0.1121
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.81s
                        Total time: 1212.95s
                               ETA: 663 mins 41.2 s

################################################################################
                     Learning iteration 1478/50000                      

                       Computation: 114038 steps/s (collection: 0.717s, learning 0.145s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.5422
       Mean episode rew_ang_vel_xy: -0.0797
          Mean episode rew_dof_acc: -0.3039
   Mean episode rew_dof_pos_limits: -0.0422
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1472
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.2019
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1029
        Mean episode terrain_level: 0.1114
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.86s
                        Total time: 1213.81s
                               ETA: 663 mins 41.8 s

################################################################################
                     Learning iteration 1479/50000                      

                       Computation: 123956 steps/s (collection: 0.671s, learning 0.122s)
               Value function loss: 0.1306
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.16
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6413
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.3289
   Mean episode rew_dof_pos_limits: -0.0366
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.1809
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0934
        Mean episode terrain_level: 0.1084
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.79s
                        Total time: 1214.60s
                               ETA: 663 mins 40.1 s

################################################################################
                     Learning iteration 1480/50000                      

                       Computation: 118213 steps/s (collection: 0.710s, learning 0.122s)
               Value function loss: 0.1292
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.16
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.4008
       Mean episode rew_ang_vel_xy: -0.0837
          Mean episode rew_dof_acc: -0.3090
   Mean episode rew_dof_pos_limits: -0.0441
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.1247
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.1093
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.83s
                        Total time: 1215.43s
                               ETA: 663 mins 39.6 s

################################################################################
                     Learning iteration 1481/50000                      

                       Computation: 124388 steps/s (collection: 0.667s, learning 0.123s)
               Value function loss: 0.1307
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.17
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3751
       Mean episode rew_ang_vel_xy: -0.0800
          Mean episode rew_dof_acc: -0.3120
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1506
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -4.0913
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1943
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.1032
        Mean episode terrain_level: 0.1110
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.79s
                        Total time: 1216.22s
                               ETA: 663 mins 37.8 s

################################################################################
                     Learning iteration 1482/50000                      

                       Computation: 127633 steps/s (collection: 0.649s, learning 0.122s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.17
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9578
       Mean episode rew_ang_vel_xy: -0.0873
          Mean episode rew_dof_acc: -0.3443
   Mean episode rew_dof_pos_limits: -0.0401
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1565
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.4096
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.1038
        Mean episode terrain_level: 0.1112
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.77s
                        Total time: 1216.99s
                               ETA: 663 mins 35.3 s

################################################################################
                     Learning iteration 1483/50000                      

                       Computation: 117676 steps/s (collection: 0.710s, learning 0.126s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.17
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1170
       Mean episode rew_ang_vel_xy: -0.0824
          Mean episode rew_dof_acc: -0.3391
   Mean episode rew_dof_pos_limits: -0.0455
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0167
      Mean episode rew_orientation: -0.0117
       Mean episode rew_smoothness: -4.5107
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0183
 Mean episode rew_tracking_ang_vel: 0.0261
 Mean episode rew_tracking_lin_vel: 0.1073
        Mean episode terrain_level: 0.1118
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.84s
                        Total time: 1217.83s
                               ETA: 663 mins 35.0 s

################################################################################
                     Learning iteration 1484/50000                      

                       Computation: 124583 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.1350
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.17
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3695
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.3083
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1500
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.1186
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1919
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.0969
        Mean episode terrain_level: 0.1082
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.79s
                        Total time: 1218.62s
                               ETA: 663 mins 33.1 s

################################################################################
                     Learning iteration 1485/50000                      

                       Computation: 121913 steps/s (collection: 0.682s, learning 0.124s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.17
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1230
       Mean episode rew_ang_vel_xy: -0.0835
          Mean episode rew_dof_acc: -0.3381
   Mean episode rew_dof_pos_limits: -0.0490
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1505
           Mean episode rew_no_fly: 0.0169
      Mean episode rew_orientation: -0.0120
       Mean episode rew_smoothness: -4.5565
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1907
          Mean episode rew_torques: -0.0186
 Mean episode rew_tracking_ang_vel: 0.0277
 Mean episode rew_tracking_lin_vel: 0.1156
        Mean episode terrain_level: 0.1065
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.81s
                        Total time: 1219.42s
                               ETA: 663 mins 31.8 s

################################################################################
                     Learning iteration 1486/50000                      

                       Computation: 123941 steps/s (collection: 0.671s, learning 0.122s)
               Value function loss: 0.1319
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.18
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -7.8807
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.2979
   Mean episode rew_dof_pos_limits: -0.0348
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.8665
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0848
        Mean episode terrain_level: 0.1076
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.79s
                        Total time: 1220.22s
                               ETA: 663 mins 30.1 s

################################################################################
                     Learning iteration 1487/50000                      

                       Computation: 118283 steps/s (collection: 0.702s, learning 0.129s)
               Value function loss: 0.1332
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.18
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6737
       Mean episode rew_ang_vel_xy: -0.0834
          Mean episode rew_dof_acc: -0.3244
   Mean episode rew_dof_pos_limits: -0.0390
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1465
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.2343
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0965
        Mean episode terrain_level: 0.1075
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.83s
                        Total time: 1221.05s
                               ETA: 663 mins 29.6 s

################################################################################
                     Learning iteration 1488/50000                      

                       Computation: 116609 steps/s (collection: 0.696s, learning 0.147s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.18
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8902
       Mean episode rew_ang_vel_xy: -0.0826
          Mean episode rew_dof_acc: -0.3358
   Mean episode rew_dof_pos_limits: -0.0443
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1548
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0112
       Mean episode rew_smoothness: -4.3769
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1928
          Mean episode rew_torques: -0.0178
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1061
        Mean episode terrain_level: 0.1069
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.84s
                        Total time: 1221.89s
                               ETA: 663 mins 29.5 s

################################################################################
                     Learning iteration 1489/50000                      

                       Computation: 122123 steps/s (collection: 0.683s, learning 0.122s)
               Value function loss: 0.1319
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.18
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7578
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3294
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1566
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.2604
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.1000
        Mean episode terrain_level: 0.1060
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.80s
                        Total time: 1222.70s
                               ETA: 663 mins 28.2 s

################################################################################
                     Learning iteration 1490/50000                      

                       Computation: 123387 steps/s (collection: 0.676s, learning 0.121s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8073
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3271
   Mean episode rew_dof_pos_limits: -0.0442
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -4.3050
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.1115
        Mean episode terrain_level: 0.1053
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.80s
                        Total time: 1223.49s
                               ETA: 663 mins 26.6 s

################################################################################
                     Learning iteration 1491/50000                      

                       Computation: 111250 steps/s (collection: 0.761s, learning 0.123s)
               Value function loss: 0.1333
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.4255
       Mean episode rew_ang_vel_xy: -0.0805
          Mean episode rew_dof_acc: -0.3204
   Mean episode rew_dof_pos_limits: -0.0405
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.1055
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.1003
        Mean episode terrain_level: 0.1057
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.88s
                        Total time: 1224.38s
                               ETA: 663 mins 27.8 s

################################################################################
                     Learning iteration 1492/50000                      

                       Computation: 127854 steps/s (collection: 0.646s, learning 0.123s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.19
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3018
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3179
   Mean episode rew_dof_pos_limits: -0.0392
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -4.0713
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0894
        Mean episode terrain_level: 0.1043
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.77s
                        Total time: 1225.15s
                               ETA: 663 mins 25.3 s

################################################################################
                     Learning iteration 1493/50000                      

                       Computation: 121239 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.19
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3167
       Mean episode rew_ang_vel_xy: -0.0838
          Mean episode rew_dof_acc: -0.3154
   Mean episode rew_dof_pos_limits: -0.0363
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -4.0142
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0879
        Mean episode terrain_level: 0.1037
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.81s
                        Total time: 1225.96s
                               ETA: 663 mins 24.2 s

################################################################################
                     Learning iteration 1494/50000                      

                       Computation: 120946 steps/s (collection: 0.684s, learning 0.129s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.20
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9214
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3213
   Mean episode rew_dof_pos_limits: -0.0428
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1551
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.3359
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1073
        Mean episode terrain_level: 0.1037
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.81s
                        Total time: 1226.77s
                               ETA: 663 mins 23.1 s

################################################################################
                     Learning iteration 1495/50000                      

                       Computation: 108539 steps/s (collection: 0.773s, learning 0.132s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.20
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3065
       Mean episode rew_ang_vel_xy: -0.0836
          Mean episode rew_dof_acc: -0.3333
   Mean episode rew_dof_pos_limits: -0.0422
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0114
       Mean episode rew_smoothness: -4.5420
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1054
        Mean episode terrain_level: 0.1041
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.91s
                        Total time: 1227.68s
                               ETA: 663 mins 25.1 s

################################################################################
                     Learning iteration 1496/50000                      

                       Computation: 110617 steps/s (collection: 0.751s, learning 0.138s)
               Value function loss: 0.1356
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.20
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8218
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.3263
   Mean episode rew_dof_pos_limits: -0.0436
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1467
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -4.2917
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1057
        Mean episode terrain_level: 0.1027
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.89s
                        Total time: 1228.56s
                               ETA: 663 mins 26.5 s

################################################################################
                     Learning iteration 1497/50000                      

                       Computation: 114967 steps/s (collection: 0.729s, learning 0.126s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.21
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1083
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.3312
   Mean episode rew_dof_pos_limits: -0.0445
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1516
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0114
       Mean episode rew_smoothness: -4.4230
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1052
        Mean episode terrain_level: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.86s
                        Total time: 1229.42s
                               ETA: 663 mins 26.7 s

################################################################################
                     Learning iteration 1498/50000                      

                       Computation: 111273 steps/s (collection: 0.760s, learning 0.124s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.21
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7238
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3280
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.1825
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1955
          Mean episode rew_torques: -0.0168
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0966
        Mean episode terrain_level: 0.1068
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.88s
                        Total time: 1230.30s
                               ETA: 663 mins 28.0 s

################################################################################
                     Learning iteration 1499/50000                      

                       Computation: 123217 steps/s (collection: 0.671s, learning 0.127s)
               Value function loss: 0.1288
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.21
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3388
       Mean episode rew_ang_vel_xy: -0.0846
          Mean episode rew_dof_acc: -0.3267
   Mean episode rew_dof_pos_limits: -0.0441
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0160
      Mean episode rew_orientation: -0.0112
       Mean episode rew_smoothness: -4.5529
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1915
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0256
 Mean episode rew_tracking_lin_vel: 0.1105
        Mean episode terrain_level: 0.1079
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.80s
                        Total time: 1231.10s
                               ETA: 663 mins 26.4 s

################################################################################
                     Learning iteration 1500/50000                      

                       Computation: 108553 steps/s (collection: 0.783s, learning 0.123s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.22
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1843
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3375
   Mean episode rew_dof_pos_limits: -0.0466
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1572
           Mean episode rew_no_fly: 0.0163
      Mean episode rew_orientation: -0.0116
       Mean episode rew_smoothness: -4.4637
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1923
          Mean episode rew_torques: -0.0182
 Mean episode rew_tracking_ang_vel: 0.0262
 Mean episode rew_tracking_lin_vel: 0.1074
        Mean episode terrain_level: 0.1061
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.91s
                        Total time: 1232.01s
                               ETA: 663 mins 28.3 s

################################################################################
                     Learning iteration 1501/50000                      

                       Computation: 108942 steps/s (collection: 0.729s, learning 0.173s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.22
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.5115
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3010
   Mean episode rew_dof_pos_limits: -0.0419
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1513
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -4.1000
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0887
        Mean episode terrain_level: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.90s
                        Total time: 1232.91s
                               ETA: 663 mins 30.1 s

################################################################################
                     Learning iteration 1502/50000                      

                       Computation: 117506 steps/s (collection: 0.687s, learning 0.150s)
               Value function loss: 0.1340
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.22
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1456
       Mean episode rew_ang_vel_xy: -0.0793
          Mean episode rew_dof_acc: -0.2914
   Mean episode rew_dof_pos_limits: -0.0371
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -3.9136
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0865
        Mean episode terrain_level: 0.1028
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.84s
                        Total time: 1233.74s
                               ETA: 663 mins 29.8 s

################################################################################
                     Learning iteration 1503/50000                      

                       Computation: 122354 steps/s (collection: 0.678s, learning 0.126s)
               Value function loss: 0.1212
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.22
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9333
       Mean episode rew_ang_vel_xy: -0.0828
          Mean episode rew_dof_acc: -0.3219
   Mean episode rew_dof_pos_limits: -0.0410
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.3688
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1921
          Mean episode rew_torques: -0.0171
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.1076
        Mean episode terrain_level: 0.1031
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.80s
                        Total time: 1234.55s
                               ETA: 663 mins 28.4 s

################################################################################
                     Learning iteration 1504/50000                      

                       Computation: 113687 steps/s (collection: 0.735s, learning 0.130s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.2808
       Mean episode rew_ang_vel_xy: -0.0786
          Mean episode rew_dof_acc: -0.3081
   Mean episode rew_dof_pos_limits: -0.0349
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -3.9936
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0153
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0803
        Mean episode terrain_level: 0.1049
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.86s
                        Total time: 1235.41s
                               ETA: 663 mins 29.0 s

################################################################################
                     Learning iteration 1505/50000                      

                       Computation: 118117 steps/s (collection: 0.710s, learning 0.122s)
               Value function loss: 0.1323
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.3405
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.3003
   Mean episode rew_dof_pos_limits: -0.0432
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.0855
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0949
        Mean episode terrain_level: 0.1041
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.83s
                        Total time: 1236.24s
                               ETA: 663 mins 28.6 s

################################################################################
                     Learning iteration 1506/50000                      

                       Computation: 112431 steps/s (collection: 0.752s, learning 0.123s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9685
       Mean episode rew_ang_vel_xy: -0.0842
          Mean episode rew_dof_acc: -0.3277
   Mean episode rew_dof_pos_limits: -0.0376
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1554
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.3050
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0947
        Mean episode terrain_level: 0.1037
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.87s
                        Total time: 1237.12s
                               ETA: 663 mins 29.5 s

################################################################################
                     Learning iteration 1507/50000                      

                       Computation: 113589 steps/s (collection: 0.721s, learning 0.144s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.1684
       Mean episode rew_ang_vel_xy: -0.0783
          Mean episode rew_dof_acc: -0.2903
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1441
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -3.9530
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0929
        Mean episode terrain_level: 0.1005
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.87s
                        Total time: 1237.98s
                               ETA: 663 mins 30.1 s

################################################################################
                     Learning iteration 1508/50000                      

                       Computation: 101685 steps/s (collection: 0.810s, learning 0.157s)
               Value function loss: 0.1235
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8120
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3054
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.2322
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1940
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.1005
        Mean episode terrain_level: 0.0991
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.97s
                        Total time: 1238.95s
                               ETA: 663 mins 33.9 s

################################################################################
                     Learning iteration 1509/50000                      

                       Computation: 118584 steps/s (collection: 0.703s, learning 0.126s)
               Value function loss: 0.1292
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.23
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7702
       Mean episode rew_ang_vel_xy: -0.0812
          Mean episode rew_dof_acc: -0.3129
   Mean episode rew_dof_pos_limits: -0.0443
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1486
           Mean episode rew_no_fly: 0.0160
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.2928
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.1011
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.83s
                        Total time: 1239.78s
                               ETA: 663 mins 33.4 s

################################################################################
                     Learning iteration 1510/50000                      

                       Computation: 116605 steps/s (collection: 0.720s, learning 0.123s)
               Value function loss: 0.1306
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.24
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2777
       Mean episode rew_ang_vel_xy: -0.0831
          Mean episode rew_dof_acc: -0.3575
   Mean episode rew_dof_pos_limits: -0.0385
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1610
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0110
       Mean episode rew_smoothness: -4.5138
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1941
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.1023
        Mean episode terrain_level: 0.1014
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.84s
                        Total time: 1240.62s
                               ETA: 663 mins 33.3 s

################################################################################
                     Learning iteration 1511/50000                      

                       Computation: 125324 steps/s (collection: 0.661s, learning 0.123s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.24
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.4666
       Mean episode rew_ang_vel_xy: -0.0881
          Mean episode rew_dof_acc: -0.3382
   Mean episode rew_dof_pos_limits: -0.0412
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1551
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0111
       Mean episode rew_smoothness: -4.5832
      Mean episode rew_stand_still: -0.0060
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1067
        Mean episode terrain_level: 0.1020
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.78s
                        Total time: 1241.41s
                               ETA: 663 mins 31.3 s

################################################################################
                     Learning iteration 1512/50000                      

                       Computation: 101966 steps/s (collection: 0.826s, learning 0.138s)
               Value function loss: 0.1302
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.24
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7398
       Mean episode rew_ang_vel_xy: -0.0837
          Mean episode rew_dof_acc: -0.3069
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1465
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -4.2311
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0937
        Mean episode terrain_level: 0.0994
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.96s
                        Total time: 1242.37s
                               ETA: 663 mins 35.0 s

################################################################################
                     Learning iteration 1513/50000                      

                       Computation: 123454 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.24
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6811
       Mean episode rew_ang_vel_xy: -0.0846
          Mean episode rew_dof_acc: -0.3060
   Mean episode rew_dof_pos_limits: -0.0399
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -4.1614
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0944
        Mean episode terrain_level: 0.0971
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.80s
                        Total time: 1243.17s
                               ETA: 663 mins 33.4 s

################################################################################
                     Learning iteration 1514/50000                      

                       Computation: 120727 steps/s (collection: 0.678s, learning 0.136s)
               Value function loss: 0.1189
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.24
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1140
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3190
   Mean episode rew_dof_pos_limits: -0.0478
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0165
      Mean episode rew_orientation: -0.0113
       Mean episode rew_smoothness: -4.4769
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1915
          Mean episode rew_torques: -0.0183
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1201
        Mean episode terrain_level: 0.0971
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.81s
                        Total time: 1243.98s
                               ETA: 663 mins 32.4 s

################################################################################
                     Learning iteration 1515/50000                      

                       Computation: 119027 steps/s (collection: 0.690s, learning 0.136s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.25
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.0416
       Mean episode rew_ang_vel_xy: -0.0815
          Mean episode rew_dof_acc: -0.3173
   Mean episode rew_dof_pos_limits: -0.0406
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -4.3950
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0171
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0982
        Mean episode terrain_level: 0.0948
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.83s
                        Total time: 1244.81s
                               ETA: 663 mins 31.7 s

################################################################################
                     Learning iteration 1516/50000                      

                       Computation: 103258 steps/s (collection: 0.816s, learning 0.136s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.25
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2066
       Mean episode rew_ang_vel_xy: -0.0851
          Mean episode rew_dof_acc: -0.3300
   Mean episode rew_dof_pos_limits: -0.0422
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.4814
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0976
        Mean episode terrain_level: 0.0950
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.95s
                        Total time: 1245.76s
                               ETA: 663 mins 35.1 s

################################################################################
                     Learning iteration 1517/50000                      

                       Computation: 112057 steps/s (collection: 0.749s, learning 0.128s)
               Value function loss: 0.1363
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.25
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.0233
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3242
   Mean episode rew_dof_pos_limits: -0.0389
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.4249
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.0996
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.88s
                        Total time: 1246.64s
                               ETA: 663 mins 36.0 s

################################################################################
                     Learning iteration 1518/50000                      

                       Computation: 116017 steps/s (collection: 0.718s, learning 0.129s)
               Value function loss: 0.1324
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.25
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9250
       Mean episode rew_ang_vel_xy: -0.0835
          Mean episode rew_dof_acc: -0.3351
   Mean episode rew_dof_pos_limits: -0.0448
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.4003
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1917
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1044
        Mean episode terrain_level: 0.1027
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.85s
                        Total time: 1247.49s
                               ETA: 663 mins 36.0 s

################################################################################
                     Learning iteration 1519/50000                      

                       Computation: 116997 steps/s (collection: 0.719s, learning 0.121s)
               Value function loss: 0.1332
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.4545
       Mean episode rew_ang_vel_xy: -0.0883
          Mean episode rew_dof_acc: -0.3445
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.5734
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0948
        Mean episode terrain_level: 0.1034
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.84s
                        Total time: 1248.33s
                               ETA: 663 mins 35.8 s

################################################################################
                     Learning iteration 1520/50000                      

                       Computation: 107158 steps/s (collection: 0.786s, learning 0.131s)
               Value function loss: 0.1292
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6719
       Mean episode rew_ang_vel_xy: -0.0823
          Mean episode rew_dof_acc: -0.3097
   Mean episode rew_dof_pos_limits: -0.0363
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -4.1571
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0914
        Mean episode terrain_level: 0.1013
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.92s
                        Total time: 1249.24s
                               ETA: 663 mins 38.1 s

################################################################################
                     Learning iteration 1521/50000                      

                       Computation: 121695 steps/s (collection: 0.680s, learning 0.128s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7495
       Mean episode rew_ang_vel_xy: -0.0847
          Mean episode rew_dof_acc: -0.3190
   Mean episode rew_dof_pos_limits: -0.0405
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1527
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -4.2879
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.1028
        Mean episode terrain_level: 0.0997
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.81s
                        Total time: 1250.05s
                               ETA: 663 mins 36.8 s

################################################################################
                     Learning iteration 1522/50000                      

                       Computation: 103955 steps/s (collection: 0.810s, learning 0.136s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2408
       Mean episode rew_ang_vel_xy: -0.0842
          Mean episode rew_dof_acc: -0.3256
   Mean episode rew_dof_pos_limits: -0.0390
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.4694
      Mean episode rew_stand_still: -0.0047
      Mean episode rew_termination: -0.1957
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0948
        Mean episode terrain_level: 0.1016
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.95s
                        Total time: 1251.00s
                               ETA: 663 mins 40.0 s

################################################################################
                     Learning iteration 1523/50000                      

                       Computation: 111382 steps/s (collection: 0.732s, learning 0.151s)
               Value function loss: 0.1345
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.4489
       Mean episode rew_ang_vel_xy: -0.0847
          Mean episode rew_dof_acc: -0.3111
   Mean episode rew_dof_pos_limits: -0.0334
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1553
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -4.1152
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0151
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0802
        Mean episode terrain_level: 0.1034
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.88s
                        Total time: 1251.88s
                               ETA: 663 mins 41.1 s

################################################################################
                     Learning iteration 1524/50000                      

                       Computation: 106021 steps/s (collection: 0.796s, learning 0.131s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8676
       Mean episode rew_ang_vel_xy: -0.0843
          Mean episode rew_dof_acc: -0.3226
   Mean episode rew_dof_pos_limits: -0.0432
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1605
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.3319
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1023
        Mean episode terrain_level: 0.1028
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.93s
                        Total time: 1252.81s
                               ETA: 663 mins 43.6 s

################################################################################
                     Learning iteration 1525/50000                      

                       Computation: 106770 steps/s (collection: 0.752s, learning 0.169s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.7353
       Mean episode rew_ang_vel_xy: -0.0838
          Mean episode rew_dof_acc: -0.3390
   Mean episode rew_dof_pos_limits: -0.0442
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1596
           Mean episode rew_no_fly: 0.0164
      Mean episode rew_orientation: -0.0118
       Mean episode rew_smoothness: -4.7425
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0185
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1121
        Mean episode terrain_level: 0.1027
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.92s
                        Total time: 1253.73s
                               ETA: 663 mins 45.9 s

################################################################################
                     Learning iteration 1526/50000                      

                       Computation: 109677 steps/s (collection: 0.754s, learning 0.142s)
               Value function loss: 0.1344
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.26
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3183
       Mean episode rew_ang_vel_xy: -0.0853
          Mean episode rew_dof_acc: -0.3234
   Mean episode rew_dof_pos_limits: -0.0440
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.5501
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1925
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1049
        Mean episode terrain_level: 0.1029
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.90s
                        Total time: 1254.62s
                               ETA: 663 mins 47.5 s

################################################################################
                     Learning iteration 1527/50000                      

                       Computation: 116334 steps/s (collection: 0.700s, learning 0.145s)
               Value function loss: 0.1345
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.27
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.0140
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.3256
   Mean episode rew_dof_pos_limits: -0.0393
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -4.3812
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0918
        Mean episode terrain_level: 0.1020
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.85s
                        Total time: 1255.47s
                               ETA: 663 mins 47.4 s

################################################################################
                     Learning iteration 1528/50000                      

                       Computation: 104910 steps/s (collection: 0.793s, learning 0.144s)
               Value function loss: 0.1325
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.27
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5298
       Mean episode rew_ang_vel_xy: -0.0868
          Mean episode rew_dof_acc: -0.3297
   Mean episode rew_dof_pos_limits: -0.0433
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1581
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0114
       Mean episode rew_smoothness: -4.5832
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0181
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1126
        Mean episode terrain_level: 0.1042
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.94s
                        Total time: 1256.40s
                               ETA: 663 mins 50.3 s

################################################################################
                     Learning iteration 1529/50000                      

                       Computation: 101154 steps/s (collection: 0.837s, learning 0.135s)
               Value function loss: 0.1295
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.27
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2489
       Mean episode rew_ang_vel_xy: -0.0833
          Mean episode rew_dof_acc: -0.3252
   Mean episode rew_dof_pos_limits: -0.0415
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1507
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.5199
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0174
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0994
        Mean episode terrain_level: 0.1047
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.97s
                        Total time: 1257.38s
                               ETA: 663 mins 54.2 s

################################################################################
                     Learning iteration 1530/50000                      

                       Computation: 110952 steps/s (collection: 0.738s, learning 0.148s)
               Value function loss: 0.1273
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.27
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3739
       Mean episode rew_ang_vel_xy: -0.0852
          Mean episode rew_dof_acc: -0.3387
   Mean episode rew_dof_pos_limits: -0.0432
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0112
       Mean episode rew_smoothness: -4.5896
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1075
        Mean episode terrain_level: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.89s
                        Total time: 1258.26s
                               ETA: 663 mins 55.4 s

################################################################################
                     Learning iteration 1531/50000                      

                       Computation: 103120 steps/s (collection: 0.802s, learning 0.151s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1493
       Mean episode rew_ang_vel_xy: -0.0876
          Mean episode rew_dof_acc: -0.3349
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1605
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.4088
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0896
        Mean episode terrain_level: 0.1003
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.95s
                        Total time: 1259.22s
                               ETA: 663 mins 58.7 s

################################################################################
                     Learning iteration 1532/50000                      

                       Computation: 108759 steps/s (collection: 0.766s, learning 0.138s)
               Value function loss: 0.1290
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.28
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.0358
       Mean episode rew_ang_vel_xy: -0.0846
          Mean episode rew_dof_acc: -0.3203
   Mean episode rew_dof_pos_limits: -0.0381
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1472
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.3431
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1949
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0911
        Mean episode terrain_level: 0.1027
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.90s
                        Total time: 1260.12s
                               ETA: 664 mins 0.5 s

################################################################################
                     Learning iteration 1533/50000                      

                       Computation: 108435 steps/s (collection: 0.776s, learning 0.131s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.28
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3647
       Mean episode rew_ang_vel_xy: -0.0841
          Mean episode rew_dof_acc: -0.3388
   Mean episode rew_dof_pos_limits: -0.0408
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1500
           Mean episode rew_no_fly: 0.0148
      Mean episode rew_orientation: -0.0111
       Mean episode rew_smoothness: -4.5414
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1944
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.1037
        Mean episode terrain_level: 0.1022
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.91s
                        Total time: 1261.03s
                               ETA: 664 mins 2.4 s

################################################################################
                     Learning iteration 1534/50000                      

                       Computation: 104300 steps/s (collection: 0.797s, learning 0.145s)
               Value function loss: 0.1287
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.28
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7965
       Mean episode rew_ang_vel_xy: -0.0820
          Mean episode rew_dof_acc: -0.3120
   Mean episode rew_dof_pos_limits: -0.0406
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1532
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -4.2596
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0925
        Mean episode terrain_level: 0.1026
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.94s
                        Total time: 1261.97s
                               ETA: 664 mins 5.3 s

################################################################################
                     Learning iteration 1535/50000                      

                       Computation: 107829 steps/s (collection: 0.761s, learning 0.151s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.28
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9221
       Mean episode rew_ang_vel_xy: -0.0845
          Mean episode rew_dof_acc: -0.3064
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.3365
      Mean episode rew_stand_still: -0.0041
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0885
        Mean episode terrain_level: 0.1042
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.91s
                        Total time: 1262.88s
                               ETA: 664 mins 7.3 s

################################################################################
                     Learning iteration 1536/50000                      

                       Computation: 106677 steps/s (collection: 0.776s, learning 0.146s)
               Value function loss: 0.1235
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9291
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.3018
   Mean episode rew_dof_pos_limits: -0.0422
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.2926
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.0955
        Mean episode terrain_level: 0.1014
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.92s
                        Total time: 1263.80s
                               ETA: 664 mins 9.6 s

################################################################################
                     Learning iteration 1537/50000                      

                       Computation: 102374 steps/s (collection: 0.823s, learning 0.137s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1426
       Mean episode rew_ang_vel_xy: -0.0844
          Mean episode rew_dof_acc: -0.3159
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1550
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.4542
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1931
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0227
 Mean episode rew_tracking_lin_vel: 0.1006
        Mean episode terrain_level: 0.1011
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.96s
                        Total time: 1264.76s
                               ETA: 664 mins 13.2 s

################################################################################
                     Learning iteration 1538/50000                      

                       Computation: 103532 steps/s (collection: 0.804s, learning 0.146s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.4580
       Mean episode rew_ang_vel_xy: -0.0841
          Mean episode rew_dof_acc: -0.3200
   Mean episode rew_dof_pos_limits: -0.0440
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0115
       Mean episode rew_smoothness: -4.6398
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1038
        Mean episode terrain_level: 0.1021
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.95s
                        Total time: 1265.71s
                               ETA: 664 mins 16.4 s

################################################################################
                     Learning iteration 1539/50000                      

                       Computation: 113190 steps/s (collection: 0.730s, learning 0.139s)
               Value function loss: 0.1340
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.8472
       Mean episode rew_ang_vel_xy: -0.0854
          Mean episode rew_dof_acc: -0.3425
   Mean episode rew_dof_pos_limits: -0.0462
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0117
       Mean episode rew_smoothness: -4.8475
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1918
          Mean episode rew_torques: -0.0188
 Mean episode rew_tracking_ang_vel: 0.0254
 Mean episode rew_tracking_lin_vel: 0.1109
        Mean episode terrain_level: 0.1002
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.87s
                        Total time: 1266.58s
                               ETA: 664 mins 17.0 s

################################################################################
                     Learning iteration 1540/50000                      

                       Computation: 117427 steps/s (collection: 0.710s, learning 0.128s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.6671
       Mean episode rew_ang_vel_xy: -0.0821
          Mean episode rew_dof_acc: -0.3025
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1559
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -4.1596
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0969
        Mean episode terrain_level: 0.1009
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.84s
                        Total time: 1267.42s
                               ETA: 664 mins 16.6 s

################################################################################
                     Learning iteration 1541/50000                      

                       Computation: 102483 steps/s (collection: 0.822s, learning 0.138s)
               Value function loss: 0.1239
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7782
       Mean episode rew_ang_vel_xy: -0.0824
          Mean episode rew_dof_acc: -0.3082
   Mean episode rew_dof_pos_limits: -0.0435
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1540
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.2231
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1037
        Mean episode terrain_level: 0.1010
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.96s
                        Total time: 1268.38s
                               ETA: 664 mins 20.1 s

################################################################################
                     Learning iteration 1542/50000                      

                       Computation: 108119 steps/s (collection: 0.786s, learning 0.123s)
               Value function loss: 0.1241
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3826
       Mean episode rew_ang_vel_xy: -0.0868
          Mean episode rew_dof_acc: -0.3222
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1565
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.5083
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0171
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0934
        Mean episode terrain_level: 0.1035
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.91s
                        Total time: 1269.29s
                               ETA: 664 mins 22.0 s

################################################################################
                     Learning iteration 1543/50000                      

                       Computation: 124916 steps/s (collection: 0.658s, learning 0.129s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5434
       Mean episode rew_ang_vel_xy: -0.0857
          Mean episode rew_dof_acc: -0.3284
   Mean episode rew_dof_pos_limits: -0.0468
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1595
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0115
       Mean episode rew_smoothness: -4.6811
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0184
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1093
        Mean episode terrain_level: 0.1047
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.79s
                        Total time: 1270.07s
                               ETA: 664 mins 20.1 s

################################################################################
                     Learning iteration 1544/50000                      

                       Computation: 124676 steps/s (collection: 0.667s, learning 0.122s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1447
       Mean episode rew_ang_vel_xy: -0.0824
          Mean episode rew_dof_acc: -0.3175
   Mean episode rew_dof_pos_limits: -0.0386
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0108
       Mean episode rew_smoothness: -4.3997
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0168
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1033
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.79s
                        Total time: 1270.86s
                               ETA: 664 mins 18.2 s

################################################################################
                     Learning iteration 1545/50000                      

                       Computation: 103491 steps/s (collection: 0.810s, learning 0.139s)
               Value function loss: 0.1323
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.8178
       Mean episode rew_ang_vel_xy: -0.0825
          Mean episode rew_dof_acc: -0.3149
   Mean episode rew_dof_pos_limits: -0.0391
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -4.2596
      Mean episode rew_stand_still: -0.0044
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0227
 Mean episode rew_tracking_lin_vel: 0.0980
        Mean episode terrain_level: 0.1034
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.95s
                        Total time: 1271.81s
                               ETA: 664 mins 21.3 s

################################################################################
                     Learning iteration 1546/50000                      

                       Computation: 117882 steps/s (collection: 0.705s, learning 0.129s)
               Value function loss: 0.1279
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9667
       Mean episode rew_ang_vel_xy: -0.0835
          Mean episode rew_dof_acc: -0.3059
   Mean episode rew_dof_pos_limits: -0.0384
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1519
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -4.3066
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0907
        Mean episode terrain_level: 0.1046
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.83s
                        Total time: 1272.65s
                               ETA: 664 mins 20.9 s

################################################################################
                     Learning iteration 1547/50000                      

                       Computation: 123293 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.30
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2519
       Mean episode rew_ang_vel_xy: -0.0845
          Mean episode rew_dof_acc: -0.3047
   Mean episode rew_dof_pos_limits: -0.0402
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1536
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0105
       Mean episode rew_smoothness: -4.4656
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0168
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0936
        Mean episode terrain_level: 0.1077
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.80s
                        Total time: 1273.44s
                               ETA: 664 mins 19.2 s

################################################################################
                     Learning iteration 1548/50000                      

                       Computation: 115559 steps/s (collection: 0.722s, learning 0.128s)
               Value function loss: 0.1238
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.31
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.7688
       Mean episode rew_ang_vel_xy: -0.0802
          Mean episode rew_dof_acc: -0.3136
   Mean episode rew_dof_pos_limits: -0.0403
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1506
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.2323
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.1012
        Mean episode terrain_level: 0.1091
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.85s
                        Total time: 1274.29s
                               ETA: 664 mins 19.3 s

################################################################################
                     Learning iteration 1549/50000                      

                       Computation: 106995 steps/s (collection: 0.783s, learning 0.136s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.31
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.9368
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.3005
   Mean episode rew_dof_pos_limits: -0.0427
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.3024
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.0971
        Mean episode terrain_level: 0.1100
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.92s
                        Total time: 1275.21s
                               ETA: 664 mins 21.5 s

################################################################################
                     Learning iteration 1550/50000                      

                       Computation: 106834 steps/s (collection: 0.777s, learning 0.143s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.31
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -8.5578
       Mean episode rew_ang_vel_xy: -0.0803
          Mean episode rew_dof_acc: -0.3023
   Mean episode rew_dof_pos_limits: -0.0373
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1427
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -4.1610
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0932
        Mean episode terrain_level: 0.1088
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.92s
                        Total time: 1276.13s
                               ETA: 664 mins 23.7 s

################################################################################
                     Learning iteration 1551/50000                      

                       Computation: 119364 steps/s (collection: 0.699s, learning 0.124s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.32
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.5396
       Mean episode rew_ang_vel_xy: -0.0920
          Mean episode rew_dof_acc: -0.3557
   Mean episode rew_dof_pos_limits: -0.0447
      Mean episode rew_joint_power: -0.0077
        Mean episode rew_lin_vel_z: -0.1721
           Mean episode rew_no_fly: 0.0165
      Mean episode rew_orientation: -0.0124
       Mean episode rew_smoothness: -5.1023
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1126
        Mean episode terrain_level: 0.1089
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.82s
                        Total time: 1276.96s
                               ETA: 664 mins 22.9 s

################################################################################
                     Learning iteration 1552/50000                      

                       Computation: 115554 steps/s (collection: 0.727s, learning 0.123s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.32
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.9933
       Mean episode rew_ang_vel_xy: -0.0858
          Mean episode rew_dof_acc: -0.3261
   Mean episode rew_dof_pos_limits: -0.0435
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0161
      Mean episode rew_orientation: -0.0116
       Mean episode rew_smoothness: -4.7819
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0184
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.1081
        Mean episode terrain_level: 0.1095
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.85s
                        Total time: 1277.81s
                               ETA: 664 mins 22.9 s

################################################################################
                     Learning iteration 1553/50000                      

                       Computation: 122169 steps/s (collection: 0.678s, learning 0.127s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.32
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.6062
       Mean episode rew_ang_vel_xy: -0.0863
          Mean episode rew_dof_acc: -0.3336
   Mean episode rew_dof_pos_limits: -0.0410
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0111
       Mean episode rew_smoothness: -4.6847
      Mean episode rew_stand_still: -0.0060
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0174
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1002
        Mean episode terrain_level: 0.1083
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.80s
                        Total time: 1278.61s
                               ETA: 664 mins 21.6 s

################################################################################
                     Learning iteration 1554/50000                      

                       Computation: 115681 steps/s (collection: 0.725s, learning 0.125s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.32
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.0577
       Mean episode rew_ang_vel_xy: -0.0888
          Mean episode rew_dof_acc: -0.3454
   Mean episode rew_dof_pos_limits: -0.0477
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1540
           Mean episode rew_no_fly: 0.0170
      Mean episode rew_orientation: -0.0120
       Mean episode rew_smoothness: -4.8804
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0191
 Mean episode rew_tracking_ang_vel: 0.0261
 Mean episode rew_tracking_lin_vel: 0.1139
        Mean episode terrain_level: 0.1099
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.85s
                        Total time: 1279.46s
                               ETA: 664 mins 21.6 s

################################################################################
                     Learning iteration 1555/50000                      

                       Computation: 115541 steps/s (collection: 0.729s, learning 0.121s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.32
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.8264
       Mean episode rew_ang_vel_xy: -0.0879
          Mean episode rew_dof_acc: -0.3444
   Mean episode rew_dof_pos_limits: -0.0410
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1530
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0113
       Mean episode rew_smoothness: -4.7200
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1054
        Mean episode terrain_level: 0.1087
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.85s
                        Total time: 1280.31s
                               ETA: 664 mins 21.6 s

################################################################################
                     Learning iteration 1556/50000                      

                       Computation: 104935 steps/s (collection: 0.788s, learning 0.149s)
               Value function loss: 0.1235
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.33
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.9665
       Mean episode rew_ang_vel_xy: -0.0837
          Mean episode rew_dof_acc: -0.3180
   Mean episode rew_dof_pos_limits: -0.0500
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1569
           Mean episode rew_no_fly: 0.0171
      Mean episode rew_orientation: -0.0122
       Mean episode rew_smoothness: -4.8609
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1904
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0281
 Mean episode rew_tracking_lin_vel: 0.1228
        Mean episode terrain_level: 0.1090
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.94s
                        Total time: 1281.25s
                               ETA: 664 mins 24.4 s

################################################################################
                     Learning iteration 1557/50000                      

                       Computation: 115802 steps/s (collection: 0.718s, learning 0.131s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3784
       Mean episode rew_ang_vel_xy: -0.0834
          Mean episode rew_dof_acc: -0.3171
   Mean episode rew_dof_pos_limits: -0.0436
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1528
           Mean episode rew_no_fly: 0.0155
      Mean episode rew_orientation: -0.0110
       Mean episode rew_smoothness: -4.5551
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0255
 Mean episode rew_tracking_lin_vel: 0.1073
        Mean episode terrain_level: 0.1105
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.85s
                        Total time: 1282.10s
                               ETA: 664 mins 24.3 s

################################################################################
                     Learning iteration 1558/50000                      

                       Computation: 105303 steps/s (collection: 0.805s, learning 0.129s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.33
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.6124
       Mean episode rew_ang_vel_xy: -0.0875
          Mean episode rew_dof_acc: -0.3155
   Mean episode rew_dof_pos_limits: -0.0412
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1547
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.6207
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0173
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.1089
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.93s
                        Total time: 1283.03s
                               ETA: 664 mins 27.0 s

################################################################################
                     Learning iteration 1559/50000                      

                       Computation: 102692 steps/s (collection: 0.810s, learning 0.147s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.33
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.5658
       Mean episode rew_ang_vel_xy: -0.0896
          Mean episode rew_dof_acc: -0.3626
   Mean episode rew_dof_pos_limits: -0.0429
      Mean episode rew_joint_power: -0.0077
        Mean episode rew_lin_vel_z: -0.1534
           Mean episode rew_no_fly: 0.0163
      Mean episode rew_orientation: -0.0121
       Mean episode rew_smoothness: -5.0891
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0187
 Mean episode rew_tracking_ang_vel: 0.0244
 Mean episode rew_tracking_lin_vel: 0.1031
        Mean episode terrain_level: 0.1088
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.96s
                        Total time: 1283.99s
                               ETA: 664 mins 30.3 s

################################################################################
                     Learning iteration 1560/50000                      

                       Computation: 103383 steps/s (collection: 0.801s, learning 0.150s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.34
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.7059
       Mean episode rew_ang_vel_xy: -0.0872
          Mean episode rew_dof_acc: -0.3202
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.6013
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1953
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0966
        Mean episode terrain_level: 0.1095
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.95s
                        Total time: 1284.94s
                               ETA: 664 mins 33.4 s

################################################################################
                     Learning iteration 1561/50000                      

                       Computation: 103575 steps/s (collection: 0.799s, learning 0.150s)
               Value function loss: 0.1301
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.34
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 207.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.6434
       Mean episode rew_ang_vel_xy: -0.0900
          Mean episode rew_dof_acc: -0.3391
   Mean episode rew_dof_pos_limits: -0.0462
      Mean episode rew_joint_power: -0.0075
        Mean episode rew_lin_vel_z: -0.1536
           Mean episode rew_no_fly: 0.0169
      Mean episode rew_orientation: -0.0125
       Mean episode rew_smoothness: -5.1175
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1933
          Mean episode rew_torques: -0.0194
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.1113
        Mean episode terrain_level: 0.1100
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.95s
                        Total time: 1285.89s
                               ETA: 664 mins 36.5 s

################################################################################
                     Learning iteration 1562/50000                      

                       Computation: 104104 steps/s (collection: 0.815s, learning 0.130s)
               Value function loss: 0.1336
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.34
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5752
       Mean episode rew_ang_vel_xy: -0.0879
          Mean episode rew_dof_acc: -0.3306
   Mean episode rew_dof_pos_limits: -0.0415
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1631
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0109
       Mean episode rew_smoothness: -4.5913
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1942
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0943
        Mean episode terrain_level: 0.1096
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.94s
                        Total time: 1286.83s
                               ETA: 664 mins 39.5 s

################################################################################
                     Learning iteration 1563/50000                      

                       Computation: 115670 steps/s (collection: 0.720s, learning 0.129s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.35
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.0538
       Mean episode rew_ang_vel_xy: -0.0906
          Mean episode rew_dof_acc: -0.3465
   Mean episode rew_dof_pos_limits: -0.0443
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1630
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0115
       Mean episode rew_smoothness: -4.8522
      Mean episode rew_stand_still: -0.0054
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0184
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1063
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.85s
                        Total time: 1287.68s
                               ETA: 664 mins 39.5 s

################################################################################
                     Learning iteration 1564/50000                      

                       Computation: 108248 steps/s (collection: 0.769s, learning 0.139s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.35
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.2170
       Mean episode rew_ang_vel_xy: -0.0870
          Mean episode rew_dof_acc: -0.3368
   Mean episode rew_dof_pos_limits: -0.0454
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1539
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0120
       Mean episode rew_smoothness: -4.9357
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0186
 Mean episode rew_tracking_ang_vel: 0.0276
 Mean episode rew_tracking_lin_vel: 0.1065
        Mean episode terrain_level: 0.1102
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.91s
                        Total time: 1288.59s
                               ETA: 664 mins 41.3 s

################################################################################
                     Learning iteration 1565/50000                      

                       Computation: 108124 steps/s (collection: 0.758s, learning 0.151s)
               Value function loss: 0.1345
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.36
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.6315
       Mean episode rew_ang_vel_xy: -0.0831
          Mean episode rew_dof_acc: -0.3116
   Mean episode rew_dof_pos_limits: -0.0507
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0165
      Mean episode rew_orientation: -0.0116
       Mean episode rew_smoothness: -4.6532
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1901
          Mean episode rew_torques: -0.0188
 Mean episode rew_tracking_ang_vel: 0.0261
 Mean episode rew_tracking_lin_vel: 0.1159
        Mean episode terrain_level: 0.1096
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.91s
                        Total time: 1289.50s
                               ETA: 664 mins 43.1 s

################################################################################
                     Learning iteration 1566/50000                      

                       Computation: 110934 steps/s (collection: 0.755s, learning 0.131s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.36
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5369
       Mean episode rew_ang_vel_xy: -0.0848
          Mean episode rew_dof_acc: -0.3175
   Mean episode rew_dof_pos_limits: -0.0378
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -4.5283
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0978
        Mean episode terrain_level: 0.1045
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.89s
                        Total time: 1290.39s
                               ETA: 664 mins 44.2 s

################################################################################
                     Learning iteration 1567/50000                      

                       Computation: 116670 steps/s (collection: 0.715s, learning 0.128s)
               Value function loss: 0.1302
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.36
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.1344
       Mean episode rew_ang_vel_xy: -0.0845
          Mean episode rew_dof_acc: -0.3213
   Mean episode rew_dof_pos_limits: -0.0474
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0162
      Mean episode rew_orientation: -0.0121
       Mean episode rew_smoothness: -4.8421
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1928
          Mean episode rew_torques: -0.0188
 Mean episode rew_tracking_ang_vel: 0.0269
 Mean episode rew_tracking_lin_vel: 0.1173
        Mean episode terrain_level: 0.1046
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.84s
                        Total time: 1291.23s
                               ETA: 664 mins 44.0 s

################################################################################
                     Learning iteration 1568/50000                      

                       Computation: 115486 steps/s (collection: 0.714s, learning 0.137s)
               Value function loss: 0.1381
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.36
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5333
       Mean episode rew_ang_vel_xy: -0.0859
          Mean episode rew_dof_acc: -0.3222
   Mean episode rew_dof_pos_limits: -0.0446
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1539
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0110
       Mean episode rew_smoothness: -4.5319
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.1060
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.85s
                        Total time: 1292.08s
                               ETA: 664 mins 44.0 s

################################################################################
                     Learning iteration 1569/50000                      

                       Computation: 104998 steps/s (collection: 0.812s, learning 0.125s)
               Value function loss: 0.1316
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.37
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.3059
       Mean episode rew_ang_vel_xy: -0.0818
          Mean episode rew_dof_acc: -0.3150
   Mean episode rew_dof_pos_limits: -0.0394
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1487
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.4415
      Mean episode rew_stand_still: -0.0066
      Mean episode rew_termination: -0.1947
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0995
        Mean episode terrain_level: 0.1056
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.94s
                        Total time: 1293.02s
                               ETA: 664 mins 46.6 s

################################################################################
                     Learning iteration 1570/50000                      

                       Computation: 121042 steps/s (collection: 0.682s, learning 0.130s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.37
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.6689
       Mean episode rew_ang_vel_xy: -0.0821
          Mean episode rew_dof_acc: -0.3208
   Mean episode rew_dof_pos_limits: -0.0422
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1576
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0112
       Mean episode rew_smoothness: -4.6239
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0950
        Mean episode terrain_level: 0.1064
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.81s
                        Total time: 1293.83s
                               ETA: 664 mins 45.5 s

################################################################################
                     Learning iteration 1571/50000                      

                       Computation: 121827 steps/s (collection: 0.685s, learning 0.122s)
               Value function loss: 0.1316
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.37
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.9513
       Mean episode rew_ang_vel_xy: -0.0863
          Mean episode rew_dof_acc: -0.3314
   Mean episode rew_dof_pos_limits: -0.0452
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1540
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0116
       Mean episode rew_smoothness: -4.7585
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0182
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.1076
        Mean episode terrain_level: 0.1054
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.81s
                        Total time: 1294.63s
                               ETA: 664 mins 44.1 s

################################################################################
                     Learning iteration 1572/50000                      

                       Computation: 105192 steps/s (collection: 0.786s, learning 0.148s)
               Value function loss: 0.1256
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.37
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.1137
       Mean episode rew_ang_vel_xy: -0.0816
          Mean episode rew_dof_acc: -0.2905
   Mean episode rew_dof_pos_limits: -0.0379
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0100
       Mean episode rew_smoothness: -4.3614
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0962
        Mean episode terrain_level: 0.1028
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.93s
                        Total time: 1295.57s
                               ETA: 664 mins 46.7 s

################################################################################
                     Learning iteration 1573/50000                      

                       Computation: 112151 steps/s (collection: 0.737s, learning 0.139s)
               Value function loss: 0.1377
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.38
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.4573
       Mean episode rew_ang_vel_xy: -0.0873
          Mean episode rew_dof_acc: -0.3126
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1543
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -4.5380
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0934
        Mean episode terrain_level: 0.1036
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.88s
                        Total time: 1296.45s
                               ETA: 664 mins 47.5 s

################################################################################
                     Learning iteration 1574/50000                      

                       Computation: 105541 steps/s (collection: 0.806s, learning 0.125s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.38
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.7998
       Mean episode rew_ang_vel_xy: -0.0862
          Mean episode rew_dof_acc: -0.3244
   Mean episode rew_dof_pos_limits: -0.0455
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1583
           Mean episode rew_no_fly: 0.0152
      Mean episode rew_orientation: -0.0113
       Mean episode rew_smoothness: -4.7269
      Mean episode rew_stand_still: -0.0060
      Mean episode rew_termination: -0.1919
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1030
        Mean episode terrain_level: 0.1016
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.93s
                        Total time: 1297.38s
                               ETA: 664 mins 50.0 s

################################################################################
                     Learning iteration 1575/50000                      

                       Computation: 108806 steps/s (collection: 0.768s, learning 0.136s)
               Value function loss: 0.1270
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.38
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.3660
       Mean episode rew_ang_vel_xy: -0.0892
          Mean episode rew_dof_acc: -0.3360
   Mean episode rew_dof_pos_limits: -0.0417
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1536
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0115
       Mean episode rew_smoothness: -4.8923
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0182
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.1025
        Mean episode terrain_level: 0.1024
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.90s
                        Total time: 1298.28s
                               ETA: 664 mins 51.6 s

################################################################################
                     Learning iteration 1576/50000                      

                       Computation: 106277 steps/s (collection: 0.783s, learning 0.142s)
               Value function loss: 0.1278
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.38
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.2158
       Mean episode rew_ang_vel_xy: -0.0880
          Mean episode rew_dof_acc: -0.3169
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1586
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -4.3587
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1948
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0812
        Mean episode terrain_level: 0.1030
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.92s
                        Total time: 1299.21s
                               ETA: 664 mins 53.9 s

################################################################################
                     Learning iteration 1577/50000                      

                       Computation: 115417 steps/s (collection: 0.723s, learning 0.129s)
               Value function loss: 0.1257
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.38
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.0090
       Mean episode rew_ang_vel_xy: -0.0805
          Mean episode rew_dof_acc: -0.2905
   Mean episode rew_dof_pos_limits: -0.0388
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1494
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0104
       Mean episode rew_smoothness: -4.2379
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0959
        Mean episode terrain_level: 0.1056
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.85s
                        Total time: 1300.06s
                               ETA: 664 mins 54.0 s

################################################################################
                     Learning iteration 1578/50000                      

                       Computation: 110259 steps/s (collection: 0.741s, learning 0.150s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.39
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.7495
       Mean episode rew_ang_vel_xy: -0.0897
          Mean episode rew_dof_acc: -0.3486
   Mean episode rew_dof_pos_limits: -0.0474
      Mean episode rew_joint_power: -0.0075
        Mean episode rew_lin_vel_z: -0.1499
           Mean episode rew_no_fly: 0.0175
      Mean episode rew_orientation: -0.0124
       Mean episode rew_smoothness: -5.1312
      Mean episode rew_stand_still: -0.0051
      Mean episode rew_termination: -0.1922
          Mean episode rew_torques: -0.0196
 Mean episode rew_tracking_ang_vel: 0.0270
 Mean episode rew_tracking_lin_vel: 0.1180
        Mean episode terrain_level: 0.1050
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.89s
                        Total time: 1300.95s
                               ETA: 664 mins 55.2 s

################################################################################
                     Learning iteration 1579/50000                      

                       Computation: 109532 steps/s (collection: 0.774s, learning 0.124s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -10.0527
       Mean episode rew_ang_vel_xy: -0.0851
          Mean episode rew_dof_acc: -0.3093
   Mean episode rew_dof_pos_limits: -0.0415
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0113
       Mean episode rew_smoothness: -4.7952
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1934
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.1067
        Mean episode terrain_level: 0.1011
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.90s
                        Total time: 1301.85s
                               ETA: 664 mins 56.6 s

################################################################################
                     Learning iteration 1580/50000                      

                       Computation: 125265 steps/s (collection: 0.653s, learning 0.132s)
               Value function loss: 0.1355
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 4.39
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -9.5809
       Mean episode rew_ang_vel_xy: -0.0846
          Mean episode rew_dof_acc: -0.3255
   Mean episode rew_dof_pos_limits: -0.0374
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1574
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0107
       Mean episode rew_smoothness: -4.5147
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1952
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0918
        Mean episode terrain_level: 0.1017
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.78s
                        Total time: 1302.63s
                               ETA: 664 mins 54.6 s

