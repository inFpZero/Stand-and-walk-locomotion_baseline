swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_124349-748fca5e
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run dream_waq10_Jun01_12-43-49 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/7eoxem1snpvtil734eckq
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 18.82 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 31493 steps/s (collection: 2.943s, learning 0.178s)
               Value function loss: 3.0361
                    Surrogate loss: 0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0177
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0008
       Mean episode rew_smoothness: -0.0061
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1362
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 1.6491
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 3.12s
                        Total time: 3.12s
                               ETA: 2601 mins 9.7 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 152846 steps/s (collection: 0.520s, learning 0.123s)
               Value function loss: 3.5717
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0209
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0637
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 1.0701
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.64s
                        Total time: 3.76s
                               ETA: 1568 mins 31.8 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 147807 steps/s (collection: 0.543s, learning 0.122s)
               Value function loss: 2.8682
                    Surrogate loss: 0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0209
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0652
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.6167
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.67s
                        Total time: 4.43s
                               ETA: 1230 mins 24.2 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 144457 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 2.3914
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0648
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.3286
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.68s
                        Total time: 5.11s
                               ETA: 1064 mins 32.9 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 141605 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 1.8289
                    Surrogate loss: 0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0653
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0127
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.1642
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.69s
                        Total time: 5.80s
                               ETA: 967 mins 18.8 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 137723 steps/s (collection: 0.590s, learning 0.124s)
               Value function loss: 1.4729
                    Surrogate loss: 0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0215
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0429
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0642
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0810
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.71s
                        Total time: 6.52s
                               ETA: 905 mins 12.2 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 136019 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 1.1600
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0644
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0130
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0387
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.72s
                        Total time: 7.24s
                               ETA: 861 mins 54.1 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 136128 steps/s (collection: 0.601s, learning 0.121s)
               Value function loss: 0.8488
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0227
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0443
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0650
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0132
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.72s
                        Total time: 7.96s
                               ETA: 829 mins 21.7 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 133138 steps/s (collection: 0.616s, learning 0.122s)
               Value function loss: 0.6351
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0444
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0678
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0134
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.74s
                        Total time: 8.70s
                               ETA: 805 mins 33.1 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 131530 steps/s (collection: 0.625s, learning 0.122s)
               Value function loss: 0.4508
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0663
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0136
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.75s
                        Total time: 9.45s
                               ETA: 787 mins 15.1 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 131658 steps/s (collection: 0.626s, learning 0.121s)
               Value function loss: 0.3544
                    Surrogate loss: 0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0691
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0138
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.75s
                        Total time: 10.20s
                               ETA: 772 mins 13.4 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 132819 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.2087
                    Surrogate loss: 0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0669
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0141
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.74s
                        Total time: 10.94s
                               ETA: 759 mins 14.7 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 133497 steps/s (collection: 0.613s, learning 0.124s)
               Value function loss: 0.1486
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0673
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0141
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.74s
                        Total time: 11.67s
                               ETA: 748 mins 1.1 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 132803 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0995
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0664
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0145
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.74s
                        Total time: 12.41s
                               ETA: 738 mins 37.5 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 133074 steps/s (collection: 0.617s, learning 0.122s)
               Value function loss: 0.0687
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0242
          Mean episode rew_dof_acc: -0.0468
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0704
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0146
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.74s
                        Total time: 13.15s
                               ETA: 730 mins 23.8 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 131864 steps/s (collection: 0.623s, learning 0.122s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0257
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0473
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0660
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0150
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.75s
                        Total time: 13.90s
                               ETA: 723 mins 33.0 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 132849 steps/s (collection: 0.618s, learning 0.122s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0485
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0659
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0154
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.74s
                        Total time: 14.64s
                               ETA: 717 mins 14.1 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 132561 steps/s (collection: 0.619s, learning 0.122s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0494
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0676
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0159
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.74s
                        Total time: 15.38s
                               ETA: 711 mins 41.7 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 132569 steps/s (collection: 0.620s, learning 0.122s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0503
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0662
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0163
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.74s
                        Total time: 16.12s
                               ETA: 706 mins 44.2 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 131906 steps/s (collection: 0.623s, learning 0.122s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0514
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0703
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0165
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.75s
                        Total time: 16.86s
                               ETA: 702 mins 25.6 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 132424 steps/s (collection: 0.620s, learning 0.122s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0508
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0705
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.74s
                        Total time: 17.61s
                               ETA: 698 mins 24.6 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 132660 steps/s (collection: 0.619s, learning 0.122s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0297
       Mean episode rew_ang_vel_xy: -0.0258
          Mean episode rew_dof_acc: -0.0535
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0708
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0173
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.74s
                        Total time: 18.35s
                               ETA: 694 mins 42.5 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 133049 steps/s (collection: 0.614s, learning 0.124s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0520
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0693
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0175
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.74s
                        Total time: 19.09s
                               ETA: 691 mins 14.9 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 135427 steps/s (collection: 0.604s, learning 0.122s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0242
          Mean episode rew_dof_acc: -0.0535
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0695
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.73s
                        Total time: 19.81s
                               ETA: 687 mins 37.5 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 132880 steps/s (collection: 0.617s, learning 0.122s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0535
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0659
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0183
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.74s
                        Total time: 20.55s
                               ETA: 684 mins 45.3 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 133997 steps/s (collection: 0.611s, learning 0.123s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0540
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0673
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0187
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.73s
                        Total time: 21.29s
                               ETA: 681 mins 54.4 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 133162 steps/s (collection: 0.616s, learning 0.122s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0546
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0652
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0190
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.74s
                        Total time: 22.02s
                               ETA: 679 mins 24.7 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 135383 steps/s (collection: 0.602s, learning 0.124s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0551
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0670
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0193
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.73s
                        Total time: 22.75s
                               ETA: 676 mins 43.9 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 134605 steps/s (collection: 0.608s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0334
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0550
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0672
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0196
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.73s
                        Total time: 23.48s
                               ETA: 674 mins 21.5 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 134235 steps/s (collection: 0.608s, learning 0.124s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0344
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0568
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0658
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0204
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.73s
                        Total time: 24.21s
                               ETA: 672 mins 11.8 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 130861 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0344
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0565
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0656
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0203
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.75s
                        Total time: 24.96s
                               ETA: 670 mins 40.9 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 134337 steps/s (collection: 0.609s, learning 0.123s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0356
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0584
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0684
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0210
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.73s
                        Total time: 25.70s
                               ETA: 668 mins 45.3 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 129624 steps/s (collection: 0.633s, learning 0.125s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0360
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0584
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0638
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0212
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.76s
                        Total time: 26.45s
                               ETA: 667 mins 36.9 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 127550 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0360
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0573
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0648
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0212
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.77s
                        Total time: 27.23s
                               ETA: 666 mins 50.6 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 129286 steps/s (collection: 0.638s, learning 0.123s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0371
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0593
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0619
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0219
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.76s
                        Total time: 27.99s
                               ETA: 665 mins 52.2 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 131755 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0371
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0591
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0642
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0218
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.75s
                        Total time: 28.73s
                               ETA: 664 mins 37.1 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 131222 steps/s (collection: 0.611s, learning 0.138s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0381
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0601
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0225
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.75s
                        Total time: 29.48s
                               ETA: 663 mins 30.2 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 124886 steps/s (collection: 0.651s, learning 0.136s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0389
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0605
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0654
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0230
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0128
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.79s
                        Total time: 30.27s
                               ETA: 663 mins 16.8 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 116143 steps/s (collection: 0.700s, learning 0.146s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0390
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0602
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0635
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0228
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.85s
                        Total time: 31.11s
                               ETA: 664 mins 19.9 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 123988 steps/s (collection: 0.650s, learning 0.143s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0401
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0613
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0642
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0236
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.79s
                        Total time: 31.91s
                               ETA: 664 mins 12.9 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 128955 steps/s (collection: 0.621s, learning 0.141s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0403
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0604
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0622
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0235
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.76s
                        Total time: 32.67s
                               ETA: 663 mins 29.0 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 124097 steps/s (collection: 0.656s, learning 0.136s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0409
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0619
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0632
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0240
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.79s
                        Total time: 33.46s
                               ETA: 663 mins 22.6 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 129520 steps/s (collection: 0.637s, learning 0.122s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0420
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0630
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0655
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0247
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.76s
                        Total time: 34.22s
                               ETA: 662 mins 38.0 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 137287 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0425
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0629
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0621
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0251
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.72s
                        Total time: 34.94s
                               ETA: 661 mins 6.6 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 116739 steps/s (collection: 0.720s, learning 0.122s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0432
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0627
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0610
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0256
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.84s
                        Total time: 35.78s
                               ETA: 661 mins 59.2 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 118910 steps/s (collection: 0.704s, learning 0.123s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0438
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0643
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0669
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0258
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.83s
                        Total time: 36.61s
                               ETA: 662 mins 32.7 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 133255 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0443
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0637
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0636
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0260
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.74s
                        Total time: 37.34s
                               ETA: 661 mins 30.2 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 131218 steps/s (collection: 0.627s, learning 0.122s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0452
       Mean episode rew_ang_vel_xy: -0.0258
          Mean episode rew_dof_acc: -0.0660
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0649
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0264
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.75s
                        Total time: 38.09s
                               ETA: 660 mins 42.2 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 130123 steps/s (collection: 0.633s, learning 0.122s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0455
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0655
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0638
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0266
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.76s
                        Total time: 38.85s
                               ETA: 660 mins 2.6 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 131299 steps/s (collection: 0.624s, learning 0.125s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0461
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0655
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0270
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.75s
                        Total time: 39.60s
                               ETA: 659 mins 17.7 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 133649 steps/s (collection: 0.612s, learning 0.123s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0473
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_dof_acc: -0.0660
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0649
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0276
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.74s
                        Total time: 40.33s
                               ETA: 658 mins 21.7 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 129285 steps/s (collection: 0.637s, learning 0.123s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0480
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0670
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0627
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0279
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.76s
                        Total time: 41.09s
                               ETA: 657 mins 51.6 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 130414 steps/s (collection: 0.631s, learning 0.123s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0483
       Mean episode rew_ang_vel_xy: -0.0260
          Mean episode rew_dof_acc: -0.0667
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0661
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0280
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.75s
                        Total time: 41.85s
                               ETA: 657 mins 16.5 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 134544 steps/s (collection: 0.607s, learning 0.123s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0491
       Mean episode rew_ang_vel_xy: -0.0261
          Mean episode rew_dof_acc: -0.0675
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0286
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.73s
                        Total time: 42.58s
                               ETA: 656 mins 21.2 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 124070 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0495
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0666
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0626
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0290
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.79s
                        Total time: 43.37s
                               ETA: 656 mins 23.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 129193 steps/s (collection: 0.638s, learning 0.123s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0505
       Mean episode rew_ang_vel_xy: -0.0263
          Mean episode rew_dof_acc: -0.0676
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0641
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0295
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.76s
                        Total time: 44.13s
                               ETA: 655 mins 58.5 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 134698 steps/s (collection: 0.608s, learning 0.121s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0516
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0684
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0300
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.73s
                        Total time: 44.86s
                               ETA: 655 mins 6.7 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 128269 steps/s (collection: 0.643s, learning 0.124s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0513
       Mean episode rew_ang_vel_xy: -0.0268
          Mean episode rew_dof_acc: -0.0686
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0628
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0298
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.77s
                        Total time: 45.63s
                               ETA: 654 mins 48.1 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 130842 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0529
       Mean episode rew_ang_vel_xy: -0.0263
          Mean episode rew_dof_acc: -0.0693
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0631
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0307
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.75s
                        Total time: 46.38s
                               ETA: 654 mins 17.4 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 124880 steps/s (collection: 0.664s, learning 0.123s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0534
       Mean episode rew_ang_vel_xy: -0.0270
          Mean episode rew_dof_acc: -0.0698
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0636
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0311
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.79s
                        Total time: 47.16s
                               ETA: 654 mins 17.6 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 120586 steps/s (collection: 0.688s, learning 0.127s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0534
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0687
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0634
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0313
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.82s
                        Total time: 47.98s
                               ETA: 654 mins 40.7 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 127079 steps/s (collection: 0.643s, learning 0.131s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0546
       Mean episode rew_ang_vel_xy: -0.0275
          Mean episode rew_dof_acc: -0.0700
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0653
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0319
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.77s
                        Total time: 48.75s
                               ETA: 654 mins 29.4 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 130575 steps/s (collection: 0.629s, learning 0.124s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0547
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0709
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0632
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0321
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.75s
                        Total time: 49.51s
                               ETA: 654 mins 2.1 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 129017 steps/s (collection: 0.637s, learning 0.125s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0558
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0695
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0633
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0327
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.76s
                        Total time: 50.27s
                               ETA: 653 mins 42.6 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 121426 steps/s (collection: 0.669s, learning 0.141s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0564
       Mean episode rew_ang_vel_xy: -0.0266
          Mean episode rew_dof_acc: -0.0701
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0617
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0331
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.81s
                        Total time: 51.08s
                               ETA: 654 mins 0.4 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 122045 steps/s (collection: 0.681s, learning 0.124s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0578
       Mean episode rew_ang_vel_xy: -0.0277
          Mean episode rew_dof_acc: -0.0724
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0683
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0336
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.81s
                        Total time: 51.88s
                               ETA: 654 mins 14.5 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 121555 steps/s (collection: 0.687s, learning 0.122s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0580
       Mean episode rew_ang_vel_xy: -0.0276
          Mean episode rew_dof_acc: -0.0732
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0691
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0339
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.81s
                        Total time: 52.69s
                               ETA: 654 mins 30.5 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 130924 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0583
       Mean episode rew_ang_vel_xy: -0.0269
          Mean episode rew_dof_acc: -0.0716
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0634
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0341
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.75s
                        Total time: 53.44s
                               ETA: 654 mins 3.6 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 126270 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0596
       Mean episode rew_ang_vel_xy: -0.0274
          Mean episode rew_dof_acc: -0.0722
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0663
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0347
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.78s
                        Total time: 54.22s
                               ETA: 653 mins 57.5 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 131968 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0599
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.0734
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0720
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0348
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.74s
                        Total time: 54.97s
                               ETA: 653 mins 27.5 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 128489 steps/s (collection: 0.634s, learning 0.131s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0611
       Mean episode rew_ang_vel_xy: -0.0273
          Mean episode rew_dof_acc: -0.0742
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0671
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0357
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.77s
                        Total time: 55.73s
                               ETA: 653 mins 12.5 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 129568 steps/s (collection: 0.635s, learning 0.123s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0613
       Mean episode rew_ang_vel_xy: -0.0277
          Mean episode rew_dof_acc: -0.0723
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0652
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0356
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.76s
                        Total time: 56.49s
                               ETA: 652 mins 53.6 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 128460 steps/s (collection: 0.638s, learning 0.128s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0624
       Mean episode rew_ang_vel_xy: -0.0284
          Mean episode rew_dof_acc: -0.0734
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0707
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0363
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.77s
                        Total time: 57.26s
                               ETA: 652 mins 39.5 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 133239 steps/s (collection: 0.614s, learning 0.124s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0629
       Mean episode rew_ang_vel_xy: -0.0276
          Mean episode rew_dof_acc: -0.0729
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0686
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0365
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.74s
                        Total time: 57.99s
                               ETA: 652 mins 7.4 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 129432 steps/s (collection: 0.636s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0635
       Mean episode rew_ang_vel_xy: -0.0287
          Mean episode rew_dof_acc: -0.0741
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0719
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0370
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.76s
                        Total time: 58.75s
                               ETA: 651 mins 50.5 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 128682 steps/s (collection: 0.640s, learning 0.124s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0641
       Mean episode rew_ang_vel_xy: -0.0291
          Mean episode rew_dof_acc: -0.0759
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0712
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0372
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.76s
                        Total time: 59.52s
                               ETA: 651 mins 36.9 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 131477 steps/s (collection: 0.626s, learning 0.122s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0655
       Mean episode rew_ang_vel_xy: -0.0296
          Mean episode rew_dof_acc: -0.0739
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0710
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0381
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.75s
                        Total time: 60.26s
                               ETA: 651 mins 13.2 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 127949 steps/s (collection: 0.644s, learning 0.124s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0662
       Mean episode rew_ang_vel_xy: -0.0301
          Mean episode rew_dof_acc: -0.0761
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0758
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0385
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.77s
                        Total time: 61.03s
                               ETA: 651 mins 3.2 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 128531 steps/s (collection: 0.642s, learning 0.123s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0668
       Mean episode rew_ang_vel_xy: -0.0301
          Mean episode rew_dof_acc: -0.0761
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0779
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0389
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.76s
                        Total time: 61.80s
                               ETA: 650 mins 51.3 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 126287 steps/s (collection: 0.647s, learning 0.131s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0673
       Mean episode rew_ang_vel_xy: -0.0287
          Mean episode rew_dof_acc: -0.0767
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0683
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0391
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.78s
                        Total time: 62.58s
                               ETA: 650 mins 48.1 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 127000 steps/s (collection: 0.647s, learning 0.127s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0684
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.0757
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0712
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0399
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.77s
                        Total time: 63.35s
                               ETA: 650 mins 42.3 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 124829 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0692
       Mean episode rew_ang_vel_xy: -0.0293
          Mean episode rew_dof_acc: -0.0774
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0722
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0403
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.79s
                        Total time: 64.14s
                               ETA: 650 mins 44.8 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 129858 steps/s (collection: 0.630s, learning 0.127s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0695
       Mean episode rew_ang_vel_xy: -0.0296
          Mean episode rew_dof_acc: -0.0772
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0736
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0406
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.76s
                        Total time: 64.89s
                               ETA: 650 mins 28.9 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 128209 steps/s (collection: 0.644s, learning 0.123s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0711
       Mean episode rew_ang_vel_xy: -0.0307
          Mean episode rew_dof_acc: -0.0778
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0745
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0414
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.77s
                        Total time: 65.66s
                               ETA: 650 mins 19.1 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 130972 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0706
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.0770
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0743
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0411
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.75s
                        Total time: 66.41s
                               ETA: 650 mins 0.1 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 127551 steps/s (collection: 0.647s, learning 0.124s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0720
       Mean episode rew_ang_vel_xy: -0.0299
          Mean episode rew_dof_acc: -0.0782
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0771
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0418
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.77s
                        Total time: 67.18s
                               ETA: 649 mins 53.1 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 132283 steps/s (collection: 0.620s, learning 0.123s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0724
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.0785
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0758
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0425
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.74s
                        Total time: 67.93s
                               ETA: 649 mins 30.5 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 127450 steps/s (collection: 0.631s, learning 0.140s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0741
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.0778
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0744
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0432
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.77s
                        Total time: 68.70s
                               ETA: 649 mins 24.4 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 123677 steps/s (collection: 0.648s, learning 0.147s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0750
       Mean episode rew_ang_vel_xy: -0.0306
          Mean episode rew_dof_acc: -0.0792
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0769
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0440
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.79s
                        Total time: 69.49s
                               ETA: 649 mins 31.6 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 118425 steps/s (collection: 0.707s, learning 0.123s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0758
       Mean episode rew_ang_vel_xy: -0.0308
          Mean episode rew_dof_acc: -0.0798
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0761
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0445
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.83s
                        Total time: 70.32s
                               ETA: 649 mins 58.1 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 130990 steps/s (collection: 0.628s, learning 0.122s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0760
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.0798
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0763
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0446
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.75s
                        Total time: 71.07s
                               ETA: 649 mins 40.4 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 127423 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0782
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.0813
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0769
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0458
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.77s
                        Total time: 71.84s
                               ETA: 649 mins 34.4 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 111518 steps/s (collection: 0.759s, learning 0.123s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0779
       Mean episode rew_ang_vel_xy: -0.0316
          Mean episode rew_dof_acc: -0.0809
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0828
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0457
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.88s
                        Total time: 72.73s
                               ETA: 650 mins 27.6 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 118388 steps/s (collection: 0.700s, learning 0.131s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0790
       Mean episode rew_ang_vel_xy: -0.0314
          Mean episode rew_dof_acc: -0.0819
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0836
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0465
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.83s
                        Total time: 73.56s
                               ETA: 650 mins 52.5 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 126300 steps/s (collection: 0.654s, learning 0.124s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0798
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.0814
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0806
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0470
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.78s
                        Total time: 74.33s
                               ETA: 650 mins 49.6 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 127804 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0799
       Mean episode rew_ang_vel_xy: -0.0311
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0802
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0470
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.77s
                        Total time: 75.10s
                               ETA: 650 mins 41.9 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 130482 steps/s (collection: 0.629s, learning 0.124s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0810
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.0827
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0868
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0477
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.75s
                        Total time: 75.86s
                               ETA: 650 mins 26.2 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 129167 steps/s (collection: 0.624s, learning 0.137s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0819
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0828
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0484
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.76s
                        Total time: 76.62s
                               ETA: 650 mins 14.7 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 123175 steps/s (collection: 0.674s, learning 0.124s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0825
       Mean episode rew_ang_vel_xy: -0.0313
          Mean episode rew_dof_acc: -0.0818
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0790
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0484
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.80s
                        Total time: 77.42s
                               ETA: 650 mins 22.2 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 119049 steps/s (collection: 0.703s, learning 0.123s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0829
       Mean episode rew_ang_vel_xy: -0.0321
          Mean episode rew_dof_acc: -0.0823
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0842
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0486
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.83s
                        Total time: 78.24s
                               ETA: 650 mins 43.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 127544 steps/s (collection: 0.649s, learning 0.122s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0848
       Mean episode rew_ang_vel_xy: -0.0313
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0815
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0500
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.77s
                        Total time: 79.01s
                               ETA: 650 mins 36.7 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 123182 steps/s (collection: 0.671s, learning 0.127s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0843
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.0834
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0835
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0498
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.80s
                        Total time: 79.81s
                               ETA: 650 mins 43.6 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 125101 steps/s (collection: 0.660s, learning 0.125s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0861
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.0836
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0852
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0511
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0163
        Mean episode terrain_level: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.79s
                        Total time: 80.60s
                               ETA: 650 mins 44.4 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 124825 steps/s (collection: 0.665s, learning 0.122s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0867
       Mean episode rew_ang_vel_xy: -0.0323
          Mean episode rew_dof_acc: -0.0823
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0832
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0513
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.79s
                        Total time: 81.38s
                               ETA: 650 mins 46.1 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 128683 steps/s (collection: 0.641s, learning 0.123s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0892
       Mean episode rew_ang_vel_xy: -0.0325
          Mean episode rew_dof_acc: -0.0861
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0858
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0527
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.76s
                        Total time: 82.15s
                               ETA: 650 mins 36.4 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 131862 steps/s (collection: 0.623s, learning 0.122s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0893
       Mean episode rew_ang_vel_xy: -0.0330
          Mean episode rew_dof_acc: -0.0847
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0849
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0528
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.75s
                        Total time: 82.89s
                               ETA: 650 mins 18.3 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 130516 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0893
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.0849
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0885
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0524
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.75s
                        Total time: 83.65s
                               ETA: 650 mins 4.1 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 130379 steps/s (collection: 0.632s, learning 0.122s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0899
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.0840
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0841
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0530
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.75s
                        Total time: 84.40s
                               ETA: 649 mins 50.5 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 117139 steps/s (collection: 0.716s, learning 0.124s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0916
       Mean episode rew_ang_vel_xy: -0.0332
          Mean episode rew_dof_acc: -0.0877
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0890
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0546
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.84s
                        Total time: 85.24s
                               ETA: 650 mins 16.1 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 119808 steps/s (collection: 0.698s, learning 0.122s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0923
       Mean episode rew_ang_vel_xy: -0.0329
          Mean episode rew_dof_acc: -0.0863
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0862
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0549
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.82s
                        Total time: 86.06s
                               ETA: 650 mins 32.8 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 128501 steps/s (collection: 0.640s, learning 0.125s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0933
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.0866
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0866
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0555
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.77s
                        Total time: 86.82s
                               ETA: 650 mins 24.2 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 128325 steps/s (collection: 0.644s, learning 0.122s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0944
       Mean episode rew_ang_vel_xy: -0.0335
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0873
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0561
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.77s
                        Total time: 87.59s
                               ETA: 650 mins 16.2 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 124584 steps/s (collection: 0.655s, learning 0.134s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0960
       Mean episode rew_ang_vel_xy: -0.0338
          Mean episode rew_dof_acc: -0.0877
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0954
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0571
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.79s
                        Total time: 88.38s
                               ETA: 650 mins 18.6 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 126740 steps/s (collection: 0.653s, learning 0.122s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0984
       Mean episode rew_ang_vel_xy: -0.0342
          Mean episode rew_dof_acc: -0.0908
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0949
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0589
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.78s
                        Total time: 89.16s
                               ETA: 650 mins 14.9 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 122155 steps/s (collection: 0.680s, learning 0.125s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0979
       Mean episode rew_ang_vel_xy: -0.0334
          Mean episode rew_dof_acc: -0.0896
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0934
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0583
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.80s
                        Total time: 89.96s
                               ETA: 650 mins 24.0 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 129015 steps/s (collection: 0.639s, learning 0.123s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0981
       Mean episode rew_ang_vel_xy: -0.0340
          Mean episode rew_dof_acc: -0.0877
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0916
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0584
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.76s
                        Total time: 90.72s
                               ETA: 650 mins 14.5 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 127565 steps/s (collection: 0.646s, learning 0.125s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1003
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.0899
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0912
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0594
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.77s
                        Total time: 91.49s
                               ETA: 650 mins 8.8 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 122973 steps/s (collection: 0.676s, learning 0.124s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1000
       Mean episode rew_ang_vel_xy: -0.0339
          Mean episode rew_dof_acc: -0.0883
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0913
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0593
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.80s
                        Total time: 92.29s
                               ETA: 650 mins 15.4 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 122186 steps/s (collection: 0.682s, learning 0.123s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1010
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.0899
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0910
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0600
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.80s
                        Total time: 93.10s
                               ETA: 650 mins 24.0 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 120795 steps/s (collection: 0.692s, learning 0.122s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1027
       Mean episode rew_ang_vel_xy: -0.0348
          Mean episode rew_dof_acc: -0.0900
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0959
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0609
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.81s
                        Total time: 93.91s
                               ETA: 650 mins 36.3 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 119175 steps/s (collection: 0.700s, learning 0.125s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1037
       Mean episode rew_ang_vel_xy: -0.0348
          Mean episode rew_dof_acc: -0.0909
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0937
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0612
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.82s
                        Total time: 94.74s
                               ETA: 650 mins 52.9 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 126149 steps/s (collection: 0.657s, learning 0.122s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1053
       Mean episode rew_ang_vel_xy: -0.0347
          Mean episode rew_dof_acc: -0.0898
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0926
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0623
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.78s
                        Total time: 95.51s
                               ETA: 650 mins 50.6 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 126121 steps/s (collection: 0.657s, learning 0.123s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1067
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.0922
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0923
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0627
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.78s
                        Total time: 96.29s
                               ETA: 650 mins 48.5 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 124595 steps/s (collection: 0.646s, learning 0.143s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1068
       Mean episode rew_ang_vel_xy: -0.0358
          Mean episode rew_dof_acc: -0.0923
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0956
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0630
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.79s
                        Total time: 97.08s
                               ETA: 650 mins 50.1 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 128722 steps/s (collection: 0.640s, learning 0.124s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1082
       Mean episode rew_ang_vel_xy: -0.0357
          Mean episode rew_dof_acc: -0.0921
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0946
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0645
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.76s
                        Total time: 97.85s
                               ETA: 650 mins 41.7 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 130233 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1084
       Mean episode rew_ang_vel_xy: -0.0352
          Mean episode rew_dof_acc: -0.0933
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1009
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0643
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.75s
                        Total time: 98.60s
                               ETA: 650 mins 29.8 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 121842 steps/s (collection: 0.684s, learning 0.123s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1106
       Mean episode rew_ang_vel_xy: -0.0350
          Mean episode rew_dof_acc: -0.0912
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0940
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0654
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.81s
                        Total time: 99.41s
                               ETA: 650 mins 38.6 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 121115 steps/s (collection: 0.680s, learning 0.131s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1114
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.0946
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1010
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0662
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.81s
                        Total time: 100.22s
                               ETA: 650 mins 49.0 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 114705 steps/s (collection: 0.708s, learning 0.149s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1115
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.0941
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0661
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.86s
                        Total time: 101.08s
                               ETA: 651 mins 16.9 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 124640 steps/s (collection: 0.662s, learning 0.127s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1142
       Mean episode rew_ang_vel_xy: -0.0356
          Mean episode rew_dof_acc: -0.0933
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0680
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.79s
                        Total time: 101.87s
                               ETA: 651 mins 18.1 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 127711 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1148
       Mean episode rew_ang_vel_xy: -0.0357
          Mean episode rew_dof_acc: -0.0951
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1009
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0684
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.77s
                        Total time: 102.64s
                               ETA: 651 mins 12.0 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 129892 steps/s (collection: 0.634s, learning 0.123s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1146
       Mean episode rew_ang_vel_xy: -0.0360
          Mean episode rew_dof_acc: -0.0951
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1009
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0677
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.76s
                        Total time: 103.39s
                               ETA: 651 mins 1.2 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 129873 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1177
       Mean episode rew_ang_vel_xy: -0.0355
          Mean episode rew_dof_acc: -0.0956
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0987
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0698
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.76s
                        Total time: 104.15s
                               ETA: 650 mins 50.5 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 128044 steps/s (collection: 0.645s, learning 0.123s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1164
       Mean episode rew_ang_vel_xy: -0.0373
          Mean episode rew_dof_acc: -0.0954
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1046
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0691
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.77s
                        Total time: 104.92s
                               ETA: 650 mins 44.0 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 129780 steps/s (collection: 0.634s, learning 0.123s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1183
       Mean episode rew_ang_vel_xy: -0.0365
          Mean episode rew_dof_acc: -0.0959
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1031
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0703
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.76s
                        Total time: 105.67s
                               ETA: 650 mins 33.8 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 124631 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1177
       Mean episode rew_ang_vel_xy: -0.0369
          Mean episode rew_dof_acc: -0.0956
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1034
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0698
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.79s
                        Total time: 106.46s
                               ETA: 650 mins 35.2 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 131918 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1205
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.0976
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1055
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0719
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.75s
                        Total time: 107.21s
                               ETA: 650 mins 20.7 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 124536 steps/s (collection: 0.654s, learning 0.135s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1207
       Mean episode rew_ang_vel_xy: -0.0365
          Mean episode rew_dof_acc: -0.0977
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1033
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0720
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.79s
                        Total time: 108.00s
                               ETA: 650 mins 22.4 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 124948 steps/s (collection: 0.665s, learning 0.121s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1214
       Mean episode rew_ang_vel_xy: -0.0373
          Mean episode rew_dof_acc: -0.0970
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.1080
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0725
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.79s
                        Total time: 108.78s
                               ETA: 650 mins 23.1 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 118738 steps/s (collection: 0.682s, learning 0.146s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1238
       Mean episode rew_ang_vel_xy: -0.0375
          Mean episode rew_dof_acc: -0.0968
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1049
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0736
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.83s
                        Total time: 109.61s
                               ETA: 650 mins 38.5 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 121179 steps/s (collection: 0.682s, learning 0.129s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1247
       Mean episode rew_ang_vel_xy: -0.0378
          Mean episode rew_dof_acc: -0.0972
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1038
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0746
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.81s
                        Total time: 110.42s
                               ETA: 650 mins 47.7 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 121418 steps/s (collection: 0.673s, learning 0.137s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1252
       Mean episode rew_ang_vel_xy: -0.0378
          Mean episode rew_dof_acc: -0.1003
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1069
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0748
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.81s
                        Total time: 111.23s
                               ETA: 650 mins 56.2 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 118660 steps/s (collection: 0.697s, learning 0.131s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1272
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.0975
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1066
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0768
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.83s
                        Total time: 112.06s
                               ETA: 651 mins 11.2 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 105623 steps/s (collection: 0.789s, learning 0.142s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1277
       Mean episode rew_ang_vel_xy: -0.0393
          Mean episode rew_dof_acc: -0.0991
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1076
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0765
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.93s
                        Total time: 112.99s
                               ETA: 652 mins 1.3 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 103388 steps/s (collection: 0.782s, learning 0.169s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1284
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.0993
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1080
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0771
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.95s
                        Total time: 113.94s
                               ETA: 652 mins 57.6 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 118650 steps/s (collection: 0.690s, learning 0.138s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1293
       Mean episode rew_ang_vel_xy: -0.0386
          Mean episode rew_dof_acc: -0.0999
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1105
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0774
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.83s
                        Total time: 114.77s
                               ETA: 653 mins 11.4 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 124332 steps/s (collection: 0.668s, learning 0.123s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1318
       Mean episode rew_ang_vel_xy: -0.0391
          Mean episode rew_dof_acc: -0.1027
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1094
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0790
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.79s
                        Total time: 115.56s
                               ETA: 653 mins 12.2 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 113981 steps/s (collection: 0.734s, learning 0.128s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1323
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1006
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1011
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0797
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.86s
                        Total time: 116.42s
                               ETA: 653 mins 37.1 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 120760 steps/s (collection: 0.687s, learning 0.127s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1335
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1024
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1143
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0803
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.81s
                        Total time: 117.24s
                               ETA: 653 mins 45.5 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 117038 steps/s (collection: 0.707s, learning 0.133s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1347
       Mean episode rew_ang_vel_xy: -0.0379
          Mean episode rew_dof_acc: -0.1024
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1089
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0810
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.84s
                        Total time: 118.08s
                               ETA: 654 mins 2.3 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 119131 steps/s (collection: 0.687s, learning 0.139s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1345
       Mean episode rew_ang_vel_xy: -0.0386
          Mean episode rew_dof_acc: -0.1028
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1081
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0819
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.83s
                        Total time: 118.90s
                               ETA: 654 mins 14.1 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 110865 steps/s (collection: 0.762s, learning 0.125s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1370
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1029
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1063
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0828
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.89s
                        Total time: 119.79s
                               ETA: 654 mins 45.9 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 124304 steps/s (collection: 0.655s, learning 0.136s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1364
       Mean episode rew_ang_vel_xy: -0.0385
          Mean episode rew_dof_acc: -0.1020
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1073
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0823
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.79s
                        Total time: 120.58s
                               ETA: 654 mins 46.0 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 124139 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1421
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1045
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1110
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0855
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.79s
                        Total time: 121.37s
                               ETA: 654 mins 46.4 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 127093 steps/s (collection: 0.652s, learning 0.121s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1402
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1025
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.1117
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0839
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.77s
                        Total time: 122.15s
                               ETA: 654 mins 40.9 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 108112 steps/s (collection: 0.786s, learning 0.124s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1418
       Mean episode rew_ang_vel_xy: -0.0405
          Mean episode rew_dof_acc: -0.1050
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1112
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0848
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.91s
                        Total time: 123.06s
                               ETA: 655 mins 18.8 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 125705 steps/s (collection: 0.660s, learning 0.122s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1450
       Mean episode rew_ang_vel_xy: -0.0390
          Mean episode rew_dof_acc: -0.1030
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1066
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0869
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.78s
                        Total time: 123.84s
                               ETA: 655 mins 15.9 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 118839 steps/s (collection: 0.695s, learning 0.132s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1432
       Mean episode rew_ang_vel_xy: -0.0400
          Mean episode rew_dof_acc: -0.1060
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1175
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0856
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.83s
                        Total time: 124.67s
                               ETA: 655 mins 27.2 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 114187 steps/s (collection: 0.731s, learning 0.130s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1464
       Mean episode rew_ang_vel_xy: -0.0404
          Mean episode rew_dof_acc: -0.1055
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1097
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0876
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.86s
                        Total time: 125.53s
                               ETA: 655 mins 49.0 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 121803 steps/s (collection: 0.685s, learning 0.122s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1468
       Mean episode rew_ang_vel_xy: -0.0392
          Mean episode rew_dof_acc: -0.1053
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1193
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0873
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.81s
                        Total time: 126.33s
                               ETA: 655 mins 53.6 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 124308 steps/s (collection: 0.668s, learning 0.122s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1482
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1054
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1137
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0879
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.79s
                        Total time: 127.12s
                               ETA: 655 mins 53.2 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 126101 steps/s (collection: 0.656s, learning 0.123s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1517
       Mean episode rew_ang_vel_xy: -0.0409
          Mean episode rew_dof_acc: -0.1071
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1157
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0900
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.78s
                        Total time: 127.90s
                               ETA: 655 mins 49.4 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 121319 steps/s (collection: 0.683s, learning 0.127s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1488
       Mean episode rew_ang_vel_xy: -0.0399
          Mean episode rew_dof_acc: -0.1058
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1172
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0880
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.81s
                        Total time: 128.71s
                               ETA: 655 mins 54.9 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 114641 steps/s (collection: 0.723s, learning 0.135s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1524
       Mean episode rew_ang_vel_xy: -0.0399
          Mean episode rew_dof_acc: -0.1056
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1143
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0906
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.86s
                        Total time: 129.57s
                               ETA: 656 mins 14.7 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 120192 steps/s (collection: 0.694s, learning 0.123s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1511
       Mean episode rew_ang_vel_xy: -0.0400
          Mean episode rew_dof_acc: -0.1074
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1212
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0896
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.82s
                        Total time: 130.39s
                               ETA: 656 mins 22.4 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 111054 steps/s (collection: 0.758s, learning 0.128s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1547
       Mean episode rew_ang_vel_xy: -0.0403
          Mean episode rew_dof_acc: -0.1085
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1192
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0914
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.89s
                        Total time: 131.27s
                               ETA: 656 mins 50.1 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 120480 steps/s (collection: 0.693s, learning 0.123s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1554
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1051
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1145
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0921
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.82s
                        Total time: 132.09s
                               ETA: 656 mins 56.8 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 120284 steps/s (collection: 0.666s, learning 0.151s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1567
       Mean episode rew_ang_vel_xy: -0.0406
          Mean episode rew_dof_acc: -0.1085
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1218
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0924
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.82s
                        Total time: 132.91s
                               ETA: 657 mins 3.8 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 120791 steps/s (collection: 0.691s, learning 0.123s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1602
       Mean episode rew_ang_vel_xy: -0.0392
          Mean episode rew_dof_acc: -0.1080
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1135
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0945
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.81s
                        Total time: 133.72s
                               ETA: 657 mins 9.7 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 120433 steps/s (collection: 0.678s, learning 0.139s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1590
       Mean episode rew_ang_vel_xy: -0.0394
          Mean episode rew_dof_acc: -0.1048
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1130
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0937
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.82s
                        Total time: 134.54s
                               ETA: 657 mins 16.2 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 113900 steps/s (collection: 0.730s, learning 0.133s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1625
       Mean episode rew_ang_vel_xy: -0.0411
          Mean episode rew_dof_acc: -0.1094
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1168
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0957
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.86s
                        Total time: 135.40s
                               ETA: 657 mins 36.3 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 120400 steps/s (collection: 0.693s, learning 0.124s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1637
       Mean episode rew_ang_vel_xy: -0.0406
          Mean episode rew_dof_acc: -0.1076
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1122
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0955
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.82s
                        Total time: 136.22s
                               ETA: 657 mins 42.7 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 122867 steps/s (collection: 0.677s, learning 0.123s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1686
       Mean episode rew_ang_vel_xy: -0.0414
          Mean episode rew_dof_acc: -0.1107
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1206
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0982
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.80s
                        Total time: 137.02s
                               ETA: 657 mins 44.2 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 121895 steps/s (collection: 0.672s, learning 0.134s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1677
       Mean episode rew_ang_vel_xy: -0.0425
          Mean episode rew_dof_acc: -0.1107
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0980
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.81s
                        Total time: 137.82s
                               ETA: 657 mins 47.6 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 121254 steps/s (collection: 0.675s, learning 0.135s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1683
       Mean episode rew_ang_vel_xy: -0.0411
          Mean episode rew_dof_acc: -0.1082
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1168
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0979
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.81s
                        Total time: 138.63s
                               ETA: 657 mins 52.1 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 119925 steps/s (collection: 0.695s, learning 0.125s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1717
       Mean episode rew_ang_vel_xy: -0.0417
          Mean episode rew_dof_acc: -0.1122
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1209
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0999
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.82s
                        Total time: 139.45s
                               ETA: 657 mins 59.1 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 128019 steps/s (collection: 0.646s, learning 0.122s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0166
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1721
       Mean episode rew_ang_vel_xy: -0.0411
          Mean episode rew_dof_acc: -0.1103
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1208
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1007
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.77s
                        Total time: 140.22s
                               ETA: 657 mins 51.4 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 112989 steps/s (collection: 0.747s, learning 0.123s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1700
       Mean episode rew_ang_vel_xy: -0.0414
          Mean episode rew_dof_acc: -0.1084
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.1195
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0985
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.87s
                        Total time: 141.09s
                               ETA: 658 mins 12.4 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 122297 steps/s (collection: 0.673s, learning 0.131s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1758
       Mean episode rew_ang_vel_xy: -0.0444
          Mean episode rew_dof_acc: -0.1127
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1268
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1024
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.80s
                        Total time: 141.90s
                               ETA: 658 mins 14.7 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 119677 steps/s (collection: 0.698s, learning 0.124s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1757
       Mean episode rew_ang_vel_xy: -0.0434
          Mean episode rew_dof_acc: -0.1129
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1306
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1021
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.82s
                        Total time: 142.72s
                               ETA: 658 mins 21.8 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 124764 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1813
       Mean episode rew_ang_vel_xy: -0.0437
          Mean episode rew_dof_acc: -0.1122
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1279
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1060
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.79s
                        Total time: 143.51s
                               ETA: 658 mins 19.7 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 108724 steps/s (collection: 0.766s, learning 0.138s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1795
       Mean episode rew_ang_vel_xy: -0.0437
          Mean episode rew_dof_acc: -0.1118
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1233
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1041
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.90s
                        Total time: 144.41s
                               ETA: 658 mins 49.4 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 125230 steps/s (collection: 0.647s, learning 0.138s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1826
       Mean episode rew_ang_vel_xy: -0.0436
          Mean episode rew_dof_acc: -0.1161
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1285
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1061
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.78s
                        Total time: 145.19s
                               ETA: 658 mins 46.3 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 121702 steps/s (collection: 0.685s, learning 0.123s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1838
       Mean episode rew_ang_vel_xy: -0.0432
          Mean episode rew_dof_acc: -0.1138
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1217
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1079
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0195
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.81s
                        Total time: 146.00s
                               ETA: 658 mins 49.3 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 117794 steps/s (collection: 0.696s, learning 0.139s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1847
       Mean episode rew_ang_vel_xy: -0.0438
          Mean episode rew_dof_acc: -0.1135
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1202
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1071
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.83s
                        Total time: 146.84s
                               ETA: 658 mins 59.6 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 121266 steps/s (collection: 0.670s, learning 0.141s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1858
       Mean episode rew_ang_vel_xy: -0.0429
          Mean episode rew_dof_acc: -0.1121
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1252
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1087
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.81s
                        Total time: 147.65s
                               ETA: 659 mins 3.3 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 121880 steps/s (collection: 0.669s, learning 0.138s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1901
       Mean episode rew_ang_vel_xy: -0.0430
          Mean episode rew_dof_acc: -0.1162
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1273
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1101
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.81s
                        Total time: 148.45s
                               ETA: 659 mins 6.0 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 116450 steps/s (collection: 0.714s, learning 0.130s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1898
       Mean episode rew_ang_vel_xy: -0.0442
          Mean episode rew_dof_acc: -0.1158
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1284
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.84s
                        Total time: 149.30s
                               ETA: 659 mins 18.5 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 123559 steps/s (collection: 0.673s, learning 0.123s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1909
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1160
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1296
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1116
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.80s
                        Total time: 150.09s
                               ETA: 659 mins 18.1 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 118567 steps/s (collection: 0.706s, learning 0.123s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1933
       Mean episode rew_ang_vel_xy: -0.0443
          Mean episode rew_dof_acc: -0.1176
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1277
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1130
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.83s
                        Total time: 150.92s
                               ETA: 659 mins 26.4 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 127194 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1984
       Mean episode rew_ang_vel_xy: -0.0453
          Mean episode rew_dof_acc: -0.1192
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1358
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1157
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.77s
                        Total time: 151.70s
                               ETA: 659 mins 20.1 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 123107 steps/s (collection: 0.666s, learning 0.133s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1947
       Mean episode rew_ang_vel_xy: -0.0434
          Mean episode rew_dof_acc: -0.1149
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1264
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1138
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.80s
                        Total time: 152.49s
                               ETA: 659 mins 20.4 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 115611 steps/s (collection: 0.727s, learning 0.124s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1981
       Mean episode rew_ang_vel_xy: -0.0445
          Mean episode rew_dof_acc: -0.1164
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1287
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1154
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.85s
                        Total time: 153.34s
                               ETA: 659 mins 34.0 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 122193 steps/s (collection: 0.681s, learning 0.123s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1967
       Mean episode rew_ang_vel_xy: -0.0443
          Mean episode rew_dof_acc: -0.1147
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1308
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1140
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.80s
                        Total time: 154.15s
                               ETA: 659 mins 35.8 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 119538 steps/s (collection: 0.689s, learning 0.134s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2026
       Mean episode rew_ang_vel_xy: -0.0441
          Mean episode rew_dof_acc: -0.1170
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1271
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1182
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.82s
                        Total time: 154.97s
                               ETA: 659 mins 42.1 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 125231 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2005
       Mean episode rew_ang_vel_xy: -0.0453
          Mean episode rew_dof_acc: -0.1183
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1343
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1162
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.78s
                        Total time: 155.76s
                               ETA: 659 mins 38.8 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 126517 steps/s (collection: 0.654s, learning 0.123s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2024
       Mean episode rew_ang_vel_xy: -0.0439
          Mean episode rew_dof_acc: -0.1149
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.1277
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1184
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.78s
                        Total time: 156.53s
                               ETA: 659 mins 33.6 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 113367 steps/s (collection: 0.728s, learning 0.139s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2046
       Mean episode rew_ang_vel_xy: -0.0462
          Mean episode rew_dof_acc: -0.1191
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1377
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1188
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.87s
                        Total time: 157.40s
                               ETA: 659 mins 51.0 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 125136 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2090
       Mean episode rew_ang_vel_xy: -0.0455
          Mean episode rew_dof_acc: -0.1187
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1307
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1221
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.79s
                        Total time: 158.19s
                               ETA: 659 mins 47.9 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 114073 steps/s (collection: 0.739s, learning 0.123s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2074
       Mean episode rew_ang_vel_xy: -0.0450
          Mean episode rew_dof_acc: -0.1199
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1397
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1208
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.86s
                        Total time: 159.05s
                               ETA: 660 mins 3.7 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 124356 steps/s (collection: 0.667s, learning 0.123s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2075
       Mean episode rew_ang_vel_xy: -0.0445
          Mean episode rew_dof_acc: -0.1182
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1311
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1212
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.79s
                        Total time: 159.84s
                               ETA: 660 mins 1.8 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 122495 steps/s (collection: 0.679s, learning 0.123s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2129
       Mean episode rew_ang_vel_xy: -0.0458
          Mean episode rew_dof_acc: -0.1192
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1359
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1248
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.80s
                        Total time: 160.64s
                               ETA: 660 mins 2.8 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 124686 steps/s (collection: 0.666s, learning 0.122s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2122
       Mean episode rew_ang_vel_xy: -0.0457
          Mean episode rew_dof_acc: -0.1203
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1371
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1241
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0226
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.79s
                        Total time: 161.43s
                               ETA: 660 mins 0.3 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 124994 steps/s (collection: 0.664s, learning 0.123s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2155
       Mean episode rew_ang_vel_xy: -0.0454
          Mean episode rew_dof_acc: -0.1212
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1345
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1269
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.79s
                        Total time: 162.22s
                               ETA: 659 mins 57.4 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 120765 steps/s (collection: 0.666s, learning 0.148s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2138
       Mean episode rew_ang_vel_xy: -0.0460
          Mean episode rew_dof_acc: -0.1203
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1395
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1251
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0261
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.81s
                        Total time: 163.03s
                               ETA: 660 mins 1.1 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 119779 steps/s (collection: 0.695s, learning 0.126s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0168
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2191
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1243
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1428
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1294
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.82s
                        Total time: 163.85s
                               ETA: 660 mins 6.5 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 125992 steps/s (collection: 0.652s, learning 0.129s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2152
       Mean episode rew_ang_vel_xy: -0.0450
          Mean episode rew_dof_acc: -0.1207
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1322
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1268
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.78s
                        Total time: 164.63s
                               ETA: 660 mins 2.1 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 123596 steps/s (collection: 0.673s, learning 0.123s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2196
       Mean episode rew_ang_vel_xy: -0.0460
          Mean episode rew_dof_acc: -0.1205
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1335
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1294
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.80s
                        Total time: 165.43s
                               ETA: 660 mins 1.3 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 123979 steps/s (collection: 0.668s, learning 0.125s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2231
       Mean episode rew_ang_vel_xy: -0.0474
          Mean episode rew_dof_acc: -0.1235
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1396
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1305
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.79s
                        Total time: 166.22s
                               ETA: 659 mins 59.9 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 123501 steps/s (collection: 0.674s, learning 0.122s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2252
       Mean episode rew_ang_vel_xy: -0.0477
          Mean episode rew_dof_acc: -0.1240
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1407
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1332
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.80s
                        Total time: 167.02s
                               ETA: 659 mins 59.3 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 119680 steps/s (collection: 0.679s, learning 0.142s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2264
       Mean episode rew_ang_vel_xy: -0.0457
          Mean episode rew_dof_acc: -0.1223
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1342
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1335
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.82s
                        Total time: 167.84s
                               ETA: 660 mins 4.6 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 116901 steps/s (collection: 0.699s, learning 0.142s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2266
       Mean episode rew_ang_vel_xy: -0.0473
          Mean episode rew_dof_acc: -0.1241
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1425
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1331
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0258
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.84s
                        Total time: 168.68s
                               ETA: 660 mins 14.5 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 118937 steps/s (collection: 0.690s, learning 0.136s)
               Value function loss: 0.0459
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2293
       Mean episode rew_ang_vel_xy: -0.0484
          Mean episode rew_dof_acc: -0.1251
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1429
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1339
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.83s
                        Total time: 169.50s
                               ETA: 660 mins 20.9 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 121259 steps/s (collection: 0.687s, learning 0.123s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2311
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1234
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1384
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1358
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.81s
                        Total time: 170.31s
                               ETA: 660 mins 23.6 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 116082 steps/s (collection: 0.724s, learning 0.123s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0163
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2279
       Mean episode rew_ang_vel_xy: -0.0479
          Mean episode rew_dof_acc: -0.1232
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1438
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1329
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0239
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.85s
                        Total time: 171.16s
                               ETA: 660 mins 34.6 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 116425 steps/s (collection: 0.722s, learning 0.122s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2368
       Mean episode rew_ang_vel_xy: -0.0478
          Mean episode rew_dof_acc: -0.1255
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1410
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1384
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0241
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.84s
                        Total time: 172.01s
                               ETA: 660 mins 44.9 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 109334 steps/s (collection: 0.773s, learning 0.127s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2345
       Mean episode rew_ang_vel_xy: -0.0503
          Mean episode rew_dof_acc: -0.1255
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1370
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.90s
                        Total time: 172.90s
                               ETA: 661 mins 7.7 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 114475 steps/s (collection: 0.735s, learning 0.124s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2377
       Mean episode rew_ang_vel_xy: -0.0480
          Mean episode rew_dof_acc: -0.1249
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1477
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1392
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.86s
                        Total time: 173.76s
                               ETA: 661 mins 21.1 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 110310 steps/s (collection: 0.751s, learning 0.140s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2401
       Mean episode rew_ang_vel_xy: -0.0479
          Mean episode rew_dof_acc: -0.1257
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1334
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1399
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.89s
                        Total time: 174.65s
                               ETA: 661 mins 41.7 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 126686 steps/s (collection: 0.652s, learning 0.124s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2411
       Mean episode rew_ang_vel_xy: -0.0479
          Mean episode rew_dof_acc: -0.1251
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1401
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1402
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.78s
                        Total time: 175.43s
                               ETA: 661 mins 36.0 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 117338 steps/s (collection: 0.696s, learning 0.142s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2387
       Mean episode rew_ang_vel_xy: -0.0482
          Mean episode rew_dof_acc: -0.1262
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1451
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1392
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.84s
                        Total time: 176.27s
                               ETA: 661 mins 44.3 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 119089 steps/s (collection: 0.702s, learning 0.123s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2421
       Mean episode rew_ang_vel_xy: -0.0483
          Mean episode rew_dof_acc: -0.1264
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1456
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1414
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.83s
                        Total time: 177.09s
                               ETA: 661 mins 49.7 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 115449 steps/s (collection: 0.708s, learning 0.144s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2358
       Mean episode rew_ang_vel_xy: -0.0479
          Mean episode rew_dof_acc: -0.1235
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.1389
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1384
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.85s
                        Total time: 177.95s
                               ETA: 662 mins 0.9 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 122741 steps/s (collection: 0.677s, learning 0.124s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2473
       Mean episode rew_ang_vel_xy: -0.0484
          Mean episode rew_dof_acc: -0.1289
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1434
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1450
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.80s
                        Total time: 178.75s
                               ETA: 662 mins 0.8 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 120020 steps/s (collection: 0.683s, learning 0.136s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0167
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2459
       Mean episode rew_ang_vel_xy: -0.0477
          Mean episode rew_dof_acc: -0.1261
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1398
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1439
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.82s
                        Total time: 179.57s
                               ETA: 662 mins 4.7 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 122836 steps/s (collection: 0.677s, learning 0.123s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2449
       Mean episode rew_ang_vel_xy: -0.0479
          Mean episode rew_dof_acc: -0.1252
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1398
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1438
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.80s
                        Total time: 180.37s
                               ETA: 662 mins 4.3 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 120113 steps/s (collection: 0.689s, learning 0.129s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2522
       Mean episode rew_ang_vel_xy: -0.0501
          Mean episode rew_dof_acc: -0.1303
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1442
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1478
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.82s
                        Total time: 181.18s
                               ETA: 662 mins 8.0 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 123891 steps/s (collection: 0.670s, learning 0.123s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2535
       Mean episode rew_ang_vel_xy: -0.0503
          Mean episode rew_dof_acc: -0.1298
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1482
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0271
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.79s
                        Total time: 181.98s
                               ETA: 662 mins 6.2 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 125366 steps/s (collection: 0.660s, learning 0.124s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0166
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2555
       Mean episode rew_ang_vel_xy: -0.0500
          Mean episode rew_dof_acc: -0.1300
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1454
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1501
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0272
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.78s
                        Total time: 182.76s
                               ETA: 662 mins 2.3 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 122951 steps/s (collection: 0.676s, learning 0.123s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2546
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.1274
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.1427
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1501
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.80s
                        Total time: 183.56s
                               ETA: 662 mins 1.9 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 125855 steps/s (collection: 0.659s, learning 0.122s)
               Value function loss: 0.0442
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2573
       Mean episode rew_ang_vel_xy: -0.0508
          Mean episode rew_dof_acc: -0.1313
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1501
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1512
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.78s
                        Total time: 184.34s
                               ETA: 661 mins 57.4 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 116908 steps/s (collection: 0.715s, learning 0.125s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0166
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2569
       Mean episode rew_ang_vel_xy: -0.0499
          Mean episode rew_dof_acc: -0.1290
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1459
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1512
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.84s
                        Total time: 185.18s
                               ETA: 662 mins 5.8 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 113982 steps/s (collection: 0.740s, learning 0.123s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2607
       Mean episode rew_ang_vel_xy: -0.0493
          Mean episode rew_dof_acc: -0.1286
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1503
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1545
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.86s
                        Total time: 186.05s
                               ETA: 662 mins 18.7 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 119843 steps/s (collection: 0.698s, learning 0.122s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2619
       Mean episode rew_ang_vel_xy: -0.0509
          Mean episode rew_dof_acc: -0.1313
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1554
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0314
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.82s
                        Total time: 186.87s
                               ETA: 662 mins 22.5 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 126430 steps/s (collection: 0.654s, learning 0.124s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2633
       Mean episode rew_ang_vel_xy: -0.0494
          Mean episode rew_dof_acc: -0.1313
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1452
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1565
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.78s
                        Total time: 187.64s
                               ETA: 662 mins 17.3 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 116684 steps/s (collection: 0.686s, learning 0.157s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2645
       Mean episode rew_ang_vel_xy: -0.0506
          Mean episode rew_dof_acc: -0.1305
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1462
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1570
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.84s
                        Total time: 188.49s
                               ETA: 662 mins 25.8 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 120626 steps/s (collection: 0.677s, learning 0.138s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2675
       Mean episode rew_ang_vel_xy: -0.0501
          Mean episode rew_dof_acc: -0.1324
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1588
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.81s
                        Total time: 189.30s
                               ETA: 662 mins 28.4 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 124596 steps/s (collection: 0.651s, learning 0.138s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2676
       Mean episode rew_ang_vel_xy: -0.0498
          Mean episode rew_dof_acc: -0.1305
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1468
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1593
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.79s
                        Total time: 190.09s
                               ETA: 662 mins 25.5 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 120453 steps/s (collection: 0.676s, learning 0.140s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2688
       Mean episode rew_ang_vel_xy: -0.0511
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1576
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1593
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.82s
                        Total time: 190.91s
                               ETA: 662 mins 28.4 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 120440 steps/s (collection: 0.672s, learning 0.145s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2744
       Mean episode rew_ang_vel_xy: -0.0510
          Mean episode rew_dof_acc: -0.1334
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1491
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1628
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.82s
                        Total time: 191.72s
                               ETA: 662 mins 31.2 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 113081 steps/s (collection: 0.723s, learning 0.146s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2726
       Mean episode rew_ang_vel_xy: -0.0500
          Mean episode rew_dof_acc: -0.1338
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1618
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.87s
                        Total time: 192.59s
                               ETA: 662 mins 44.9 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 119970 steps/s (collection: 0.679s, learning 0.141s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2765
       Mean episode rew_ang_vel_xy: -0.0502
          Mean episode rew_dof_acc: -0.1330
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1480
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1636
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.82s
                        Total time: 193.41s
                               ETA: 662 mins 48.3 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 118214 steps/s (collection: 0.692s, learning 0.139s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2738
       Mean episode rew_ang_vel_xy: -0.0504
          Mean episode rew_dof_acc: -0.1329
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1432
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1628
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0276
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.83s
                        Total time: 194.24s
                               ETA: 662 mins 54.1 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 115239 steps/s (collection: 0.722s, learning 0.131s)
               Value function loss: 0.0489
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2776
       Mean episode rew_ang_vel_xy: -0.0496
          Mean episode rew_dof_acc: -0.1324
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1455
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1643
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0261
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.85s
                        Total time: 195.10s
                               ETA: 663 mins 4.3 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 124650 steps/s (collection: 0.667s, learning 0.121s)
               Value function loss: 0.0466
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2799
       Mean episode rew_ang_vel_xy: -0.0514
          Mean episode rew_dof_acc: -0.1331
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1657
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0277
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.79s
                        Total time: 195.88s
                               ETA: 663 mins 1.3 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 115969 steps/s (collection: 0.726s, learning 0.122s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2808
       Mean episode rew_ang_vel_xy: -0.0512
          Mean episode rew_dof_acc: -0.1345
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1522
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1663
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.85s
                        Total time: 196.73s
                               ETA: 663 mins 10.2 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 115599 steps/s (collection: 0.717s, learning 0.133s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2789
       Mean episode rew_ang_vel_xy: -0.0503
          Mean episode rew_dof_acc: -0.1336
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1492
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1658
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0308
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.85s
                        Total time: 197.58s
                               ETA: 663 mins 19.6 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 114069 steps/s (collection: 0.728s, learning 0.134s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2834
       Mean episode rew_ang_vel_xy: -0.0530
          Mean episode rew_dof_acc: -0.1372
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1580
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1674
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.86s
                        Total time: 198.44s
                               ETA: 663 mins 31.2 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 109998 steps/s (collection: 0.764s, learning 0.129s)
               Value function loss: 0.0553
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2854
       Mean episode rew_ang_vel_xy: -0.0497
          Mean episode rew_dof_acc: -0.1332
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1518
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1713
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0314
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.89s
                        Total time: 199.34s
                               ETA: 663 mins 49.1 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 123989 steps/s (collection: 0.670s, learning 0.123s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2866
       Mean episode rew_ang_vel_xy: -0.0518
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1556
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1692
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.79s
                        Total time: 200.13s
                               ETA: 663 mins 46.8 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 112182 steps/s (collection: 0.743s, learning 0.134s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2908
       Mean episode rew_ang_vel_xy: -0.0516
          Mean episode rew_dof_acc: -0.1357
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1493
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1726
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0299
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.88s
                        Total time: 201.01s
                               ETA: 664 mins 1.0 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 114148 steps/s (collection: 0.735s, learning 0.126s)
               Value function loss: 0.0524
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2843
       Mean episode rew_ang_vel_xy: -0.0516
          Mean episode rew_dof_acc: -0.1343
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1548
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1688
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.86s
                        Total time: 201.87s
                               ETA: 664 mins 12.1 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 125141 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0522
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2897
       Mean episode rew_ang_vel_xy: -0.0534
          Mean episode rew_dof_acc: -0.1376
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1624
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1710
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0297
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.79s
                        Total time: 202.65s
                               ETA: 664 mins 8.3 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 116884 steps/s (collection: 0.706s, learning 0.135s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2937
       Mean episode rew_ang_vel_xy: -0.0517
          Mean episode rew_dof_acc: -0.1347
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1746
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.84s
                        Total time: 203.49s
                               ETA: 664 mins 15.3 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 123467 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2879
       Mean episode rew_ang_vel_xy: -0.0508
          Mean episode rew_dof_acc: -0.1321
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1707
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.80s
                        Total time: 204.29s
                               ETA: 664 mins 13.5 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 123446 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2954
       Mean episode rew_ang_vel_xy: -0.0514
          Mean episode rew_dof_acc: -0.1379
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1752
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.80s
                        Total time: 205.09s
                               ETA: 664 mins 11.8 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 121124 steps/s (collection: 0.688s, learning 0.124s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2925
       Mean episode rew_ang_vel_xy: -0.0520
          Mean episode rew_dof_acc: -0.1348
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.1508
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1727
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.81s
                        Total time: 205.90s
                               ETA: 664 mins 13.0 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 105607 steps/s (collection: 0.807s, learning 0.124s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3013
       Mean episode rew_ang_vel_xy: -0.0511
          Mean episode rew_dof_acc: -0.1329
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1448
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1791
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.93s
                        Total time: 206.83s
                               ETA: 664 mins 37.2 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 119302 steps/s (collection: 0.703s, learning 0.121s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2984
       Mean episode rew_ang_vel_xy: -0.0521
          Mean episode rew_dof_acc: -0.1391
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1586
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1756
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.82s
                        Total time: 207.65s
                               ETA: 664 mins 40.7 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 111332 steps/s (collection: 0.750s, learning 0.133s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.2991
       Mean episode rew_ang_vel_xy: -0.0516
          Mean episode rew_dof_acc: -0.1347
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1583
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1760
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.88s
                        Total time: 208.54s
                               ETA: 664 mins 55.4 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 114814 steps/s (collection: 0.713s, learning 0.143s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3105
       Mean episode rew_ang_vel_xy: -0.0538
          Mean episode rew_dof_acc: -0.1404
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1547
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1846
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0320
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.86s
                        Total time: 209.39s
                               ETA: 665 mins 5.0 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 110694 steps/s (collection: 0.760s, learning 0.128s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3030
       Mean episode rew_ang_vel_xy: -0.0515
          Mean episode rew_dof_acc: -0.1387
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1795
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0327
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.89s
                        Total time: 210.28s
                               ETA: 665 mins 20.4 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 119651 steps/s (collection: 0.700s, learning 0.121s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3182
       Mean episode rew_ang_vel_xy: -0.0539
          Mean episode rew_dof_acc: -0.1432
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1553
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1904
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.82s
                        Total time: 211.10s
                               ETA: 665 mins 23.2 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 118581 steps/s (collection: 0.707s, learning 0.122s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3133
       Mean episode rew_ang_vel_xy: -0.0523
          Mean episode rew_dof_acc: -0.1392
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1510
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1861
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0351
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.83s
                        Total time: 211.93s
                               ETA: 665 mins 27.4 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 111440 steps/s (collection: 0.757s, learning 0.125s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3191
       Mean episode rew_ang_vel_xy: -0.0522
          Mean episode rew_dof_acc: -0.1399
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1896
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0377
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.88s
                        Total time: 212.81s
                               ETA: 665 mins 41.5 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 121625 steps/s (collection: 0.682s, learning 0.126s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3190
       Mean episode rew_ang_vel_xy: -0.0535
          Mean episode rew_dof_acc: -0.1406
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1895
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.81s
                        Total time: 213.62s
                               ETA: 665 mins 41.6 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 117344 steps/s (collection: 0.714s, learning 0.124s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3124
       Mean episode rew_ang_vel_xy: -0.0501
          Mean episode rew_dof_acc: -0.1344
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1482
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1861
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0380
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.84s
                        Total time: 214.46s
                               ETA: 665 mins 47.3 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 118717 steps/s (collection: 0.688s, learning 0.141s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3154
       Mean episode rew_ang_vel_xy: -0.0504
          Mean episode rew_dof_acc: -0.1351
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1418
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1877
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0353
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.83s
                        Total time: 215.29s
                               ETA: 665 mins 51.1 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 114617 steps/s (collection: 0.718s, learning 0.140s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3213
       Mean episode rew_ang_vel_xy: -0.0510
          Mean episode rew_dof_acc: -0.1364
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1912
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.86s
                        Total time: 216.15s
                               ETA: 666 mins 0.3 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 116427 steps/s (collection: 0.704s, learning 0.141s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3201
       Mean episode rew_ang_vel_xy: -0.0537
          Mean episode rew_dof_acc: -0.1412
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1590
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1899
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0369
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.84s
                        Total time: 216.99s
                               ETA: 666 mins 7.1 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 113948 steps/s (collection: 0.741s, learning 0.121s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3243
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.1385
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1534
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1947
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0378
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.86s
                        Total time: 217.85s
                               ETA: 666 mins 17.1 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 109878 steps/s (collection: 0.772s, learning 0.122s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3216
       Mean episode rew_ang_vel_xy: -0.0534
          Mean episode rew_dof_acc: -0.1381
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1543
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1939
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.89s
                        Total time: 218.75s
                               ETA: 666 mins 32.9 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 112009 steps/s (collection: 0.735s, learning 0.143s)
               Value function loss: 0.0655
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3260
       Mean episode rew_ang_vel_xy: -0.0518
          Mean episode rew_dof_acc: -0.1377
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1566
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1954
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.88s
                        Total time: 219.62s
                               ETA: 666 mins 45.4 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 120467 steps/s (collection: 0.692s, learning 0.124s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3238
       Mean episode rew_ang_vel_xy: -0.0545
          Mean episode rew_dof_acc: -0.1396
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1597
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1956
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.82s
                        Total time: 220.44s
                               ETA: 666 mins 46.7 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 115297 steps/s (collection: 0.718s, learning 0.134s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3274
       Mean episode rew_ang_vel_xy: -0.0519
          Mean episode rew_dof_acc: -0.1380
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1979
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0433
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.85s
                        Total time: 221.29s
                               ETA: 666 mins 54.6 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 113892 steps/s (collection: 0.729s, learning 0.134s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3270
       Mean episode rew_ang_vel_xy: -0.0532
          Mean episode rew_dof_acc: -0.1415
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1567
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1980
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0422
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.86s
                        Total time: 222.16s
                               ETA: 667 mins 4.3 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 108324 steps/s (collection: 0.784s, learning 0.123s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3369
       Mean episode rew_ang_vel_xy: -0.0544
          Mean episode rew_dof_acc: -0.1421
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2054
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0404
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.91s
                        Total time: 223.06s
                               ETA: 667 mins 22.0 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 105549 steps/s (collection: 0.787s, learning 0.145s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3333
       Mean episode rew_ang_vel_xy: -0.0530
          Mean episode rew_dof_acc: -0.1416
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2043
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0435
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.93s
                        Total time: 224.00s
                               ETA: 667 mins 43.7 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 115110 steps/s (collection: 0.721s, learning 0.133s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3404
       Mean episode rew_ang_vel_xy: -0.0536
          Mean episode rew_dof_acc: -0.1414
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1531
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2054
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0444
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.85s
                        Total time: 224.85s
                               ETA: 667 mins 51.5 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 110056 steps/s (collection: 0.750s, learning 0.143s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3333
       Mean episode rew_ang_vel_xy: -0.0529
          Mean episode rew_dof_acc: -0.1398
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2038
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0413
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.89s
                        Total time: 225.74s
                               ETA: 668 mins 6.2 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 112650 steps/s (collection: 0.748s, learning 0.125s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.65
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3385
       Mean episode rew_ang_vel_xy: -0.0526
          Mean episode rew_dof_acc: -0.1384
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2053
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0421
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.87s
                        Total time: 226.61s
                               ETA: 668 mins 17.1 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 114850 steps/s (collection: 0.732s, learning 0.124s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3440
       Mean episode rew_ang_vel_xy: -0.0535
          Mean episode rew_dof_acc: -0.1422
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2092
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.86s
                        Total time: 227.47s
                               ETA: 668 mins 25.1 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 113775 steps/s (collection: 0.741s, learning 0.123s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3460
       Mean episode rew_ang_vel_xy: -0.0524
          Mean episode rew_dof_acc: -0.1450
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1541
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2089
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0393
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.86s
                        Total time: 228.33s
                               ETA: 668 mins 34.3 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 105184 steps/s (collection: 0.812s, learning 0.123s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3450
       Mean episode rew_ang_vel_xy: -0.0537
          Mean episode rew_dof_acc: -0.1397
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1567
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.2098
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1998
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0377
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.93s
                        Total time: 229.27s
                               ETA: 668 mins 55.9 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 115076 steps/s (collection: 0.726s, learning 0.129s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3493
       Mean episode rew_ang_vel_xy: -0.0546
          Mean episode rew_dof_acc: -0.1441
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1553
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2137
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0390
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.85s
                        Total time: 230.12s
                               ETA: 669 mins 3.3 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 102086 steps/s (collection: 0.825s, learning 0.138s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.66
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3558
       Mean episode rew_ang_vel_xy: -0.0549
          Mean episode rew_dof_acc: -0.1443
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1551
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2160
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0374
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.96s
                        Total time: 231.09s
                               ETA: 669 mins 29.5 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 107821 steps/s (collection: 0.770s, learning 0.141s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3506
       Mean episode rew_ang_vel_xy: -0.0556
          Mean episode rew_dof_acc: -0.1443
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1570
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2126
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0265
        Mean episode terrain_level: 0.0359
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.91s
                        Total time: 232.00s
                               ETA: 669 mins 46.7 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 107875 steps/s (collection: 0.747s, learning 0.164s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3566
       Mean episode rew_ang_vel_xy: -0.0543
          Mean episode rew_dof_acc: -0.1450
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1547
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2182
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0306
        Mean episode terrain_level: 0.0367
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.91s
                        Total time: 232.91s
                               ETA: 670 mins 3.6 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 102297 steps/s (collection: 0.819s, learning 0.142s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3624
       Mean episode rew_ang_vel_xy: -0.0549
          Mean episode rew_dof_acc: -0.1425
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1537
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2213
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0351
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.96s
                        Total time: 233.87s
                               ETA: 670 mins 29.0 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 112580 steps/s (collection: 0.750s, learning 0.123s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.67
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3874
       Mean episode rew_ang_vel_xy: -0.0556
          Mean episode rew_dof_acc: -0.1504
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1573
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2372
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0327
        Mean episode terrain_level: 0.0340
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.87s
                        Total time: 234.74s
                               ETA: 670 mins 39.1 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 106909 steps/s (collection: 0.788s, learning 0.131s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3556
       Mean episode rew_ang_vel_xy: -0.0533
          Mean episode rew_dof_acc: -0.1407
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1443
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2158
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0331
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.92s
                        Total time: 235.66s
                               ETA: 670 mins 57.1 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 103402 steps/s (collection: 0.822s, learning 0.129s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3774
       Mean episode rew_ang_vel_xy: -0.0558
          Mean episode rew_dof_acc: -0.1494
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1572
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2307
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.95s
                        Total time: 236.61s
                               ETA: 671 mins 20.3 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 110931 steps/s (collection: 0.763s, learning 0.123s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.68
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3651
       Mean episode rew_ang_vel_xy: -0.0530
          Mean episode rew_dof_acc: -0.1447
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1479
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2226
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.89s
                        Total time: 237.50s
                               ETA: 671 mins 32.4 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 112285 steps/s (collection: 0.751s, learning 0.125s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3733
       Mean episode rew_ang_vel_xy: -0.0546
          Mean episode rew_dof_acc: -0.1435
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1585
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2281
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.88s
                        Total time: 238.38s
                               ETA: 671 mins 42.5 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 102235 steps/s (collection: 0.836s, learning 0.126s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3741
       Mean episode rew_ang_vel_xy: -0.0545
          Mean episode rew_dof_acc: -0.1447
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1557
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.2330
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.96s
                        Total time: 239.34s
                               ETA: 672 mins 7.1 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 98279 steps/s (collection: 0.873s, learning 0.127s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3874
       Mean episode rew_ang_vel_xy: -0.0544
          Mean episode rew_dof_acc: -0.1478
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1541
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2380
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0382
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.00s
                        Total time: 240.34s
                               ETA: 672 mins 38.0 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 104848 steps/s (collection: 0.815s, learning 0.123s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.69
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3819
       Mean episode rew_ang_vel_xy: -0.0551
          Mean episode rew_dof_acc: -0.1491
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1541
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2359
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1993
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0409
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.94s
                        Total time: 241.28s
                               ETA: 672 mins 58.2 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 105035 steps/s (collection: 0.795s, learning 0.141s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4083
       Mean episode rew_ang_vel_xy: -0.0557
          Mean episode rew_dof_acc: -0.1507
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2544
      Mean episode rew_stand_still: -0.0020
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0390
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.94s
                        Total time: 242.21s
                               ETA: 673 mins 18.0 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 109072 steps/s (collection: 0.773s, learning 0.128s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4038
       Mean episode rew_ang_vel_xy: -0.0565
          Mean episode rew_dof_acc: -0.1505
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1532
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2511
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1987
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.90s
                        Total time: 243.11s
                               ETA: 673 mins 31.9 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 101235 steps/s (collection: 0.849s, learning 0.122s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4071
       Mean episode rew_ang_vel_xy: -0.0548
          Mean episode rew_dof_acc: -0.1468
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1512
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2525
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1981
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.97s
                        Total time: 244.08s
                               ETA: 673 mins 57.3 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 106847 steps/s (collection: 0.798s, learning 0.122s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3842
       Mean episode rew_ang_vel_xy: -0.0546
          Mean episode rew_dof_acc: -0.1451
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2367
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1991
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0324
        Mean episode terrain_level: 0.0372
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.92s
                        Total time: 245.00s
                               ETA: 674 mins 14.0 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 108404 steps/s (collection: 0.783s, learning 0.123s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.70
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4051
       Mean episode rew_ang_vel_xy: -0.0533
          Mean episode rew_dof_acc: -0.1451
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1508
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2530
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1982
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0368
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.91s
                        Total time: 245.91s
                               ETA: 674 mins 28.5 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 108711 steps/s (collection: 0.762s, learning 0.142s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3951
       Mean episode rew_ang_vel_xy: -0.0554
          Mean episode rew_dof_acc: -0.1482
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1556
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2455
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1986
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0392
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.90s
                        Total time: 246.81s
                               ETA: 674 mins 42.5 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 104078 steps/s (collection: 0.823s, learning 0.121s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4157
       Mean episode rew_ang_vel_xy: -0.0555
          Mean episode rew_dof_acc: -0.1520
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2613
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1974
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.94s
                        Total time: 247.76s
                               ETA: 675 mins 2.9 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 108980 steps/s (collection: 0.780s, learning 0.122s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3916
       Mean episode rew_ang_vel_xy: -0.0540
          Mean episode rew_dof_acc: -0.1444
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2445
      Mean episode rew_stand_still: -0.0017
      Mean episode rew_termination: -0.1992
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0400
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.90s
                        Total time: 248.66s
                               ETA: 675 mins 16.3 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 104482 steps/s (collection: 0.817s, learning 0.124s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.71
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4255
       Mean episode rew_ang_vel_xy: -0.0551
          Mean episode rew_dof_acc: -0.1524
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1463
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.2653
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1979
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0399
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.94s
                        Total time: 249.60s
                               ETA: 675 mins 35.8 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 107365 steps/s (collection: 0.792s, learning 0.123s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.3905
       Mean episode rew_ang_vel_xy: -0.0551
          Mean episode rew_dof_acc: -0.1457
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2402
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1989
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0298
        Mean episode terrain_level: 0.0380
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.92s
                        Total time: 250.52s
                               ETA: 675 mins 51.2 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 99859 steps/s (collection: 0.863s, learning 0.122s)
               Value function loss: 0.0907
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4340
       Mean episode rew_ang_vel_xy: -0.0561
          Mean episode rew_dof_acc: -0.1541
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1470
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.2734
      Mean episode rew_stand_still: -0.0018
      Mean episode rew_termination: -0.1976
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0363
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.98s
                        Total time: 251.50s
                               ETA: 676 mins 17.5 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 94668 steps/s (collection: 0.888s, learning 0.150s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4247
       Mean episode rew_ang_vel_xy: -0.0567
          Mean episode rew_dof_acc: -0.1522
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1592
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2642
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1977
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0358
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.04s
                        Total time: 252.54s
                               ETA: 676 mins 52.4 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 103263 steps/s (collection: 0.821s, learning 0.131s)
               Value function loss: 0.0940
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.72
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4635
       Mean episode rew_ang_vel_xy: -0.0565
          Mean episode rew_dof_acc: -0.1559
   Mean episode rew_dof_pos_limits: -0.0152
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1504
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.2924
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0385
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.95s
                        Total time: 253.49s
                               ETA: 677 mins 13.2 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 100936 steps/s (collection: 0.851s, learning 0.123s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4565
       Mean episode rew_ang_vel_xy: -0.0557
          Mean episode rew_dof_acc: -0.1510
   Mean episode rew_dof_pos_limits: -0.0142
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1433
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.2848
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1970
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0429
        Mean episode terrain_level: 0.0395
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.97s
                        Total time: 254.47s
                               ETA: 677 mins 37.3 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 103162 steps/s (collection: 0.829s, learning 0.124s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4236
       Mean episode rew_ang_vel_xy: -0.0556
          Mean episode rew_dof_acc: -0.1535
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1566
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2643
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1984
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0377
        Mean episode terrain_level: 0.0382
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.95s
                        Total time: 255.42s
                               ETA: 677 mins 57.9 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 105033 steps/s (collection: 0.813s, learning 0.123s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.73
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4503
       Mean episode rew_ang_vel_xy: -0.0558
          Mean episode rew_dof_acc: -0.1547
   Mean episode rew_dof_pos_limits: -0.0145
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.2804
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0394
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.94s
                        Total time: 256.36s
                               ETA: 678 mins 15.7 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 103175 steps/s (collection: 0.829s, learning 0.123s)
               Value function loss: 0.0944
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4698
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.1609
   Mean episode rew_dof_pos_limits: -0.0157
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1609
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.2957
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0500
        Mean episode terrain_level: 0.0370
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.95s
                        Total time: 257.31s
                               ETA: 678 mins 36.1 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 98883 steps/s (collection: 0.866s, learning 0.129s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4856
       Mean episode rew_ang_vel_xy: -0.0574
          Mean episode rew_dof_acc: -0.1560
   Mean episode rew_dof_pos_limits: -0.0164
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1539
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3058
      Mean episode rew_stand_still: -0.0025
      Mean episode rew_termination: -0.1956
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.99s
                        Total time: 258.30s
                               ETA: 679 mins 2.8 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 99952 steps/s (collection: 0.856s, learning 0.128s)
               Value function loss: 0.0932
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.74
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4674
       Mean episode rew_ang_vel_xy: -0.0544
          Mean episode rew_dof_acc: -0.1532
   Mean episode rew_dof_pos_limits: -0.0156
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1472
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.2959
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.98s
                        Total time: 259.29s
                               ETA: 679 mins 27.7 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 94426 steps/s (collection: 0.901s, learning 0.140s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4868
       Mean episode rew_ang_vel_xy: -0.0574
          Mean episode rew_dof_acc: -0.1601
   Mean episode rew_dof_pos_limits: -0.0158
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1476
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3075
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1958
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0536
        Mean episode terrain_level: 0.0375
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 1.04s
                        Total time: 260.33s
                               ETA: 680 mins 1.5 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 102179 steps/s (collection: 0.838s, learning 0.124s)
               Value function loss: 0.0966
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.75
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4895
       Mean episode rew_ang_vel_xy: -0.0573
          Mean episode rew_dof_acc: -0.1609
   Mean episode rew_dof_pos_limits: -0.0157
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1573
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3064
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1968
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0393
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.96s
                        Total time: 261.29s
                               ETA: 680 mins 22.6 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 101253 steps/s (collection: 0.832s, learning 0.139s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4786
       Mean episode rew_ang_vel_xy: -0.0556
          Mean episode rew_dof_acc: -0.1574
   Mean episode rew_dof_pos_limits: -0.0156
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1537
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3016
      Mean episode rew_stand_still: -0.0022
      Mean episode rew_termination: -0.1963
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0406
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.97s
                        Total time: 262.26s
                               ETA: 680 mins 45.1 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 99789 steps/s (collection: 0.848s, learning 0.137s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4768
       Mean episode rew_ang_vel_xy: -0.0572
          Mean episode rew_dof_acc: -0.1505
   Mean episode rew_dof_pos_limits: -0.0164
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1478
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.2990
      Mean episode rew_stand_still: -0.0021
      Mean episode rew_termination: -0.1960
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.99s
                        Total time: 263.24s
                               ETA: 681 mins 9.5 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 94705 steps/s (collection: 0.914s, learning 0.124s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4759
       Mean episode rew_ang_vel_xy: -0.0583
          Mean episode rew_dof_acc: -0.1578
   Mean episode rew_dof_pos_limits: -0.0157
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1562
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.2960
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0398
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 1.04s
                        Total time: 264.28s
                               ETA: 681 mins 42.1 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 100670 steps/s (collection: 0.849s, learning 0.127s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.76
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4703
       Mean episode rew_ang_vel_xy: -0.0552
          Mean episode rew_dof_acc: -0.1489
   Mean episode rew_dof_pos_limits: -0.0162
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.2958
      Mean episode rew_stand_still: -0.0019
      Mean episode rew_termination: -0.1959
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0491
        Mean episode terrain_level: 0.0391
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.98s
                        Total time: 265.26s
                               ETA: 682 mins 4.9 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 97784 steps/s (collection: 0.877s, learning 0.129s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4822
       Mean episode rew_ang_vel_xy: -0.0568
          Mean episode rew_dof_acc: -0.1582
   Mean episode rew_dof_pos_limits: -0.0158
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1584
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3037
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1962
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0370
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 1.01s
                        Total time: 266.26s
                               ETA: 682 mins 32.0 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 104312 steps/s (collection: 0.819s, learning 0.124s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.4841
       Mean episode rew_ang_vel_xy: -0.0588
          Mean episode rew_dof_acc: -0.1545
   Mean episode rew_dof_pos_limits: -0.0156
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1533
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3018
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1965
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0543
        Mean episode terrain_level: 0.0369
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.94s
                        Total time: 267.21s
                               ETA: 682 mins 49.2 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 94707 steps/s (collection: 0.903s, learning 0.134s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.77
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5524
       Mean episode rew_ang_vel_xy: -0.0583
          Mean episode rew_dof_acc: -0.1641
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1542
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3511
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 1.04s
                        Total time: 268.24s
                               ETA: 683 mins 21.0 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 93378 steps/s (collection: 0.907s, learning 0.146s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5475
       Mean episode rew_ang_vel_xy: -0.0595
          Mean episode rew_dof_acc: -0.1645
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1556
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3476
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0413
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 1.05s
                        Total time: 269.30s
                               ETA: 683 mins 54.8 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 95621 steps/s (collection: 0.903s, learning 0.125s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5288
       Mean episode rew_ang_vel_xy: -0.0577
          Mean episode rew_dof_acc: -0.1589
   Mean episode rew_dof_pos_limits: -0.0178
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1520
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3355
      Mean episode rew_stand_still: -0.0024
      Mean episode rew_termination: -0.1946
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0404
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 1.03s
                        Total time: 270.33s
                               ETA: 684 mins 24.7 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 92889 steps/s (collection: 0.926s, learning 0.133s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5067
       Mean episode rew_ang_vel_xy: -0.0560
          Mean episode rew_dof_acc: -0.1569
   Mean episode rew_dof_pos_limits: -0.0173
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3209
      Mean episode rew_stand_still: -0.0016
      Mean episode rew_termination: -0.1954
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0524
        Mean episode terrain_level: 0.0397
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 1.06s
                        Total time: 271.38s
                               ETA: 684 mins 58.9 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 96177 steps/s (collection: 0.884s, learning 0.138s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.78
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5279
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.1595
   Mean episode rew_dof_pos_limits: -0.0183
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1566
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.3344
      Mean episode rew_stand_still: -0.0023
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0394
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 1.02s
                        Total time: 272.41s
                               ETA: 685 mins 27.5 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 97627 steps/s (collection: 0.864s, learning 0.143s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5460
       Mean episode rew_ang_vel_xy: -0.0593
          Mean episode rew_dof_acc: -0.1626
   Mean episode rew_dof_pos_limits: -0.0202
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1570
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.3480
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0385
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 1.01s
                        Total time: 273.41s
                               ETA: 685 mins 53.6 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 104130 steps/s (collection: 0.821s, learning 0.123s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.79
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5578
       Mean episode rew_ang_vel_xy: -0.0600
          Mean episode rew_dof_acc: -0.1639
   Mean episode rew_dof_pos_limits: -0.0205
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1578
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3521
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1939
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0393
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.94s
                        Total time: 274.36s
                               ETA: 686 mins 10.1 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 97361 steps/s (collection: 0.878s, learning 0.132s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5137
       Mean episode rew_ang_vel_xy: -0.0570
          Mean episode rew_dof_acc: -0.1554
   Mean episode rew_dof_pos_limits: -0.0177
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1523
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0036
       Mean episode rew_smoothness: -0.3251
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0400
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 1.01s
                        Total time: 275.37s
                               ETA: 686 mins 36.3 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 104739 steps/s (collection: 0.814s, learning 0.124s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5754
       Mean episode rew_ang_vel_xy: -0.0586
          Mean episode rew_dof_acc: -0.1600
   Mean episode rew_dof_pos_limits: -0.0216
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1497
           Mean episode rew_no_fly: 0.0084
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3648
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1932
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0402
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.94s
                        Total time: 276.31s
                               ETA: 686 mins 51.8 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 98997 steps/s (collection: 0.862s, learning 0.131s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.80
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5621
       Mean episode rew_ang_vel_xy: -0.0615
          Mean episode rew_dof_acc: -0.1663
   Mean episode rew_dof_pos_limits: -0.0191
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1637
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.3605
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1938
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.0400
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.99s
                        Total time: 277.30s
                               ETA: 687 mins 15.2 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 96439 steps/s (collection: 0.896s, learning 0.123s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5850
       Mean episode rew_ang_vel_xy: -0.0586
          Mean episode rew_dof_acc: -0.1596
   Mean episode rew_dof_pos_limits: -0.0212
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1509
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.3749
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0743
        Mean episode terrain_level: 0.0403
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 1.02s
                        Total time: 278.32s
                               ETA: 687 mins 42.4 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 90356 steps/s (collection: 0.955s, learning 0.133s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.81
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5675
       Mean episode rew_ang_vel_xy: -0.0608
          Mean episode rew_dof_acc: -0.1649
   Mean episode rew_dof_pos_limits: -0.0203
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1552
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.3567
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1937
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0618
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 1.09s
                        Total time: 279.41s
                               ETA: 688 mins 19.6 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 96613 steps/s (collection: 0.882s, learning 0.136s)
               Value function loss: 0.0930
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5637
       Mean episode rew_ang_vel_xy: -0.0589
          Mean episode rew_dof_acc: -0.1587
   Mean episode rew_dof_pos_limits: -0.0200
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0037
       Mean episode rew_smoothness: -0.3632
      Mean episode rew_stand_still: -0.0026
      Mean episode rew_termination: -0.1935
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0626
        Mean episode terrain_level: 0.0404
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 1.02s
                        Total time: 280.42s
                               ETA: 688 mins 46.2 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 96997 steps/s (collection: 0.875s, learning 0.138s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5763
       Mean episode rew_ang_vel_xy: -0.0591
          Mean episode rew_dof_acc: -0.1620
   Mean episode rew_dof_pos_limits: -0.0216
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1498
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3688
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1926
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0659
        Mean episode terrain_level: 0.0399
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 1.01s
                        Total time: 281.44s
                               ETA: 689 mins 12.0 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 95782 steps/s (collection: 0.905s, learning 0.122s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.82
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5935
       Mean episode rew_ang_vel_xy: -0.0597
          Mean episode rew_dof_acc: -0.1649
   Mean episode rew_dof_pos_limits: -0.0223
      Mean episode rew_joint_power: -0.0034
        Mean episode rew_lin_vel_z: -0.1534
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3802
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0669
        Mean episode terrain_level: 0.0388
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 1.03s
                        Total time: 282.46s
                               ETA: 689 mins 39.5 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 102624 steps/s (collection: 0.835s, learning 0.123s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5642
       Mean episode rew_ang_vel_xy: -0.0575
          Mean episode rew_dof_acc: -0.1571
   Mean episode rew_dof_pos_limits: -0.0210
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1534
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.3591
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1936
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0595
        Mean episode terrain_level: 0.0387
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.96s
                        Total time: 283.42s
                               ETA: 689 mins 56.9 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 94312 steps/s (collection: 0.915s, learning 0.127s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6078
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.1717
   Mean episode rew_dof_pos_limits: -0.0223
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1554
           Mean episode rew_no_fly: 0.0085
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.3910
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1929
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.0432
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 1.04s
                        Total time: 284.46s
                               ETA: 690 mins 26.5 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 97077 steps/s (collection: 0.891s, learning 0.121s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5877
       Mean episode rew_ang_vel_xy: -0.0609
          Mean episode rew_dof_acc: -0.1619
   Mean episode rew_dof_pos_limits: -0.0217
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3741
      Mean episode rew_stand_still: -0.0029
      Mean episode rew_termination: -0.1930
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0729
        Mean episode terrain_level: 0.0424
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 1.01s
                        Total time: 285.48s
                               ETA: 690 mins 51.6 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 100748 steps/s (collection: 0.853s, learning 0.123s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.83
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5462
       Mean episode rew_ang_vel_xy: -0.0571
          Mean episode rew_dof_acc: -0.1537
   Mean episode rew_dof_pos_limits: -0.0185
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1514
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0035
       Mean episode rew_smoothness: -0.3517
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1951
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0420
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.98s
                        Total time: 286.45s
                               ETA: 691 mins 11.1 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 101667 steps/s (collection: 0.844s, learning 0.123s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5839
       Mean episode rew_ang_vel_xy: -0.0606
          Mean episode rew_dof_acc: -0.1608
   Mean episode rew_dof_pos_limits: -0.0225
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1583
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.3722
      Mean episode rew_stand_still: -0.0039
      Mean episode rew_termination: -0.1927
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0740
        Mean episode terrain_level: 0.0407
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.97s
                        Total time: 287.42s
                               ETA: 691 mins 29.3 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 98368 steps/s (collection: 0.876s, learning 0.123s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.84
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5770
       Mean episode rew_ang_vel_xy: -0.0587
          Mean episode rew_dof_acc: -0.1634
   Mean episode rew_dof_pos_limits: -0.0202
      Mean episode rew_joint_power: -0.0033
        Mean episode rew_lin_vel_z: -0.1578
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.3673
      Mean episode rew_stand_still: -0.0033
      Mean episode rew_termination: -0.1950
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0413
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.00s
                        Total time: 288.42s
                               ETA: 691 mins 52.1 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 97106 steps/s (collection: 0.880s, learning 0.133s)
               Value function loss: 0.0997
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6775
       Mean episode rew_ang_vel_xy: -0.0603
          Mean episode rew_dof_acc: -0.1726
   Mean episode rew_dof_pos_limits: -0.0276
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.4391
      Mean episode rew_stand_still: -0.0032
      Mean episode rew_termination: -0.1892
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0839
        Mean episode terrain_level: 0.0413
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 1.01s
                        Total time: 289.43s
                               ETA: 692 mins 16.5 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 97613 steps/s (collection: 0.884s, learning 0.123s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.85
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.5538
       Mean episode rew_ang_vel_xy: -0.0592
          Mean episode rew_dof_acc: -0.1549
   Mean episode rew_dof_pos_limits: -0.0201
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1562
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0038
       Mean episode rew_smoothness: -0.3512
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1945
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.0415
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 1.01s
                        Total time: 290.44s
                               ETA: 692 mins 40.1 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 101204 steps/s (collection: 0.848s, learning 0.123s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6738
       Mean episode rew_ang_vel_xy: -0.0605
          Mean episode rew_dof_acc: -0.1748
   Mean episode rew_dof_pos_limits: -0.0255
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.4374
      Mean episode rew_stand_still: -0.0035
      Mean episode rew_termination: -0.1911
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.0384
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.97s
                        Total time: 291.41s
                               ETA: 692 mins 58.4 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 91355 steps/s (collection: 0.932s, learning 0.144s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6383
       Mean episode rew_ang_vel_xy: -0.0617
          Mean episode rew_dof_acc: -0.1704
   Mean episode rew_dof_pos_limits: -0.0238
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1610
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0040
       Mean episode rew_smoothness: -0.4116
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1923
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0795
        Mean episode terrain_level: 0.0397
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.08s
                        Total time: 292.48s
                               ETA: 693 mins 31.6 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 89610 steps/s (collection: 0.964s, learning 0.133s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.86
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6892
       Mean episode rew_ang_vel_xy: -0.0601
          Mean episode rew_dof_acc: -0.1732
   Mean episode rew_dof_pos_limits: -0.0272
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.4438
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1897
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0914
        Mean episode terrain_level: 0.0425
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.10s
                        Total time: 293.58s
                               ETA: 694 mins 7.5 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 101416 steps/s (collection: 0.847s, learning 0.122s)
               Value function loss: 0.0974
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7029
       Mean episode rew_ang_vel_xy: -0.0603
          Mean episode rew_dof_acc: -0.1739
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1582
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.4511
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1900
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.0433
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.97s
                        Total time: 294.55s
                               ETA: 694 mins 25.1 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 101388 steps/s (collection: 0.846s, learning 0.124s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.87
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6589
       Mean episode rew_ang_vel_xy: -0.0605
          Mean episode rew_dof_acc: -0.1688
   Mean episode rew_dof_pos_limits: -0.0257
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1599
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.4212
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1911
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.0410
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.97s
                        Total time: 295.52s
                               ETA: 694 mins 42.6 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 91962 steps/s (collection: 0.946s, learning 0.123s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6505
       Mean episode rew_ang_vel_xy: -0.0598
          Mean episode rew_dof_acc: -0.1636
   Mean episode rew_dof_pos_limits: -0.0252
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1576
           Mean episode rew_no_fly: 0.0086
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.4140
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1913
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0807
        Mean episode terrain_level: 0.0408
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.07s
                        Total time: 296.59s
                               ETA: 695 mins 14.1 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 100530 steps/s (collection: 0.854s, learning 0.124s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.88
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6430
       Mean episode rew_ang_vel_xy: -0.0596
          Mean episode rew_dof_acc: -0.1688
   Mean episode rew_dof_pos_limits: -0.0233
      Mean episode rew_joint_power: -0.0035
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0082
      Mean episode rew_orientation: -0.0039
       Mean episode rew_smoothness: -0.4132
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1924
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0785
        Mean episode terrain_level: 0.0386
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.98s
                        Total time: 297.57s
                               ETA: 695 mins 32.5 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 97810 steps/s (collection: 0.881s, learning 0.124s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6896
       Mean episode rew_ang_vel_xy: -0.0605
          Mean episode rew_dof_acc: -0.1711
   Mean episode rew_dof_pos_limits: -0.0270
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1517
           Mean episode rew_no_fly: 0.0089
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.4458
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1903
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0884
        Mean episode terrain_level: 0.0370
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.01s
                        Total time: 298.57s
                               ETA: 695 mins 54.7 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 100513 steps/s (collection: 0.854s, learning 0.124s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7741
       Mean episode rew_ang_vel_xy: -0.0637
          Mean episode rew_dof_acc: -0.1851
   Mean episode rew_dof_pos_limits: -0.0340
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1603
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0053
       Mean episode rew_smoothness: -0.5016
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1857
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0980
        Mean episode terrain_level: 0.0362
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.98s
                        Total time: 299.55s
                               ETA: 696 mins 13.0 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 99241 steps/s (collection: 0.867s, learning 0.124s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.89
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7327
       Mean episode rew_ang_vel_xy: -0.0622
          Mean episode rew_dof_acc: -0.1767
   Mean episode rew_dof_pos_limits: -0.0297
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1580
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.4714
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1883
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0928
        Mean episode terrain_level: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.99s
                        Total time: 300.54s
                               ETA: 696 mins 32.8 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 93224 steps/s (collection: 0.930s, learning 0.125s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.6936
       Mean episode rew_ang_vel_xy: -0.0605
          Mean episode rew_dof_acc: -0.1708
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0036
        Mean episode rew_lin_vel_z: -0.1572
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.4449
      Mean episode rew_stand_still: -0.0037
      Mean episode rew_termination: -0.1897
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0849
        Mean episode terrain_level: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.05s
                        Total time: 301.60s
                               ETA: 697 mins 1.5 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 96139 steps/s (collection: 0.878s, learning 0.145s)
               Value function loss: 0.1020
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.90
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7589
       Mean episode rew_ang_vel_xy: -0.0636
          Mean episode rew_dof_acc: -0.1796
   Mean episode rew_dof_pos_limits: -0.0309
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1535
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0047
       Mean episode rew_smoothness: -0.4953
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1884
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.0374
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.02s
                        Total time: 302.62s
                               ETA: 697 mins 25.5 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 98504 steps/s (collection: 0.876s, learning 0.122s)
               Value function loss: 0.0866
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7599
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.1755
   Mean episode rew_dof_pos_limits: -0.0322
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1485
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.4946
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1870
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.1004
        Mean episode terrain_level: 0.0371
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.00s
                        Total time: 303.62s
                               ETA: 697 mins 46.1 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 99727 steps/s (collection: 0.860s, learning 0.125s)
               Value function loss: 0.0917
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7432
       Mean episode rew_ang_vel_xy: -0.0650
          Mean episode rew_dof_acc: -0.1836
   Mean episode rew_dof_pos_limits: -0.0293
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1654
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.4847
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1880
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0939
        Mean episode terrain_level: 0.0380
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.99s
                        Total time: 304.60s
                               ETA: 698 mins 4.8 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 95005 steps/s (collection: 0.910s, learning 0.124s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.91
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7688
       Mean episode rew_ang_vel_xy: -0.0644
          Mean episode rew_dof_acc: -0.1865
   Mean episode rew_dof_pos_limits: -0.0301
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1626
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.4990
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1873
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.1033
        Mean episode terrain_level: 0.0370
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.03s
                        Total time: 305.64s
                               ETA: 698 mins 30.2 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 92585 steps/s (collection: 0.938s, learning 0.124s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7265
       Mean episode rew_ang_vel_xy: -0.0603
          Mean episode rew_dof_acc: -0.1782
   Mean episode rew_dof_pos_limits: -0.0280
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1520
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0043
       Mean episode rew_smoothness: -0.4737
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1895
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0957
        Mean episode terrain_level: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.06s
                        Total time: 306.70s
                               ETA: 698 mins 59.0 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 99413 steps/s (collection: 0.864s, learning 0.124s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.92
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7401
       Mean episode rew_ang_vel_xy: -0.0666
          Mean episode rew_dof_acc: -0.1816
   Mean episode rew_dof_pos_limits: -0.0292
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1688
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.4752
      Mean episode rew_stand_still: -0.0030
      Mean episode rew_termination: -0.1898
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0860
        Mean episode terrain_level: 0.0392
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.99s
                        Total time: 307.69s
                               ETA: 699 mins 17.8 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 95116 steps/s (collection: 0.902s, learning 0.131s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7335
       Mean episode rew_ang_vel_xy: -0.0637
          Mean episode rew_dof_acc: -0.1746
   Mean episode rew_dof_pos_limits: -0.0304
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1609
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0049
       Mean episode rew_smoothness: -0.4755
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1879
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0978
        Mean episode terrain_level: 0.0385
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.03s
                        Total time: 308.72s
                               ETA: 699 mins 42.6 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 98267 steps/s (collection: 0.877s, learning 0.123s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7700
       Mean episode rew_ang_vel_xy: -0.0657
          Mean episode rew_dof_acc: -0.1867
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1645
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.5025
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1875
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0955
        Mean episode terrain_level: 0.0381
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.00s
                        Total time: 309.72s
                               ETA: 700 mins 2.7 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 92279 steps/s (collection: 0.942s, learning 0.124s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.93
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7375
       Mean episode rew_ang_vel_xy: -0.0620
          Mean episode rew_dof_acc: -0.1772
   Mean episode rew_dof_pos_limits: -0.0277
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1443
           Mean episode rew_no_fly: 0.0090
      Mean episode rew_orientation: -0.0042
       Mean episode rew_smoothness: -0.4822
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1892
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0952
        Mean episode terrain_level: 0.0369
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.07s
                        Total time: 310.79s
                               ETA: 700 mins 31.5 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 89165 steps/s (collection: 0.981s, learning 0.122s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.94
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7608
       Mean episode rew_ang_vel_xy: -0.0623
          Mean episode rew_dof_acc: -0.1840
   Mean episode rew_dof_pos_limits: -0.0289
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1528
           Mean episode rew_no_fly: 0.0092
      Mean episode rew_orientation: -0.0044
       Mean episode rew_smoothness: -0.4968
      Mean episode rew_stand_still: -0.0036
      Mean episode rew_termination: -0.1886
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0881
        Mean episode terrain_level: 0.0373
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.10s
                        Total time: 311.89s
                               ETA: 701 mins 5.1 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 96482 steps/s (collection: 0.887s, learning 0.132s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.94
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7820
       Mean episode rew_ang_vel_xy: -0.0636
          Mean episode rew_dof_acc: -0.1755
   Mean episode rew_dof_pos_limits: -0.0332
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1551
           Mean episode rew_no_fly: 0.0099
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.5085
      Mean episode rew_stand_still: -0.0028
      Mean episode rew_termination: -0.1871
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.0367
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.02s
                        Total time: 312.91s
                               ETA: 701 mins 27.3 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 91967 steps/s (collection: 0.943s, learning 0.126s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 100.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7637
       Mean episode rew_ang_vel_xy: -0.0636
          Mean episode rew_dof_acc: -0.1826
   Mean episode rew_dof_pos_limits: -0.0289
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1495
           Mean episode rew_no_fly: 0.0088
      Mean episode rew_orientation: -0.0041
       Mean episode rew_smoothness: -0.4998
      Mean episode rew_stand_still: -0.0027
      Mean episode rew_termination: -0.1888
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.07s
                        Total time: 313.98s
                               ETA: 701 mins 56.1 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 87380 steps/s (collection: 0.974s, learning 0.151s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7944
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.1855
   Mean episode rew_dof_pos_limits: -0.0316
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1580
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.5154
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1879
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.1073
        Mean episode terrain_level: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.13s
                        Total time: 315.10s
                               ETA: 702 mins 32.2 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 95631 steps/s (collection: 0.904s, learning 0.123s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8973
       Mean episode rew_ang_vel_xy: -0.0671
          Mean episode rew_dof_acc: -0.2001
   Mean episode rew_dof_pos_limits: -0.0384
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1591
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.5903
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1834
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1177
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.03s
                        Total time: 316.13s
                               ETA: 702 mins 55.2 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 93335 steps/s (collection: 0.913s, learning 0.140s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.95
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7449
       Mean episode rew_ang_vel_xy: -0.0613
          Mean episode rew_dof_acc: -0.1687
   Mean episode rew_dof_pos_limits: -0.0298
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1526
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.4845
      Mean episode rew_stand_still: -0.0034
      Mean episode rew_termination: -0.1883
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0854
        Mean episode terrain_level: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.05s
                        Total time: 317.18s
                               ETA: 703 mins 21.4 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 90417 steps/s (collection: 0.956s, learning 0.131s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7871
       Mean episode rew_ang_vel_xy: -0.0675
          Mean episode rew_dof_acc: -0.1849
   Mean episode rew_dof_pos_limits: -0.0307
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1521
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.5099
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1879
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0954
        Mean episode terrain_level: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.09s
                        Total time: 318.27s
                               ETA: 703 mins 52.0 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 94649 steps/s (collection: 0.915s, learning 0.124s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8872
       Mean episode rew_ang_vel_xy: -0.0640
          Mean episode rew_dof_acc: -0.1893
   Mean episode rew_dof_pos_limits: -0.0390
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -0.5875
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1846
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1275
        Mean episode terrain_level: 0.0340
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.04s
                        Total time: 319.31s
                               ETA: 704 mins 16.0 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 86851 steps/s (collection: 0.990s, learning 0.142s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.96
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7858
       Mean episode rew_ang_vel_xy: -0.0620
          Mean episode rew_dof_acc: -0.1765
   Mean episode rew_dof_pos_limits: -0.0299
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1584
           Mean episode rew_no_fly: 0.0093
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.5131
      Mean episode rew_stand_still: -0.0042
      Mean episode rew_termination: -0.1885
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0934
        Mean episode terrain_level: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.13s
                        Total time: 320.44s
                               ETA: 704 mins 52.1 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 95832 steps/s (collection: 0.902s, learning 0.124s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9028
       Mean episode rew_ang_vel_xy: -0.0657
          Mean episode rew_dof_acc: -0.1892
   Mean episode rew_dof_pos_limits: -0.0407
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1515
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -0.5960
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1823
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0255
 Mean episode rew_tracking_lin_vel: 0.1211
        Mean episode terrain_level: 0.0353
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.03s
                        Total time: 321.47s
                               ETA: 705 mins 14.1 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 90212 steps/s (collection: 0.950s, learning 0.140s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7868
       Mean episode rew_ang_vel_xy: -0.0615
          Mean episode rew_dof_acc: -0.1770
   Mean episode rew_dof_pos_limits: -0.0311
      Mean episode rew_joint_power: -0.0038
        Mean episode rew_lin_vel_z: -0.1483
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0046
       Mean episode rew_smoothness: -0.5136
      Mean episode rew_stand_still: -0.0045
      Mean episode rew_termination: -0.1877
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0916
        Mean episode terrain_level: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.09s
                        Total time: 322.56s
                               ETA: 705 mins 44.4 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 91205 steps/s (collection: 0.939s, learning 0.138s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.97
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8662
       Mean episode rew_ang_vel_xy: -0.0629
          Mean episode rew_dof_acc: -0.1819
   Mean episode rew_dof_pos_limits: -0.0372
      Mean episode rew_joint_power: -0.0039
        Mean episode rew_lin_vel_z: -0.1488
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.5690
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1847
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1137
        Mean episode terrain_level: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.08s
                        Total time: 323.63s
                               ETA: 706 mins 12.9 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 90957 steps/s (collection: 0.947s, learning 0.134s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8832
       Mean episode rew_ang_vel_xy: -0.0656
          Mean episode rew_dof_acc: -0.1934
   Mean episode rew_dof_pos_limits: -0.0354
      Mean episode rew_joint_power: -0.0043
        Mean episode rew_lin_vel_z: -0.1594
           Mean episode rew_no_fly: 0.0100
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.5829
      Mean episode rew_stand_still: -0.0052
      Mean episode rew_termination: -0.1846
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.1172
        Mean episode terrain_level: 0.0332
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.08s
                        Total time: 324.71s
                               ETA: 706 mins 41.7 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 89300 steps/s (collection: 0.976s, learning 0.125s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.98
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8600
       Mean episode rew_ang_vel_xy: -0.0641
          Mean episode rew_dof_acc: -0.1889
   Mean episode rew_dof_pos_limits: -0.0339
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1538
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.5668
      Mean episode rew_stand_still: -0.0046
      Mean episode rew_termination: -0.1858
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.1067
        Mean episode terrain_level: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.10s
                        Total time: 325.81s
                               ETA: 707 mins 12.9 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 88469 steps/s (collection: 0.972s, learning 0.139s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8552
       Mean episode rew_ang_vel_xy: -0.0686
          Mean episode rew_dof_acc: -0.1897
   Mean episode rew_dof_pos_limits: -0.0339
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1643
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.5598
      Mean episode rew_stand_still: -0.0038
      Mean episode rew_termination: -0.1857
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.1048
        Mean episode terrain_level: 0.0360
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.11s
                        Total time: 326.93s
                               ETA: 707 mins 45.3 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 91298 steps/s (collection: 0.929s, learning 0.148s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.99
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9773
       Mean episode rew_ang_vel_xy: -0.0705
          Mean episode rew_dof_acc: -0.2063
   Mean episode rew_dof_pos_limits: -0.0394
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1662
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.6477
      Mean episode rew_stand_still: -0.0048
      Mean episode rew_termination: -0.1810
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1230
        Mean episode terrain_level: 0.0379
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.08s
                        Total time: 328.00s
                               ETA: 708 mins 13.1 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 87941 steps/s (collection: 0.992s, learning 0.126s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.00
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9212
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.1861
   Mean episode rew_dof_pos_limits: -0.0395
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1554
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.6073
      Mean episode rew_stand_still: -0.0065
      Mean episode rew_termination: -0.1828
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.1289
        Mean episode terrain_level: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.12s
                        Total time: 329.12s
                               ETA: 708 mins 46.0 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 96212 steps/s (collection: 0.884s, learning 0.138s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.00
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.7985
       Mean episode rew_ang_vel_xy: -0.0639
          Mean episode rew_dof_acc: -0.1764
   Mean episode rew_dof_pos_limits: -0.0300
      Mean episode rew_joint_power: -0.0037
        Mean episode rew_lin_vel_z: -0.1496
           Mean episode rew_no_fly: 0.0091
      Mean episode rew_orientation: -0.0045
       Mean episode rew_smoothness: -0.5193
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1895
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0869
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 1.02s
                        Total time: 330.14s
                               ETA: 709 mins 6.4 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 95282 steps/s (collection: 0.908s, learning 0.124s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9156
       Mean episode rew_ang_vel_xy: -0.0631
          Mean episode rew_dof_acc: -0.1888
   Mean episode rew_dof_pos_limits: -0.0382
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1471
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0055
       Mean episode rew_smoothness: -0.6041
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1848
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1134
        Mean episode terrain_level: 0.0330
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.03s
                        Total time: 331.17s
                               ETA: 709 mins 27.9 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 93960 steps/s (collection: 0.922s, learning 0.124s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.01
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.8900
       Mean episode rew_ang_vel_xy: -0.0643
          Mean episode rew_dof_acc: -0.1910
   Mean episode rew_dof_pos_limits: -0.0347
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1599
           Mean episode rew_no_fly: 0.0097
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.5822
      Mean episode rew_stand_still: -0.0040
      Mean episode rew_termination: -0.1855
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.1036
        Mean episode terrain_level: 0.0326
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.05s
                        Total time: 332.22s
                               ETA: 709 mins 51.2 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 96382 steps/s (collection: 0.896s, learning 0.124s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 197.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0524
       Mean episode rew_ang_vel_xy: -0.0678
          Mean episode rew_dof_acc: -0.2053
   Mean episode rew_dof_pos_limits: -0.0467
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1525
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.7050
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1792
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0295
 Mean episode rew_tracking_lin_vel: 0.1496
        Mean episode terrain_level: 0.0332
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.02s
                        Total time: 333.24s
                               ETA: 710 mins 11.0 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 92064 steps/s (collection: 0.944s, learning 0.124s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.02
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9212
       Mean episode rew_ang_vel_xy: -0.0618
          Mean episode rew_dof_acc: -0.1816
   Mean episode rew_dof_pos_limits: -0.0399
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1568
           Mean episode rew_no_fly: 0.0106
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -0.6086
      Mean episode rew_stand_still: -0.0072
      Mean episode rew_termination: -0.1849
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1203
        Mean episode terrain_level: 0.0327
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 1.07s
                        Total time: 334.31s
                               ETA: 710 mins 36.7 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 85469 steps/s (collection: 1.004s, learning 0.146s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9021
       Mean episode rew_ang_vel_xy: -0.0648
          Mean episode rew_dof_acc: -0.1921
   Mean episode rew_dof_pos_limits: -0.0336
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1457
           Mean episode rew_no_fly: 0.0094
      Mean episode rew_orientation: -0.0048
       Mean episode rew_smoothness: -0.5943
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1865
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.1097
        Mean episode terrain_level: 0.0329
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 1.15s
                        Total time: 335.46s
                               ETA: 711 mins 12.9 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 91825 steps/s (collection: 0.947s, learning 0.123s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.03
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0857
       Mean episode rew_ang_vel_xy: -0.0671
          Mean episode rew_dof_acc: -0.2040
   Mean episode rew_dof_pos_limits: -0.0491
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1552
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -0.7241
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1782
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0318
 Mean episode rew_tracking_lin_vel: 0.1501
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 1.07s
                        Total time: 336.53s
                               ETA: 711 mins 38.7 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 89259 steps/s (collection: 0.976s, learning 0.126s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.04
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9391
       Mean episode rew_ang_vel_xy: -0.0651
          Mean episode rew_dof_acc: -0.1812
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1547
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.6194
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1837
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1171
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 1.10s
                        Total time: 337.63s
                               ETA: 712 mins 8.3 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 88853 steps/s (collection: 0.983s, learning 0.123s)
               Value function loss: 0.0844
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0647
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.2045
   Mean episode rew_dof_pos_limits: -0.0453
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1550
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -0.7051
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1801
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0283
 Mean episode rew_tracking_lin_vel: 0.1413
        Mean episode terrain_level: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 1.11s
                        Total time: 338.74s
                               ETA: 712 mins 38.4 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 89106 steps/s (collection: 0.976s, learning 0.127s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.05
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0104
       Mean episode rew_ang_vel_xy: -0.0659
          Mean episode rew_dof_acc: -0.1868
   Mean episode rew_dof_pos_limits: -0.0459
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1572
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -0.6737
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1808
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0287
 Mean episode rew_tracking_lin_vel: 0.1528
        Mean episode terrain_level: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.10s
                        Total time: 339.84s
                               ETA: 713 mins 7.9 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 91347 steps/s (collection: 0.924s, learning 0.152s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0738
       Mean episode rew_ang_vel_xy: -0.0688
          Mean episode rew_dof_acc: -0.2077
   Mean episode rew_dof_pos_limits: -0.0458
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1623
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -0.7205
      Mean episode rew_stand_still: -0.0060
      Mean episode rew_termination: -0.1794
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0294
 Mean episode rew_tracking_lin_vel: 0.1361
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 1.08s
                        Total time: 340.92s
                               ETA: 713 mins 33.9 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 92199 steps/s (collection: 0.926s, learning 0.141s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.06
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9658
       Mean episode rew_ang_vel_xy: -0.0669
          Mean episode rew_dof_acc: -0.1859
   Mean episode rew_dof_pos_limits: -0.0380
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1475
           Mean episode rew_no_fly: 0.0098
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.6300
      Mean episode rew_stand_still: -0.0058
      Mean episode rew_termination: -0.1835
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1265
        Mean episode terrain_level: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.07s
                        Total time: 341.98s
                               ETA: 713 mins 58.4 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 90763 steps/s (collection: 0.943s, learning 0.140s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.07
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0022
       Mean episode rew_ang_vel_xy: -0.0667
          Mean episode rew_dof_acc: -0.1868
   Mean episode rew_dof_pos_limits: -0.0436
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1624
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -0.6646
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1822
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0269
 Mean episode rew_tracking_lin_vel: 0.1252
        Mean episode terrain_level: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 1.08s
                        Total time: 343.07s
                               ETA: 714 mins 25.0 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 92756 steps/s (collection: 0.937s, learning 0.123s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1274
       Mean episode rew_ang_vel_xy: -0.0676
          Mean episode rew_dof_acc: -0.2080
   Mean episode rew_dof_pos_limits: -0.0505
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1524
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -0.7555
      Mean episode rew_stand_still: -0.0076
      Mean episode rew_termination: -0.1777
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0315
 Mean episode rew_tracking_lin_vel: 0.1595
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 1.06s
                        Total time: 344.12s
                               ETA: 714 mins 48.5 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 94255 steps/s (collection: 0.920s, learning 0.123s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0619
       Mean episode rew_ang_vel_xy: -0.0685
          Mean episode rew_dof_acc: -0.2023
   Mean episode rew_dof_pos_limits: -0.0446
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1583
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -0.7025
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1791
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1482
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.04s
                        Total time: 345.17s
                               ETA: 715 mins 9.8 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 90031 steps/s (collection: 0.960s, learning 0.131s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.08
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0181
       Mean episode rew_ang_vel_xy: -0.0667
          Mean episode rew_dof_acc: -0.1920
   Mean episode rew_dof_pos_limits: -0.0404
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1617
           Mean episode rew_no_fly: 0.0103
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.6758
      Mean episode rew_stand_still: -0.0043
      Mean episode rew_termination: -0.1822
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0254
 Mean episode rew_tracking_lin_vel: 0.1162
        Mean episode terrain_level: 0.0326
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 1.09s
                        Total time: 346.26s
                               ETA: 715 mins 37.1 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 87652 steps/s (collection: 0.967s, learning 0.155s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.09
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1326
       Mean episode rew_ang_vel_xy: -0.0712
          Mean episode rew_dof_acc: -0.2059
   Mean episode rew_dof_pos_limits: -0.0479
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1642
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -0.7542
      Mean episode rew_stand_still: -0.0077
      Mean episode rew_termination: -0.1777
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0304
 Mean episode rew_tracking_lin_vel: 0.1569
        Mean episode terrain_level: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 1.12s
                        Total time: 347.38s
                               ETA: 716 mins 7.9 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 93238 steps/s (collection: 0.919s, learning 0.135s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0869
       Mean episode rew_ang_vel_xy: -0.0695
          Mean episode rew_dof_acc: -0.1993
   Mean episode rew_dof_pos_limits: -0.0438
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1605
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0057
       Mean episode rew_smoothness: -0.7237
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1799
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1408
        Mean episode terrain_level: 0.0347
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 1.05s
                        Total time: 348.44s
                               ETA: 716 mins 30.2 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 94841 steps/s (collection: 0.915s, learning 0.122s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1393
       Mean episode rew_ang_vel_xy: -0.0678
          Mean episode rew_dof_acc: -0.2060
   Mean episode rew_dof_pos_limits: -0.0490
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1592
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.7664
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1785
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0307
 Mean episode rew_tracking_lin_vel: 0.1444
        Mean episode terrain_level: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.04s
                        Total time: 349.47s
                               ETA: 716 mins 50.2 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 87516 steps/s (collection: 0.996s, learning 0.128s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.10
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1051
       Mean episode rew_ang_vel_xy: -0.0663
          Mean episode rew_dof_acc: -0.1921
   Mean episode rew_dof_pos_limits: -0.0473
      Mean episode rew_joint_power: -0.0044
        Mean episode rew_lin_vel_z: -0.1549
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -0.7322
      Mean episode rew_stand_still: -0.0056
      Mean episode rew_termination: -0.1793
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0295
 Mean episode rew_tracking_lin_vel: 0.1407
        Mean episode terrain_level: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.12s
                        Total time: 350.60s
                               ETA: 717 mins 20.8 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 88126 steps/s (collection: 0.993s, learning 0.122s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.11
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0010
       Mean episode rew_ang_vel_xy: -0.0674
          Mean episode rew_dof_acc: -0.1856
   Mean episode rew_dof_pos_limits: -0.0412
      Mean episode rew_joint_power: -0.0041
        Mean episode rew_lin_vel_z: -0.1650
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -0.6612
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1818
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0256
 Mean episode rew_tracking_lin_vel: 0.1225
        Mean episode terrain_level: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 1.12s
                        Total time: 351.71s
                               ETA: 717 mins 50.3 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 91383 steps/s (collection: 0.952s, learning 0.124s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1598
       Mean episode rew_ang_vel_xy: -0.0701
          Mean episode rew_dof_acc: -0.2154
   Mean episode rew_dof_pos_limits: -0.0472
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1645
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -0.7757
      Mean episode rew_stand_still: -0.0077
      Mean episode rew_termination: -0.1783
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0291
 Mean episode rew_tracking_lin_vel: 0.1456
        Mean episode terrain_level: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 1.08s
                        Total time: 352.79s
                               ETA: 718 mins 14.7 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 95720 steps/s (collection: 0.904s, learning 0.123s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.12
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.9651
       Mean episode rew_ang_vel_xy: -0.0658
          Mean episode rew_dof_acc: -0.1819
   Mean episode rew_dof_pos_limits: -0.0362
      Mean episode rew_joint_power: -0.0040
        Mean episode rew_lin_vel_z: -0.1616
           Mean episode rew_no_fly: 0.0096
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.6292
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1853
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.1064
        Mean episode terrain_level: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.03s
                        Total time: 353.81s
                               ETA: 718 mins 33.1 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 95107 steps/s (collection: 0.910s, learning 0.123s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.13
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1483
       Mean episode rew_ang_vel_xy: -0.0702
          Mean episode rew_dof_acc: -0.2084
   Mean episode rew_dof_pos_limits: -0.0482
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1627
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.7703
      Mean episode rew_stand_still: -0.0072
      Mean episode rew_termination: -0.1786
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0298
 Mean episode rew_tracking_lin_vel: 0.1474
        Mean episode terrain_level: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 1.03s
                        Total time: 354.85s
                               ETA: 718 mins 52.2 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 90020 steps/s (collection: 0.963s, learning 0.129s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0632
       Mean episode rew_ang_vel_xy: -0.0686
          Mean episode rew_dof_acc: -0.2005
   Mean episode rew_dof_pos_limits: -0.0396
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1733
           Mean episode rew_no_fly: 0.0105
      Mean episode rew_orientation: -0.0054
       Mean episode rew_smoothness: -0.7059
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1827
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0257
 Mean episode rew_tracking_lin_vel: 0.1239
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 1.09s
                        Total time: 355.94s
                               ETA: 719 mins 18.3 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 94661 steps/s (collection: 0.911s, learning 0.127s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.14
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2210
       Mean episode rew_ang_vel_xy: -0.0737
          Mean episode rew_dof_acc: -0.2179
   Mean episode rew_dof_pos_limits: -0.0487
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1697
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -0.8106
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1771
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0307
 Mean episode rew_tracking_lin_vel: 0.1507
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.04s
                        Total time: 356.98s
                               ETA: 719 mins 37.8 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 92241 steps/s (collection: 0.942s, learning 0.123s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.15
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0593
       Mean episode rew_ang_vel_xy: -0.0661
          Mean episode rew_dof_acc: -0.1885
   Mean episode rew_dof_pos_limits: -0.0413
      Mean episode rew_joint_power: -0.0042
        Mean episode rew_lin_vel_z: -0.1560
           Mean episode rew_no_fly: 0.0104
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -0.6944
      Mean episode rew_stand_still: -0.0055
      Mean episode rew_termination: -0.1822
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1302
        Mean episode terrain_level: 0.0316
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 1.07s
                        Total time: 358.04s
                               ETA: 720 mins 0.4 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 89859 steps/s (collection: 0.971s, learning 0.123s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2282
       Mean episode rew_ang_vel_xy: -0.0760
          Mean episode rew_dof_acc: -0.2240
   Mean episode rew_dof_pos_limits: -0.0469
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1552
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0058
       Mean episode rew_smoothness: -0.8146
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1764
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0302
 Mean episode rew_tracking_lin_vel: 0.1673
        Mean episode terrain_level: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.09s
                        Total time: 359.14s
                               ETA: 720 mins 26.4 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 89262 steps/s (collection: 0.979s, learning 0.123s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.16
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.0805
       Mean episode rew_ang_vel_xy: -0.0702
          Mean episode rew_dof_acc: -0.2048
   Mean episode rew_dof_pos_limits: -0.0398
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1669
           Mean episode rew_no_fly: 0.0102
      Mean episode rew_orientation: -0.0052
       Mean episode rew_smoothness: -0.7136
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1825
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1239
        Mean episode terrain_level: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.10s
                        Total time: 360.24s
                               ETA: 720 mins 53.1 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 88324 steps/s (collection: 0.991s, learning 0.122s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.1271
       Mean episode rew_ang_vel_xy: -0.0733
          Mean episode rew_dof_acc: -0.1964
   Mean episode rew_dof_pos_limits: -0.0439
      Mean episode rew_joint_power: -0.0045
        Mean episode rew_lin_vel_z: -0.1673
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0059
       Mean episode rew_smoothness: -0.7484
      Mean episode rew_stand_still: -0.0068
      Mean episode rew_termination: -0.1794
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0282
 Mean episode rew_tracking_lin_vel: 0.1292
        Mean episode terrain_level: 0.0320
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.11s
                        Total time: 361.35s
                               ETA: 721 mins 21.0 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 86997 steps/s (collection: 1.007s, learning 0.123s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.17
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2547
       Mean episode rew_ang_vel_xy: -0.0750
          Mean episode rew_dof_acc: -0.2088
   Mean episode rew_dof_pos_limits: -0.0517
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1655
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -0.8378
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1753
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0313
 Mean episode rew_tracking_lin_vel: 0.1534
        Mean episode terrain_level: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 1.13s
                        Total time: 362.48s
                               ETA: 721 mins 50.9 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 89483 steps/s (collection: 0.976s, learning 0.123s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3190
       Mean episode rew_ang_vel_xy: -0.0716
          Mean episode rew_dof_acc: -0.2093
   Mean episode rew_dof_pos_limits: -0.0577
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1611
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -0.8828
      Mean episode rew_stand_still: -0.0080
      Mean episode rew_termination: -0.1730
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0350
 Mean episode rew_tracking_lin_vel: 0.1735
        Mean episode terrain_level: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 1.10s
                        Total time: 363.58s
                               ETA: 722 mins 16.8 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 90277 steps/s (collection: 0.965s, learning 0.124s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.18
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2093
       Mean episode rew_ang_vel_xy: -0.0690
          Mean episode rew_dof_acc: -0.2009
   Mean episode rew_dof_pos_limits: -0.0490
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1639
           Mean episode rew_no_fly: 0.0118
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -0.7991
      Mean episode rew_stand_still: -0.0049
      Mean episode rew_termination: -0.1779
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0300
 Mean episode rew_tracking_lin_vel: 0.1591
        Mean episode terrain_level: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.09s
                        Total time: 364.67s
                               ETA: 722 mins 41.5 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 89384 steps/s (collection: 0.962s, learning 0.137s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.19
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2489
       Mean episode rew_ang_vel_xy: -0.0743
          Mean episode rew_dof_acc: -0.2124
   Mean episode rew_dof_pos_limits: -0.0520
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1840
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -0.8281
      Mean episode rew_stand_still: -0.0057
      Mean episode rew_termination: -0.1755
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0320
 Mean episode rew_tracking_lin_vel: 0.1595
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 1.10s
                        Total time: 365.77s
                               ETA: 723 mins 7.4 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 90425 steps/s (collection: 0.964s, learning 0.123s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3967
       Mean episode rew_ang_vel_xy: -0.0781
          Mean episode rew_dof_acc: -0.2303
   Mean episode rew_dof_pos_limits: -0.0547
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1659
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0068
       Mean episode rew_smoothness: -0.9361
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1731
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0351
 Mean episode rew_tracking_lin_vel: 0.1875
        Mean episode terrain_level: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 1.09s
                        Total time: 366.86s
                               ETA: 723 mins 31.6 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 90856 steps/s (collection: 0.959s, learning 0.123s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.20
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3430
       Mean episode rew_ang_vel_xy: -0.0769
          Mean episode rew_dof_acc: -0.2295
   Mean episode rew_dof_pos_limits: -0.0516
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1634
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.8983
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1745
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0331
 Mean episode rew_tracking_lin_vel: 0.1696
        Mean episode terrain_level: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 1.08s
                        Total time: 367.94s
                               ETA: 723 mins 55.1 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 87365 steps/s (collection: 1.002s, learning 0.123s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.21
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4292
       Mean episode rew_ang_vel_xy: -0.0761
          Mean episode rew_dof_acc: -0.2328
   Mean episode rew_dof_pos_limits: -0.0609
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1655
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -0.9641
      Mean episode rew_stand_still: -0.0090
      Mean episode rew_termination: -0.1707
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0368
 Mean episode rew_tracking_lin_vel: 0.1978
        Mean episode terrain_level: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 1.13s
                        Total time: 369.06s
                               ETA: 724 mins 23.6 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 88730 steps/s (collection: 0.968s, learning 0.140s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4374
       Mean episode rew_ang_vel_xy: -0.0771
          Mean episode rew_dof_acc: -0.2317
   Mean episode rew_dof_pos_limits: -0.0581
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1595
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -0.9724
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1709
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0365
 Mean episode rew_tracking_lin_vel: 0.1699
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.11s
                        Total time: 370.17s
                               ETA: 724 mins 49.8 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 86575 steps/s (collection: 1.002s, learning 0.134s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.22
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2831
       Mean episode rew_ang_vel_xy: -0.0719
          Mean episode rew_dof_acc: -0.2055
   Mean episode rew_dof_pos_limits: -0.0531
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1742
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -0.8551
      Mean episode rew_stand_still: -0.0050
      Mean episode rew_termination: -0.1767
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0330
 Mean episode rew_tracking_lin_vel: 0.1463
        Mean episode terrain_level: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 1.14s
                        Total time: 371.31s
                               ETA: 725 mins 19.2 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 92268 steps/s (collection: 0.925s, learning 0.141s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.23
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3731
       Mean episode rew_ang_vel_xy: -0.0768
          Mean episode rew_dof_acc: -0.2259
   Mean episode rew_dof_pos_limits: -0.0518
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1527
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -0.9218
      Mean episode rew_stand_still: -0.0053
      Mean episode rew_termination: -0.1748
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0325
 Mean episode rew_tracking_lin_vel: 0.1537
        Mean episode terrain_level: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 1.07s
                        Total time: 372.37s
                               ETA: 725 mins 40.3 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 84249 steps/s (collection: 1.022s, learning 0.145s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.24
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3352
       Mean episode rew_ang_vel_xy: -0.0761
          Mean episode rew_dof_acc: -0.2173
   Mean episode rew_dof_pos_limits: -0.0513
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1647
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -0.8931
      Mean episode rew_stand_still: -0.0078
      Mean episode rew_termination: -0.1761
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0324
 Mean episode rew_tracking_lin_vel: 0.1635
        Mean episode terrain_level: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.17s
                        Total time: 373.54s
                               ETA: 726 mins 13.1 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 91534 steps/s (collection: 0.933s, learning 0.141s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.24
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4192
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.2226
   Mean episode rew_dof_pos_limits: -0.0552
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1598
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -0.9565
      Mean episode rew_stand_still: -0.0089
      Mean episode rew_termination: -0.1732
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0348
 Mean episode rew_tracking_lin_vel: 0.1844
        Mean episode terrain_level: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 1.07s
                        Total time: 374.61s
                               ETA: 726 mins 34.9 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 84985 steps/s (collection: 1.015s, learning 0.141s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4623
       Mean episode rew_ang_vel_xy: -0.0800
          Mean episode rew_dof_acc: -0.2333
   Mean episode rew_dof_pos_limits: -0.0537
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1593
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.9836
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1728
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0355
 Mean episode rew_tracking_lin_vel: 0.1925
        Mean episode terrain_level: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 1.16s
                        Total time: 375.77s
                               ETA: 727 mins 6.2 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 88101 steps/s (collection: 0.975s, learning 0.141s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.25
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5265
       Mean episode rew_ang_vel_xy: -0.0807
          Mean episode rew_dof_acc: -0.2385
   Mean episode rew_dof_pos_limits: -0.0611
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1601
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.0267
      Mean episode rew_stand_still: -0.0064
      Mean episode rew_termination: -0.1693
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.1964
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.12s
                        Total time: 376.89s
                               ETA: 727 mins 32.7 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 90702 steps/s (collection: 0.946s, learning 0.137s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.26
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 251.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5783
       Mean episode rew_ang_vel_xy: -0.0857
          Mean episode rew_dof_acc: -0.2493
   Mean episode rew_dof_pos_limits: -0.0588
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1701
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.0625
      Mean episode rew_stand_still: -0.0084
      Mean episode rew_termination: -0.1700
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0377
 Mean episode rew_tracking_lin_vel: 0.1954
        Mean episode terrain_level: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 1.08s
                        Total time: 377.97s
                               ETA: 727 mins 55.3 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 89022 steps/s (collection: 0.982s, learning 0.122s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4524
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.2236
   Mean episode rew_dof_pos_limits: -0.0588
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1619
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -0.9788
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1734
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0364
 Mean episode rew_tracking_lin_vel: 0.1860
        Mean episode terrain_level: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 1.10s
                        Total time: 379.07s
                               ETA: 728 mins 20.1 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 85610 steps/s (collection: 1.022s, learning 0.126s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.27
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3588
       Mean episode rew_ang_vel_xy: -0.0765
          Mean episode rew_dof_acc: -0.2168
   Mean episode rew_dof_pos_limits: -0.0508
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1646
           Mean episode rew_no_fly: 0.0111
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -0.9059
      Mean episode rew_stand_still: -0.0084
      Mean episode rew_termination: -0.1756
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0315
 Mean episode rew_tracking_lin_vel: 0.1638
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 1.15s
                        Total time: 380.22s
                               ETA: 728 mins 49.9 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 92023 steps/s (collection: 0.945s, learning 0.123s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.28
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 230.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.4644
       Mean episode rew_ang_vel_xy: -0.0785
          Mean episode rew_dof_acc: -0.2214
   Mean episode rew_dof_pos_limits: -0.0567
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1599
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -0.9866
      Mean episode rew_stand_still: -0.0065
      Mean episode rew_termination: -0.1742
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0350
 Mean episode rew_tracking_lin_vel: 0.1841
        Mean episode terrain_level: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.07s
                        Total time: 381.29s
                               ETA: 729 mins 10.4 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 93425 steps/s (collection: 0.927s, learning 0.125s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.29
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 223.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5837
       Mean episode rew_ang_vel_xy: -0.0809
          Mean episode rew_dof_acc: -0.2468
   Mean episode rew_dof_pos_limits: -0.0609
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1703
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.0628
      Mean episode rew_stand_still: -0.0084
      Mean episode rew_termination: -0.1691
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0392
 Mean episode rew_tracking_lin_vel: 0.2032
        Mean episode terrain_level: 0.0289
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.05s
                        Total time: 382.34s
                               ETA: 729 mins 28.9 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 93928 steps/s (collection: 0.923s, learning 0.124s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.30
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.3946
       Mean episode rew_ang_vel_xy: -0.0758
          Mean episode rew_dof_acc: -0.2225
   Mean episode rew_dof_pos_limits: -0.0517
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1676
           Mean episode rew_no_fly: 0.0115
      Mean episode rew_orientation: -0.0064
       Mean episode rew_smoothness: -0.9364
      Mean episode rew_stand_still: -0.0082
      Mean episode rew_termination: -0.1734
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0330
 Mean episode rew_tracking_lin_vel: 0.1720
        Mean episode terrain_level: 0.0289
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.05s
                        Total time: 383.39s
                               ETA: 729 mins 46.7 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 91655 steps/s (collection: 0.950s, learning 0.122s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.31
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.2651
       Mean episode rew_ang_vel_xy: -0.0792
          Mean episode rew_dof_acc: -0.2199
   Mean episode rew_dof_pos_limits: -0.0399
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1675
           Mean episode rew_no_fly: 0.0095
      Mean episode rew_orientation: -0.0050
       Mean episode rew_smoothness: -0.8396
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1799
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0261
 Mean episode rew_tracking_lin_vel: 0.1376
        Mean episode terrain_level: 0.0284
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 1.07s
                        Total time: 384.46s
                               ETA: 730 mins 7.4 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 85333 steps/s (collection: 1.017s, learning 0.135s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.31
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5235
       Mean episode rew_ang_vel_xy: -0.0788
          Mean episode rew_dof_acc: -0.2225
   Mean episode rew_dof_pos_limits: -0.0600
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1584
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.0077
      Mean episode rew_stand_still: -0.0069
      Mean episode rew_termination: -0.1727
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0380
 Mean episode rew_tracking_lin_vel: 0.1810
        Mean episode terrain_level: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 1.15s
                        Total time: 385.61s
                               ETA: 730 mins 37.0 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 89595 steps/s (collection: 0.974s, learning 0.123s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.32
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 249.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7430
       Mean episode rew_ang_vel_xy: -0.0865
          Mean episode rew_dof_acc: -0.2490
   Mean episode rew_dof_pos_limits: -0.0690
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1701
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -1.1742
      Mean episode rew_stand_still: -0.0081
      Mean episode rew_termination: -0.1647
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0431
 Mean episode rew_tracking_lin_vel: 0.2198
        Mean episode terrain_level: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 1.10s
                        Total time: 386.71s
                               ETA: 731 mins 0.2 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 89282 steps/s (collection: 0.978s, learning 0.123s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5716
       Mean episode rew_ang_vel_xy: -0.0779
          Mean episode rew_dof_acc: -0.2259
   Mean episode rew_dof_pos_limits: -0.0645
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1641
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -1.0587
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1713
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.1839
        Mean episode terrain_level: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 1.10s
                        Total time: 387.81s
                               ETA: 731 mins 23.8 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 88764 steps/s (collection: 0.985s, learning 0.123s)
               Value function loss: 0.0877
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6548
       Mean episode rew_ang_vel_xy: -0.0817
          Mean episode rew_dof_acc: -0.2250
   Mean episode rew_dof_pos_limits: -0.0674
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1646
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.1078
      Mean episode rew_stand_still: -0.0099
      Mean episode rew_termination: -0.1689
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2102
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 1.11s
                        Total time: 388.92s
                               ETA: 731 mins 48.0 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 92197 steps/s (collection: 0.943s, learning 0.123s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.33
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 234.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6780
       Mean episode rew_ang_vel_xy: -0.0869
          Mean episode rew_dof_acc: -0.2520
   Mean episode rew_dof_pos_limits: -0.0629
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1719
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.1343
      Mean episode rew_stand_still: -0.0082
      Mean episode rew_termination: -0.1684
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0401
 Mean episode rew_tracking_lin_vel: 0.2080
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 1.07s
                        Total time: 389.99s
                               ETA: 732 mins 7.4 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 89181 steps/s (collection: 0.958s, learning 0.144s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0109
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 223.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7139
       Mean episode rew_ang_vel_xy: -0.0822
          Mean episode rew_dof_acc: -0.2489
   Mean episode rew_dof_pos_limits: -0.0631
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1645
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.1528
      Mean episode rew_stand_still: -0.0081
      Mean episode rew_termination: -0.1674
          Mean episode rew_torques: -0.0131
 Mean episode rew_tracking_ang_vel: 0.0417
 Mean episode rew_tracking_lin_vel: 0.2054
        Mean episode terrain_level: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.10s
                        Total time: 391.09s
                               ETA: 732 mins 30.8 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 84384 steps/s (collection: 1.036s, learning 0.129s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.34
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5835
       Mean episode rew_ang_vel_xy: -0.0795
          Mean episode rew_dof_acc: -0.2248
   Mean episode rew_dof_pos_limits: -0.0609
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1656
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.0604
      Mean episode rew_stand_still: -0.0076
      Mean episode rew_termination: -0.1727
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1823
        Mean episode terrain_level: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 1.16s
                        Total time: 392.25s
                               ETA: 733 mins 1.1 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 85872 steps/s (collection: 1.023s, learning 0.122s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.36
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7398
       Mean episode rew_ang_vel_xy: -0.0854
          Mean episode rew_dof_acc: -0.2579
   Mean episode rew_dof_pos_limits: -0.0630
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1707
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.1681
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1674
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.2036
        Mean episode terrain_level: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 1.14s
                        Total time: 393.40s
                               ETA: 733 mins 29.0 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 90921 steps/s (collection: 0.931s, learning 0.150s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.36
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6656
       Mean episode rew_ang_vel_xy: -0.0850
          Mean episode rew_dof_acc: -0.2326
   Mean episode rew_dof_pos_limits: -0.0613
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1656
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.1050
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1699
          Mean episode rew_torques: -0.0127
 Mean episode rew_tracking_ang_vel: 0.0375
 Mean episode rew_tracking_lin_vel: 0.1770
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 1.08s
                        Total time: 394.48s
                               ETA: 733 mins 49.7 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 89361 steps/s (collection: 0.959s, learning 0.141s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 237.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6883
       Mean episode rew_ang_vel_xy: -0.0880
          Mean episode rew_dof_acc: -0.2549
   Mean episode rew_dof_pos_limits: -0.0569
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1719
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.1258
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1705
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0372
 Mean episode rew_tracking_lin_vel: 0.1974
        Mean episode terrain_level: 0.0297
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 1.10s
                        Total time: 395.58s
                               ETA: 734 mins 12.3 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 85966 steps/s (collection: 1.008s, learning 0.135s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.37
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 195.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5754
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.2149
   Mean episode rew_dof_pos_limits: -0.0606
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1705
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.0443
      Mean episode rew_stand_still: -0.0076
      Mean episode rew_termination: -0.1720
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0371
 Mean episode rew_tracking_lin_vel: 0.1809
        Mean episode terrain_level: 0.0302
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.14s
                        Total time: 396.72s
                               ETA: 734 mins 39.7 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 88816 steps/s (collection: 0.983s, learning 0.123s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 186.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7287
       Mean episode rew_ang_vel_xy: -0.0812
          Mean episode rew_dof_acc: -0.2391
   Mean episode rew_dof_pos_limits: -0.0658
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1740
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.1568
      Mean episode rew_stand_still: -0.0099
      Mean episode rew_termination: -0.1668
          Mean episode rew_torques: -0.0135
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2303
        Mean episode terrain_level: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.11s
                        Total time: 397.83s
                               ETA: 735 mins 2.9 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 90507 steps/s (collection: 0.960s, learning 0.127s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.38
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6198
       Mean episode rew_ang_vel_xy: -0.0843
          Mean episode rew_dof_acc: -0.2326
   Mean episode rew_dof_pos_limits: -0.0563
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1685
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.0708
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1724
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0338
 Mean episode rew_tracking_lin_vel: 0.1796
        Mean episode terrain_level: 0.0295
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 1.09s
                        Total time: 398.91s
                               ETA: 735 mins 23.7 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 87384 steps/s (collection: 0.997s, learning 0.128s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 193.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7278
       Mean episode rew_ang_vel_xy: -0.0819
          Mean episode rew_dof_acc: -0.2431
   Mean episode rew_dof_pos_limits: -0.0638
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1680
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.1536
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1698
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.1973
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 1.12s
                        Total time: 400.04s
                               ETA: 735 mins 48.7 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 84137 steps/s (collection: 1.045s, learning 0.123s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.39
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 218.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7730
       Mean episode rew_ang_vel_xy: -0.0810
          Mean episode rew_dof_acc: -0.2174
   Mean episode rew_dof_pos_limits: -0.0743
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1677
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -1.1849
      Mean episode rew_stand_still: -0.0084
      Mean episode rew_termination: -0.1669
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2176
        Mean episode terrain_level: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 1.17s
                        Total time: 401.21s
                               ETA: 736 mins 18.4 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 92521 steps/s (collection: 0.940s, learning 0.122s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.40
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5126
       Mean episode rew_ang_vel_xy: -0.0740
          Mean episode rew_dof_acc: -0.1924
   Mean episode rew_dof_pos_limits: -0.0611
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1725
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -0.9985
      Mean episode rew_stand_still: -0.0091
      Mean episode rew_termination: -0.1734
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0359
 Mean episode rew_tracking_lin_vel: 0.1792
        Mean episode terrain_level: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.06s
                        Total time: 402.27s
                               ETA: 736 mins 36.3 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 90828 steps/s (collection: 0.943s, learning 0.139s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.41
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5009
       Mean episode rew_ang_vel_xy: -0.0749
          Mean episode rew_dof_acc: -0.2009
   Mean episode rew_dof_pos_limits: -0.0561
      Mean episode rew_joint_power: -0.0048
        Mean episode rew_lin_vel_z: -0.1625
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -0.9908
      Mean episode rew_stand_still: -0.0076
      Mean episode rew_termination: -0.1754
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0349
 Mean episode rew_tracking_lin_vel: 0.1521
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.08s
                        Total time: 403.35s
                               ETA: 736 mins 56.2 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 90870 steps/s (collection: 0.946s, learning 0.136s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 236.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7651
       Mean episode rew_ang_vel_xy: -0.0851
          Mean episode rew_dof_acc: -0.2482
   Mean episode rew_dof_pos_limits: -0.0621
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1768
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.1790
      Mean episode rew_stand_still: -0.0068
      Mean episode rew_termination: -0.1693
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0371
 Mean episode rew_tracking_lin_vel: 0.1775
        Mean episode terrain_level: 0.0276
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 1.08s
                        Total time: 404.43s
                               ETA: 737 mins 16.1 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 91560 steps/s (collection: 0.949s, learning 0.125s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.42
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7338
       Mean episode rew_ang_vel_xy: -0.0855
          Mean episode rew_dof_acc: -0.2473
   Mean episode rew_dof_pos_limits: -0.0614
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1731
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.1593
      Mean episode rew_stand_still: -0.0075
      Mean episode rew_termination: -0.1705
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0379
 Mean episode rew_tracking_lin_vel: 0.1940
        Mean episode terrain_level: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 1.07s
                        Total time: 405.51s
                               ETA: 737 mins 34.9 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 90915 steps/s (collection: 0.959s, learning 0.123s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5465
       Mean episode rew_ang_vel_xy: -0.0836
          Mean episode rew_dof_acc: -0.2157
   Mean episode rew_dof_pos_limits: -0.0569
      Mean episode rew_joint_power: -0.0051
        Mean episode rew_lin_vel_z: -0.1798
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.0173
      Mean episode rew_stand_still: -0.0071
      Mean episode rew_termination: -0.1744
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0329
 Mean episode rew_tracking_lin_vel: 0.1590
        Mean episode terrain_level: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.08s
                        Total time: 406.59s
                               ETA: 737 mins 54.5 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 91741 steps/s (collection: 0.942s, learning 0.129s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.43
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.5748
       Mean episode rew_ang_vel_xy: -0.0826
          Mean episode rew_dof_acc: -0.2221
   Mean episode rew_dof_pos_limits: -0.0534
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1730
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0063
       Mean episode rew_smoothness: -1.0446
      Mean episode rew_stand_still: -0.0081
      Mean episode rew_termination: -0.1755
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0320
 Mean episode rew_tracking_lin_vel: 0.1779
        Mean episode terrain_level: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.07s
                        Total time: 407.66s
                               ETA: 738 mins 12.9 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 89606 steps/s (collection: 0.957s, learning 0.140s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.44
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 202.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6351
       Mean episode rew_ang_vel_xy: -0.0776
          Mean episode rew_dof_acc: -0.2197
   Mean episode rew_dof_pos_limits: -0.0595
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1631
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.0924
      Mean episode rew_stand_still: -0.0061
      Mean episode rew_termination: -0.1739
          Mean episode rew_torques: -0.0123
 Mean episode rew_tracking_ang_vel: 0.0354
 Mean episode rew_tracking_lin_vel: 0.1788
        Mean episode terrain_level: 0.0260
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.10s
                        Total time: 408.76s
                               ETA: 738 mins 34.1 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 87093 steps/s (collection: 0.992s, learning 0.137s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 212.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6886
       Mean episode rew_ang_vel_xy: -0.0805
          Mean episode rew_dof_acc: -0.2185
   Mean episode rew_dof_pos_limits: -0.0655
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1779
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.1266
      Mean episode rew_stand_still: -0.0125
      Mean episode rew_termination: -0.1718
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0376
 Mean episode rew_tracking_lin_vel: 0.2103
        Mean episode terrain_level: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 1.13s
                        Total time: 409.89s
                               ETA: 738 mins 58.5 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 94587 steps/s (collection: 0.917s, learning 0.123s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.45
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 187.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7565
       Mean episode rew_ang_vel_xy: -0.0821
          Mean episode rew_dof_acc: -0.2279
   Mean episode rew_dof_pos_limits: -0.0647
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1628
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.1733
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1704
          Mean episode rew_torques: -0.0132
 Mean episode rew_tracking_ang_vel: 0.0384
 Mean episode rew_tracking_lin_vel: 0.1837
        Mean episode terrain_level: 0.0260
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.04s
                        Total time: 410.93s
                               ETA: 739 mins 13.2 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 88865 steps/s (collection: 0.983s, learning 0.123s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.46
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 221.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8360
       Mean episode rew_ang_vel_xy: -0.0898
          Mean episode rew_dof_acc: -0.2395
   Mean episode rew_dof_pos_limits: -0.0656
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1786
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.2249
      Mean episode rew_stand_still: -0.0092
      Mean episode rew_termination: -0.1686
          Mean episode rew_torques: -0.0136
 Mean episode rew_tracking_ang_vel: 0.0400
 Mean episode rew_tracking_lin_vel: 0.2224
        Mean episode terrain_level: 0.0261
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 1.11s
                        Total time: 412.03s
                               ETA: 739 mins 35.0 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 89801 steps/s (collection: 0.973s, learning 0.122s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.47
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8876
       Mean episode rew_ang_vel_xy: -0.0872
          Mean episode rew_dof_acc: -0.2483
   Mean episode rew_dof_pos_limits: -0.0659
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1609
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.2568
      Mean episode rew_stand_still: -0.0089
      Mean episode rew_termination: -0.1663
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0400
 Mean episode rew_tracking_lin_vel: 0.2227
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 1.09s
                        Total time: 413.13s
                               ETA: 739 mins 55.5 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 87811 steps/s (collection: 0.993s, learning 0.127s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.47
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6113
       Mean episode rew_ang_vel_xy: -0.0832
          Mean episode rew_dof_acc: -0.2244
   Mean episode rew_dof_pos_limits: -0.0551
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1766
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.0681
      Mean episode rew_stand_still: -0.0078
      Mean episode rew_termination: -0.1741
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0335
 Mean episode rew_tracking_lin_vel: 0.1682
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 1.12s
                        Total time: 414.25s
                               ETA: 740 mins 18.6 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 83723 steps/s (collection: 1.039s, learning 0.136s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8915
       Mean episode rew_ang_vel_xy: -0.0814
          Mean episode rew_dof_acc: -0.2296
   Mean episode rew_dof_pos_limits: -0.0747
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1647
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -1.2655
      Mean episode rew_stand_still: -0.0078
      Mean episode rew_termination: -0.1680
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0429
 Mean episode rew_tracking_lin_vel: 0.2096
        Mean episode terrain_level: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 1.17s
                        Total time: 415.42s
                               ETA: 740 mins 47.3 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 88239 steps/s (collection: 0.977s, learning 0.137s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.48
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.6101
       Mean episode rew_ang_vel_xy: -0.0762
          Mean episode rew_dof_acc: -0.1988
   Mean episode rew_dof_pos_limits: -0.0619
      Mean episode rew_joint_power: -0.0047
        Mean episode rew_lin_vel_z: -0.1619
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.0708
      Mean episode rew_stand_still: -0.0089
      Mean episode rew_termination: -0.1737
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0346
 Mean episode rew_tracking_lin_vel: 0.1744
        Mean episode terrain_level: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 1.11s
                        Total time: 416.53s
                               ETA: 741 mins 9.6 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 89864 steps/s (collection: 0.970s, learning 0.123s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.49
               Mean reward (total): -3.00
                Mean reward (task): -3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 294.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8898
       Mean episode rew_ang_vel_xy: -0.0875
          Mean episode rew_dof_acc: -0.2528
   Mean episode rew_dof_pos_limits: -0.0650
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1700
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0070
       Mean episode rew_smoothness: -1.2682
      Mean episode rew_stand_still: -0.0085
      Mean episode rew_termination: -0.1679
          Mean episode rew_torques: -0.0139
 Mean episode rew_tracking_ang_vel: 0.0403
 Mean episode rew_tracking_lin_vel: 0.1935
        Mean episode terrain_level: 0.0247
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 1.09s
                        Total time: 417.63s
                               ETA: 741 mins 29.6 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 84769 steps/s (collection: 1.036s, learning 0.123s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.50
               Mean reward (total): -3.52
                Mean reward (task): -3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7730
       Mean episode rew_ang_vel_xy: -0.0815
          Mean episode rew_dof_acc: -0.2216
   Mean episode rew_dof_pos_limits: -0.0648
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1705
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.1759
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1711
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1823
        Mean episode terrain_level: 0.0261
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 1.16s
                        Total time: 418.79s
                               ETA: 741 mins 56.5 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 94025 steps/s (collection: 0.922s, learning 0.124s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7950
       Mean episode rew_ang_vel_xy: -0.0893
          Mean episode rew_dof_acc: -0.2351
   Mean episode rew_dof_pos_limits: -0.0616
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1770
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.1867
      Mean episode rew_stand_still: -0.0085
      Mean episode rew_termination: -0.1710
          Mean episode rew_torques: -0.0133
 Mean episode rew_tracking_ang_vel: 0.0364
 Mean episode rew_tracking_lin_vel: 0.1965
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 1.05s
                        Total time: 419.83s
                               ETA: 742 mins 11.2 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 90920 steps/s (collection: 0.957s, learning 0.124s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.51
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 251.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2684
       Mean episode rew_ang_vel_xy: -0.0940
          Mean episode rew_dof_acc: -0.2701
   Mean episode rew_dof_pos_limits: -0.0849
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1840
           Mean episode rew_no_fly: 0.0159
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -1.5310
      Mean episode rew_stand_still: -0.0138
      Mean episode rew_termination: -0.1582
          Mean episode rew_torques: -0.0173
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2677
        Mean episode terrain_level: 0.0266
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 1.08s
                        Total time: 420.92s
                               ETA: 742 mins 29.6 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 83194 steps/s (collection: 1.059s, learning 0.123s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0218
       Mean episode rew_ang_vel_xy: -0.0865
          Mean episode rew_dof_acc: -0.2460
   Mean episode rew_dof_pos_limits: -0.0720
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1712
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.3477
      Mean episode rew_stand_still: -0.0102
      Mean episode rew_termination: -0.1646
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0417
 Mean episode rew_tracking_lin_vel: 0.2159
        Mean episode terrain_level: 0.0278
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.18s
                        Total time: 422.10s
                               ETA: 742 mins 58.5 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 84728 steps/s (collection: 1.022s, learning 0.138s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.52
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 247.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9589
       Mean episode rew_ang_vel_xy: -0.0899
          Mean episode rew_dof_acc: -0.2442
   Mean episode rew_dof_pos_limits: -0.0707
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1716
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.3047
      Mean episode rew_stand_still: -0.0081
      Mean episode rew_termination: -0.1681
          Mean episode rew_torques: -0.0146
 Mean episode rew_tracking_ang_vel: 0.0422
 Mean episode rew_tracking_lin_vel: 0.2094
        Mean episode terrain_level: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 1.16s
                        Total time: 423.26s
                               ETA: 743 mins 25.0 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 86794 steps/s (collection: 1.010s, learning 0.123s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.53
               Mean reward (total): -3.04
                Mean reward (task): -3.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 281.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0268
       Mean episode rew_ang_vel_xy: -0.0880
          Mean episode rew_dof_acc: -0.2465
   Mean episode rew_dof_pos_limits: -0.0729
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1746
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.3568
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1660
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0454
 Mean episode rew_tracking_lin_vel: 0.2277
        Mean episode terrain_level: 0.0260
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 1.13s
                        Total time: 424.39s
                               ETA: 743 mins 48.5 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 92564 steps/s (collection: 0.927s, learning 0.135s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.54
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 232.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9271
       Mean episode rew_ang_vel_xy: -0.0840
          Mean episode rew_dof_acc: -0.2376
   Mean episode rew_dof_pos_limits: -0.0690
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1635
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.2893
      Mean episode rew_stand_still: -0.0108
      Mean episode rew_termination: -0.1698
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.2072
        Mean episode terrain_level: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.06s
                        Total time: 425.45s
                               ETA: 744 mins 4.5 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 84866 steps/s (collection: 1.018s, learning 0.140s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.55
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 214.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9866
       Mean episode rew_ang_vel_xy: -0.0859
          Mean episode rew_dof_acc: -0.2482
   Mean episode rew_dof_pos_limits: -0.0691
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1700
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.3289
      Mean episode rew_stand_still: -0.0108
      Mean episode rew_termination: -0.1663
          Mean episode rew_torques: -0.0143
 Mean episode rew_tracking_ang_vel: 0.0403
 Mean episode rew_tracking_lin_vel: 0.2238
        Mean episode terrain_level: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 1.16s
                        Total time: 426.61s
                               ETA: 744 mins 30.5 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 88352 steps/s (collection: 0.990s, learning 0.123s)
               Value function loss: 0.0893
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.55
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 270.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1035
       Mean episode rew_ang_vel_xy: -0.0910
          Mean episode rew_dof_acc: -0.2465
   Mean episode rew_dof_pos_limits: -0.0742
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1722
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -1.3841
      Mean episode rew_stand_still: -0.0092
      Mean episode rew_termination: -0.1662
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0446
 Mean episode rew_tracking_lin_vel: 0.2239
        Mean episode terrain_level: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 1.11s
                        Total time: 427.72s
                               ETA: 744 mins 51.6 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 92783 steps/s (collection: 0.937s, learning 0.123s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.56
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 256.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0313
       Mean episode rew_ang_vel_xy: -0.0861
          Mean episode rew_dof_acc: -0.2295
   Mean episode rew_dof_pos_limits: -0.0749
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1621
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -1.3470
      Mean episode rew_stand_still: -0.0063
      Mean episode rew_termination: -0.1679
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2019
        Mean episode terrain_level: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 1.06s
                        Total time: 428.78s
                               ETA: 745 mins 7.1 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 86149 steps/s (collection: 1.012s, learning 0.129s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 216.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.7597
       Mean episode rew_ang_vel_xy: -0.0806
          Mean episode rew_dof_acc: -0.2181
   Mean episode rew_dof_pos_limits: -0.0575
      Mean episode rew_joint_power: -0.0053
        Mean episode rew_lin_vel_z: -0.1653
           Mean episode rew_no_fly: 0.0113
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.1687
      Mean episode rew_stand_still: -0.0068
      Mean episode rew_termination: -0.1750
          Mean episode rew_torques: -0.0126
 Mean episode rew_tracking_ang_vel: 0.0333
 Mean episode rew_tracking_lin_vel: 0.1600
        Mean episode terrain_level: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 1.14s
                        Total time: 429.92s
                               ETA: 745 mins 31.0 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 93081 steps/s (collection: 0.932s, learning 0.124s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.57
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 241.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0926
       Mean episode rew_ang_vel_xy: -0.0930
          Mean episode rew_dof_acc: -0.2703
   Mean episode rew_dof_pos_limits: -0.0676
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1770
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0067
       Mean episode rew_smoothness: -1.4011
      Mean episode rew_stand_still: -0.0083
      Mean episode rew_termination: -0.1646
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0416
 Mean episode rew_tracking_lin_vel: 0.2146
        Mean episode terrain_level: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 1.06s
                        Total time: 430.98s
                               ETA: 745 mins 46.0 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 92601 steps/s (collection: 0.938s, learning 0.124s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.58
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 215.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9541
       Mean episode rew_ang_vel_xy: -0.0879
          Mean episode rew_dof_acc: -0.2356
   Mean episode rew_dof_pos_limits: -0.0654
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1791
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.2920
      Mean episode rew_stand_still: -0.0104
      Mean episode rew_termination: -0.1702
          Mean episode rew_torques: -0.0138
 Mean episode rew_tracking_ang_vel: 0.0374
 Mean episode rew_tracking_lin_vel: 0.1962
        Mean episode terrain_level: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 1.06s
                        Total time: 432.04s
                               ETA: 746 mins 1.4 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 84797 steps/s (collection: 1.014s, learning 0.145s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 224.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2653
       Mean episode rew_ang_vel_xy: -0.0908
          Mean episode rew_dof_acc: -0.2492
   Mean episode rew_dof_pos_limits: -0.0823
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1711
           Mean episode rew_no_fly: 0.0151
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.5102
      Mean episode rew_stand_still: -0.0108
      Mean episode rew_termination: -0.1610
          Mean episode rew_torques: -0.0164
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2657
        Mean episode terrain_level: 0.0258
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 1.16s
                        Total time: 433.20s
                               ETA: 746 mins 26.9 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 84293 steps/s (collection: 1.043s, learning 0.123s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.59
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9306
       Mean episode rew_ang_vel_xy: -0.0838
          Mean episode rew_dof_acc: -0.2286
   Mean episode rew_dof_pos_limits: -0.0687
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1645
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.2848
      Mean episode rew_stand_still: -0.0110
      Mean episode rew_termination: -0.1686
          Mean episode rew_torques: -0.0141
 Mean episode rew_tracking_ang_vel: 0.0409
 Mean episode rew_tracking_lin_vel: 0.2197
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 1.17s
                        Total time: 434.37s
                               ETA: 746 mins 53.0 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 89084 steps/s (collection: 0.981s, learning 0.123s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.60
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 284.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1219
       Mean episode rew_ang_vel_xy: -0.0862
          Mean episode rew_dof_acc: -0.2318
   Mean episode rew_dof_pos_limits: -0.0771
      Mean episode rew_joint_power: -0.0058
        Mean episode rew_lin_vel_z: -0.1844
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.4127
      Mean episode rew_stand_still: -0.0099
      Mean episode rew_termination: -0.1644
          Mean episode rew_torques: -0.0156
 Mean episode rew_tracking_ang_vel: 0.0430
 Mean episode rew_tracking_lin_vel: 0.2149
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 1.10s
                        Total time: 435.47s
                               ETA: 747 mins 12.6 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 86413 steps/s (collection: 1.000s, learning 0.138s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0503
       Mean episode rew_ang_vel_xy: -0.0905
          Mean episode rew_dof_acc: -0.2527
   Mean episode rew_dof_pos_limits: -0.0672
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1706
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.3523
      Mean episode rew_stand_still: -0.0080
      Mean episode rew_termination: -0.1682
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2000
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 1.14s
                        Total time: 436.61s
                               ETA: 747 mins 35.5 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 89867 steps/s (collection: 0.970s, learning 0.124s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.61
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 248.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9012
       Mean episode rew_ang_vel_xy: -0.0772
          Mean episode rew_dof_acc: -0.2047
   Mean episode rew_dof_pos_limits: -0.0723
      Mean episode rew_joint_power: -0.0050
        Mean episode rew_lin_vel_z: -0.1675
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -1.2589
      Mean episode rew_stand_still: -0.0071
      Mean episode rew_termination: -0.1705
          Mean episode rew_torques: -0.0142
 Mean episode rew_tracking_ang_vel: 0.0455
 Mean episode rew_tracking_lin_vel: 0.2030
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 1.09s
                        Total time: 437.70s
                               ETA: 747 mins 53.9 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 89043 steps/s (collection: 0.981s, learning 0.123s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.62
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 240.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2214
       Mean episode rew_ang_vel_xy: -0.0955
          Mean episode rew_dof_acc: -0.2500
   Mean episode rew_dof_pos_limits: -0.0790
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1773
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -1.4731
      Mean episode rew_stand_still: -0.0090
      Mean episode rew_termination: -0.1633
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2288
        Mean episode terrain_level: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 1.10s
                        Total time: 438.81s
                               ETA: 748 mins 13.2 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 86507 steps/s (collection: 1.013s, learning 0.123s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.62
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 275.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2664
       Mean episode rew_ang_vel_xy: -0.0887
          Mean episode rew_dof_acc: -0.2765
   Mean episode rew_dof_pos_limits: -0.0739
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1795
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.5288
      Mean episode rew_stand_still: -0.0092
      Mean episode rew_termination: -0.1642
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2233
        Mean episode terrain_level: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 1.14s
                        Total time: 439.94s
                               ETA: 748 mins 35.8 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 94634 steps/s (collection: 0.914s, learning 0.125s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1081
       Mean episode rew_ang_vel_xy: -0.0852
          Mean episode rew_dof_acc: -0.2212
   Mean episode rew_dof_pos_limits: -0.0801
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1744
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -1.3924
      Mean episode rew_stand_still: -0.0088
      Mean episode rew_termination: -0.1662
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0461
 Mean episode rew_tracking_lin_vel: 0.2199
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 1.04s
                        Total time: 440.98s
                               ETA: 748 mins 48.3 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 93188 steps/s (collection: 0.929s, learning 0.126s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.63
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1369
       Mean episode rew_ang_vel_xy: -0.0895
          Mean episode rew_dof_acc: -0.2497
   Mean episode rew_dof_pos_limits: -0.0674
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1827
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0069
       Mean episode rew_smoothness: -1.4240
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1680
          Mean episode rew_torques: -0.0149
 Mean episode rew_tracking_ang_vel: 0.0418
 Mean episode rew_tracking_lin_vel: 0.2105
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 1.05s
                        Total time: 442.04s
                               ETA: 749 mins 2.4 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 93152 steps/s (collection: 0.929s, learning 0.126s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.64
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 260.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2316
       Mean episode rew_ang_vel_xy: -0.0899
          Mean episode rew_dof_acc: -0.2450
   Mean episode rew_dof_pos_limits: -0.0778
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1749
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.4814
      Mean episode rew_stand_still: -0.0111
      Mean episode rew_termination: -0.1636
          Mean episode rew_torques: -0.0162
 Mean episode rew_tracking_ang_vel: 0.0457
 Mean episode rew_tracking_lin_vel: 0.2291
        Mean episode terrain_level: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 1.06s
                        Total time: 443.09s
                               ETA: 749 mins 16.5 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 94158 steps/s (collection: 0.919s, learning 0.125s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.65
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.8207
       Mean episode rew_ang_vel_xy: -0.0817
          Mean episode rew_dof_acc: -0.2187
   Mean episode rew_dof_pos_limits: -0.0585
      Mean episode rew_joint_power: -0.0052
        Mean episode rew_lin_vel_z: -0.1713
           Mean episode rew_no_fly: 0.0110
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.2024
      Mean episode rew_stand_still: -0.0091
      Mean episode rew_termination: -0.1751
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0326
 Mean episode rew_tracking_lin_vel: 0.1751
        Mean episode terrain_level: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 1.04s
                        Total time: 444.13s
                               ETA: 749 mins 29.3 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 93600 steps/s (collection: 0.922s, learning 0.128s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.66
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2593
       Mean episode rew_ang_vel_xy: -0.0942
          Mean episode rew_dof_acc: -0.2508
   Mean episode rew_dof_pos_limits: -0.0762
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1780
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.5059
      Mean episode rew_stand_still: -0.0103
      Mean episode rew_termination: -0.1655
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0424
 Mean episode rew_tracking_lin_vel: 0.2189
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 1.05s
                        Total time: 445.19s
                               ETA: 749 mins 42.8 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 90046 steps/s (collection: 0.965s, learning 0.126s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.66
               Mean reward (total): -2.92
                Mean reward (task): -2.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 312.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2330
       Mean episode rew_ang_vel_xy: -0.0864
          Mean episode rew_dof_acc: -0.2309
   Mean episode rew_dof_pos_limits: -0.0810
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1783
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -1.4900
      Mean episode rew_stand_still: -0.0089
      Mean episode rew_termination: -0.1646
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2212
        Mean episode terrain_level: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 1.09s
                        Total time: 446.28s
                               ETA: 750 mins 0.3 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 86816 steps/s (collection: 0.987s, learning 0.145s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.67
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1863
       Mean episode rew_ang_vel_xy: -0.0877
          Mean episode rew_dof_acc: -0.2418
   Mean episode rew_dof_pos_limits: -0.0737
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1674
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.4608
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1673
          Mean episode rew_torques: -0.0157
 Mean episode rew_tracking_ang_vel: 0.0423
 Mean episode rew_tracking_lin_vel: 0.2175
        Mean episode terrain_level: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 1.13s
                        Total time: 447.41s
                               ETA: 750 mins 21.9 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 85937 steps/s (collection: 0.998s, learning 0.146s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.68
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1006
       Mean episode rew_ang_vel_xy: -0.0850
          Mean episode rew_dof_acc: -0.2294
   Mean episode rew_dof_pos_limits: -0.0716
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1711
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0076
       Mean episode rew_smoothness: -1.3987
      Mean episode rew_stand_still: -0.0101
      Mean episode rew_termination: -0.1688
          Mean episode rew_torques: -0.0152
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.2119
        Mean episode terrain_level: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 1.14s
                        Total time: 448.55s
                               ETA: 750 mins 44.5 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 86469 steps/s (collection: 1.009s, learning 0.128s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0720
       Mean episode rew_ang_vel_xy: -0.0841
          Mean episode rew_dof_acc: -0.2193
   Mean episode rew_dof_pos_limits: -0.0697
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1717
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.3767
      Mean episode rew_stand_still: -0.0067
      Mean episode rew_termination: -0.1703
          Mean episode rew_torques: -0.0147
 Mean episode rew_tracking_ang_vel: 0.0383
 Mean episode rew_tracking_lin_vel: 0.1897
        Mean episode terrain_level: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 1.14s
                        Total time: 449.69s
                               ETA: 751 mins 6.4 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 93263 steps/s (collection: 0.930s, learning 0.124s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.69
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 258.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3520
       Mean episode rew_ang_vel_xy: -0.0942
          Mean episode rew_dof_acc: -0.2661
   Mean episode rew_dof_pos_limits: -0.0775
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1726
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -1.5672
      Mean episode rew_stand_still: -0.0098
      Mean episode rew_termination: -0.1640
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0469
 Mean episode rew_tracking_lin_vel: 0.2283
        Mean episode terrain_level: 0.0258
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 1.05s
                        Total time: 450.74s
                               ETA: 751 mins 19.9 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 87984 steps/s (collection: 0.975s, learning 0.143s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2938
       Mean episode rew_ang_vel_xy: -0.0898
          Mean episode rew_dof_acc: -0.2490
   Mean episode rew_dof_pos_limits: -0.0801
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1837
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.5407
      Mean episode rew_stand_still: -0.0092
      Mean episode rew_termination: -0.1646
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0472
 Mean episode rew_tracking_lin_vel: 0.2286
        Mean episode terrain_level: 0.0272
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 1.12s
                        Total time: 451.86s
                               ETA: 751 mins 39.6 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 90422 steps/s (collection: 0.964s, learning 0.124s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.70
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 215.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1254
       Mean episode rew_ang_vel_xy: -0.0815
          Mean episode rew_dof_acc: -0.2240
   Mean episode rew_dof_pos_limits: -0.0765
      Mean episode rew_joint_power: -0.0055
        Mean episode rew_lin_vel_z: -0.1754
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -1.4293
      Mean episode rew_stand_still: -0.0090
      Mean episode rew_termination: -0.1668
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0437
 Mean episode rew_tracking_lin_vel: 0.2127
        Mean episode terrain_level: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 1.09s
                        Total time: 452.95s
                               ETA: 751 mins 56.2 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 89456 steps/s (collection: 0.950s, learning 0.149s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 264.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6274
       Mean episode rew_ang_vel_xy: -0.0971
          Mean episode rew_dof_acc: -0.2732
   Mean episode rew_dof_pos_limits: -0.0925
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1807
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -1.7561
      Mean episode rew_stand_still: -0.0103
      Mean episode rew_termination: -0.1561
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0557
 Mean episode rew_tracking_lin_vel: 0.2830
        Mean episode terrain_level: 0.0281
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 1.10s
                        Total time: 454.05s
                               ETA: 752 mins 13.9 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 82043 steps/s (collection: 1.055s, learning 0.143s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.71
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 260.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2726
       Mean episode rew_ang_vel_xy: -0.0868
          Mean episode rew_dof_acc: -0.2364
   Mean episode rew_dof_pos_limits: -0.0802
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1795
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -1.5211
      Mean episode rew_stand_still: -0.0068
      Mean episode rew_termination: -0.1665
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0450
 Mean episode rew_tracking_lin_vel: 0.2128
        Mean episode terrain_level: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 1.20s
                        Total time: 455.25s
                               ETA: 752 mins 41.4 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 92608 steps/s (collection: 0.920s, learning 0.141s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.72
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 270.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2870
       Mean episode rew_ang_vel_xy: -0.0847
          Mean episode rew_dof_acc: -0.2362
   Mean episode rew_dof_pos_limits: -0.0812
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1789
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -1.5245
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1643
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2232
        Mean episode terrain_level: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 1.06s
                        Total time: 456.31s
                               ETA: 752 mins 55.3 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 89518 steps/s (collection: 0.957s, learning 0.141s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.73
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 266.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5969
       Mean episode rew_ang_vel_xy: -0.0969
          Mean episode rew_dof_acc: -0.2468
   Mean episode rew_dof_pos_limits: -0.0949
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1783
           Mean episode rew_no_fly: 0.0153
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -1.7233
      Mean episode rew_stand_still: -0.0105
      Mean episode rew_termination: -0.1591
          Mean episode rew_torques: -0.0197
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2637
        Mean episode terrain_level: 0.0256
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 1.10s
                        Total time: 457.41s
                               ETA: 753 mins 12.7 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 84694 steps/s (collection: 1.022s, learning 0.139s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.74
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 193.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5612
       Mean episode rew_ang_vel_xy: -0.0904
          Mean episode rew_dof_acc: -0.2467
   Mean episode rew_dof_pos_limits: -0.0959
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1797
           Mean episode rew_no_fly: 0.0163
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -1.6987
      Mean episode rew_stand_still: -0.0105
      Mean episode rew_termination: -0.1590
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.2638
        Mean episode terrain_level: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 1.16s
                        Total time: 458.57s
                               ETA: 753 mins 36.2 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 91563 steps/s (collection: 0.951s, learning 0.123s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.1524
       Mean episode rew_ang_vel_xy: -0.0894
          Mean episode rew_dof_acc: -0.2292
   Mean episode rew_dof_pos_limits: -0.0703
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1692
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.4197
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1695
          Mean episode rew_torques: -0.0154
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.1883
        Mean episode terrain_level: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 1.07s
                        Total time: 459.64s
                               ETA: 753 mins 51.1 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 84719 steps/s (collection: 1.036s, learning 0.124s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.75
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2856
       Mean episode rew_ang_vel_xy: -0.0916
          Mean episode rew_dof_acc: -0.2401
   Mean episode rew_dof_pos_limits: -0.0759
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1775
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.5089
      Mean episode rew_stand_still: -0.0077
      Mean episode rew_termination: -0.1675
          Mean episode rew_torques: -0.0163
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2278
        Mean episode terrain_level: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 1.16s
                        Total time: 460.80s
                               ETA: 754 mins 14.4 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 93281 steps/s (collection: 0.930s, learning 0.123s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.76
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -1.9453
       Mean episode rew_ang_vel_xy: -0.0811
          Mean episode rew_dof_acc: -0.2011
   Mean episode rew_dof_pos_limits: -0.0656
      Mean episode rew_joint_power: -0.0049
        Mean episode rew_lin_vel_z: -0.1619
           Mean episode rew_no_fly: 0.0112
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -1.2762
      Mean episode rew_stand_still: -0.0062
      Mean episode rew_termination: -0.1747
          Mean episode rew_torques: -0.0140
 Mean episode rew_tracking_ang_vel: 0.0334
 Mean episode rew_tracking_lin_vel: 0.1765
        Mean episode terrain_level: 0.0258
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 1.05s
                        Total time: 461.85s
                               ETA: 754 mins 27.1 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 90761 steps/s (collection: 0.945s, learning 0.138s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.77
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 222.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4625
       Mean episode rew_ang_vel_xy: -0.0881
          Mean episode rew_dof_acc: -0.2391
   Mean episode rew_dof_pos_limits: -0.0898
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1812
           Mean episode rew_no_fly: 0.0144
      Mean episode rew_orientation: -0.0094
       Mean episode rew_smoothness: -1.6384
      Mean episode rew_stand_still: -0.0117
      Mean episode rew_termination: -0.1621
          Mean episode rew_torques: -0.0182
 Mean episode rew_tracking_ang_vel: 0.0513
 Mean episode rew_tracking_lin_vel: 0.2556
        Mean episode terrain_level: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 1.08s
                        Total time: 462.94s
                               ETA: 754 mins 42.7 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 84687 steps/s (collection: 1.038s, learning 0.123s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.78
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 186.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4160
       Mean episode rew_ang_vel_xy: -0.0883
          Mean episode rew_dof_acc: -0.2432
   Mean episode rew_dof_pos_limits: -0.0817
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1604
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -1.6024
      Mean episode rew_stand_still: -0.0121
      Mean episode rew_termination: -0.1651
          Mean episode rew_torques: -0.0174
 Mean episode rew_tracking_ang_vel: 0.0491
 Mean episode rew_tracking_lin_vel: 0.2370
        Mean episode terrain_level: 0.0258
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 1.16s
                        Total time: 464.10s
                               ETA: 755 mins 5.8 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 93184 steps/s (collection: 0.925s, learning 0.130s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.78
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 218.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2588
       Mean episode rew_ang_vel_xy: -0.0834
          Mean episode rew_dof_acc: -0.2266
   Mean episode rew_dof_pos_limits: -0.0791
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1745
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.5049
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1655
          Mean episode rew_torques: -0.0161
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2223
        Mean episode terrain_level: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 1.05s
                        Total time: 465.15s
                               ETA: 755 mins 18.5 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 83963 steps/s (collection: 1.014s, learning 0.157s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.79
               Mean reward (total): -2.76
                Mean reward (task): -2.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 341.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6087
       Mean episode rew_ang_vel_xy: -0.0966
          Mean episode rew_dof_acc: -0.2543
   Mean episode rew_dof_pos_limits: -0.0897
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1879
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -1.7215
      Mean episode rew_stand_still: -0.0102
      Mean episode rew_termination: -0.1610
          Mean episode rew_torques: -0.0188
 Mean episode rew_tracking_ang_vel: 0.0483
 Mean episode rew_tracking_lin_vel: 0.2475
        Mean episode terrain_level: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 1.17s
                        Total time: 466.32s
                               ETA: 755 mins 42.4 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 87182 steps/s (collection: 1.004s, learning 0.123s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.80
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 245.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.3347
       Mean episode rew_ang_vel_xy: -0.0860
          Mean episode rew_dof_acc: -0.2232
   Mean episode rew_dof_pos_limits: -0.0792
      Mean episode rew_joint_power: -0.0057
        Mean episode rew_lin_vel_z: -0.1778
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -1.5347
      Mean episode rew_stand_still: -0.0106
      Mean episode rew_termination: -0.1658
          Mean episode rew_torques: -0.0167
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2194
        Mean episode terrain_level: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.13s
                        Total time: 467.45s
                               ETA: 756 mins 2.0 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 89839 steps/s (collection: 0.971s, learning 0.123s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 246.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4609
       Mean episode rew_ang_vel_xy: -0.0900
          Mean episode rew_dof_acc: -0.2485
   Mean episode rew_dof_pos_limits: -0.0802
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1803
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.6300
      Mean episode rew_stand_still: -0.0097
      Mean episode rew_termination: -0.1649
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0457
 Mean episode rew_tracking_lin_vel: 0.2303
        Mean episode terrain_level: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.09s
                        Total time: 468.54s
                               ETA: 756 mins 18.3 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 86195 steps/s (collection: 1.012s, learning 0.129s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.81
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 218.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.2006
       Mean episode rew_ang_vel_xy: -0.0815
          Mean episode rew_dof_acc: -0.2104
   Mean episode rew_dof_pos_limits: -0.0752
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1716
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.4507
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1690
          Mean episode rew_torques: -0.0158
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2052
        Mean episode terrain_level: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 1.14s
                        Total time: 469.69s
                               ETA: 756 mins 39.0 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 88851 steps/s (collection: 0.981s, learning 0.125s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.82
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5366
       Mean episode rew_ang_vel_xy: -0.0927
          Mean episode rew_dof_acc: -0.2441
   Mean episode rew_dof_pos_limits: -0.0876
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1771
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -1.6711
      Mean episode rew_stand_still: -0.0132
      Mean episode rew_termination: -0.1642
          Mean episode rew_torques: -0.0183
 Mean episode rew_tracking_ang_vel: 0.0508
 Mean episode rew_tracking_lin_vel: 0.2560
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 1.11s
                        Total time: 470.79s
                               ETA: 756 mins 56.3 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 93350 steps/s (collection: 0.928s, learning 0.125s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.83
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 274.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.4421
       Mean episode rew_ang_vel_xy: -0.0881
          Mean episode rew_dof_acc: -0.2372
   Mean episode rew_dof_pos_limits: -0.0810
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1712
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.6103
      Mean episode rew_stand_still: -0.0100
      Mean episode rew_termination: -0.1664
          Mean episode rew_torques: -0.0173
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2291
        Mean episode terrain_level: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 1.05s
                        Total time: 471.84s
                               ETA: 757 mins 8.4 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 91478 steps/s (collection: 0.951s, learning 0.124s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.83
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7757
       Mean episode rew_ang_vel_xy: -0.0954
          Mean episode rew_dof_acc: -0.2412
   Mean episode rew_dof_pos_limits: -0.0973
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1726
           Mean episode rew_no_fly: 0.0150
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -1.8263
      Mean episode rew_stand_still: -0.0087
      Mean episode rew_termination: -0.1616
          Mean episode rew_torques: -0.0201
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.2514
        Mean episode terrain_level: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 1.07s
                        Total time: 472.92s
                               ETA: 757 mins 22.5 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 90519 steps/s (collection: 0.962s, learning 0.124s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.84
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 215.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7324
       Mean episode rew_ang_vel_xy: -0.0950
          Mean episode rew_dof_acc: -0.2491
   Mean episode rew_dof_pos_limits: -0.0929
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1789
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -1.7942
      Mean episode rew_stand_still: -0.0120
      Mean episode rew_termination: -0.1595
          Mean episode rew_torques: -0.0194
 Mean episode rew_tracking_ang_vel: 0.0508
 Mean episode rew_tracking_lin_vel: 0.2718
        Mean episode terrain_level: 0.0248
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 1.09s
                        Total time: 474.01s
                               ETA: 757 mins 37.7 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 89521 steps/s (collection: 0.949s, learning 0.150s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 202.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5455
       Mean episode rew_ang_vel_xy: -0.0939
          Mean episode rew_dof_acc: -0.2392
   Mean episode rew_dof_pos_limits: -0.0828
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1769
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0083
       Mean episode rew_smoothness: -1.6780
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1644
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0476
 Mean episode rew_tracking_lin_vel: 0.2325
        Mean episode terrain_level: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 1.10s
                        Total time: 475.10s
                               ETA: 757 mins 53.9 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 91906 steps/s (collection: 0.947s, learning 0.123s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.85
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 199.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.0874
       Mean episode rew_ang_vel_xy: -0.0784
          Mean episode rew_dof_acc: -0.1866
   Mean episode rew_dof_pos_limits: -0.0751
      Mean episode rew_joint_power: -0.0046
        Mean episode rew_lin_vel_z: -0.1802
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.3693
      Mean episode rew_stand_still: -0.0098
      Mean episode rew_termination: -0.1715
          Mean episode rew_torques: -0.0150
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.2084
        Mean episode terrain_level: 0.0250
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 1.07s
                        Total time: 476.17s
                               ETA: 758 mins 7.4 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 85895 steps/s (collection: 1.017s, learning 0.128s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 282.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5990
       Mean episode rew_ang_vel_xy: -0.0950
          Mean episode rew_dof_acc: -0.2498
   Mean episode rew_dof_pos_limits: -0.0831
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1881
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -1.7127
      Mean episode rew_stand_still: -0.0100
      Mean episode rew_termination: -0.1660
          Mean episode rew_torques: -0.0184
 Mean episode rew_tracking_ang_vel: 0.0490
 Mean episode rew_tracking_lin_vel: 0.2343
        Mean episode terrain_level: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 1.14s
                        Total time: 477.32s
                               ETA: 758 mins 28.0 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 88808 steps/s (collection: 0.984s, learning 0.123s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.86
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 264.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6389
       Mean episode rew_ang_vel_xy: -0.0901
          Mean episode rew_dof_acc: -0.2329
   Mean episode rew_dof_pos_limits: -0.0907
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1854
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0096
       Mean episode rew_smoothness: -1.7408
      Mean episode rew_stand_still: -0.0099
      Mean episode rew_termination: -0.1639
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0515
 Mean episode rew_tracking_lin_vel: 0.2335
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 1.11s
                        Total time: 478.42s
                               ETA: 758 mins 44.9 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 87121 steps/s (collection: 0.982s, learning 0.146s)
               Value function loss: 0.0866
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.87
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 207.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5637
       Mean episode rew_ang_vel_xy: -0.0933
          Mean episode rew_dof_acc: -0.2394
   Mean episode rew_dof_pos_limits: -0.0810
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1744
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -1.6660
      Mean episode rew_stand_still: -0.0076
      Mean episode rew_termination: -0.1660
          Mean episode rew_torques: -0.0177
 Mean episode rew_tracking_ang_vel: 0.0455
 Mean episode rew_tracking_lin_vel: 0.2285
        Mean episode terrain_level: 0.0239
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 1.13s
                        Total time: 479.55s
                               ETA: 759 mins 3.7 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 87544 steps/s (collection: 0.989s, learning 0.134s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 231.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7432
       Mean episode rew_ang_vel_xy: -0.0968
          Mean episode rew_dof_acc: -0.2506
   Mean episode rew_dof_pos_limits: -0.0896
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1859
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -1.7951
      Mean episode rew_stand_still: -0.0131
      Mean episode rew_termination: -0.1616
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0533
 Mean episode rew_tracking_lin_vel: 0.2495
        Mean episode terrain_level: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 1.12s
                        Total time: 480.68s
                               ETA: 759 mins 22.0 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 84200 steps/s (collection: 1.034s, learning 0.133s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.88
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 250.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7758
       Mean episode rew_ang_vel_xy: -0.0954
          Mean episode rew_dof_acc: -0.2572
   Mean episode rew_dof_pos_limits: -0.0916
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1984
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -1.8327
      Mean episode rew_stand_still: -0.0120
      Mean episode rew_termination: -0.1626
          Mean episode rew_torques: -0.0198
 Mean episode rew_tracking_ang_vel: 0.0481
 Mean episode rew_tracking_lin_vel: 0.2544
        Mean episode terrain_level: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 1.17s
                        Total time: 481.84s
                               ETA: 759 mins 44.4 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 88931 steps/s (collection: 0.982s, learning 0.124s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6049
       Mean episode rew_ang_vel_xy: -0.0923
          Mean episode rew_dof_acc: -0.2569
   Mean episode rew_dof_pos_limits: -0.0821
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1969
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -1.7167
      Mean episode rew_stand_still: -0.0097
      Mean episode rew_termination: -0.1645
          Mean episode rew_torques: -0.0182
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2090
        Mean episode terrain_level: 0.0226
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 1.11s
                        Total time: 482.95s
                               ETA: 760 mins 0.9 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 87955 steps/s (collection: 0.970s, learning 0.148s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.89
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 234.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7435
       Mean episode rew_ang_vel_xy: -0.0969
          Mean episode rew_dof_acc: -0.2522
   Mean episode rew_dof_pos_limits: -0.0887
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1929
           Mean episode rew_no_fly: 0.0135
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -1.8042
      Mean episode rew_stand_still: -0.0070
      Mean episode rew_termination: -0.1620
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0498
 Mean episode rew_tracking_lin_vel: 0.2390
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 1.12s
                        Total time: 484.07s
                               ETA: 760 mins 18.4 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 84972 steps/s (collection: 1.024s, learning 0.133s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.90
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7833
       Mean episode rew_ang_vel_xy: -0.1012
          Mean episode rew_dof_acc: -0.2720
   Mean episode rew_dof_pos_limits: -0.0794
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1724
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0065
       Mean episode rew_smoothness: -1.8099
      Mean episode rew_stand_still: -0.0124
      Mean episode rew_termination: -0.1637
          Mean episode rew_torques: -0.0187
 Mean episode rew_tracking_ang_vel: 0.0424
 Mean episode rew_tracking_lin_vel: 0.2329
        Mean episode terrain_level: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 1.16s
                        Total time: 485.22s
                               ETA: 760 mins 39.6 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 83020 steps/s (collection: 1.029s, learning 0.155s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 239.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7774
       Mean episode rew_ang_vel_xy: -0.0967
          Mean episode rew_dof_acc: -0.2456
   Mean episode rew_dof_pos_limits: -0.0875
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1861
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -1.8069
      Mean episode rew_stand_still: -0.0075
      Mean episode rew_termination: -0.1647
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0494
 Mean episode rew_tracking_lin_vel: 0.2302
        Mean episode terrain_level: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 1.18s
                        Total time: 486.41s
                               ETA: 761 mins 3.2 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 87412 steps/s (collection: 1.001s, learning 0.124s)
               Value function loss: 0.0880
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.91
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5493
       Mean episode rew_ang_vel_xy: -0.0907
          Mean episode rew_dof_acc: -0.2165
   Mean episode rew_dof_pos_limits: -0.0839
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1736
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -1.6658
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1700
          Mean episode rew_torques: -0.0178
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2212
        Mean episode terrain_level: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 1.12s
                        Total time: 487.53s
                               ETA: 761 mins 21.2 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 86570 steps/s (collection: 1.001s, learning 0.135s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.92
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 290.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1175
       Mean episode rew_ang_vel_xy: -0.1012
          Mean episode rew_dof_acc: -0.2693
   Mean episode rew_dof_pos_limits: -0.1021
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1855
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.0443
      Mean episode rew_stand_still: -0.0102
      Mean episode rew_termination: -0.1569
          Mean episode rew_torques: -0.0216
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.2825
        Mean episode terrain_level: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 1.14s
                        Total time: 488.67s
                               ETA: 761 mins 40.1 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 92487 steps/s (collection: 0.939s, learning 0.123s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.93
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 263.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.7670
       Mean episode rew_ang_vel_xy: -0.0910
          Mean episode rew_dof_acc: -0.2297
   Mean episode rew_dof_pos_limits: -0.0905
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1774
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0090
       Mean episode rew_smoothness: -1.8028
      Mean episode rew_stand_still: -0.0097
      Mean episode rew_termination: -0.1636
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2304
        Mean episode terrain_level: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 1.06s
                        Total time: 489.73s
                               ETA: 761 mins 52.2 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 88732 steps/s (collection: 0.985s, learning 0.123s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.93
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 212.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6836
       Mean episode rew_ang_vel_xy: -0.0904
          Mean episode rew_dof_acc: -0.2313
   Mean episode rew_dof_pos_limits: -0.0866
      Mean episode rew_joint_power: -0.0059
        Mean episode rew_lin_vel_z: -0.1788
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -1.7551
      Mean episode rew_stand_still: -0.0105
      Mean episode rew_termination: -0.1641
          Mean episode rew_torques: -0.0188
 Mean episode rew_tracking_ang_vel: 0.0486
 Mean episode rew_tracking_lin_vel: 0.2342
        Mean episode terrain_level: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 1.11s
                        Total time: 490.84s
                               ETA: 762 mins 8.4 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 88772 steps/s (collection: 0.984s, learning 0.124s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.94
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 223.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6662
       Mean episode rew_ang_vel_xy: -0.0920
          Mean episode rew_dof_acc: -0.2440
   Mean episode rew_dof_pos_limits: -0.0791
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1859
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0075
       Mean episode rew_smoothness: -1.7332
      Mean episode rew_stand_still: -0.0090
      Mean episode rew_termination: -0.1658
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0461
 Mean episode rew_tracking_lin_vel: 0.2121
        Mean episode terrain_level: 0.0221
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.11s
                        Total time: 491.95s
                               ETA: 762 mins 24.5 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 86903 steps/s (collection: 0.999s, learning 0.132s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.95
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 233.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1003
       Mean episode rew_ang_vel_xy: -0.1019
          Mean episode rew_dof_acc: -0.2774
   Mean episode rew_dof_pos_limits: -0.0913
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1825
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.0124
      Mean episode rew_stand_still: -0.0082
      Mean episode rew_termination: -0.1601
          Mean episode rew_torques: -0.0209
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2452
        Mean episode terrain_level: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 1.13s
                        Total time: 493.08s
                               ETA: 762 mins 42.7 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 89660 steps/s (collection: 0.972s, learning 0.124s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.96
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1912
       Mean episode rew_ang_vel_xy: -0.1077
          Mean episode rew_dof_acc: -0.2819
   Mean episode rew_dof_pos_limits: -0.0962
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1988
           Mean episode rew_no_fly: 0.0146
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.0704
      Mean episode rew_stand_still: -0.0073
      Mean episode rew_termination: -0.1578
          Mean episode rew_torques: -0.0215
 Mean episode rew_tracking_ang_vel: 0.0558
 Mean episode rew_tracking_lin_vel: 0.2544
        Mean episode terrain_level: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 1.10s
                        Total time: 494.17s
                               ETA: 762 mins 57.7 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 88574 steps/s (collection: 0.988s, learning 0.122s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.96
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 228.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9186
       Mean episode rew_ang_vel_xy: -0.0998
          Mean episode rew_dof_acc: -0.2494
   Mean episode rew_dof_pos_limits: -0.0889
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1968
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -1.8826
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1645
          Mean episode rew_torques: -0.0198
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2446
        Mean episode terrain_level: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 1.11s
                        Total time: 495.28s
                               ETA: 763 mins 13.8 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 88224 steps/s (collection: 0.991s, learning 0.123s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0873
       Mean episode rew_ang_vel_xy: -0.1000
          Mean episode rew_dof_acc: -0.2719
   Mean episode rew_dof_pos_limits: -0.0884
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1904
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.9961
      Mean episode rew_stand_still: -0.0110
      Mean episode rew_termination: -0.1630
          Mean episode rew_torques: -0.0202
 Mean episode rew_tracking_ang_vel: 0.0487
 Mean episode rew_tracking_lin_vel: 0.2439
        Mean episode terrain_level: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 1.11s
                        Total time: 496.40s
                               ETA: 763 mins 30.2 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 90450 steps/s (collection: 0.964s, learning 0.123s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.97
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 254.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8610
       Mean episode rew_ang_vel_xy: -0.0944
          Mean episode rew_dof_acc: -0.2575
   Mean episode rew_dof_pos_limits: -0.0810
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1782
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -1.8629
      Mean episode rew_stand_still: -0.0110
      Mean episode rew_termination: -0.1647
          Mean episode rew_torques: -0.0191
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2254
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 1.09s
                        Total time: 497.48s
                               ETA: 763 mins 44.1 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 85622 steps/s (collection: 1.002s, learning 0.146s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 237.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.6378
       Mean episode rew_ang_vel_xy: -0.0863
          Mean episode rew_dof_acc: -0.2140
   Mean episode rew_dof_pos_limits: -0.0810
      Mean episode rew_joint_power: -0.0056
        Mean episode rew_lin_vel_z: -0.1755
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -1.7063
      Mean episode rew_stand_still: -0.0091
      Mean episode rew_termination: -0.1698
          Mean episode rew_torques: -0.0179
 Mean episode rew_tracking_ang_vel: 0.0467
 Mean episode rew_tracking_lin_vel: 0.2197
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 1.15s
                        Total time: 498.63s
                               ETA: 764 mins 3.6 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 88524 steps/s (collection: 0.987s, learning 0.124s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0120
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.98
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8719
       Mean episode rew_ang_vel_xy: -0.0957
          Mean episode rew_dof_acc: -0.2391
   Mean episode rew_dof_pos_limits: -0.0854
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1816
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.8527
      Mean episode rew_stand_still: -0.0078
      Mean episode rew_termination: -0.1658
          Mean episode rew_torques: -0.0192
 Mean episode rew_tracking_ang_vel: 0.0488
 Mean episode rew_tracking_lin_vel: 0.2378
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 1.11s
                        Total time: 499.74s
                               ETA: 764 mins 19.5 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 93122 steps/s (collection: 0.933s, learning 0.123s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 214.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.9545
       Mean episode rew_ang_vel_xy: -0.0949
          Mean episode rew_dof_acc: -0.2590
   Mean episode rew_dof_pos_limits: -0.0876
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1892
           Mean episode rew_no_fly: 0.0128
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -1.9271
      Mean episode rew_stand_still: -0.0119
      Mean episode rew_termination: -0.1621
          Mean episode rew_torques: -0.0201
 Mean episode rew_tracking_ang_vel: 0.0466
 Mean episode rew_tracking_lin_vel: 0.2546
        Mean episode terrain_level: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 1.06s
                        Total time: 500.80s
                               ETA: 764 mins 30.3 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 83414 steps/s (collection: 1.044s, learning 0.135s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 2.99
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 226.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8859
       Mean episode rew_ang_vel_xy: -0.0922
          Mean episode rew_dof_acc: -0.2368
   Mean episode rew_dof_pos_limits: -0.0872
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1705
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -1.8713
      Mean episode rew_stand_still: -0.0129
      Mean episode rew_termination: -0.1637
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0487
 Mean episode rew_tracking_lin_vel: 0.2436
        Mean episode terrain_level: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 1.18s
                        Total time: 501.98s
                               ETA: 764 mins 52.4 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 88299 steps/s (collection: 0.983s, learning 0.131s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.00
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 258.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0256
       Mean episode rew_ang_vel_xy: -0.0885
          Mean episode rew_dof_acc: -0.2470
   Mean episode rew_dof_pos_limits: -0.0929
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1674
           Mean episode rew_no_fly: 0.0137
      Mean episode rew_orientation: -0.0089
       Mean episode rew_smoothness: -1.9936
      Mean episode rew_stand_still: -0.0091
      Mean episode rew_termination: -0.1622
          Mean episode rew_torques: -0.0207
 Mean episode rew_tracking_ang_vel: 0.0513
 Mean episode rew_tracking_lin_vel: 0.2433
        Mean episode terrain_level: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 1.11s
                        Total time: 503.09s
                               ETA: 765 mins 8.4 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 91664 steps/s (collection: 0.949s, learning 0.123s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 255.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2807
       Mean episode rew_ang_vel_xy: -0.0956
          Mean episode rew_dof_acc: -0.2537
   Mean episode rew_dof_pos_limits: -0.1037
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1798
           Mean episode rew_no_fly: 0.0158
      Mean episode rew_orientation: -0.0102
       Mean episode rew_smoothness: -2.1387
      Mean episode rew_stand_still: -0.0109
      Mean episode rew_termination: -0.1566
          Mean episode rew_torques: -0.0222
 Mean episode rew_tracking_ang_vel: 0.0527
 Mean episode rew_tracking_lin_vel: 0.2603
        Mean episode terrain_level: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 1.07s
                        Total time: 504.16s
                               ETA: 765 mins 20.6 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 88647 steps/s (collection: 0.986s, learning 0.123s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.01
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0509
       Mean episode rew_ang_vel_xy: -0.1010
          Mean episode rew_dof_acc: -0.2494
   Mean episode rew_dof_pos_limits: -0.0864
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1805
           Mean episode rew_no_fly: 0.0124
      Mean episode rew_orientation: -0.0081
       Mean episode rew_smoothness: -1.9672
      Mean episode rew_stand_still: -0.0111
      Mean episode rew_termination: -0.1631
          Mean episode rew_torques: -0.0201
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2330
        Mean episode terrain_level: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 1.11s
                        Total time: 505.27s
                               ETA: 765 mins 36.1 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 92572 steps/s (collection: 0.939s, learning 0.123s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 239.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1962
       Mean episode rew_ang_vel_xy: -0.0948
          Mean episode rew_dof_acc: -0.2619
   Mean episode rew_dof_pos_limits: -0.0933
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1822
           Mean episode rew_no_fly: 0.0136
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.0881
      Mean episode rew_stand_still: -0.0140
      Mean episode rew_termination: -0.1601
          Mean episode rew_torques: -0.0213
 Mean episode rew_tracking_ang_vel: 0.0508
 Mean episode rew_tracking_lin_vel: 0.2607
        Mean episode terrain_level: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 1.06s
                        Total time: 506.33s
                               ETA: 765 mins 47.2 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 84481 steps/s (collection: 1.039s, learning 0.124s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 213.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3383
       Mean episode rew_ang_vel_xy: -0.1013
          Mean episode rew_dof_acc: -0.2678
   Mean episode rew_dof_pos_limits: -0.0960
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1906
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0085
       Mean episode rew_smoothness: -2.1572
      Mean episode rew_stand_still: -0.0113
      Mean episode rew_termination: -0.1582
          Mean episode rew_torques: -0.0219
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2643
        Mean episode terrain_level: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 1.16s
                        Total time: 507.50s
                               ETA: 766 mins 7.5 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 86313 steps/s (collection: 1.003s, learning 0.136s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.02
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 250.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3547
       Mean episode rew_ang_vel_xy: -0.1061
          Mean episode rew_dof_acc: -0.2709
   Mean episode rew_dof_pos_limits: -0.0955
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1800
           Mean episode rew_no_fly: 0.0143
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.1657
      Mean episode rew_stand_still: -0.0100
      Mean episode rew_termination: -0.1596
          Mean episode rew_torques: -0.0221
 Mean episode rew_tracking_ang_vel: 0.0547
 Mean episode rew_tracking_lin_vel: 0.2601
        Mean episode terrain_level: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 1.14s
                        Total time: 508.64s
                               ETA: 766 mins 25.5 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 85443 steps/s (collection: 1.027s, learning 0.124s)
               Value function loss: 0.0952
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.24
                Mean reward (task): -3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 252.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0075
       Mean episode rew_ang_vel_xy: -0.0895
          Mean episode rew_dof_acc: -0.2254
   Mean episode rew_dof_pos_limits: -0.0925
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1744
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -1.9671
      Mean episode rew_stand_still: -0.0107
      Mean episode rew_termination: -0.1661
          Mean episode rew_torques: -0.0204
 Mean episode rew_tracking_ang_vel: 0.0484
 Mean episode rew_tracking_lin_vel: 0.2464
        Mean episode terrain_level: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 1.15s
                        Total time: 509.79s
                               ETA: 766 mins 44.5 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 89868 steps/s (collection: 0.966s, learning 0.128s)
               Value function loss: 0.0825
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 213.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0940
       Mean episode rew_ang_vel_xy: -0.0952
          Mean episode rew_dof_acc: -0.2355
   Mean episode rew_dof_pos_limits: -0.0912
      Mean episode rew_joint_power: -0.0063
        Mean episode rew_lin_vel_z: -0.1702
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0088
       Mean episode rew_smoothness: -2.0001
      Mean episode rew_stand_still: -0.0112
      Mean episode rew_termination: -0.1639
          Mean episode rew_torques: -0.0204
 Mean episode rew_tracking_ang_vel: 0.0523
 Mean episode rew_tracking_lin_vel: 0.2394
        Mean episode terrain_level: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 1.09s
                        Total time: 510.88s
                               ETA: 766 mins 58.3 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 87197 steps/s (collection: 1.003s, learning 0.124s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.03
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 228.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.8554
       Mean episode rew_ang_vel_xy: -0.0941
          Mean episode rew_dof_acc: -0.2555
   Mean episode rew_dof_pos_limits: -0.0726
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1742
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0060
       Mean episode rew_smoothness: -1.8579
      Mean episode rew_stand_still: -0.0082
      Mean episode rew_termination: -0.1699
          Mean episode rew_torques: -0.0180
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.2004
        Mean episode terrain_level: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 1.13s
                        Total time: 512.01s
                               ETA: 767 mins 15.1 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 91046 steps/s (collection: 0.956s, learning 0.123s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 210.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2073
       Mean episode rew_ang_vel_xy: -0.0916
          Mean episode rew_dof_acc: -0.2386
   Mean episode rew_dof_pos_limits: -0.0979
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1742
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0098
       Mean episode rew_smoothness: -2.0849
      Mean episode rew_stand_still: -0.0059
      Mean episode rew_termination: -0.1614
          Mean episode rew_torques: -0.0212
 Mean episode rew_tracking_ang_vel: 0.0510
 Mean episode rew_tracking_lin_vel: 0.2279
        Mean episode terrain_level: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 1.08s
                        Total time: 513.09s
                               ETA: 767 mins 27.5 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 85580 steps/s (collection: 1.025s, learning 0.123s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.04
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 248.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4374
       Mean episode rew_ang_vel_xy: -0.1051
          Mean episode rew_dof_acc: -0.2838
   Mean episode rew_dof_pos_limits: -0.0947
      Mean episode rew_joint_power: -0.0076
        Mean episode rew_lin_vel_z: -0.1828
           Mean episode rew_no_fly: 0.0134
      Mean episode rew_orientation: -0.0080
       Mean episode rew_smoothness: -2.2247
      Mean episode rew_stand_still: -0.0103
      Mean episode rew_termination: -0.1588
          Mean episode rew_torques: -0.0222
 Mean episode rew_tracking_ang_vel: 0.0503
 Mean episode rew_tracking_lin_vel: 0.2643
        Mean episode terrain_level: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 1.15s
                        Total time: 514.24s
                               ETA: 767 mins 46.1 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 87872 steps/s (collection: 0.968s, learning 0.151s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 258.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1321
       Mean episode rew_ang_vel_xy: -0.0943
          Mean episode rew_dof_acc: -0.2528
   Mean episode rew_dof_pos_limits: -0.0863
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1841
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.0363
      Mean episode rew_stand_still: -0.0088
      Mean episode rew_termination: -0.1637
          Mean episode rew_torques: -0.0203
 Mean episode rew_tracking_ang_vel: 0.0466
 Mean episode rew_tracking_lin_vel: 0.2281
        Mean episode terrain_level: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 1.12s
                        Total time: 515.35s
                               ETA: 768 mins 1.9 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 90059 steps/s (collection: 0.967s, learning 0.125s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.05
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 221.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -2.5617
       Mean episode rew_ang_vel_xy: -0.0799
          Mean episode rew_dof_acc: -0.2125
   Mean episode rew_dof_pos_limits: -0.0710
      Mean episode rew_joint_power: -0.0054
        Mean episode rew_lin_vel_z: -0.1747
           Mean episode rew_no_fly: 0.0107
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.6613
      Mean episode rew_stand_still: -0.0079
      Mean episode rew_termination: -0.1727
          Mean episode rew_torques: -0.0165
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.1759
        Mean episode terrain_level: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 1.09s
                        Total time: 516.45s
                               ETA: 768 mins 15.2 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 85289 steps/s (collection: 1.030s, learning 0.123s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.06
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 225.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1292
       Mean episode rew_ang_vel_xy: -0.0929
          Mean episode rew_dof_acc: -0.2299
   Mean episode rew_dof_pos_limits: -0.0923
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1708
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.0154
      Mean episode rew_stand_still: -0.0088
      Mean episode rew_termination: -0.1614
          Mean episode rew_torques: -0.0201
 Mean episode rew_tracking_ang_vel: 0.0523
 Mean episode rew_tracking_lin_vel: 0.2394
        Mean episode terrain_level: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 1.15s
                        Total time: 517.60s
                               ETA: 768 mins 33.9 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 90360 steps/s (collection: 0.958s, learning 0.130s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 242.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2655
       Mean episode rew_ang_vel_xy: -0.0937
          Mean episode rew_dof_acc: -0.2463
   Mean episode rew_dof_pos_limits: -0.0950
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1829
           Mean episode rew_no_fly: 0.0139
      Mean episode rew_orientation: -0.0091
       Mean episode rew_smoothness: -2.1124
      Mean episode rew_stand_still: -0.0096
      Mean episode rew_termination: -0.1605
          Mean episode rew_torques: -0.0211
 Mean episode rew_tracking_ang_vel: 0.0505
 Mean episode rew_tracking_lin_vel: 0.2478
        Mean episode terrain_level: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 1.09s
                        Total time: 518.69s
                               ETA: 768 mins 46.7 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 80346 steps/s (collection: 1.072s, learning 0.152s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.07
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 244.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3447
       Mean episode rew_ang_vel_xy: -0.0996
          Mean episode rew_dof_acc: -0.2550
   Mean episode rew_dof_pos_limits: -0.0924
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1872
           Mean episode rew_no_fly: 0.0132
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.1570
      Mean episode rew_stand_still: -0.0145
      Mean episode rew_termination: -0.1610
          Mean episode rew_torques: -0.0215
 Mean episode rew_tracking_ang_vel: 0.0510
 Mean episode rew_tracking_lin_vel: 0.2552
        Mean episode terrain_level: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 1.22s
                        Total time: 519.91s
                               ETA: 769 mins 11.6 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 87591 steps/s (collection: 0.998s, learning 0.124s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.08
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 248.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2548
       Mean episode rew_ang_vel_xy: -0.0950
          Mean episode rew_dof_acc: -0.2393
   Mean episode rew_dof_pos_limits: -0.0956
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1768
           Mean episode rew_no_fly: 0.0141
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.1070
      Mean episode rew_stand_still: -0.0098
      Mean episode rew_termination: -0.1626
          Mean episode rew_torques: -0.0214
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2372
        Mean episode terrain_level: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 1.12s
                        Total time: 521.03s
                               ETA: 769 mins 27.4 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 86405 steps/s (collection: 1.014s, learning 0.123s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.09
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 257.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6501
       Mean episode rew_ang_vel_xy: -0.1020
          Mean episode rew_dof_acc: -0.2648
   Mean episode rew_dof_pos_limits: -0.1071
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1752
           Mean episode rew_no_fly: 0.0156
      Mean episode rew_orientation: -0.0103
       Mean episode rew_smoothness: -2.3533
      Mean episode rew_stand_still: -0.0134
      Mean episode rew_termination: -0.1546
          Mean episode rew_torques: -0.0240
 Mean episode rew_tracking_ang_vel: 0.0568
 Mean episode rew_tracking_lin_vel: 0.2829
        Mean episode terrain_level: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.14s
                        Total time: 522.17s
                               ETA: 769 mins 44.5 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 85493 steps/s (collection: 1.020s, learning 0.129s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.09
               Mean reward (total): -2.80
                Mean reward (task): -2.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 362.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7994
       Mean episode rew_ang_vel_xy: -0.1069
          Mean episode rew_dof_acc: -0.2710
   Mean episode rew_dof_pos_limits: -0.1098
      Mean episode rew_joint_power: -0.0074
        Mean episode rew_lin_vel_z: -0.1821
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0106
       Mean episode rew_smoothness: -2.4616
      Mean episode rew_stand_still: -0.0112
      Mean episode rew_termination: -0.1553
          Mean episode rew_torques: -0.0246
 Mean episode rew_tracking_ang_vel: 0.0578
 Mean episode rew_tracking_lin_vel: 0.2866
        Mean episode terrain_level: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 1.15s
                        Total time: 523.32s
                               ETA: 770 mins 2.6 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 89133 steps/s (collection: 0.962s, learning 0.141s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.10
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 223.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2764
       Mean episode rew_ang_vel_xy: -0.0937
          Mean episode rew_dof_acc: -0.2453
   Mean episode rew_dof_pos_limits: -0.0917
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1760
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.1212
      Mean episode rew_stand_still: -0.0122
      Mean episode rew_termination: -0.1631
          Mean episode rew_torques: -0.0210
 Mean episode rew_tracking_ang_vel: 0.0517
 Mean episode rew_tracking_lin_vel: 0.2633
        Mean episode terrain_level: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 1.10s
                        Total time: 524.42s
                               ETA: 770 mins 16.5 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 82591 steps/s (collection: 1.049s, learning 0.141s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -2.92
                Mean reward (task): -2.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 321.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5493
       Mean episode rew_ang_vel_xy: -0.1039
          Mean episode rew_dof_acc: -0.2692
   Mean episode rew_dof_pos_limits: -0.0990
      Mean episode rew_joint_power: -0.0072
        Mean episode rew_lin_vel_z: -0.1839
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0092
       Mean episode rew_smoothness: -2.2947
      Mean episode rew_stand_still: -0.0075
      Mean episode rew_termination: -0.1596
          Mean episode rew_torques: -0.0231
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2632
        Mean episode terrain_level: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 1.19s
                        Total time: 525.61s
                               ETA: 770 mins 38.1 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 83597 steps/s (collection: 1.035s, learning 0.141s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 259.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.2341
       Mean episode rew_ang_vel_xy: -0.0972
          Mean episode rew_dof_acc: -0.2501
   Mean episode rew_dof_pos_limits: -0.0856
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1760
           Mean episode rew_no_fly: 0.0121
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.0829
      Mean episode rew_stand_still: -0.0106
      Mean episode rew_termination: -0.1635
          Mean episode rew_torques: -0.0202
 Mean episode rew_tracking_ang_vel: 0.0446
 Mean episode rew_tracking_lin_vel: 0.2246
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 1.18s
                        Total time: 526.79s
                               ETA: 770 mins 58.3 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 15869 steps/s (collection: 6.053s, learning 0.141s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.11
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 261.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4690
       Mean episode rew_ang_vel_xy: -0.1048
          Mean episode rew_dof_acc: -0.2755
   Mean episode rew_dof_pos_limits: -0.0902
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1930
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0074
       Mean episode rew_smoothness: -2.2351
      Mean episode rew_stand_still: -0.0106
      Mean episode rew_termination: -0.1619
          Mean episode rew_torques: -0.0217
 Mean episode rew_tracking_ang_vel: 0.0487
 Mean episode rew_tracking_lin_vel: 0.2319
        Mean episode terrain_level: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 6.19s
                        Total time: 532.98s
                               ETA: 778 mins 38.3 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 67481 steps/s (collection: 1.331s, learning 0.125s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.12
               Mean reward (total): -3.00
                Mean reward (task): -3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 294.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6790
       Mean episode rew_ang_vel_xy: -0.1021
          Mean episode rew_dof_acc: -0.2713
   Mean episode rew_dof_pos_limits: -0.1047
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1842
           Mean episode rew_no_fly: 0.0154
      Mean episode rew_orientation: -0.0093
       Mean episode rew_smoothness: -2.3993
      Mean episode rew_stand_still: -0.0103
      Mean episode rew_termination: -0.1544
          Mean episode rew_torques: -0.0235
 Mean episode rew_tracking_ang_vel: 0.0543
 Mean episode rew_tracking_lin_vel: 0.2666
        Mean episode terrain_level: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.46s
                        Total time: 534.44s
                               ETA: 779 mins 22.1 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 63912 steps/s (collection: 1.416s, learning 0.122s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.13
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 246.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7783
       Mean episode rew_ang_vel_xy: -0.1055
          Mean episode rew_dof_acc: -0.2944
   Mean episode rew_dof_pos_limits: -0.1032
      Mean episode rew_joint_power: -0.0077
        Mean episode rew_lin_vel_z: -0.1959
           Mean episode rew_no_fly: 0.0145
      Mean episode rew_orientation: -0.0086
       Mean episode rew_smoothness: -2.4550
      Mean episode rew_stand_still: -0.0110
      Mean episode rew_termination: -0.1551
          Mean episode rew_torques: -0.0241
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.2622
        Mean episode terrain_level: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 1.54s
                        Total time: 535.98s
                               ETA: 780 mins 12.9 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 64481 steps/s (collection: 1.403s, learning 0.122s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.13
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 238.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1006
       Mean episode rew_ang_vel_xy: -0.0961
          Mean episode rew_dof_acc: -0.2360
   Mean episode rew_dof_pos_limits: -0.0829
      Mean episode rew_joint_power: -0.0060
        Mean episode rew_lin_vel_z: -0.1873
           Mean episode rew_no_fly: 0.0119
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -1.9945
      Mean episode rew_stand_still: -0.0081
      Mean episode rew_termination: -0.1668
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2373
        Mean episode terrain_level: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 1.52s
                        Total time: 537.50s
                               ETA: 781 mins 2.3 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 61235 steps/s (collection: 1.484s, learning 0.122s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.14
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5819
       Mean episode rew_ang_vel_xy: -0.0981
          Mean episode rew_dof_acc: -0.2526
   Mean episode rew_dof_pos_limits: -0.1039
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1888
           Mean episode rew_no_fly: 0.0147
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -2.3153
      Mean episode rew_stand_still: -0.0103
      Mean episode rew_termination: -0.1578
          Mean episode rew_torques: -0.0232
 Mean episode rew_tracking_ang_vel: 0.0543
 Mean episode rew_tracking_lin_vel: 0.2637
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.61s
                        Total time: 539.11s
                               ETA: 781 mins 58.6 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 62651 steps/s (collection: 1.447s, learning 0.122s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 257.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4414
       Mean episode rew_ang_vel_xy: -0.1023
          Mean episode rew_dof_acc: -0.2541
   Mean episode rew_dof_pos_limits: -0.0929
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1860
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.2140
      Mean episode rew_stand_still: -0.0109
      Mean episode rew_termination: -0.1621
          Mean episode rew_torques: -0.0217
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2442
        Mean episode terrain_level: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 1.57s
                        Total time: 540.68s
                               ETA: 782 mins 51.5 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 65799 steps/s (collection: 1.372s, learning 0.122s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.15
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 235.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6096
       Mean episode rew_ang_vel_xy: -0.0969
          Mean episode rew_dof_acc: -0.2683
   Mean episode rew_dof_pos_limits: -0.1034
      Mean episode rew_joint_power: -0.0070
        Mean episode rew_lin_vel_z: -0.1974
           Mean episode rew_no_fly: 0.0149
      Mean episode rew_orientation: -0.0095
       Mean episode rew_smoothness: -2.3581
      Mean episode rew_stand_still: -0.0118
      Mean episode rew_termination: -0.1579
          Mean episode rew_torques: -0.0235
 Mean episode rew_tracking_ang_vel: 0.0569
 Mean episode rew_tracking_lin_vel: 0.2730
        Mean episode terrain_level: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 1.49s
                        Total time: 542.17s
                               ETA: 783 mins 37.7 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 63406 steps/s (collection: 1.412s, learning 0.138s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4780
       Mean episode rew_ang_vel_xy: -0.1043
          Mean episode rew_dof_acc: -0.2688
   Mean episode rew_dof_pos_limits: -0.0872
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1852
           Mean episode rew_no_fly: 0.0116
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -2.2269
      Mean episode rew_stand_still: -0.0105
      Mean episode rew_termination: -0.1622
          Mean episode rew_torques: -0.0214
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2454
        Mean episode terrain_level: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 1.55s
                        Total time: 543.72s
                               ETA: 784 mins 28.6 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 66916 steps/s (collection: 1.347s, learning 0.122s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.16
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 281.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -4.1091
       Mean episode rew_ang_vel_xy: -0.1083
          Mean episode rew_dof_acc: -0.3006
   Mean episode rew_dof_pos_limits: -0.1153
      Mean episode rew_joint_power: -0.0081
        Mean episode rew_lin_vel_z: -0.1938
           Mean episode rew_no_fly: 0.0157
      Mean episode rew_orientation: -0.0099
       Mean episode rew_smoothness: -2.6649
      Mean episode rew_stand_still: -0.0147
      Mean episode rew_termination: -0.1520
          Mean episode rew_torques: -0.0266
 Mean episode rew_tracking_ang_vel: 0.0627
 Mean episode rew_tracking_lin_vel: 0.2997
        Mean episode terrain_level: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 1.47s
                        Total time: 545.19s
                               ETA: 785 mins 12.3 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 67801 steps/s (collection: 1.327s, learning 0.123s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.0818
       Mean episode rew_ang_vel_xy: -0.0897
          Mean episode rew_dof_acc: -0.2396
   Mean episode rew_dof_pos_limits: -0.0801
      Mean episode rew_joint_power: -0.0062
        Mean episode rew_lin_vel_z: -0.1789
           Mean episode rew_no_fly: 0.0117
      Mean episode rew_orientation: -0.0071
       Mean episode rew_smoothness: -1.9949
      Mean episode rew_stand_still: -0.0099
      Mean episode rew_termination: -0.1689
          Mean episode rew_torques: -0.0191
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2106
        Mean episode terrain_level: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 1.45s
                        Total time: 546.64s
                               ETA: 785 mins 54.2 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 66796 steps/s (collection: 1.349s, learning 0.123s)
               Value function loss: 0.0866
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.17
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 264.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7236
       Mean episode rew_ang_vel_xy: -0.1042
          Mean episode rew_dof_acc: -0.2767
   Mean episode rew_dof_pos_limits: -0.0968
      Mean episode rew_joint_power: -0.0074
        Mean episode rew_lin_vel_z: -0.1856
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.4003
      Mean episode rew_stand_still: -0.0148
      Mean episode rew_termination: -0.1606
          Mean episode rew_torques: -0.0235
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.2694
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 1.47s
                        Total time: 548.11s
                               ETA: 786 mins 37.9 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 64014 steps/s (collection: 1.412s, learning 0.124s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.18
               Mean reward (total): -3.56
                Mean reward (task): -3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1529
       Mean episode rew_ang_vel_xy: -0.0932
          Mean episode rew_dof_acc: -0.2476
   Mean episode rew_dof_pos_limits: -0.0778
      Mean episode rew_joint_power: -0.0065
        Mean episode rew_lin_vel_z: -0.1839
           Mean episode rew_no_fly: 0.0109
      Mean episode rew_orientation: -0.0066
       Mean episode rew_smoothness: -2.0489
      Mean episode rew_stand_still: -0.0072
      Mean episode rew_termination: -0.1688
          Mean episode rew_torques: -0.0193
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2119
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 1.54s
                        Total time: 549.65s
                               ETA: 787 mins 26.8 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 66449 steps/s (collection: 1.358s, learning 0.122s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.19
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 257.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5434
       Mean episode rew_ang_vel_xy: -0.0966
          Mean episode rew_dof_acc: -0.2566
   Mean episode rew_dof_pos_limits: -0.0925
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1759
           Mean episode rew_no_fly: 0.0129
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.2897
      Mean episode rew_stand_still: -0.0095
      Mean episode rew_termination: -0.1624
          Mean episode rew_torques: -0.0218
 Mean episode rew_tracking_ang_vel: 0.0483
 Mean episode rew_tracking_lin_vel: 0.2386
        Mean episode terrain_level: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 1.48s
                        Total time: 551.13s
                               ETA: 788 mins 10.8 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 66059 steps/s (collection: 1.364s, learning 0.124s)
               Value function loss: 0.0956
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.9792
       Mean episode rew_ang_vel_xy: -0.1092
          Mean episode rew_dof_acc: -0.3002
   Mean episode rew_dof_pos_limits: -0.1027
      Mean episode rew_joint_power: -0.0079
        Mean episode rew_lin_vel_z: -0.2023
           Mean episode rew_no_fly: 0.0138
      Mean episode rew_orientation: -0.0084
       Mean episode rew_smoothness: -2.5554
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1570
          Mean episode rew_torques: -0.0249
 Mean episode rew_tracking_ang_vel: 0.0526
 Mean episode rew_tracking_lin_vel: 0.2625
        Mean episode terrain_level: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 1.49s
                        Total time: 552.62s
                               ETA: 788 mins 55.3 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 67964 steps/s (collection: 1.325s, learning 0.122s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.32
                Mean reward (task): -3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 218.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.8990
       Mean episode rew_ang_vel_xy: -0.1033
          Mean episode rew_dof_acc: -0.2634
   Mean episode rew_dof_pos_limits: -0.1080
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1920
           Mean episode rew_no_fly: 0.0142
      Mean episode rew_orientation: -0.0097
       Mean episode rew_smoothness: -2.4992
      Mean episode rew_stand_still: -0.0129
      Mean episode rew_termination: -0.1562
          Mean episode rew_torques: -0.0247
 Mean episode rew_tracking_ang_vel: 0.0638
 Mean episode rew_tracking_lin_vel: 0.2880
        Mean episode terrain_level: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 1.45s
                        Total time: 554.06s
                               ETA: 789 mins 36.2 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 68596 steps/s (collection: 1.312s, learning 0.121s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.20
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 253.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5664
       Mean episode rew_ang_vel_xy: -0.0979
          Mean episode rew_dof_acc: -0.2572
   Mean episode rew_dof_pos_limits: -0.0906
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1846
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0077
       Mean episode rew_smoothness: -2.2884
      Mean episode rew_stand_still: -0.0086
      Mean episode rew_termination: -0.1636
          Mean episode rew_torques: -0.0215
 Mean episode rew_tracking_ang_vel: 0.0505
 Mean episode rew_tracking_lin_vel: 0.2350
        Mean episode terrain_level: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.43s
                        Total time: 555.50s
                               ETA: 790 mins 15.7 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 61602 steps/s (collection: 1.455s, learning 0.140s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.21
               Mean reward (total): -3.08
                Mean reward (task): -3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 266.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.1582
       Mean episode rew_ang_vel_xy: -0.0904
          Mean episode rew_dof_acc: -0.2475
   Mean episode rew_dof_pos_limits: -0.0775
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1802
           Mean episode rew_no_fly: 0.0108
      Mean episode rew_orientation: -0.0062
       Mean episode rew_smoothness: -2.0390
      Mean episode rew_stand_still: -0.0095
      Mean episode rew_termination: -0.1674
          Mean episode rew_torques: -0.0190
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.1996
        Mean episode terrain_level: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 1.60s
                        Total time: 557.09s
                               ETA: 791 mins 9.0 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 66647 steps/s (collection: 1.351s, learning 0.124s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5774
       Mean episode rew_ang_vel_xy: -0.1003
          Mean episode rew_dof_acc: -0.2649
   Mean episode rew_dof_pos_limits: -0.0921
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1891
           Mean episode rew_no_fly: 0.0123
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.3196
      Mean episode rew_stand_still: -0.0093
      Mean episode rew_termination: -0.1616
          Mean episode rew_torques: -0.0223
 Mean episode rew_tracking_ang_vel: 0.0476
 Mean episode rew_tracking_lin_vel: 0.2464
        Mean episode terrain_level: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.47s
                        Total time: 558.57s
                               ETA: 791 mins 51.8 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 67484 steps/s (collection: 1.335s, learning 0.122s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.22
               Mean reward (total): -3.44
                Mean reward (task): -3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.3004
       Mean episode rew_ang_vel_xy: -0.0878
          Mean episode rew_dof_acc: -0.2385
   Mean episode rew_dof_pos_limits: -0.0910
      Mean episode rew_joint_power: -0.0061
        Mean episode rew_lin_vel_z: -0.1917
           Mean episode rew_no_fly: 0.0131
      Mean episode rew_orientation: -0.0087
       Mean episode rew_smoothness: -2.1537
      Mean episode rew_stand_still: -0.0128
      Mean episode rew_termination: -0.1642
          Mean episode rew_torques: -0.0208
 Mean episode rew_tracking_ang_vel: 0.0482
 Mean episode rew_tracking_lin_vel: 0.2354
        Mean episode terrain_level: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 1.46s
                        Total time: 560.02s
                               ETA: 792 mins 32.9 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 69094 steps/s (collection: 1.301s, learning 0.122s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.23
               Mean reward (total): -3.28
                Mean reward (task): -3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 223.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4151
       Mean episode rew_ang_vel_xy: -0.0956
          Mean episode rew_dof_acc: -0.2563
   Mean episode rew_dof_pos_limits: -0.0854
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1878
           Mean episode rew_no_fly: 0.0120
      Mean episode rew_orientation: -0.0072
       Mean episode rew_smoothness: -2.2116
      Mean episode rew_stand_still: -0.0080
      Mean episode rew_termination: -0.1658
          Mean episode rew_torques: -0.0209
 Mean episode rew_tracking_ang_vel: 0.0445
 Mean episode rew_tracking_lin_vel: 0.2160
        Mean episode terrain_level: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 1.42s
                        Total time: 561.45s
                               ETA: 793 mins 10.9 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 64927 steps/s (collection: 1.393s, learning 0.121s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.20
                Mean reward (task): -3.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 246.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.4487
       Mean episode rew_ang_vel_xy: -0.0977
          Mean episode rew_dof_acc: -0.2515
   Mean episode rew_dof_pos_limits: -0.0872
      Mean episode rew_joint_power: -0.0067
        Mean episode rew_lin_vel_z: -0.1787
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0079
       Mean episode rew_smoothness: -2.2216
      Mean episode rew_stand_still: -0.0146
      Mean episode rew_termination: -0.1660
          Mean episode rew_torques: -0.0214
 Mean episode rew_tracking_ang_vel: 0.0457
 Mean episode rew_tracking_lin_vel: 0.2407
        Mean episode terrain_level: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 1.51s
                        Total time: 562.96s
                               ETA: 793 mins 56.6 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 64485 steps/s (collection: 1.403s, learning 0.122s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.24
               Mean reward (total): -3.04
                Mean reward (task): -3.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 288.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5872
       Mean episode rew_ang_vel_xy: -0.0959
          Mean episode rew_dof_acc: -0.2560
   Mean episode rew_dof_pos_limits: -0.0936
      Mean episode rew_joint_power: -0.0069
        Mean episode rew_lin_vel_z: -0.1932
           Mean episode rew_no_fly: 0.0130
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.3361
      Mean episode rew_stand_still: -0.0107
      Mean episode rew_termination: -0.1615
          Mean episode rew_torques: -0.0223
 Mean episode rew_tracking_ang_vel: 0.0497
 Mean episode rew_tracking_lin_vel: 0.2435
        Mean episode terrain_level: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 1.52s
                        Total time: 564.48s
                               ETA: 794 mins 43.0 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 65910 steps/s (collection: 1.370s, learning 0.122s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.25
               Mean reward (total): -3.40
                Mean reward (task): -3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 210.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7298
       Mean episode rew_ang_vel_xy: -0.1023
          Mean episode rew_dof_acc: -0.2774
   Mean episode rew_dof_pos_limits: -0.0920
      Mean episode rew_joint_power: -0.0073
        Mean episode rew_lin_vel_z: -0.1873
           Mean episode rew_no_fly: 0.0122
      Mean episode rew_orientation: -0.0073
       Mean episode rew_smoothness: -2.4107
      Mean episode rew_stand_still: -0.0087
      Mean episode rew_termination: -0.1624
          Mean episode rew_torques: -0.0224
 Mean episode rew_tracking_ang_vel: 0.0524
 Mean episode rew_tracking_lin_vel: 0.2411
        Mean episode terrain_level: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 1.49s
                        Total time: 565.98s
                               ETA: 795 mins 26.4 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 68683 steps/s (collection: 1.310s, learning 0.122s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 277.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7582
       Mean episode rew_ang_vel_xy: -0.0990
          Mean episode rew_dof_acc: -0.2589
   Mean episode rew_dof_pos_limits: -0.0970
      Mean episode rew_joint_power: -0.0071
        Mean episode rew_lin_vel_z: -0.1874
           Mean episode rew_no_fly: 0.0133
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.4249
      Mean episode rew_stand_still: -0.0105
      Mean episode rew_termination: -0.1609
          Mean episode rew_torques: -0.0227
 Mean episode rew_tracking_ang_vel: 0.0517
 Mean episode rew_tracking_lin_vel: 0.2539
        Mean episode terrain_level: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.43s
                        Total time: 567.41s
                               ETA: 796 mins 4.6 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 65075 steps/s (collection: 1.389s, learning 0.121s)
               Value function loss: 0.0897
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.26
               Mean reward (total): -3.36
                Mean reward (task): -3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.6884
       Mean episode rew_ang_vel_xy: -0.1020
          Mean episode rew_dof_acc: -0.2535
   Mean episode rew_dof_pos_limits: -0.0934
      Mean episode rew_joint_power: -0.0068
        Mean episode rew_lin_vel_z: -0.1801
           Mean episode rew_no_fly: 0.0127
      Mean episode rew_orientation: -0.0078
       Mean episode rew_smoothness: -2.3725
      Mean episode rew_stand_still: -0.0102
      Mean episode rew_termination: -0.1614
          Mean episode rew_torques: -0.0221
 Mean episode rew_tracking_ang_vel: 0.0501
 Mean episode rew_tracking_lin_vel: 0.2460
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 1.51s
                        Total time: 568.92s
                               ETA: 796 mins 49.4 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 66799 steps/s (collection: 1.350s, learning 0.122s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.27
               Mean reward (total): -3.12
                Mean reward (task): -3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 273.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.7785
       Mean episode rew_ang_vel_xy: -0.0961
          Mean episode rew_dof_acc: -0.2313
   Mean episode rew_dof_pos_limits: -0.1049
      Mean episode rew_joint_power: -0.0064
        Mean episode rew_lin_vel_z: -0.1787
           Mean episode rew_no_fly: 0.0140
      Mean episode rew_orientation: -0.0101
       Mean episode rew_smoothness: -2.4226
      Mean episode rew_stand_still: -0.0075
      Mean episode rew_termination: -0.1590
          Mean episode rew_torques: -0.0237
 Mean episode rew_tracking_ang_vel: 0.0537
 Mean episode rew_tracking_lin_vel: 0.2430
        Mean episode terrain_level: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 1.47s
                        Total time: 570.39s
                               ETA: 797 mins 30.7 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 66343 steps/s (collection: 1.357s, learning 0.125s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0114
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 3.27
               Mean reward (total): -3.16
                Mean reward (task): -3.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 282.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -3.5949
       Mean episode rew_ang_vel_xy: -0.0987
          Mean episode rew_dof_acc: -0.2414
   Mean episode rew_dof_pos_limits: -0.0921
      Mean episode rew_joint_power: -0.0066
        Mean episode rew_lin_vel_z: -0.1866
           Mean episode rew_no_fly: 0.0126
      Mean episode rew_orientation: -0.0082
       Mean episode rew_smoothness: -2.3068
      Mean episode rew_stand_still: -0.0120
      Mean episode rew_termination: -0.1655
          Mean episode rew_torques: -0.0221
 Mean episode rew_tracking_ang_vel: 0.0478
 Mean episode rew_tracking_lin_vel: 0.2219
        Mean episode terrain_level: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 1.48s
                        Total time: 571.87s
                               ETA: 798 mins 12.7 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/7eoxem1snpvtil734eckq
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    