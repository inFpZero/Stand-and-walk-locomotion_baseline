swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_132620-8d5899d4
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yh_gym_his_Jun01_13-26-20 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/22fy96g2hxce6mlwr88ki
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.04 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 20440 steps/s (collection: 4.630s, learning 0.179s)
               Value function loss: 3.2620
                    Surrogate loss: 0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0033
       Mean episode rew_ang_vel_xy: -0.0167
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0514
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0007
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1349
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 1.6331
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 4.81s
                        Total time: 4.81s
                               ETA: 4007 mins 38.1 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 120061 steps/s (collection: 0.658s, learning 0.161s)
               Value function loss: 3.7377
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 1.0212
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.82s
                        Total time: 5.63s
                               ETA: 2344 mins 55.7 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 147717 steps/s (collection: 0.516s, learning 0.150s)
               Value function loss: 3.2053
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0600
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0170
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.5566
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.67s
                        Total time: 6.29s
                               ETA: 1748 mins 6.2 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 151346 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 2.6243
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0587
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0176
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.2772
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.65s
                        Total time: 6.94s
                               ETA: 1446 mins 21.7 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 146850 steps/s (collection: 0.548s, learning 0.121s)
               Value function loss: 1.9693
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0220
          Mean episode rew_dof_acc: -0.0403
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0176
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.1366
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.67s
                        Total time: 7.61s
                               ETA: 1268 mins 37.6 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 139684 steps/s (collection: 0.582s, learning 0.121s)
               Value function loss: 1.3459
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0606
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0182
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0660
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.70s
                        Total time: 8.32s
                               ETA: 1154 mins 54.1 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 131418 steps/s (collection: 0.623s, learning 0.125s)
               Value function loss: 1.1327
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0605
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0181
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0329
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.75s
                        Total time: 9.06s
                               ETA: 1078 mins 56.2 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 133056 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.9300
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0621
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0186
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.74s
                        Total time: 9.80s
                               ETA: 1020 mins 60.0 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 116326 steps/s (collection: 0.711s, learning 0.134s)
               Value function loss: 0.8380
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0599
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0188
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.85s
                        Total time: 10.65s
                               ETA: 985 mins 46.3 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 137035 steps/s (collection: 0.592s, learning 0.125s)
               Value function loss: 0.6894
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0190
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.72s
                        Total time: 11.37s
                               ETA: 946 mins 56.7 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 133752 steps/s (collection: 0.613s, learning 0.122s)
               Value function loss: 0.6530
                    Surrogate loss: 0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0629
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0193
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.73s
                        Total time: 12.10s
                               ETA: 916 mins 30.7 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 138516 steps/s (collection: 0.584s, learning 0.125s)
               Value function loss: 0.3607
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0620
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0196
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.71s
                        Total time: 12.81s
                               ETA: 889 mins 23.5 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 118201 steps/s (collection: 0.703s, learning 0.129s)
               Value function loss: 0.3042
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0438
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0594
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0200
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.83s
                        Total time: 13.64s
                               ETA: 874 mins 15.6 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 124736 steps/s (collection: 0.667s, learning 0.121s)
               Value function loss: 0.2134
                    Surrogate loss: 0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0430
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0594
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0200
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.79s
                        Total time: 14.43s
                               ETA: 858 mins 41.6 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 132554 steps/s (collection: 0.619s, learning 0.122s)
               Value function loss: 0.1973
                    Surrogate loss: 0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0627
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0204
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.74s
                        Total time: 15.17s
                               ETA: 842 mins 37.2 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 130070 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.1281
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0595
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0204
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.76s
                        Total time: 15.93s
                               ETA: 829 mins 17.6 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 136989 steps/s (collection: 0.589s, learning 0.129s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0633
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0208
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.72s
                        Total time: 16.64s
                               ETA: 815 mins 39.6 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 131455 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0207
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0589
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0215
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.75s
                        Total time: 17.39s
                               ETA: 804 mins 56.4 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 120679 steps/s (collection: 0.690s, learning 0.125s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0468
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0217
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.81s
                        Total time: 18.21s
                               ETA: 798 mins 16.4 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 143092 steps/s (collection: 0.565s, learning 0.122s)
               Value function loss: 0.0246
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0477
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0583
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0221
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.69s
                        Total time: 18.89s
                               ETA: 786 mins 57.5 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 134089 steps/s (collection: 0.601s, learning 0.132s)
               Value function loss: 0.0236
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0481
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0589
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0223
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.73s
                        Total time: 19.63s
                               ETA: 778 mins 33.0 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 124211 steps/s (collection: 0.658s, learning 0.133s)
               Value function loss: 0.0213
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0476
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0583
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0227
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.79s
                        Total time: 20.42s
                               ETA: 773 mins 6.7 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 118150 steps/s (collection: 0.710s, learning 0.122s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0491
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0231
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.83s
                        Total time: 21.25s
                               ETA: 769 mins 37.0 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 139792 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0496
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0552
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0235
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.70s
                        Total time: 21.95s
                               ETA: 761 mins 56.4 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 141900 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0494
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0555
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0241
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.69s
                        Total time: 22.65s
                               ETA: 754 mins 31.7 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 132644 steps/s (collection: 0.619s, learning 0.122s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0504
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0544
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0243
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.74s
                        Total time: 23.39s
                               ETA: 749 mins 14.1 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 134936 steps/s (collection: 0.582s, learning 0.147s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0506
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0520
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0247
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.73s
                        Total time: 24.12s
                               ETA: 743 mins 56.7 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 124277 steps/s (collection: 0.653s, learning 0.138s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0511
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0532
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0252
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.79s
                        Total time: 24.91s
                               ETA: 740 mins 53.4 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 136560 steps/s (collection: 0.598s, learning 0.122s)
               Value function loss: 0.0147
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0525
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0535
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0259
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.72s
                        Total time: 25.63s
                               ETA: 736 mins 0.1 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 137235 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0528
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0262
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.72s
                        Total time: 26.34s
                               ETA: 731 mins 20.4 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 131485 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0525
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0537
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0264
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.75s
                        Total time: 27.09s
                               ETA: 727 mins 49.2 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 125854 steps/s (collection: 0.659s, learning 0.122s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0534
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0531
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0268
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.78s
                        Total time: 27.87s
                               ETA: 725 mins 23.4 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 123559 steps/s (collection: 0.655s, learning 0.141s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0535
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0525
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0272
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.80s
                        Total time: 28.67s
                               ETA: 723 mins 28.4 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 133605 steps/s (collection: 0.601s, learning 0.135s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0539
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0551
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0275
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.74s
                        Total time: 29.40s
                               ETA: 720 mins 12.1 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 126167 steps/s (collection: 0.657s, learning 0.122s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0548
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0283
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.78s
                        Total time: 30.18s
                               ETA: 718 mins 9.0 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 143420 steps/s (collection: 0.564s, learning 0.121s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0551
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0517
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0285
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.69s
                        Total time: 30.87s
                               ETA: 714 mins 2.5 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 136234 steps/s (collection: 0.598s, learning 0.123s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0559
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0523
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0290
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.72s
                        Total time: 31.59s
                               ETA: 710 mins 58.2 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 131443 steps/s (collection: 0.604s, learning 0.143s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0553
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0509
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0291
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.75s
                        Total time: 32.34s
                               ETA: 708 mins 38.1 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 122237 steps/s (collection: 0.671s, learning 0.133s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0569
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0297
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.80s
                        Total time: 33.14s
                               ETA: 707 mins 37.3 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 127604 steps/s (collection: 0.644s, learning 0.126s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0568
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0301
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.77s
                        Total time: 33.91s
                               ETA: 705 mins 57.3 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 133692 steps/s (collection: 0.612s, learning 0.124s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0565
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0488
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0305
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.74s
                        Total time: 34.65s
                               ETA: 703 mins 39.3 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 133292 steps/s (collection: 0.612s, learning 0.126s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0572
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0486
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0309
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.74s
                        Total time: 35.39s
                               ETA: 701 mins 30.6 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 128793 steps/s (collection: 0.635s, learning 0.129s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0590
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0312
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.76s
                        Total time: 36.15s
                               ETA: 699 mins 57.7 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 133330 steps/s (collection: 0.602s, learning 0.135s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0583
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0467
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0315
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.74s
                        Total time: 36.89s
                               ETA: 697 mins 59.5 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 140440 steps/s (collection: 0.569s, learning 0.131s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0591
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0320
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.70s
                        Total time: 37.59s
                               ETA: 695 mins 25.0 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 125127 steps/s (collection: 0.639s, learning 0.147s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0593
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0465
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0319
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.79s
                        Total time: 38.37s
                               ETA: 694 mins 30.3 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 125477 steps/s (collection: 0.643s, learning 0.141s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0116
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0592
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0321
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.78s
                        Total time: 39.15s
                               ETA: 693 mins 35.6 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 128335 steps/s (collection: 0.626s, learning 0.140s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0595
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0474
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0329
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.77s
                        Total time: 39.92s
                               ETA: 692 mins 24.9 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 135942 steps/s (collection: 0.574s, learning 0.149s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0605
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0483
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0330
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.72s
                        Total time: 40.64s
                               ETA: 690 mins 33.5 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 133605 steps/s (collection: 0.597s, learning 0.139s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0600
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0475
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0337
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.74s
                        Total time: 41.38s
                               ETA: 688 mins 59.0 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 134804 steps/s (collection: 0.605s, learning 0.124s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0602
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0468
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0339
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.73s
                        Total time: 42.11s
                               ETA: 687 mins 21.9 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 133494 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0608
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0345
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.74s
                        Total time: 42.85s
                               ETA: 685 mins 55.3 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 124047 steps/s (collection: 0.663s, learning 0.130s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0611
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0347
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.79s
                        Total time: 43.64s
                               ETA: 685 mins 24.8 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 131772 steps/s (collection: 0.621s, learning 0.125s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0613
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0456
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0355
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.75s
                        Total time: 44.38s
                               ETA: 684 mins 12.4 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 119426 steps/s (collection: 0.694s, learning 0.129s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0130
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0625
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0497
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0358
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.82s
                        Total time: 45.21s
                               ETA: 684 mins 12.7 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 118530 steps/s (collection: 0.688s, learning 0.142s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0133
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0623
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0367
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.83s
                        Total time: 46.04s
                               ETA: 684 mins 18.5 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 123236 steps/s (collection: 0.651s, learning 0.147s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0633
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0504
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0368
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.80s
                        Total time: 46.83s
                               ETA: 683 mins 56.3 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 118418 steps/s (collection: 0.677s, learning 0.153s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0136
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0633
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0464
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0372
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.83s
                        Total time: 47.66s
                               ETA: 684 mins 2.8 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 124849 steps/s (collection: 0.665s, learning 0.122s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0136
       Mean episode rew_ang_vel_xy: -0.0192
          Mean episode rew_dof_acc: -0.0629
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0373
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.79s
                        Total time: 48.45s
                               ETA: 683 mins 32.9 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 124020 steps/s (collection: 0.665s, learning 0.128s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0138
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0639
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0459
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0378
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.79s
                        Total time: 49.24s
                               ETA: 683 mins 8.3 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 141308 steps/s (collection: 0.572s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0644
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0492
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0382
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.70s
                        Total time: 49.94s
                               ETA: 681 mins 25.1 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 130967 steps/s (collection: 0.610s, learning 0.141s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0645
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0389
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.75s
                        Total time: 50.69s
                               ETA: 680 mins 29.4 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 127167 steps/s (collection: 0.627s, learning 0.146s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0642
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0466
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0390
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.77s
                        Total time: 51.46s
                               ETA: 679 mins 53.3 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/22fy96g2hxce6mlwr88ki
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    