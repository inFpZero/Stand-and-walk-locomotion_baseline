swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240531_174606-f7a40a9d
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run test_May31_17-46-06 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/7yaepr9xj19mcaabcs9yg
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 18.04 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=512, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=512, out_features=256, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=256, out_features=128, bias=True)
  (7): ELU(alpha=1.0)
  (8): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=512, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=512, out_features=256, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=256, out_features=128, bias=True)
  (7): ELU(alpha=1.0)
  (8): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 11661 steps/s (collection: 8.060s, learning 0.370s)
               Value function loss: 0.0987
                    Surrogate loss: 0.0244
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0023
       Mean episode rew_ang_vel_xy: -0.0167
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0012
        Mean episode rew_lin_vel_z: -0.0546
           Mean episode rew_no_fly: 0.0008
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0015
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 1.6600
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.43s
                        Total time: 8.43s
                               ETA: 7024 mins 48.0 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 24902 steps/s (collection: 3.674s, learning 0.274s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0643
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 1.0989
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.95s
                        Total time: 12.38s
                               ETA: 5157 mins 5.0 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 126952 steps/s (collection: 0.553s, learning 0.221s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0651
           Mean episode rew_no_fly: 0.0017
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.6482
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.77s
                        Total time: 13.15s
                               ETA: 3653 mins 4.4 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 123657 steps/s (collection: 0.575s, learning 0.220s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0648
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.3518
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.79s
                        Total time: 13.95s
                               ETA: 2905 mins 21.5 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 119127 steps/s (collection: 0.604s, learning 0.221s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0462
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0643
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.1863
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.83s
                        Total time: 14.77s
                               ETA: 2461 mins 45.8 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 118115 steps/s (collection: 0.612s, learning 0.221s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0656
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0973
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.83s
                        Total time: 15.60s
                               ETA: 2167 mins 0.6 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 108317 steps/s (collection: 0.672s, learning 0.235s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0467
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0676
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0507
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.91s
                        Total time: 16.51s
                               ETA: 1965 mins 25.7 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 98731 steps/s (collection: 0.768s, learning 0.228s)
               Value function loss: 0.0013
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0471
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0661
           Mean episode rew_no_fly: 0.0018
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 1.00s
                        Total time: 17.51s
                               ETA: 1823 mins 25.0 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 109005 steps/s (collection: 0.681s, learning 0.221s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0472
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0668
           Mean episode rew_no_fly: 0.0019
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.90s
                        Total time: 18.41s
                               ETA: 1704 mins 16.3 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 106839 steps/s (collection: 0.688s, learning 0.232s)
               Value function loss: 22.0992
                    Surrogate loss: 0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0471
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0639
           Mean episode rew_no_fly: 0.0019
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.92s
                        Total time: 19.33s
                               ETA: 1610 mins 28.5 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 102411 steps/s (collection: 0.736s, learning 0.224s)
               Value function loss: 4.5719
                    Surrogate loss: 0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0484
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0670
           Mean episode rew_no_fly: 0.0019
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.96s
                        Total time: 20.29s
                               ETA: 1536 mins 44.6 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 99231 steps/s (collection: 0.763s, learning 0.227s)
               Value function loss: 0.3929
                    Surrogate loss: 0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0482
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0655
           Mean episode rew_no_fly: 0.0019
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.99s
                        Total time: 21.28s
                               ETA: 1477 mins 26.0 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 93176 steps/s (collection: 0.710s, learning 0.345s)
               Value function loss: 0.1997
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0491
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0670
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.06s
                        Total time: 22.33s
                               ETA: 1431 mins 22.3 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 102763 steps/s (collection: 0.719s, learning 0.237s)
               Value function loss: 0.1023
                    Surrogate loss: 0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0491
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0688
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.96s
                        Total time: 23.29s
                               ETA: 1386 mins 1.8 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 103350 steps/s (collection: 0.720s, learning 0.232s)
               Value function loss: 0.0560
                    Surrogate loss: 0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0501
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0693
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.95s
                        Total time: 24.24s
                               ETA: 1346 mins 25.8 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 108237 steps/s (collection: 0.675s, learning 0.233s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0506
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0706
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.91s
                        Total time: 25.15s
                               ETA: 1309 mins 32.6 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 97492 steps/s (collection: 0.779s, learning 0.229s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0507
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0700
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.01s
                        Total time: 26.16s
                               ETA: 1281 mins 53.9 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 97629 steps/s (collection: 0.780s, learning 0.227s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0258
          Mean episode rew_dof_acc: -0.0497
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0705
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.01s
                        Total time: 27.17s
                               ETA: 1257 mins 15.4 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 104234 steps/s (collection: 0.713s, learning 0.230s)
               Value function loss: 0.0031
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0497
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0694
           Mean episode rew_no_fly: 0.0020
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.94s
                        Total time: 28.11s
                               ETA: 1232 mins 24.7 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 98346 steps/s (collection: 0.763s, learning 0.236s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0506
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0687
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.00s
                        Total time: 29.11s
                               ETA: 1212 mins 24.0 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 109950 steps/s (collection: 0.671s, learning 0.224s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0506
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0694
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.89s
                        Total time: 30.00s
                               ETA: 1190 mins 6.5 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 112624 steps/s (collection: 0.644s, learning 0.228s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0523
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0702
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.87s
                        Total time: 30.88s
                               ETA: 1169 mins 2.3 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 94355 steps/s (collection: 0.792s, learning 0.250s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0517
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0679
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.04s
                        Total time: 31.92s
                               ETA: 1155 mins 55.2 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 104533 steps/s (collection: 0.667s, learning 0.273s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0516
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0685
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.94s
                        Total time: 32.86s
                               ETA: 1140 mins 22.4 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 100731 steps/s (collection: 0.741s, learning 0.235s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0520
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0657
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.98s
                        Total time: 33.83s
                               ETA: 1127 mins 15.0 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 110561 steps/s (collection: 0.655s, learning 0.234s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0259
          Mean episode rew_dof_acc: -0.0530
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0691
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.89s
                        Total time: 34.72s
                               ETA: 1112 mins 21.4 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 118492 steps/s (collection: 0.608s, learning 0.221s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0527
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0646
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.83s
                        Total time: 35.55s
                               ETA: 1096 mins 43.7 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 100038 steps/s (collection: 0.758s, learning 0.225s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0527
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0663
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.98s
                        Total time: 36.54s
                               ETA: 1086 mins 46.1 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 116277 steps/s (collection: 0.624s, learning 0.221s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0531
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0658
           Mean episode rew_no_fly: 0.0021
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.85s
                        Total time: 37.38s
                               ETA: 1073 mins 33.2 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 106443 steps/s (collection: 0.689s, learning 0.235s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0536
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0648
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.92s
                        Total time: 38.30s
                               ETA: 1063 mins 23.2 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 117355 steps/s (collection: 0.617s, learning 0.220s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0536
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0654
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.84s
                        Total time: 39.14s
                               ETA: 1051 mins 34.0 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 103504 steps/s (collection: 0.729s, learning 0.221s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0250
          Mean episode rew_dof_acc: -0.0531
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0642
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.95s
                        Total time: 40.09s
                               ETA: 1043 mins 24.2 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 107929 steps/s (collection: 0.683s, learning 0.228s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0259
          Mean episode rew_dof_acc: -0.0537
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0659
           Mean episode rew_no_fly: 0.0022
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.91s
                        Total time: 41.00s
                               ETA: 1034 mins 45.0 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 119221 steps/s (collection: 0.604s, learning 0.220s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0258
          Mean episode rew_dof_acc: -0.0540
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0650
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.82s
                        Total time: 41.83s
                               ETA: 1024 mins 29.6 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 115518 steps/s (collection: 0.610s, learning 0.241s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0544
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0629
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.85s
                        Total time: 42.68s
                               ETA: 1015 mins 27.0 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 99932 steps/s (collection: 0.742s, learning 0.242s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0544
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0637
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.98s
                        Total time: 43.66s
                               ETA: 1009 mins 58.7 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 107405 steps/s (collection: 0.677s, learning 0.238s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0534
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0616
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.92s
                        Total time: 44.58s
                               ETA: 1003 mins 15.6 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 114850 steps/s (collection: 0.627s, learning 0.228s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0556
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0636
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.86s
                        Total time: 45.43s
                               ETA: 995 mins 35.8 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 108633 steps/s (collection: 0.684s, learning 0.221s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0548
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0648
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.90s
                        Total time: 46.34s
                               ETA: 989 mins 22.2 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 115512 steps/s (collection: 0.627s, learning 0.224s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0257
          Mean episode rew_dof_acc: -0.0551
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0586
           Mean episode rew_no_fly: 0.0023
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.85s
                        Total time: 47.19s
                               ETA: 982 mins 19.9 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 98973 steps/s (collection: 0.744s, learning 0.249s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0539
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.99s
                        Total time: 48.18s
                               ETA: 978 mins 31.5 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 113483 steps/s (collection: 0.628s, learning 0.238s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0259
          Mean episode rew_dof_acc: -0.0556
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0624
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.87s
                        Total time: 49.05s
                               ETA: 972 mins 22.8 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 112119 steps/s (collection: 0.638s, learning 0.239s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0551
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0622
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.88s
                        Total time: 49.92s
                               ETA: 966 mins 43.5 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 112179 steps/s (collection: 0.635s, learning 0.241s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0621
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.88s
                        Total time: 50.80s
                               ETA: 961 mins 19.1 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 100777 steps/s (collection: 0.742s, learning 0.233s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0554
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0613
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.98s
                        Total time: 51.78s
                               ETA: 957 mins 59.1 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 112267 steps/s (collection: 0.651s, learning 0.224s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0128
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0552
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.88s
                        Total time: 52.65s
                               ETA: 952 mins 59.3 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 111045 steps/s (collection: 0.665s, learning 0.220s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0552
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.89s
                        Total time: 53.54s
                               ETA: 948 mins 22.5 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 111358 steps/s (collection: 0.646s, learning 0.236s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0116
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.88s
                        Total time: 54.42s
                               ETA: 943 mins 54.6 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 110228 steps/s (collection: 0.672s, learning 0.220s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0557
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0574
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.89s
                        Total time: 55.31s
                               ETA: 939 mins 46.9 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 114323 steps/s (collection: 0.639s, learning 0.221s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0550
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0591
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.86s
                        Total time: 56.17s
                               ETA: 935 mins 17.1 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 115846 steps/s (collection: 0.629s, learning 0.220s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0551
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0583
           Mean episode rew_no_fly: 0.0024
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.85s
                        Total time: 57.02s
                               ETA: 930 mins 46.7 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 103434 steps/s (collection: 0.724s, learning 0.226s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0555
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.95s
                        Total time: 57.97s
                               ETA: 928 mins 4.6 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 108535 steps/s (collection: 0.671s, learning 0.234s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0553
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0576
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.91s
                        Total time: 58.88s
                               ETA: 924 mins 46.4 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 110148 steps/s (collection: 0.672s, learning 0.220s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0557
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.89s
                        Total time: 59.77s
                               ETA: 921 mins 23.3 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 107108 steps/s (collection: 0.693s, learning 0.225s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0235
          Mean episode rew_dof_acc: -0.0542
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0534
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.92s
                        Total time: 60.69s
                               ETA: 918 mins 30.5 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 117037 steps/s (collection: 0.619s, learning 0.221s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0555
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0551
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.84s
                        Total time: 61.53s
                               ETA: 914 mins 34.4 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 110714 steps/s (collection: 0.656s, learning 0.232s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0562
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.89s
                        Total time: 62.41s
                               ETA: 911 mins 28.6 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 106323 steps/s (collection: 0.700s, learning 0.225s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0562
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0563
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.92s
                        Total time: 63.34s
                               ETA: 909 mins 0.7 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 114438 steps/s (collection: 0.640s, learning 0.219s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0554
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.86s
                        Total time: 64.20s
                               ETA: 905 mins 42.4 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 107908 steps/s (collection: 0.686s, learning 0.225s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0545
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.91s
                        Total time: 65.11s
                               ETA: 903 mins 13.9 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 107759 steps/s (collection: 0.685s, learning 0.228s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0552
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.91s
                        Total time: 66.02s
                               ETA: 900 mins 51.2 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 107864 steps/s (collection: 0.692s, learning 0.219s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0564
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0522
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.91s
                        Total time: 66.93s
                               ETA: 898 mins 32.5 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 112214 steps/s (collection: 0.643s, learning 0.233s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0255
          Mean episode rew_dof_acc: -0.0558
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0538
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.88s
                        Total time: 67.81s
                               ETA: 895 mins 50.0 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 116927 steps/s (collection: 0.621s, learning 0.220s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0239
          Mean episode rew_dof_acc: -0.0557
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0025
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.84s
                        Total time: 68.65s
                               ETA: 892 mins 45.1 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 99092 steps/s (collection: 0.752s, learning 0.240s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0563
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0520
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.99s
                        Total time: 69.64s
                               ETA: 891 mins 42.1 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 112447 steps/s (collection: 0.649s, learning 0.225s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0569
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0530
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.87s
                        Total time: 70.52s
                               ETA: 889 mins 11.9 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 118940 steps/s (collection: 0.605s, learning 0.221s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0538
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.83s
                        Total time: 71.34s
                               ETA: 886 mins 10.5 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 110944 steps/s (collection: 0.648s, learning 0.238s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0565
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0548
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.89s
                        Total time: 72.23s
                               ETA: 883 mins 58.2 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 112154 steps/s (collection: 0.646s, learning 0.231s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0567
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.88s
                        Total time: 73.11s
                               ETA: 881 mins 42.7 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 111919 steps/s (collection: 0.658s, learning 0.221s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0553
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.88s
                        Total time: 73.98s
                               ETA: 879 mins 32.5 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 115085 steps/s (collection: 0.634s, learning 0.220s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.85s
                        Total time: 74.84s
                               ETA: 877 mins 8.8 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 114923 steps/s (collection: 0.628s, learning 0.228s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0567
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0547
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.86s
                        Total time: 75.69s
                               ETA: 874 mins 50.0 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 103242 steps/s (collection: 0.713s, learning 0.239s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.95s
                        Total time: 76.65s
                               ETA: 873 mins 41.2 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 102977 steps/s (collection: 0.724s, learning 0.231s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0544
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.95s
                        Total time: 77.60s
                               ETA: 872 mins 35.8 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 111347 steps/s (collection: 0.650s, learning 0.233s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0556
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0518
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.88s
                        Total time: 78.48s
                               ETA: 870 mins 44.4 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 100551 steps/s (collection: 0.730s, learning 0.247s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0563
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0516
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.98s
                        Total time: 79.46s
                               ETA: 869 mins 58.2 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 100164 steps/s (collection: 0.739s, learning 0.243s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.98s
                        Total time: 80.44s
                               ETA: 869 mins 15.6 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 105234 steps/s (collection: 0.700s, learning 0.234s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0591
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0536
           Mean episode rew_no_fly: 0.0026
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.93s
                        Total time: 81.38s
                               ETA: 868 mins 3.7 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 102939 steps/s (collection: 0.726s, learning 0.229s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0573
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.95s
                        Total time: 82.33s
                               ETA: 867 mins 6.9 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 108695 steps/s (collection: 0.679s, learning 0.225s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0568
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.90s
                        Total time: 83.24s
                               ETA: 865 mins 39.9 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 107727 steps/s (collection: 0.674s, learning 0.238s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0568
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.91s
                        Total time: 84.15s
                               ETA: 864 mins 20.0 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 107759 steps/s (collection: 0.672s, learning 0.240s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0567
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.91s
                        Total time: 85.06s
                               ETA: 863 mins 1.9 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 99182 steps/s (collection: 0.747s, learning 0.244s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0242
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.99s
                        Total time: 86.05s
                               ETA: 862 mins 33.1 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 106068 steps/s (collection: 0.682s, learning 0.245s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0582
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0491
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.93s
                        Total time: 86.98s
                               ETA: 861 mins 26.7 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 105089 steps/s (collection: 0.696s, learning 0.240s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0574
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0491
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.94s
                        Total time: 87.91s
                               ETA: 860 mins 26.9 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 109061 steps/s (collection: 0.648s, learning 0.253s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0238
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.90s
                        Total time: 88.81s
                               ETA: 859 mins 8.8 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 102885 steps/s (collection: 0.718s, learning 0.238s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0574
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0509
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.96s
                        Total time: 89.77s
                               ETA: 858 mins 23.4 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 110249 steps/s (collection: 0.650s, learning 0.241s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0577
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0497
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.89s
                        Total time: 90.66s
                               ETA: 857 mins 2.8 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 101264 steps/s (collection: 0.727s, learning 0.244s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0580
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0496
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.97s
                        Total time: 91.63s
                               ETA: 856 mins 28.5 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 111528 steps/s (collection: 0.636s, learning 0.246s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0580
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0027
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.88s
                        Total time: 92.51s
                               ETA: 855 mins 5.3 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 103620 steps/s (collection: 0.707s, learning 0.241s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0577
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.95s
                        Total time: 93.46s
                               ETA: 854 mins 20.8 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 102462 steps/s (collection: 0.734s, learning 0.226s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0578
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.96s
                        Total time: 94.42s
                               ETA: 853 mins 43.1 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 111180 steps/s (collection: 0.661s, learning 0.223s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0583
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0504
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.88s
                        Total time: 95.31s
                               ETA: 852 mins 25.7 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 111901 steps/s (collection: 0.657s, learning 0.221s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0239
          Mean episode rew_dof_acc: -0.0573
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0497
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.88s
                        Total time: 96.18s
                               ETA: 851 mins 7.0 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 115175 steps/s (collection: 0.626s, learning 0.228s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0164
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0575
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0482
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.85s
                        Total time: 97.04s
                               ETA: 849 mins 36.9 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 115035 steps/s (collection: 0.635s, learning 0.220s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0575
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.85s
                        Total time: 97.89s
                               ETA: 848 mins 9.1 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 105109 steps/s (collection: 0.707s, learning 0.228s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0574
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0488
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.94s
                        Total time: 98.83s
                               ETA: 847 mins 24.6 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 110180 steps/s (collection: 0.672s, learning 0.221s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0573
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0474
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.89s
                        Total time: 99.72s
                               ETA: 846 mins 19.1 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 106971 steps/s (collection: 0.688s, learning 0.231s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0566
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.92s
                        Total time: 100.64s
                               ETA: 845 mins 28.4 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 100295 steps/s (collection: 0.727s, learning 0.253s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0577
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0518
           Mean episode rew_no_fly: 0.0028
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.98s
                        Total time: 101.62s
                               ETA: 845 mins 9.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 114232 steps/s (collection: 0.621s, learning 0.240s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0234
          Mean episode rew_dof_acc: -0.0577
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0472
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.86s
                        Total time: 102.48s
                               ETA: 843 mins 51.3 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 115262 steps/s (collection: 0.614s, learning 0.239s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0237
          Mean episode rew_dof_acc: -0.0572
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0462
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.85s
                        Total time: 103.33s
                               ETA: 842 mins 31.1 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 99933 steps/s (collection: 0.734s, learning 0.250s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0237
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0478
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.98s
                        Total time: 104.32s
                               ETA: 842 mins 15.9 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 98572 steps/s (collection: 0.756s, learning 0.242s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0500
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.00s
                        Total time: 105.31s
                               ETA: 842 mins 7.4 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 22740 steps/s (collection: 4.102s, learning 0.221s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0242
          Mean episode rew_dof_acc: -0.0589
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0484
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 4.32s
                        Total time: 109.64s
                               ETA: 868 mins 19.4 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 109824 steps/s (collection: 0.662s, learning 0.233s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0572
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0465
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.90s
                        Total time: 110.53s
                               ETA: 867 mins 8.2 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 108048 steps/s (collection: 0.688s, learning 0.222s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0574
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0482
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.91s
                        Total time: 111.44s
                               ETA: 866 mins 5.2 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 111831 steps/s (collection: 0.645s, learning 0.234s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0237
          Mean episode rew_dof_acc: -0.0580
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0474
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.88s
                        Total time: 112.32s
                               ETA: 864 mins 49.1 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 22574 steps/s (collection: 4.120s, learning 0.235s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0578
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0484
           Mean episode rew_no_fly: 0.0029
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 4.35s
                        Total time: 116.68s
                               ETA: 890 mins 5.2 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/7yaepr9xj19mcaabcs9yg
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    