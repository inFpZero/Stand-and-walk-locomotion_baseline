swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_132822-e7cfd534
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yh_gym_his_Jun01_13-28-22 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/nvkvpjdo7r3495z0bk4pj
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 18.43 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 12008 steps/s (collection: 7.997s, learning 0.189s)
               Value function loss: 7.8983
                    Surrogate loss: 0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -6.56
                Mean reward (task): -6.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0033
       Mean episode rew_ang_vel_xy: -0.0170
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0523
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0007
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1355
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 1.6337
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.19s
                        Total time: 8.19s
                               ETA: 6822 mins 4.9 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 21931 steps/s (collection: 4.362s, learning 0.121s)
               Value function loss: 7.9955
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -6.71
                Mean reward (task): -6.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0588
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 1.0179
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 4.48s
                        Total time: 12.67s
                               ETA: 5278 mins 36.0 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 146936 steps/s (collection: 0.547s, learning 0.122s)
               Value function loss: 6.6060
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -6.59
                Mean reward (task): -6.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0403
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0603
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0170
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.5552
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.67s
                        Total time: 13.34s
                               ETA: 3704 mins 49.7 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 139681 steps/s (collection: 0.555s, learning 0.149s)
               Value function loss: 5.9024
                    Surrogate loss: 0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -6.72
                Mean reward (task): -6.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0585
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0173
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.2759
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.70s
                        Total time: 14.04s
                               ETA: 2925 mins 10.6 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 138908 steps/s (collection: 0.586s, learning 0.122s)
               Value function loss: 5.1948
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -6.70
                Mean reward (task): -6.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0176
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.1328
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.71s
                        Total time: 14.75s
                               ETA: 2458 mins 2.0 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 129271 steps/s (collection: 0.609s, learning 0.151s)
               Value function loss: 4.7542
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.94
                Mean reward (task): -6.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0606
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0175
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0657
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.76s
                        Total time: 15.51s
                               ETA: 2153 mins 55.6 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 143079 steps/s (collection: 0.566s, learning 0.121s)
               Value function loss: 3.8140
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.55
                Mean reward (task): -6.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0574
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0176
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0360
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.69s
                        Total time: 16.20s
                               ETA: 1927 mins 58.2 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 121992 steps/s (collection: 0.667s, learning 0.139s)
               Value function loss: 2.8803
                    Surrogate loss: 0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.60
                Mean reward (task): -6.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0578
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0177
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.81s
                        Total time: 17.00s
                               ETA: 1770 mins 52.0 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 128053 steps/s (collection: 0.645s, learning 0.123s)
               Value function loss: 2.8422
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.82
                Mean reward (task): -6.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.77s
                        Total time: 17.77s
                               ETA: 1645 mins 8.6 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 127103 steps/s (collection: 0.646s, learning 0.128s)
               Value function loss: 2.1592
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.54
                Mean reward (task): -6.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0181
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.77s
                        Total time: 18.54s
                               ETA: 1545 mins 2.3 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 122088 steps/s (collection: 0.678s, learning 0.127s)
               Value function loss: 1.7156
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.21
                Mean reward (task): -6.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0563
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0184
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.81s
                        Total time: 19.35s
                               ETA: 1465 mins 32.4 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 134123 steps/s (collection: 0.607s, learning 0.126s)
               Value function loss: 1.4755
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.48
                Mean reward (task): -6.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0185
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.73s
                        Total time: 20.08s
                               ETA: 1394 mins 16.3 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 117363 steps/s (collection: 0.709s, learning 0.128s)
               Value function loss: 1.2361
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.79
                Mean reward (task): -6.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0546
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0188
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.84s
                        Total time: 20.92s
                               ETA: 1340 mins 40.4 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 129423 steps/s (collection: 0.631s, learning 0.129s)
               Value function loss: 1.0712
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.76
                Mean reward (task): -6.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0548
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0187
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.76s
                        Total time: 21.68s
                               ETA: 1290 mins 5.2 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 116529 steps/s (collection: 0.713s, learning 0.131s)
               Value function loss: 1.0566
                    Surrogate loss: 0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.59
                Mean reward (task): -6.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0541
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0192
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.84s
                        Total time: 22.52s
                               ETA: 1250 mins 54.6 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 131157 steps/s (collection: 0.628s, learning 0.122s)
               Value function loss: 0.9197
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.46
                Mean reward (task): -6.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0508
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.75s
                        Total time: 23.27s
                               ETA: 1211 mins 43.8 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 129914 steps/s (collection: 0.622s, learning 0.134s)
               Value function loss: 0.9024
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.50
                Mean reward (task): -6.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0515
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.76s
                        Total time: 24.03s
                               ETA: 1177 mins 30.5 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 126395 steps/s (collection: 0.656s, learning 0.121s)
               Value function loss: 0.6892
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.66
                Mean reward (task): -6.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.78s
                        Total time: 24.81s
                               ETA: 1148 mins 3.8 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 120194 steps/s (collection: 0.687s, learning 0.131s)
               Value function loss: 0.7287
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.76
                Mean reward (task): -6.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0504
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0190
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.82s
                        Total time: 25.62s
                               ETA: 1123 mins 28.6 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 114733 steps/s (collection: 0.709s, learning 0.147s)
               Value function loss: 0.6814
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.43
                Mean reward (task): -6.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0491
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0194
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.86s
                        Total time: 26.48s
                               ETA: 1102 mins 58.1 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 122616 steps/s (collection: 0.678s, learning 0.124s)
               Value function loss: 0.6556
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.38
                Mean reward (task): -6.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0469
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0194
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.80s
                        Total time: 27.28s
                               ETA: 1082 mins 13.6 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 124710 steps/s (collection: 0.641s, learning 0.148s)
               Value function loss: 0.5986
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.47
                Mean reward (task): -6.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0404
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0475
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0196
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.79s
                        Total time: 28.07s
                               ETA: 1062 mins 51.6 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 132311 steps/s (collection: 0.618s, learning 0.125s)
               Value function loss: 0.5360
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.37
                Mean reward (task): -6.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0404
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0460
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0196
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.74s
                        Total time: 28.81s
                               ETA: 1043 mins 32.1 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 137891 steps/s (collection: 0.591s, learning 0.122s)
               Value function loss: 0.5168
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.39
                Mean reward (task): -6.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0440
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0195
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.71s
                        Total time: 29.53s
                               ETA: 1024 mins 46.6 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 131581 steps/s (collection: 0.625s, learning 0.122s)
               Value function loss: 0.6221
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.42
                Mean reward (task): -6.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0451
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0194
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.75s
                        Total time: 30.27s
                               ETA: 1008 mins 39.4 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 118220 steps/s (collection: 0.682s, learning 0.150s)
               Value function loss: 0.6054
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.50
                Mean reward (task): -6.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0186
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0437
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.83s
                        Total time: 31.11s
                               ETA: 996 mins 28.9 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 127690 steps/s (collection: 0.631s, learning 0.139s)
               Value function loss: 0.5872
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.30
                Mean reward (task): -6.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0185
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0436
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.77s
                        Total time: 31.88s
                               ETA: 983 mins 18.3 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 135103 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.5641
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.61
                Mean reward (task): -6.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0185
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0407
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0189
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.73s
                        Total time: 32.60s
                               ETA: 969 mins 48.7 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 143479 steps/s (collection: 0.564s, learning 0.121s)
               Value function loss: 0.5076
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.09
                Mean reward (task): -6.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0185
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0390
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0188
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.69s
                        Total time: 33.29s
                               ETA: 956 mins 1.7 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 130484 steps/s (collection: 0.629s, learning 0.125s)
               Value function loss: 0.4337
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.32
                Mean reward (task): -6.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0180
          Mean episode rew_dof_acc: -0.0376
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0397
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0185
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.75s
                        Total time: 34.04s
                               ETA: 945 mins 3.4 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 121957 steps/s (collection: 0.674s, learning 0.132s)
               Value function loss: 0.4386
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.33
                Mean reward (task): -6.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0182
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0382
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0186
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.81s
                        Total time: 34.85s
                               ETA: 936 mins 12.5 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 143098 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.4386
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.22
                Mean reward (task): -6.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0179
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0380
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0183
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.69s
                        Total time: 35.53s
                               ETA: 924 mins 48.7 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 133129 steps/s (collection: 0.602s, learning 0.137s)
               Value function loss: 0.3404
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.91
                Mean reward (task): -5.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0174
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0346
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0182
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.74s
                        Total time: 36.27s
                               ETA: 915 mins 24.2 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 121785 steps/s (collection: 0.653s, learning 0.154s)
               Value function loss: 0.4145
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.03
                Mean reward (task): -6.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0176
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0354
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0180
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.81s
                        Total time: 37.08s
                               ETA: 908 mins 14.0 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 123281 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 0.3754
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.22
                Mean reward (task): -6.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0175
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0355
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0180
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.80s
                        Total time: 37.88s
                               ETA: 901 mins 14.3 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 137639 steps/s (collection: 0.592s, learning 0.122s)
               Value function loss: 0.3908
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.04
                Mean reward (task): -6.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0174
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0351
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.71s
                        Total time: 38.59s
                               ETA: 892 mins 42.5 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 138419 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.3419
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.09
                Mean reward (task): -6.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0178
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.71s
                        Total time: 39.30s
                               ETA: 884 mins 32.8 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 120808 steps/s (collection: 0.691s, learning 0.123s)
               Value function loss: 0.3561
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.84
                Mean reward (task): -5.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0333
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0180
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.81s
                        Total time: 40.12s
                               ETA: 879 mins 5.0 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 122988 steps/s (collection: 0.668s, learning 0.131s)
               Value function loss: 0.4371
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.98
                Mean reward (task): -5.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0169
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0332
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.80s
                        Total time: 40.92s
                               ETA: 873 mins 35.5 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 128086 steps/s (collection: 0.642s, learning 0.126s)
               Value function loss: 0.2979
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.97
                Mean reward (task): -5.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0165
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0323
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0177
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.77s
                        Total time: 41.68s
                               ETA: 867 mins 42.7 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 127185 steps/s (collection: 0.643s, learning 0.130s)
               Value function loss: 0.2888
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.02
                Mean reward (task): -6.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0167
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0306
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0176
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.77s
                        Total time: 42.46s
                               ETA: 862 mins 13.7 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 129085 steps/s (collection: 0.627s, learning 0.135s)
               Value function loss: 0.2901
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.83
                Mean reward (task): -5.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0167
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0296
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0174
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.76s
                        Total time: 43.22s
                               ETA: 856 mins 46.8 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 125197 steps/s (collection: 0.660s, learning 0.125s)
               Value function loss: 0.2675
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.81
                Mean reward (task): -5.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0164
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0173
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.79s
                        Total time: 44.00s
                               ETA: 852 mins 2.5 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 131504 steps/s (collection: 0.618s, learning 0.129s)
               Value function loss: 0.2949
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.87
                Mean reward (task): -5.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0160
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.75s
                        Total time: 44.75s
                               ETA: 846 mins 48.4 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 124424 steps/s (collection: 0.659s, learning 0.132s)
               Value function loss: 0.2854
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.92
                Mean reward (task): -5.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0161
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0172
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.79s
                        Total time: 45.54s
                               ETA: 842 mins 35.4 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 133166 steps/s (collection: 0.604s, learning 0.134s)
               Value function loss: 0.1873
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.71
                Mean reward (task): -5.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0160
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0166
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.74s
                        Total time: 46.28s
                               ETA: 837 mins 37.1 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 129255 steps/s (collection: 0.632s, learning 0.129s)
               Value function loss: 0.2302
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.93
                Mean reward (task): -5.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0160
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.76s
                        Total time: 47.04s
                               ETA: 833 mins 15.1 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 122451 steps/s (collection: 0.664s, learning 0.139s)
               Value function loss: 0.2241
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.89
                Mean reward (task): -5.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0161
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0161
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.80s
                        Total time: 47.84s
                               ETA: 829 mins 48.0 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 130316 steps/s (collection: 0.612s, learning 0.142s)
               Value function loss: 0.1952
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.72
                Mean reward (task): -5.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0158
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0160
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.75s
                        Total time: 48.60s
                               ETA: 825 mins 40.0 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 137402 steps/s (collection: 0.589s, learning 0.127s)
               Value function loss: 0.1842
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.98
                Mean reward (task): -5.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0158
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0158
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.72s
                        Total time: 49.31s
                               ETA: 821 mins 3.0 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 138231 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.2871
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.93
                Mean reward (task): -5.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0156
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0158
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.71s
                        Total time: 50.02s
                               ETA: 816 mins 32.6 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 136641 steps/s (collection: 0.589s, learning 0.130s)
               Value function loss: 0.1786
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.72
                Mean reward (task): -5.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0156
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0157
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.72s
                        Total time: 50.74s
                               ETA: 812 mins 20.5 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 145272 steps/s (collection: 0.554s, learning 0.122s)
               Value function loss: 0.1961
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0152
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0156
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.68s
                        Total time: 51.42s
                               ETA: 807 mins 37.6 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 134092 steps/s (collection: 0.598s, learning 0.135s)
               Value function loss: 0.1968
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -5.78
                Mean reward (task): -5.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0152
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0153
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.73s
                        Total time: 52.15s
                               ETA: 803 mins 57.4 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 138516 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.2486
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0153
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0151
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.71s
                        Total time: 52.86s
                               ETA: 800 mins 3.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 142576 steps/s (collection: 0.560s, learning 0.129s)
               Value function loss: 0.1685
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0154
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0148
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.69s
                        Total time: 53.55s
                               ETA: 796 mins 0.6 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 140461 steps/s (collection: 0.575s, learning 0.125s)
               Value function loss: 0.2680
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0152
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0146
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.70s
                        Total time: 54.25s
                               ETA: 792 mins 15.0 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 135543 steps/s (collection: 0.602s, learning 0.123s)
               Value function loss: 0.1993
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -5.75
                Mean reward (task): -5.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0152
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0144
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.73s
                        Total time: 54.98s
                               ETA: 788 mins 59.0 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 139644 steps/s (collection: 0.566s, learning 0.138s)
               Value function loss: 0.1510
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0152
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0143
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.70s
                        Total time: 55.68s
                               ETA: 785 mins 31.6 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 140507 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.1739
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0150
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0144
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.70s
                        Total time: 56.38s
                               ETA: 782 mins 7.5 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 130461 steps/s (collection: 0.629s, learning 0.125s)
               Value function loss: 0.2206
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0150
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.75s
                        Total time: 57.13s
                               ETA: 779 mins 34.2 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 137041 steps/s (collection: 0.597s, learning 0.121s)
               Value function loss: 0.1640
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0151
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.72s
                        Total time: 57.85s
                               ETA: 776 mins 36.6 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 129670 steps/s (collection: 0.633s, learning 0.125s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0149
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0138
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.76s
                        Total time: 58.61s
                               ETA: 774 mins 17.0 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 142978 steps/s (collection: 0.564s, learning 0.124s)
               Value function loss: 0.1474
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0148
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.69s
                        Total time: 59.30s
                               ETA: 771 mins 6.7 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 134347 steps/s (collection: 0.608s, learning 0.124s)
               Value function loss: 0.1599
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.64
                Mean reward (task): -5.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0148
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0136
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.73s
                        Total time: 60.03s
                               ETA: 768 mins 36.1 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 128229 steps/s (collection: 0.646s, learning 0.121s)
               Value function loss: 0.1598
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0148
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0134
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.77s
                        Total time: 60.79s
                               ETA: 766 mins 36.5 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 129109 steps/s (collection: 0.641s, learning 0.121s)
               Value function loss: 0.1473
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0147
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0132
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.76s
                        Total time: 61.56s
                               ETA: 764 mins 36.5 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 137028 steps/s (collection: 0.591s, learning 0.127s)
               Value function loss: 0.1350
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0146
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0130
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.72s
                        Total time: 62.27s
                               ETA: 762 mins 7.7 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 140158 steps/s (collection: 0.559s, learning 0.142s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0144
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0131
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.70s
                        Total time: 62.97s
                               ETA: 759 mins 31.7 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 129225 steps/s (collection: 0.635s, learning 0.126s)
               Value function loss: 0.1595
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0146
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.76s
                        Total time: 63.74s
                               ETA: 757 mins 42.4 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 141515 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.1455
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0142
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.69s
                        Total time: 64.43s
                               ETA: 755 mins 9.7 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 129503 steps/s (collection: 0.636s, learning 0.123s)
               Value function loss: 0.2718
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.66
                Mean reward (task): -5.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0140
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.76s
                        Total time: 65.19s
                               ETA: 753 mins 25.9 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 138016 steps/s (collection: 0.591s, learning 0.121s)
               Value function loss: 0.1483
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0140
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.71s
                        Total time: 65.90s
                               ETA: 751 mins 12.9 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 145557 steps/s (collection: 0.554s, learning 0.121s)
               Value function loss: 0.2278
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0141
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.68s
                        Total time: 66.58s
                               ETA: 748 mins 38.5 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 128638 steps/s (collection: 0.640s, learning 0.124s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0140
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0127
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.76s
                        Total time: 67.34s
                               ETA: 747 mins 7.4 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 141239 steps/s (collection: 0.571s, learning 0.125s)
               Value function loss: 0.1468
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0140
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.70s
                        Total time: 68.04s
                               ETA: 744 mins 53.9 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 140178 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.1351
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0139
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.70s
                        Total time: 68.74s
                               ETA: 742 mins 47.3 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 144206 steps/s (collection: 0.561s, learning 0.121s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0137
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.68s
                        Total time: 69.42s
                               ETA: 740 mins 31.3 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 138402 steps/s (collection: 0.588s, learning 0.122s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0139
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.71s
                        Total time: 70.13s
                               ETA: 738 mins 36.9 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 142014 steps/s (collection: 0.570s, learning 0.122s)
               Value function loss: 0.1348
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0135
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.69s
                        Total time: 70.82s
                               ETA: 736 mins 34.0 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 143185 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.1320
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0136
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.69s
                        Total time: 71.51s
                               ETA: 734 mins 30.6 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 140596 steps/s (collection: 0.577s, learning 0.122s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0135
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.70s
                        Total time: 72.21s
                               ETA: 732 mins 38.0 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 143558 steps/s (collection: 0.564s, learning 0.121s)
               Value function loss: 0.1255
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0133
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.68s
                        Total time: 72.89s
                               ETA: 730 mins 39.3 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 125727 steps/s (collection: 0.659s, learning 0.122s)
               Value function loss: 0.1511
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0134
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.78s
                        Total time: 73.67s
                               ETA: 729 mins 41.2 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 146908 steps/s (collection: 0.546s, learning 0.123s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0135
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.67s
                        Total time: 74.34s
                               ETA: 727 mins 38.2 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 132282 steps/s (collection: 0.604s, learning 0.140s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0134
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.74s
                        Total time: 75.09s
                               ETA: 726 mins 21.0 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 147870 steps/s (collection: 0.543s, learning 0.121s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0134
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0182
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.66s
                        Total time: 75.75s
                               ETA: 724 mins 20.6 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 131533 steps/s (collection: 0.627s, learning 0.121s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0134
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0179
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.75s
                        Total time: 76.50s
                               ETA: 723 mins 9.8 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 140283 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0133
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0180
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.70s
                        Total time: 77.20s
                               ETA: 721 mins 34.4 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 129901 steps/s (collection: 0.634s, learning 0.122s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0130
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0172
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.76s
                        Total time: 77.96s
                               ETA: 720 mins 32.1 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 143047 steps/s (collection: 0.561s, learning 0.127s)
               Value function loss: 0.1532
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0131
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0175
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.69s
                        Total time: 78.64s
                               ETA: 718 mins 53.1 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 141429 steps/s (collection: 0.556s, learning 0.139s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0130
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0176
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.70s
                        Total time: 79.34s
                               ETA: 717 mins 20.5 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 136943 steps/s (collection: 0.596s, learning 0.122s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0130
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0174
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.72s
                        Total time: 80.06s
                               ETA: 716 mins 2.1 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 128094 steps/s (collection: 0.638s, learning 0.129s)
               Value function loss: 0.1023
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0128
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0169
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.77s
                        Total time: 80.82s
                               ETA: 715 mins 11.6 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 138463 steps/s (collection: 0.582s, learning 0.128s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0126
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0163
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.71s
                        Total time: 81.53s
                               ETA: 713 mins 52.0 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 142850 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0127
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0166
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.69s
                        Total time: 82.22s
                               ETA: 712 mins 22.8 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 147193 steps/s (collection: 0.545s, learning 0.123s)
               Value function loss: 0.1191
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0127
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0172
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.67s
                        Total time: 82.89s
                               ETA: 710 mins 44.9 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 132307 steps/s (collection: 0.610s, learning 0.133s)
               Value function loss: 0.0969
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0125
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0163
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.74s
                        Total time: 83.63s
                               ETA: 709 mins 47.2 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 142875 steps/s (collection: 0.550s, learning 0.138s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0125
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0162
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.69s
                        Total time: 84.32s
                               ETA: 708 mins 23.0 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 123413 steps/s (collection: 0.669s, learning 0.127s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0124
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0160
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.80s
                        Total time: 85.12s
                               ETA: 707 mins 54.6 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 120491 steps/s (collection: 0.679s, learning 0.137s)
               Value function loss: 0.1406
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0122
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0162
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.82s
                        Total time: 85.93s
                               ETA: 707 mins 36.3 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 138625 steps/s (collection: 0.587s, learning 0.122s)
               Value function loss: 0.1228
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0123
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0162
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.71s
                        Total time: 86.64s
                               ETA: 706 mins 26.1 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 147133 steps/s (collection: 0.540s, learning 0.129s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0123
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0158
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.67s
                        Total time: 87.31s
                               ETA: 704 mins 57.5 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 135897 steps/s (collection: 0.601s, learning 0.122s)
               Value function loss: 0.0994
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0122
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0153
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.72s
                        Total time: 88.03s
                               ETA: 703 mins 57.0 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 128155 steps/s (collection: 0.626s, learning 0.141s)
               Value function loss: 0.1081
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0121
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0158
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.77s
                        Total time: 88.80s
                               ETA: 703 mins 18.4 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 128285 steps/s (collection: 0.644s, learning 0.122s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0120
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0152
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.77s
                        Total time: 89.57s
                               ETA: 702 mins 40.1 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 126239 steps/s (collection: 0.655s, learning 0.124s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0120
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0160
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.78s
                        Total time: 90.35s
                               ETA: 702 mins 8.4 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 143314 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0119
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0156
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.69s
                        Total time: 91.03s
                               ETA: 700 mins 54.4 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 143988 steps/s (collection: 0.541s, learning 0.142s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0117
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0152
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.68s
                        Total time: 91.71s
                               ETA: 699 mins 40.2 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 133296 steps/s (collection: 0.603s, learning 0.135s)
               Value function loss: 0.1085
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0117
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.74s
                        Total time: 92.45s
                               ETA: 698 mins 52.2 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 144006 steps/s (collection: 0.561s, learning 0.122s)
               Value function loss: 0.1066
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0118
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0153
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.68s
                        Total time: 93.14s
                               ETA: 697 mins 40.4 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 148948 steps/s (collection: 0.538s, learning 0.122s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0114
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0152
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.66s
                        Total time: 93.80s
                               ETA: 696 mins 19.8 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 130443 steps/s (collection: 0.631s, learning 0.123s)
               Value function loss: 0.1112
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0116
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0149
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.75s
                        Total time: 94.55s
                               ETA: 695 mins 42.0 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 126252 steps/s (collection: 0.650s, learning 0.129s)
               Value function loss: 0.1106
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0114
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0149
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.78s
                        Total time: 95.33s
                               ETA: 695 mins 15.7 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 147258 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0112
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0151
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.67s
                        Total time: 95.99s
                               ETA: 694 mins 1.7 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 145575 steps/s (collection: 0.546s, learning 0.129s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0113
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0150
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.68s
                        Total time: 96.67s
                               ETA: 692 mins 52.3 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 135522 steps/s (collection: 0.603s, learning 0.122s)
               Value function loss: 0.1873
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0110
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0146
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.73s
                        Total time: 97.40s
                               ETA: 692 mins 5.5 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 144302 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0111
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0143
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.68s
                        Total time: 98.08s
                               ETA: 691 mins 0.7 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 138619 steps/s (collection: 0.587s, learning 0.122s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0109
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0144
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0127
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.71s
                        Total time: 98.79s
                               ETA: 690 mins 8.7 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 144806 steps/s (collection: 0.556s, learning 0.123s)
               Value function loss: 0.0938
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0108
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0139
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.68s
                        Total time: 99.46s
                               ETA: 689 mins 5.0 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 142185 steps/s (collection: 0.569s, learning 0.123s)
               Value function loss: 0.1139
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0108
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0138
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0131
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.69s
                        Total time: 100.16s
                               ETA: 688 mins 7.5 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 144776 steps/s (collection: 0.557s, learning 0.122s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0105
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0131
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0132
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.68s
                        Total time: 100.84s
                               ETA: 687 mins 5.9 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 129236 steps/s (collection: 0.633s, learning 0.127s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0105
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0129
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0136
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.76s
                        Total time: 101.60s
                               ETA: 686 mins 38.3 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 138927 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0944
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0104
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0131
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0135
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.71s
                        Total time: 102.30s
                               ETA: 685 mins 49.9 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 145317 steps/s (collection: 0.546s, learning 0.130s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0101
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0124
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0138
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.68s
                        Total time: 102.98s
                               ETA: 684 mins 49.8 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 144655 steps/s (collection: 0.531s, learning 0.149s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0102
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0126
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.68s
                        Total time: 103.66s
                               ETA: 683 mins 51.9 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 135185 steps/s (collection: 0.590s, learning 0.137s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0101
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0126
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.73s
                        Total time: 104.39s
                               ETA: 683 mins 13.5 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 141388 steps/s (collection: 0.572s, learning 0.124s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0099
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0121
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.70s
                        Total time: 105.08s
                               ETA: 682 mins 23.4 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 129175 steps/s (collection: 0.622s, learning 0.139s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0117
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.76s
                        Total time: 105.84s
                               ETA: 681 mins 59.4 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 141138 steps/s (collection: 0.562s, learning 0.134s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0115
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.70s
                        Total time: 106.54s
                               ETA: 681 mins 11.0 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 139936 steps/s (collection: 0.568s, learning 0.135s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
          Mean episode rew_dof_acc: -0.0255
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0119
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.70s
                        Total time: 107.24s
                               ETA: 680 mins 25.6 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 131237 steps/s (collection: 0.620s, learning 0.129s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -4.82
                Mean reward (task): -4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0114
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.75s
                        Total time: 107.99s
                               ETA: 679 mins 58.5 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 144611 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0109
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.68s
                        Total time: 108.67s
                               ETA: 679 mins 5.8 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 150503 steps/s (collection: 0.531s, learning 0.123s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0092
          Mean episode rew_dof_acc: -0.0252
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0138
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.65s
                        Total time: 109.32s
                               ETA: 678 mins 4.0 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 144052 steps/s (collection: 0.537s, learning 0.146s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0091
          Mean episode rew_dof_acc: -0.0250
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0140
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.68s
                        Total time: 110.01s
                               ETA: 677 mins 13.9 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 138308 steps/s (collection: 0.588s, learning 0.122s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0091
          Mean episode rew_dof_acc: -0.0252
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0101
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.71s
                        Total time: 110.72s
                               ETA: 676 mins 34.9 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 147765 steps/s (collection: 0.542s, learning 0.123s)
               Value function loss: 0.1090
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0090
          Mean episode rew_dof_acc: -0.0245
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.67s
                        Total time: 111.38s
                               ETA: 675 mins 39.9 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 144002 steps/s (collection: 0.561s, learning 0.122s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -4.69
                Mean reward (task): -4.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0088
          Mean episode rew_dof_acc: -0.0242
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.68s
                        Total time: 112.07s
                               ETA: 674 mins 52.0 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 141540 steps/s (collection: 0.573s, learning 0.121s)
               Value function loss: 0.0917
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -4.74
                Mean reward (task): -4.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0088
          Mean episode rew_dof_acc: -0.0243
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.69s
                        Total time: 112.76s
                               ETA: 674 mins 9.0 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 148317 steps/s (collection: 0.541s, learning 0.122s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -4.79
                Mean reward (task): -4.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0086
          Mean episode rew_dof_acc: -0.0236
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0093
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0135
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.66s
                        Total time: 113.42s
                               ETA: 673 mins 15.4 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 152705 steps/s (collection: 0.522s, learning 0.122s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -4.75
                Mean reward (task): -4.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0086
          Mean episode rew_dof_acc: -0.0236
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0092
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0136
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.64s
                        Total time: 114.07s
                               ETA: 672 mins 15.7 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 131819 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0961
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -4.83
                Mean reward (task): -4.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0085
          Mean episode rew_dof_acc: -0.0229
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0094
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0134
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.75s
                        Total time: 114.81s
                               ETA: 671 mins 52.7 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 146571 steps/s (collection: 0.543s, learning 0.127s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -4.71
                Mean reward (task): -4.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0085
          Mean episode rew_dof_acc: -0.0231
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0134
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.67s
                        Total time: 115.48s
                               ETA: 671 mins 3.8 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 142253 steps/s (collection: 0.570s, learning 0.121s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -4.77
                Mean reward (task): -4.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0084
          Mean episode rew_dof_acc: -0.0223
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0096
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0134
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.69s
                        Total time: 116.17s
                               ETA: 670 mins 22.7 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 142642 steps/s (collection: 0.554s, learning 0.135s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -4.64
                Mean reward (task): -4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0082
          Mean episode rew_dof_acc: -0.0226
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0133
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.69s
                        Total time: 116.86s
                               ETA: 669 mins 41.4 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 138051 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -4.69
                Mean reward (task): -4.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0081
          Mean episode rew_dof_acc: -0.0219
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0133
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.71s
                        Total time: 117.57s
                               ETA: 669 mins 8.6 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 152273 steps/s (collection: 0.524s, learning 0.122s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -4.72
                Mean reward (task): -4.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0081
          Mean episode rew_dof_acc: -0.0219
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0133
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.65s
                        Total time: 118.22s
                               ETA: 668 mins 13.6 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 144957 steps/s (collection: 0.549s, learning 0.130s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -4.67
                Mean reward (task): -4.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0080
          Mean episode rew_dof_acc: -0.0218
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0130
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.68s
                        Total time: 118.90s
                               ETA: 667 mins 30.3 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 145724 steps/s (collection: 0.543s, learning 0.131s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -4.74
                Mean reward (task): -4.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0081
          Mean episode rew_dof_acc: -0.0214
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0130
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.67s
                        Total time: 119.57s
                               ETA: 666 mins 46.4 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 149802 steps/s (collection: 0.529s, learning 0.127s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0080
          Mean episode rew_dof_acc: -0.0211
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.66s
                        Total time: 120.23s
                               ETA: 665 mins 57.0 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 146684 steps/s (collection: 0.539s, learning 0.131s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -4.80
                Mean reward (task): -4.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0079
          Mean episode rew_dof_acc: -0.0207
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.67s
                        Total time: 120.90s
                               ETA: 665 mins 12.9 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 144400 steps/s (collection: 0.544s, learning 0.137s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -4.75
                Mean reward (task): -4.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0080
          Mean episode rew_dof_acc: -0.0206
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.68s
                        Total time: 121.58s
                               ETA: 664 mins 32.7 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 138081 steps/s (collection: 0.579s, learning 0.133s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0079
          Mean episode rew_dof_acc: -0.0209
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.71s
                        Total time: 122.29s
                               ETA: 664 mins 3.3 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 154187 steps/s (collection: 0.508s, learning 0.130s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -4.76
                Mean reward (task): -4.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0078
          Mean episode rew_dof_acc: -0.0202
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0129
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.64s
                        Total time: 122.93s
                               ETA: 663 mins 10.1 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 121739 steps/s (collection: 0.658s, learning 0.149s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -4.58
                Mean reward (task): -4.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0078
          Mean episode rew_dof_acc: -0.0201
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.81s
                        Total time: 123.74s
                               ETA: 663 mins 12.3 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 143757 steps/s (collection: 0.553s, learning 0.130s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -4.53
                Mean reward (task): -4.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0077
          Mean episode rew_dof_acc: -0.0197
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.68s
                        Total time: 124.42s
                               ETA: 662 mins 34.9 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 143043 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -4.57
                Mean reward (task): -4.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0076
          Mean episode rew_dof_acc: -0.0201
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.69s
                        Total time: 125.11s
                               ETA: 661 mins 59.1 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 145477 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0475
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -4.61
                Mean reward (task): -4.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0074
          Mean episode rew_dof_acc: -0.0191
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0069
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.68s
                        Total time: 125.78s
                               ETA: 661 mins 20.1 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 147032 steps/s (collection: 0.546s, learning 0.122s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -4.47
                Mean reward (task): -4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0074
          Mean episode rew_dof_acc: -0.0193
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.67s
                        Total time: 126.45s
                               ETA: 660 mins 39.3 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 134482 steps/s (collection: 0.605s, learning 0.126s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.68
                Mean reward (task): -4.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0073
          Mean episode rew_dof_acc: -0.0191
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0067
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.73s
                        Total time: 127.18s
                               ETA: 660 mins 18.5 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 137453 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.73
                Mean reward (task): -4.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0073
          Mean episode rew_dof_acc: -0.0194
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.72s
                        Total time: 127.90s
                               ETA: 659 mins 53.0 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 147275 steps/s (collection: 0.545s, learning 0.122s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.63
                Mean reward (task): -4.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0073
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.67s
                        Total time: 128.57s
                               ETA: 659 mins 13.2 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 151706 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.53
                Mean reward (task): -4.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0073
          Mean episode rew_dof_acc: -0.0190
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0071
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.65s
                        Total time: 129.21s
                               ETA: 658 mins 27.9 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 136269 steps/s (collection: 0.591s, learning 0.131s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.46
                Mean reward (task): -4.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0073
          Mean episode rew_dof_acc: -0.0190
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.72s
                        Total time: 129.94s
                               ETA: 658 mins 5.4 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 150722 steps/s (collection: 0.527s, learning 0.126s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.45
                Mean reward (task): -4.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0071
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0070
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.65s
                        Total time: 130.59s
                               ETA: 657 mins 22.3 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 149538 steps/s (collection: 0.535s, learning 0.123s)
               Value function loss: 0.0502
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.35
                Mean reward (task): -4.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0070
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0065
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.66s
                        Total time: 131.25s
                               ETA: 656 mins 41.3 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 143273 steps/s (collection: 0.564s, learning 0.122s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.52
                Mean reward (task): -4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0071
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.69s
                        Total time: 131.93s
                               ETA: 656 mins 9.3 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 155044 steps/s (collection: 0.512s, learning 0.122s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.47
                Mean reward (task): -4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0070
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0065
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.63s
                        Total time: 132.57s
                               ETA: 655 mins 22.2 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 141394 steps/s (collection: 0.573s, learning 0.122s)
               Value function loss: 0.0527
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.62
                Mean reward (task): -4.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0069
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0063
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.70s
                        Total time: 133.26s
                               ETA: 654 mins 53.8 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 148003 steps/s (collection: 0.538s, learning 0.127s)
               Value function loss: 0.0556
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.59
                Mean reward (task): -4.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0069
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0066
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.66s
                        Total time: 133.92s
                               ETA: 654 mins 16.5 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 133437 steps/s (collection: 0.598s, learning 0.139s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.47
                Mean reward (task): -4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0068
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0061
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.74s
                        Total time: 134.66s
                               ETA: 654 mins 0.9 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 144635 steps/s (collection: 0.557s, learning 0.123s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.50
                Mean reward (task): -4.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0069
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0064
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.68s
                        Total time: 135.34s
                               ETA: 653 mins 28.8 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 152529 steps/s (collection: 0.523s, learning 0.122s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.42
                Mean reward (task): -4.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0069
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0063
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.64s
                        Total time: 135.99s
                               ETA: 652 mins 47.0 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 153988 steps/s (collection: 0.516s, learning 0.123s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.35
                Mean reward (task): -4.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0068
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0062
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.64s
                        Total time: 136.62s
                               ETA: 652 mins 4.0 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 151819 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.50
                Mean reward (task): -4.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0067
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0058
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.65s
                        Total time: 137.27s
                               ETA: 651 mins 24.0 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 151975 steps/s (collection: 0.516s, learning 0.131s)
               Value function loss: 0.0503
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.44
                Mean reward (task): -4.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0066
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0062
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.65s
                        Total time: 137.92s
                               ETA: 650 mins 44.3 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 135301 steps/s (collection: 0.584s, learning 0.143s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.38
                Mean reward (task): -4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0068
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0066
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.73s
                        Total time: 138.65s
                               ETA: 650 mins 27.4 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 139697 steps/s (collection: 0.576s, learning 0.128s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.33
                Mean reward (task): -4.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0067
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0064
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.70s
                        Total time: 139.35s
                               ETA: 650 mins 4.3 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 145923 steps/s (collection: 0.532s, learning 0.142s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): -4.52
                Mean reward (task): -4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0068
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0066
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.67s
                        Total time: 140.02s
                               ETA: 649 mins 33.2 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 129745 steps/s (collection: 0.624s, learning 0.134s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): -4.54
                Mean reward (task): -4.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0066
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0061
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.76s
                        Total time: 140.78s
                               ETA: 649 mins 25.6 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 141677 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): -4.38
                Mean reward (task): -4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0065
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0091
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.69s
                        Total time: 141.47s
                               ETA: 649 mins 0.5 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 139199 steps/s (collection: 0.566s, learning 0.140s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): -4.49
                Mean reward (task): -4.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0066
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0092
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0064
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.71s
                        Total time: 142.18s
                               ETA: 648 mins 39.1 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 157367 steps/s (collection: 0.502s, learning 0.123s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.39
                Mean reward (task): -4.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0065
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0093
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0064
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.62s
                        Total time: 142.80s
                               ETA: 647 mins 55.7 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 148413 steps/s (collection: 0.539s, learning 0.123s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.34
                Mean reward (task): -4.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0065
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0094
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0059
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.66s
                        Total time: 143.47s
                               ETA: 647 mins 23.0 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 148042 steps/s (collection: 0.513s, learning 0.151s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.38
                Mean reward (task): -4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0063
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0058
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.66s
                        Total time: 144.13s
                               ETA: 646 mins 51.0 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 148068 steps/s (collection: 0.526s, learning 0.138s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.46
                Mean reward (task): -4.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0064
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.66s
                        Total time: 144.80s
                               ETA: 646 mins 19.4 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 149116 steps/s (collection: 0.520s, learning 0.139s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.56
                Mean reward (task): -4.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0064
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0061
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.66s
                        Total time: 145.45s
                               ETA: 645 mins 46.9 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 145953 steps/s (collection: 0.552s, learning 0.122s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.33
                Mean reward (task): -4.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0063
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0249
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.67s
                        Total time: 146.13s
                               ETA: 645 mins 18.5 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 135240 steps/s (collection: 0.581s, learning 0.146s)
               Value function loss: 0.0503
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): -4.44
                Mean reward (task): -4.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0063
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0095
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.73s
                        Total time: 146.85s
                               ETA: 645 mins 4.4 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 142151 steps/s (collection: 0.544s, learning 0.148s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.34
                Mean reward (task): -4.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0062
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.69s
                        Total time: 147.55s
                               ETA: 644 mins 41.2 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 149944 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.47
                Mean reward (task): -4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0063
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.66s
                        Total time: 148.20s
                               ETA: 644 mins 8.9 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 151886 steps/s (collection: 0.521s, learning 0.126s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.42
                Mean reward (task): -4.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0063
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.65s
                        Total time: 148.85s
                               ETA: 643 mins 34.7 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 151726 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.29
                Mean reward (task): -4.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0061
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0055
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.65s
                        Total time: 149.50s
                               ETA: 643 mins 1.1 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 144767 steps/s (collection: 0.555s, learning 0.124s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.46
                Mean reward (task): -4.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0061
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.68s
                        Total time: 150.18s
                               ETA: 642 mins 35.8 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 140164 steps/s (collection: 0.562s, learning 0.140s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.24
                Mean reward (task): -4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0062
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.70s
                        Total time: 150.88s
                               ETA: 642 mins 16.4 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 145564 steps/s (collection: 0.539s, learning 0.137s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.31
                Mean reward (task): -4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0062
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.68s
                        Total time: 151.55s
                               ETA: 641 mins 50.6 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 141550 steps/s (collection: 0.570s, learning 0.124s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.35
                Mean reward (task): -4.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0062
          Mean episode rew_dof_acc: -0.0148
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0058
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.69s
                        Total time: 152.25s
                               ETA: 641 mins 30.0 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 149931 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.36
                Mean reward (task): -4.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0061
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0096
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.66s
                        Total time: 152.90s
                               ETA: 640 mins 59.7 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 156841 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.16
                Mean reward (task): -4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0061
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.63s
                        Total time: 153.53s
                               ETA: 640 mins 22.5 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 145459 steps/s (collection: 0.547s, learning 0.129s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.31
                Mean reward (task): -4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0060
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.68s
                        Total time: 154.21s
                               ETA: 639 mins 57.9 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 145342 steps/s (collection: 0.537s, learning 0.139s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): -4.30
                Mean reward (task): -4.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0148
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.68s
                        Total time: 154.88s
                               ETA: 639 mins 33.7 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 147507 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.22
                Mean reward (task): -4.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0265
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.67s
                        Total time: 155.55s
                               ETA: 639 mins 7.3 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 147941 steps/s (collection: 0.528s, learning 0.137s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.34
                Mean reward (task): -4.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0097
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.66s
                        Total time: 156.21s
                               ETA: 638 mins 40.6 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 138556 steps/s (collection: 0.570s, learning 0.140s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.34
                Mean reward (task): -4.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0047
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.71s
                        Total time: 156.92s
                               ETA: 638 mins 25.2 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 153456 steps/s (collection: 0.518s, learning 0.122s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.35
                Mean reward (task): -4.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.64s
                        Total time: 157.56s
                               ETA: 637 mins 53.2 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 146874 steps/s (collection: 0.545s, learning 0.124s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.25
                Mean reward (task): -4.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.67s
                        Total time: 158.23s
                               ETA: 637 mins 28.4 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 136580 steps/s (collection: 0.597s, learning 0.122s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.30
                Mean reward (task): -4.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.72s
                        Total time: 158.95s
                               ETA: 637 mins 16.0 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 149722 steps/s (collection: 0.533s, learning 0.123s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.24
                Mean reward (task): -4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0264
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.66s
                        Total time: 159.61s
                               ETA: 636 mins 48.6 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 149350 steps/s (collection: 0.534s, learning 0.124s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.31
                Mean reward (task): -4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0143
   Mean episode rew_dof_pos_limits: -0.0098
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.66s
                        Total time: 160.27s
                               ETA: 636 mins 21.8 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 153201 steps/s (collection: 0.519s, learning 0.123s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0148
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.64s
                        Total time: 160.91s
                               ETA: 635 mins 51.4 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 145516 steps/s (collection: 0.547s, learning 0.129s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.33
                Mean reward (task): -4.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.68s
                        Total time: 161.58s
                               ETA: 635 mins 29.2 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 144264 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.18
                Mean reward (task): -4.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.68s
                        Total time: 162.27s
                               ETA: 635 mins 8.6 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 141411 steps/s (collection: 0.572s, learning 0.123s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.19
                Mean reward (task): -4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.70s
                        Total time: 162.96s
                               ETA: 634 mins 51.4 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 138928 steps/s (collection: 0.579s, learning 0.129s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.25
                Mean reward (task): -4.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.71s
                        Total time: 163.67s
                               ETA: 634 mins 37.3 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 150836 steps/s (collection: 0.531s, learning 0.121s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.21
                Mean reward (task): -4.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0141
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.65s
                        Total time: 164.32s
                               ETA: 634 mins 10.4 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 143727 steps/s (collection: 0.557s, learning 0.127s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0267
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.68s
                        Total time: 165.00s
                               ETA: 633 mins 51.1 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 150350 steps/s (collection: 0.512s, learning 0.141s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.14
                Mean reward (task): -4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.65s
                        Total time: 165.66s
                               ETA: 633 mins 25.1 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 149129 steps/s (collection: 0.528s, learning 0.131s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.26
                Mean reward (task): -4.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0143
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.66s
                        Total time: 166.32s
                               ETA: 633 mins 0.5 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 132659 steps/s (collection: 0.605s, learning 0.136s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.16
                Mean reward (task): -4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0278
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.74s
                        Total time: 167.06s
                               ETA: 632 mins 54.8 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 140202 steps/s (collection: 0.557s, learning 0.144s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0139
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.70s
                        Total time: 167.76s
                               ETA: 632 mins 40.0 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 138622 steps/s (collection: 0.581s, learning 0.128s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.28
                Mean reward (task): -4.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.71s
                        Total time: 168.47s
                               ETA: 632 mins 27.3 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 148329 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.24
                Mean reward (task): -4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0141
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.66s
                        Total time: 169.13s
                               ETA: 632 mins 4.2 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 150350 steps/s (collection: 0.530s, learning 0.124s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.25
                Mean reward (task): -4.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0140
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.65s
                        Total time: 169.78s
                               ETA: 631 mins 39.3 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 156925 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.24
                Mean reward (task): -4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0138
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.63s
                        Total time: 170.41s
                               ETA: 631 mins 8.6 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 139024 steps/s (collection: 0.579s, learning 0.128s)
               Value function loss: 0.0393
                    Surrogate loss: 0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.21
                Mean reward (task): -4.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0139
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.71s
                        Total time: 171.12s
                               ETA: 630 mins 55.9 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 158969 steps/s (collection: 0.496s, learning 0.123s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.19
                Mean reward (task): -4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0135
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.62s
                        Total time: 171.74s
                               ETA: 630 mins 23.9 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 143754 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.27
                Mean reward (task): -4.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0141
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.68s
                        Total time: 172.42s
                               ETA: 630 mins 6.4 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 143543 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0487
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0137
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.68s
                        Total time: 173.11s
                               ETA: 629 mins 49.3 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 131926 steps/s (collection: 0.599s, learning 0.146s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.18
                Mean reward (task): -4.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0139
   Mean episode rew_dof_pos_limits: -0.0099
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.75s
                        Total time: 173.85s
                               ETA: 629 mins 45.5 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 155149 steps/s (collection: 0.510s, learning 0.124s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.15
                Mean reward (task): -4.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0137
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0281
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.63s
                        Total time: 174.48s
                               ETA: 629 mins 17.6 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 146564 steps/s (collection: 0.548s, learning 0.123s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.15
                Mean reward (task): -4.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0139
   Mean episode rew_dof_pos_limits: -0.0100
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.67s
                        Total time: 175.15s
                               ETA: 628 mins 57.9 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 146164 steps/s (collection: 0.551s, learning 0.122s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): -4.08
                Mean reward (task): -4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0134
   Mean episode rew_dof_pos_limits: -0.0101
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.67s
                        Total time: 175.83s
                               ETA: 628 mins 38.8 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 155059 steps/s (collection: 0.510s, learning 0.124s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.14
                Mean reward (task): -4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0134
   Mean episode rew_dof_pos_limits: -0.0102
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0288
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.63s
                        Total time: 176.46s
                               ETA: 628 mins 11.5 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 156207 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.15
                Mean reward (task): -4.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0134
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.63s
                        Total time: 177.09s
                               ETA: 627 mins 43.5 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 156915 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.18
                Mean reward (task): -4.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0131
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.63s
                        Total time: 177.72s
                               ETA: 627 mins 15.2 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 149372 steps/s (collection: 0.533s, learning 0.125s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.17
                Mean reward (task): -4.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0130
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0292
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.66s
                        Total time: 178.38s
                               ETA: 626 mins 53.7 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 151627 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.05
                Mean reward (task): -4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0133
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0290
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.65s
                        Total time: 179.02s
                               ETA: 626 mins 30.4 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 143698 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0130
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.68s
                        Total time: 179.71s
                               ETA: 626 mins 14.8 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 154818 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.06
                Mean reward (task): -4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0134
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.63s
                        Total time: 180.34s
                               ETA: 625 mins 49.0 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 139150 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.23
                Mean reward (task): -4.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0132
   Mean episode rew_dof_pos_limits: -0.0103
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0048
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.71s
                        Total time: 181.05s
                               ETA: 625 mins 38.3 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 137928 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0131
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0297
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.71s
                        Total time: 181.76s
                               ETA: 625 mins 28.9 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 150325 steps/s (collection: 0.514s, learning 0.140s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.07
                Mean reward (task): -4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0134
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.65s
                        Total time: 182.42s
                               ETA: 625 mins 7.5 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 149587 steps/s (collection: 0.517s, learning 0.140s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.16
                Mean reward (task): -4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.66s
                        Total time: 183.07s
                               ETA: 624 mins 47.0 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 155243 steps/s (collection: 0.494s, learning 0.139s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): -4.31
                Mean reward (task): -4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0133
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.63s
                        Total time: 183.71s
                               ETA: 624 mins 21.7 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 153228 steps/s (collection: 0.521s, learning 0.121s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -3.95
                Mean reward (task): -3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0127
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.64s
                        Total time: 184.35s
                               ETA: 623 mins 58.4 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 137774 steps/s (collection: 0.592s, learning 0.121s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.17
                Mean reward (task): -4.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0131
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0288
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.71s
                        Total time: 185.06s
                               ETA: 623 mins 49.7 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 156529 steps/s (collection: 0.506s, learning 0.122s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.14
                Mean reward (task): -4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0129
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.63s
                        Total time: 185.69s
                               ETA: 623 mins 24.0 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 155521 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.01
                Mean reward (task): -4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0295
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.63s
                        Total time: 186.32s
                               ETA: 622 mins 59.2 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 156314 steps/s (collection: 0.505s, learning 0.124s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.19
                Mean reward (task): -4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.63s
                        Total time: 186.95s
                               ETA: 622 mins 34.0 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 144701 steps/s (collection: 0.548s, learning 0.131s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.11
                Mean reward (task): -4.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0295
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.68s
                        Total time: 187.63s
                               ETA: 622 mins 19.0 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 147529 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0360
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.17
                Mean reward (task): -4.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0129
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.67s
                        Total time: 188.30s
                               ETA: 622 mins 1.6 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 141747 steps/s (collection: 0.567s, learning 0.127s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.16
                Mean reward (task): -4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0125
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0297
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.69s
                        Total time: 188.99s
                               ETA: 621 mins 49.7 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 158056 steps/s (collection: 0.500s, learning 0.122s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.12
                Mean reward (task): -4.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0125
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.62s
                        Total time: 189.61s
                               ETA: 621 mins 23.7 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 157816 steps/s (collection: 0.499s, learning 0.124s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0292
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.62s
                        Total time: 190.23s
                               ETA: 620 mins 58.2 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 155873 steps/s (collection: 0.507s, learning 0.124s)
               Value function loss: 0.0476
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0127
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.63s
                        Total time: 190.86s
                               ETA: 620 mins 34.4 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 159213 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.10
                Mean reward (task): -4.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.62s
                        Total time: 191.48s
                               ETA: 620 mins 8.2 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 161055 steps/s (collection: 0.487s, learning 0.124s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.03
                Mean reward (task): -4.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0126
   Mean episode rew_dof_pos_limits: -0.0105
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.61s
                        Total time: 192.09s
                               ETA: 619 mins 40.8 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 158155 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0373
                    Surrogate loss: 0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0127
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.62s
                        Total time: 192.71s
                               ETA: 619 mins 15.8 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 159600 steps/s (collection: 0.490s, learning 0.126s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): -4.14
                Mean reward (task): -4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0127
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0305
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.62s
                        Total time: 193.33s
                               ETA: 618 mins 49.9 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 156386 steps/s (collection: 0.507s, learning 0.121s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0104
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.63s
                        Total time: 193.96s
                               ETA: 618 mins 26.6 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 159410 steps/s (collection: 0.495s, learning 0.122s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.06
                Mean reward (task): -4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0126
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.62s
                        Total time: 194.58s
                               ETA: 618 mins 1.2 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 158225 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.07
                Mean reward (task): -4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0301
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.62s
                        Total time: 195.20s
                               ETA: 617 mins 36.8 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 156276 steps/s (collection: 0.507s, learning 0.122s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.04
                Mean reward (task): -4.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0125
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0300
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.63s
                        Total time: 195.83s
                               ETA: 617 mins 14.2 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 160789 steps/s (collection: 0.487s, learning 0.124s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.07
                Mean reward (task): -4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.61s
                        Total time: 196.44s
                               ETA: 616 mins 48.3 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 144384 steps/s (collection: 0.552s, learning 0.129s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0106
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.68s
                        Total time: 197.12s
                               ETA: 616 mins 35.7 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 148691 steps/s (collection: 0.538s, learning 0.123s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0128
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0305
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.66s
                        Total time: 197.78s
                               ETA: 616 mins 19.5 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 157239 steps/s (collection: 0.503s, learning 0.122s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.08
                Mean reward (task): -4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.63s
                        Total time: 198.40s
                               ETA: 615 mins 56.7 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 156693 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.09
                Mean reward (task): -4.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0125
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.63s
                        Total time: 199.03s
                               ETA: 615 mins 34.5 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 138308 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.10
                Mean reward (task): -4.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0305
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.71s
                        Total time: 199.74s
                               ETA: 615 mins 27.9 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 138574 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.71s
                        Total time: 200.45s
                               ETA: 615 mins 21.0 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 156436 steps/s (collection: 0.507s, learning 0.122s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.02
                Mean reward (task): -4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.63s
                        Total time: 201.08s
                               ETA: 614 mins 59.3 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 137218 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.05
                Mean reward (task): -4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.72s
                        Total time: 201.80s
                               ETA: 614 mins 53.9 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 129465 steps/s (collection: 0.628s, learning 0.132s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.03
                Mean reward (task): -4.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0107
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.76s
                        Total time: 202.56s
                               ETA: 614 mins 56.4 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 128837 steps/s (collection: 0.629s, learning 0.134s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.05
                Mean reward (task): -4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.76s
                        Total time: 203.32s
                               ETA: 614 mins 59.4 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 139834 steps/s (collection: 0.571s, learning 0.132s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.09
                Mean reward (task): -4.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0310
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.70s
                        Total time: 204.02s
                               ETA: 614 mins 51.6 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 160244 steps/s (collection: 0.493s, learning 0.121s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.12
                Mean reward (task): -4.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.61s
                        Total time: 204.64s
                               ETA: 614 mins 27.7 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 155764 steps/s (collection: 0.499s, learning 0.132s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.13
                Mean reward (task): -4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.63s
                        Total time: 205.27s
                               ETA: 614 mins 7.2 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 149424 steps/s (collection: 0.523s, learning 0.135s)
               Value function loss: 0.0367
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.08
                Mean reward (task): -4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.66s
                        Total time: 205.92s
                               ETA: 613 mins 51.6 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 146242 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.67s
                        Total time: 206.60s
                               ETA: 613 mins 38.6 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 133985 steps/s (collection: 0.611s, learning 0.122s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.06
                Mean reward (task): -4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.73s
                        Total time: 207.33s
                               ETA: 613 mins 36.7 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 162272 steps/s (collection: 0.483s, learning 0.123s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.61s
                        Total time: 207.94s
                               ETA: 613 mins 12.1 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 158876 steps/s (collection: 0.496s, learning 0.123s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.62s
                        Total time: 208.55s
                               ETA: 612 mins 50.0 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 146730 steps/s (collection: 0.546s, learning 0.124s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.05
                Mean reward (task): -4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0112
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0323
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.67s
                        Total time: 209.22s
                               ETA: 612 mins 37.0 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 158638 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.05
                Mean reward (task): -4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0310
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.62s
                        Total time: 209.84s
                               ETA: 612 mins 15.4 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 155073 steps/s (collection: 0.511s, learning 0.123s)
               Value function loss: 0.0369
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.63s
                        Total time: 210.48s
                               ETA: 611 mins 56.3 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 149040 steps/s (collection: 0.536s, learning 0.123s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0313
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.66s
                        Total time: 211.14s
                               ETA: 611 mins 41.8 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 155006 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.95
                Mean reward (task): -3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.63s
                        Total time: 211.77s
                               ETA: 611 mins 23.1 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 154331 steps/s (collection: 0.514s, learning 0.123s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0316
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.64s
                        Total time: 212.41s
                               ETA: 611 mins 4.9 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 152076 steps/s (collection: 0.524s, learning 0.122s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.01
                Mean reward (task): -4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0108
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.65s
                        Total time: 213.06s
                               ETA: 610 mins 48.5 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 147084 steps/s (collection: 0.526s, learning 0.142s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.67s
                        Total time: 213.72s
                               ETA: 610 mins 36.0 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 140764 steps/s (collection: 0.548s, learning 0.150s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.02
                Mean reward (task): -4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0109
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.70s
                        Total time: 214.42s
                               ETA: 610 mins 28.6 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 149577 steps/s (collection: 0.534s, learning 0.123s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.66s
                        Total time: 215.08s
                               ETA: 610 mins 14.3 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 156802 steps/s (collection: 0.501s, learning 0.126s)
               Value function loss: 0.0337
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.95
                Mean reward (task): -3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.63s
                        Total time: 215.71s
                               ETA: 609 mins 55.0 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 153311 steps/s (collection: 0.508s, learning 0.134s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0110
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0311
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.64s
                        Total time: 216.35s
                               ETA: 609 mins 38.2 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 145368 steps/s (collection: 0.552s, learning 0.124s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.02
                Mean reward (task): -4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0110
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.68s
                        Total time: 217.02s
                               ETA: 609 mins 27.4 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 154596 steps/s (collection: 0.512s, learning 0.124s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.14
                Mean reward (task): -4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.64s
                        Total time: 217.66s
                               ETA: 609 mins 9.9 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 146534 steps/s (collection: 0.550s, learning 0.121s)
               Value function loss: 0.0389
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.67s
                        Total time: 218.33s
                               ETA: 608 mins 58.4 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 156115 steps/s (collection: 0.508s, learning 0.122s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.08
                Mean reward (task): -4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.63s
                        Total time: 218.96s
                               ETA: 608 mins 40.1 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 157455 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.62s
                        Total time: 219.58s
                               ETA: 608 mins 21.0 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 150373 steps/s (collection: 0.528s, learning 0.126s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.24
                Mean reward (task): -4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0318
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.65s
                        Total time: 220.24s
                               ETA: 608 mins 6.9 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 140943 steps/s (collection: 0.572s, learning 0.125s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.09
                Mean reward (task): -4.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0111
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.70s
                        Total time: 220.94s
                               ETA: 608 mins 0.1 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 146284 steps/s (collection: 0.540s, learning 0.132s)
               Value function loss: 0.0383
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.67s
                        Total time: 221.61s
                               ETA: 607 mins 49.2 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 150064 steps/s (collection: 0.526s, learning 0.129s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0327
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.66s
                        Total time: 222.26s
                               ETA: 607 mins 35.5 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 126300 steps/s (collection: 0.646s, learning 0.132s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.78s
                        Total time: 223.04s
                               ETA: 607 mins 42.1 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 130890 steps/s (collection: 0.625s, learning 0.126s)
               Value function loss: 0.0340
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.75s
                        Total time: 223.79s
                               ETA: 607 mins 44.2 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 157681 steps/s (collection: 0.498s, learning 0.126s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.02
                Mean reward (task): -4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.62s
                        Total time: 224.42s
                               ETA: 607 mins 25.6 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 146386 steps/s (collection: 0.542s, learning 0.129s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.67s
                        Total time: 225.09s
                               ETA: 607 mins 14.8 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 145368 steps/s (collection: 0.532s, learning 0.144s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.08
                Mean reward (task): -4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.68s
                        Total time: 225.76s
                               ETA: 607 mins 4.9 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 140919 steps/s (collection: 0.573s, learning 0.125s)
               Value function loss: 0.0375
                    Surrogate loss: 0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.70s
                        Total time: 226.46s
                               ETA: 606 mins 58.5 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 153819 steps/s (collection: 0.515s, learning 0.124s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.01
                Mean reward (task): -4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0113
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.64s
                        Total time: 227.10s
                               ETA: 606 mins 42.7 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 154304 steps/s (collection: 0.511s, learning 0.126s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.64s
                        Total time: 227.74s
                               ETA: 606 mins 26.7 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 135240 steps/s (collection: 0.596s, learning 0.131s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0327
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.73s
                        Total time: 228.46s
                               ETA: 606 mins 25.1 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 156868 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.04
                Mean reward (task): -4.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.63s
                        Total time: 229.09s
                               ETA: 606 mins 7.6 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 155512 steps/s (collection: 0.508s, learning 0.124s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.01
                Mean reward (task): -4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.63s
                        Total time: 229.72s
                               ETA: 605 mins 51.1 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 140849 steps/s (collection: 0.575s, learning 0.123s)
               Value function loss: 0.0382
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.70s
                        Total time: 230.42s
                               ETA: 605 mins 45.1 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 152441 steps/s (collection: 0.519s, learning 0.126s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -4.03
                Mean reward (task): -4.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0331
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.64s
                        Total time: 231.07s
                               ETA: 605 mins 30.7 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 146400 steps/s (collection: 0.547s, learning 0.125s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.67s
                        Total time: 231.74s
                               ETA: 605 mins 20.6 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 153960 steps/s (collection: 0.516s, learning 0.123s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0112
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.64s
                        Total time: 232.38s
                               ETA: 605 mins 5.4 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 157134 steps/s (collection: 0.503s, learning 0.123s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.63s
                        Total time: 233.00s
                               ETA: 604 mins 48.3 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 148798 steps/s (collection: 0.526s, learning 0.135s)
               Value function loss: 0.0337
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.66s
                        Total time: 233.66s
                               ETA: 604 mins 36.8 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 147252 steps/s (collection: 0.546s, learning 0.122s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.67s
                        Total time: 234.33s
                               ETA: 604 mins 26.3 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 145184 steps/s (collection: 0.553s, learning 0.124s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.68s
                        Total time: 235.01s
                               ETA: 604 mins 17.4 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 143626 steps/s (collection: 0.562s, learning 0.123s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.68s
                        Total time: 235.69s
                               ETA: 604 mins 9.7 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 145714 steps/s (collection: 0.552s, learning 0.122s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.67s
                        Total time: 236.37s
                               ETA: 604 mins 0.6 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 150572 steps/s (collection: 0.529s, learning 0.124s)
               Value function loss: 0.0459
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0337
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.65s
                        Total time: 237.02s
                               ETA: 603 mins 48.1 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 146558 steps/s (collection: 0.536s, learning 0.134s)
               Value function loss: 0.0459
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0338
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.67s
                        Total time: 237.69s
                               ETA: 603 mins 38.5 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 153595 steps/s (collection: 0.498s, learning 0.142s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.64s
                        Total time: 238.33s
                               ETA: 603 mins 24.2 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 135119 steps/s (collection: 0.578s, learning 0.149s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.73s
                        Total time: 239.06s
                               ETA: 603 mins 23.3 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 138046 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.71s
                        Total time: 239.77s
                               ETA: 603 mins 20.0 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 156387 steps/s (collection: 0.504s, learning 0.125s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.63s
                        Total time: 240.40s
                               ETA: 603 mins 4.2 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 155333 steps/s (collection: 0.506s, learning 0.127s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.63s
                        Total time: 241.03s
                               ETA: 602 mins 49.1 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 144679 steps/s (collection: 0.549s, learning 0.131s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.68s
                        Total time: 241.71s
                               ETA: 602 mins 41.1 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 159316 steps/s (collection: 0.494s, learning 0.123s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.87
                Mean reward (task): -3.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.62s
                        Total time: 242.33s
                               ETA: 602 mins 23.8 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 151608 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 0.0364
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.65s
                        Total time: 242.98s
                               ETA: 602 mins 11.3 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 149627 steps/s (collection: 0.535s, learning 0.122s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.66s
                        Total time: 243.63s
                               ETA: 602 mins 0.1 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 146401 steps/s (collection: 0.549s, learning 0.122s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.67s
                        Total time: 244.30s
                               ETA: 601 mins 51.2 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 154517 steps/s (collection: 0.512s, learning 0.124s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.64s
                        Total time: 244.94s
                               ETA: 601 mins 37.0 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 160061 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0349
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.61s
                        Total time: 245.55s
                               ETA: 601 mins 19.8 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 150597 steps/s (collection: 0.530s, learning 0.122s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0343
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.65s
                        Total time: 246.21s
                               ETA: 601 mins 8.2 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 152274 steps/s (collection: 0.520s, learning 0.126s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0049
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.65s
                        Total time: 246.85s
                               ETA: 600 mins 55.7 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 139462 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.70s
                        Total time: 247.56s
                               ETA: 600 mins 51.9 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 158522 steps/s (collection: 0.498s, learning 0.122s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.62s
                        Total time: 248.18s
                               ETA: 600 mins 35.8 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 156547 steps/s (collection: 0.506s, learning 0.122s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.63s
                        Total time: 248.81s
                               ETA: 600 mins 20.9 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 144240 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0319
                    Surrogate loss: 0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.68s
                        Total time: 249.49s
                               ETA: 600 mins 13.9 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 148970 steps/s (collection: 0.510s, learning 0.150s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.66s
                        Total time: 250.15s
                               ETA: 600 mins 3.8 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 157025 steps/s (collection: 0.505s, learning 0.121s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.63s
                        Total time: 250.77s
                               ETA: 599 mins 48.8 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 151162 steps/s (collection: 0.527s, learning 0.123s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0348
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.65s
                        Total time: 251.42s
                               ETA: 599 mins 37.4 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 155877 steps/s (collection: 0.508s, learning 0.122s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0357
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.63s
                        Total time: 252.05s
                               ETA: 599 mins 23.3 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 147734 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.67s
                        Total time: 252.72s
                               ETA: 599 mins 14.2 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 139721 steps/s (collection: 0.579s, learning 0.124s)
               Value function loss: 0.0320
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.70s
                        Total time: 253.42s
                               ETA: 599 mins 10.6 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 145204 steps/s (collection: 0.554s, learning 0.123s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.68s
                        Total time: 254.10s
                               ETA: 599 mins 3.2 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 158386 steps/s (collection: 0.500s, learning 0.121s)
               Value function loss: 0.0364
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0350
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.62s
                        Total time: 254.72s
                               ETA: 598 mins 47.9 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 132214 steps/s (collection: 0.612s, learning 0.132s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0341
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.74s
                        Total time: 255.46s
                               ETA: 598 mins 50.0 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 140385 steps/s (collection: 0.566s, learning 0.134s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.70s
                        Total time: 256.16s
                               ETA: 598 mins 46.0 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 151673 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.65s
                        Total time: 256.81s
                               ETA: 598 mins 34.7 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 155735 steps/s (collection: 0.489s, learning 0.142s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.63s
                        Total time: 257.44s
                               ETA: 598 mins 21.1 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 146650 steps/s (collection: 0.532s, learning 0.138s)
               Value function loss: 0.0332
                    Surrogate loss: 0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0032
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.67s
                        Total time: 258.11s
                               ETA: 598 mins 13.0 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 137220 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0032
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.72s
                        Total time: 258.83s
                               ETA: 598 mins 11.4 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 142004 steps/s (collection: 0.569s, learning 0.123s)
               Value function loss: 0.0511
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0050
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.69s
                        Total time: 259.52s
                               ETA: 598 mins 6.4 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 153696 steps/s (collection: 0.517s, learning 0.123s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.64s
                        Total time: 260.16s
                               ETA: 597 mins 54.2 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 149098 steps/s (collection: 0.535s, learning 0.124s)
               Value function loss: 0.0321
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0032
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.66s
                        Total time: 260.82s
                               ETA: 597 mins 44.8 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 161154 steps/s (collection: 0.487s, learning 0.123s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.61s
                        Total time: 261.43s
                               ETA: 597 mins 28.6 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 146633 steps/s (collection: 0.533s, learning 0.138s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.67s
                        Total time: 262.10s
                               ETA: 597 mins 20.8 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 161565 steps/s (collection: 0.485s, learning 0.123s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.61s
                        Total time: 262.71s
                               ETA: 597 mins 4.6 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 160642 steps/s (collection: 0.488s, learning 0.124s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.61s
                        Total time: 263.32s
                               ETA: 596 mins 49.0 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 157191 steps/s (collection: 0.502s, learning 0.123s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0354
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.63s
                        Total time: 263.95s
                               ETA: 596 mins 35.2 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 160255 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.61s
                        Total time: 264.56s
                               ETA: 596 mins 19.9 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 145723 steps/s (collection: 0.547s, learning 0.127s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.87
                Mean reward (task): -3.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.67s
                        Total time: 265.24s
                               ETA: 596 mins 13.0 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 142266 steps/s (collection: 0.563s, learning 0.128s)
               Value function loss: 0.0368
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.69s
                        Total time: 265.93s
                               ETA: 596 mins 8.2 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 147099 steps/s (collection: 0.531s, learning 0.138s)
               Value function loss: 0.0379
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.67s
                        Total time: 266.60s
                               ETA: 596 mins 0.5 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 150169 steps/s (collection: 0.532s, learning 0.122s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.65s
                        Total time: 267.25s
                               ETA: 595 mins 50.9 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 155967 steps/s (collection: 0.489s, learning 0.141s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.63s
                        Total time: 267.88s
                               ETA: 595 mins 38.2 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 137953 steps/s (collection: 0.573s, learning 0.139s)
               Value function loss: 0.0380
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.71s
                        Total time: 268.59s
                               ETA: 595 mins 36.5 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 134880 steps/s (collection: 0.571s, learning 0.158s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.71
                Mean reward (task): -3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.73s
                        Total time: 269.32s
                               ETA: 595 mins 36.9 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 154181 steps/s (collection: 0.513s, learning 0.124s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.64s
                        Total time: 269.96s
                               ETA: 595 mins 25.3 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 139578 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.70s
                        Total time: 270.66s
                               ETA: 595 mins 22.5 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 138953 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0364
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.71s
                        Total time: 271.37s
                               ETA: 595 mins 20.2 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 151590 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.65s
                        Total time: 272.02s
                               ETA: 595 mins 10.1 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 156922 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0093
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.63s
                        Total time: 272.65s
                               ETA: 594 mins 57.2 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 154167 steps/s (collection: 0.514s, learning 0.124s)
               Value function loss: 0.0322
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.64s
                        Total time: 273.28s
                               ETA: 594 mins 45.8 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 155400 steps/s (collection: 0.503s, learning 0.130s)
               Value function loss: 0.0383
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.63s
                        Total time: 273.92s
                               ETA: 594 mins 33.8 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 153248 steps/s (collection: 0.519s, learning 0.122s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.64s
                        Total time: 274.56s
                               ETA: 594 mins 23.0 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 161641 steps/s (collection: 0.485s, learning 0.124s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.61s
                        Total time: 275.17s
                               ETA: 594 mins 7.9 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 149642 steps/s (collection: 0.533s, learning 0.123s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.66s
                        Total time: 275.82s
                               ETA: 593 mins 59.3 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 153199 steps/s (collection: 0.519s, learning 0.123s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0093
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0051
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.64s
                        Total time: 276.46s
                               ETA: 593 mins 48.7 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 147447 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0362
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.67s
                        Total time: 277.13s
                               ETA: 593 mins 41.4 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 154609 steps/s (collection: 0.514s, learning 0.121s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0371
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.64s
                        Total time: 277.77s
                               ETA: 593 mins 30.1 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 160196 steps/s (collection: 0.488s, learning 0.126s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.61s
                        Total time: 278.38s
                               ETA: 593 mins 16.1 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 155528 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0335
                    Surrogate loss: 0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0363
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.63s
                        Total time: 279.01s
                               ETA: 593 mins 4.5 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 152876 steps/s (collection: 0.519s, learning 0.124s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.64s
                        Total time: 279.66s
                               ETA: 592 mins 54.3 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 136776 steps/s (collection: 0.580s, learning 0.139s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.72s
                        Total time: 280.37s
                               ETA: 592 mins 53.8 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 159784 steps/s (collection: 0.493s, learning 0.123s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.62s
                        Total time: 280.99s
                               ETA: 592 mins 40.2 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 152080 steps/s (collection: 0.523s, learning 0.124s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.65s
                        Total time: 281.64s
                               ETA: 592 mins 30.6 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 137357 steps/s (collection: 0.578s, learning 0.137s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.72s
                        Total time: 282.35s
                               ETA: 592 mins 29.8 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 143787 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.68s
                        Total time: 283.04s
                               ETA: 592 mins 24.9 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 155468 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.63s
                        Total time: 283.67s
                               ETA: 592 mins 13.6 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 140882 steps/s (collection: 0.567s, learning 0.130s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0052
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.70s
                        Total time: 284.37s
                               ETA: 592 mins 10.6 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 159991 steps/s (collection: 0.492s, learning 0.122s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.61s
                        Total time: 284.98s
                               ETA: 591 mins 57.2 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 155956 steps/s (collection: 0.503s, learning 0.127s)
               Value function loss: 0.0329
                    Surrogate loss: 0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0033
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.63s
                        Total time: 285.61s
                               ETA: 591 mins 45.8 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 145686 steps/s (collection: 0.550s, learning 0.125s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0365
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.67s
                        Total time: 286.28s
                               ETA: 591 mins 40.0 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 161873 steps/s (collection: 0.484s, learning 0.123s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.61s
                        Total time: 286.89s
                               ETA: 591 mins 25.9 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 157365 steps/s (collection: 0.503s, learning 0.122s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0377
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.62s
                        Total time: 287.52s
                               ETA: 591 mins 14.0 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 154809 steps/s (collection: 0.513s, learning 0.122s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.64s
                        Total time: 288.15s
                               ETA: 591 mins 3.4 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 158112 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.71
                Mean reward (task): -3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.62s
                        Total time: 288.77s
                               ETA: 590 mins 51.2 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 133997 steps/s (collection: 0.610s, learning 0.123s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.73s
                        Total time: 289.51s
                               ETA: 590 mins 52.8 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 159669 steps/s (collection: 0.491s, learning 0.125s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.62s
                        Total time: 290.12s
                               ETA: 590 mins 40.0 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 153834 steps/s (collection: 0.516s, learning 0.123s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.64s
                        Total time: 290.76s
                               ETA: 590 mins 30.1 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 159001 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0053
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.62s
                        Total time: 291.38s
                               ETA: 590 mins 17.7 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 149587 steps/s (collection: 0.533s, learning 0.124s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.66s
                        Total time: 292.04s
                               ETA: 590 mins 10.0 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 145315 steps/s (collection: 0.547s, learning 0.129s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.68s
                        Total time: 292.71s
                               ETA: 590 mins 4.8 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 152260 steps/s (collection: 0.523s, learning 0.123s)
               Value function loss: 0.0400
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.65s
                        Total time: 293.36s
                               ETA: 589 mins 55.8 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 146278 steps/s (collection: 0.544s, learning 0.128s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.67s
                        Total time: 294.03s
                               ETA: 589 mins 50.1 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 151005 steps/s (collection: 0.515s, learning 0.136s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.65s
                        Total time: 294.68s
                               ETA: 589 mins 41.8 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 161325 steps/s (collection: 0.486s, learning 0.123s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.61s
                        Total time: 295.29s
                               ETA: 589 mins 28.7 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 163131 steps/s (collection: 0.481s, learning 0.122s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.60s
                        Total time: 295.89s
                               ETA: 589 mins 14.7 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 158028 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0371
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.62s
                        Total time: 296.52s
                               ETA: 589 mins 3.2 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 154820 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.63s
                        Total time: 297.15s
                               ETA: 588 mins 53.2 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 137107 steps/s (collection: 0.569s, learning 0.148s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.72s
                        Total time: 297.87s
                               ETA: 588 mins 53.0 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 147036 steps/s (collection: 0.542s, learning 0.126s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.67s
                        Total time: 298.54s
                               ETA: 588 mins 47.1 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 150533 steps/s (collection: 0.531s, learning 0.122s)
               Value function loss: 0.0378
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.93
                Mean reward (task): -3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.65s
                        Total time: 299.19s
                               ETA: 588 mins 39.4 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 161819 steps/s (collection: 0.486s, learning 0.122s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0378
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.61s
                        Total time: 299.80s
                               ETA: 588 mins 26.3 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 128779 steps/s (collection: 0.623s, learning 0.140s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.76s
                        Total time: 300.56s
                               ETA: 588 mins 31.6 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 160754 steps/s (collection: 0.489s, learning 0.122s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0035
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.61s
                        Total time: 301.17s
                               ETA: 588 mins 19.1 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 151392 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.65s
                        Total time: 301.82s
                               ETA: 588 mins 11.0 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 164296 steps/s (collection: 0.476s, learning 0.123s)
               Value function loss: 0.0404
                    Surrogate loss: 0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.60s
                        Total time: 302.42s
                               ETA: 587 mins 57.1 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 163851 steps/s (collection: 0.475s, learning 0.125s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.60s
                        Total time: 303.02s
                               ETA: 587 mins 43.4 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 162234 steps/s (collection: 0.481s, learning 0.125s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.61s
                        Total time: 303.63s
                               ETA: 587 mins 30.4 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 142666 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.69s
                        Total time: 304.31s
                               ETA: 587 mins 27.2 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 164195 steps/s (collection: 0.475s, learning 0.123s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.60s
                        Total time: 304.91s
                               ETA: 587 mins 13.5 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 164511 steps/s (collection: 0.474s, learning 0.124s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.60s
                        Total time: 305.51s
                               ETA: 586 mins 59.7 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 163022 steps/s (collection: 0.481s, learning 0.122s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0034
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.60s
                        Total time: 306.11s
                               ETA: 586 mins 46.7 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 151503 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0360
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0094
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.65s
                        Total time: 306.76s
                               ETA: 586 mins 38.9 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 162472 steps/s (collection: 0.483s, learning 0.122s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0384
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.61s
                        Total time: 307.37s
                               ETA: 586 mins 26.2 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 154085 steps/s (collection: 0.515s, learning 0.123s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.64s
                        Total time: 308.01s
                               ETA: 586 mins 17.3 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 162917 steps/s (collection: 0.480s, learning 0.123s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.60s
                        Total time: 308.61s
                               ETA: 586 mins 4.4 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 148222 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0391
                    Surrogate loss: 0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.66s
                        Total time: 309.27s
                               ETA: 585 mins 58.5 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 162418 steps/s (collection: 0.481s, learning 0.124s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0054
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.61s
                        Total time: 309.88s
                               ETA: 585 mins 46.0 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 161683 steps/s (collection: 0.484s, learning 0.124s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.61s
                        Total time: 310.49s
                               ETA: 585 mins 33.8 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 166071 steps/s (collection: 0.469s, learning 0.123s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.59s
                        Total time: 311.08s
                               ETA: 585 mins 19.9 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 154684 steps/s (collection: 0.513s, learning 0.123s)
               Value function loss: 0.0354
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.64s
                        Total time: 311.71s
                               ETA: 585 mins 11.0 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 151559 steps/s (collection: 0.527s, learning 0.121s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.87
                Mean reward (task): -3.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.65s
                        Total time: 312.36s
                               ETA: 585 mins 3.5 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 154600 steps/s (collection: 0.513s, learning 0.123s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.64s
                        Total time: 313.00s
                               ETA: 584 mins 54.7 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 145177 steps/s (collection: 0.531s, learning 0.146s)
               Value function loss: 0.0397
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0377
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.68s
                        Total time: 313.67s
                               ETA: 584 mins 50.5 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 157612 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.62s
                        Total time: 314.30s
                               ETA: 584 mins 40.4 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 161968 steps/s (collection: 0.483s, learning 0.123s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.61s
                        Total time: 314.91s
                               ETA: 584 mins 28.4 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 153035 steps/s (collection: 0.502s, learning 0.140s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0095
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0386
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.64s
                        Total time: 315.55s
                               ETA: 584 mins 20.5 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 147907 steps/s (collection: 0.523s, learning 0.141s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.66s
                        Total time: 316.21s
                               ETA: 584 mins 15.0 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 138009 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0055
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0375
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.71s
                        Total time: 316.92s
                               ETA: 584 mins 14.9 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 139603 steps/s (collection: 0.569s, learning 0.136s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0373
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.70s
                        Total time: 317.63s
                               ETA: 584 mins 13.8 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 147342 steps/s (collection: 0.545s, learning 0.122s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0036
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.67s
                        Total time: 318.30s
                               ETA: 584 mins 8.6 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 158645 steps/s (collection: 0.496s, learning 0.124s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.62s
                        Total time: 318.92s
                               ETA: 583 mins 58.3 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 143503 steps/s (collection: 0.557s, learning 0.128s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.69s
                        Total time: 319.60s
                               ETA: 583 mins 55.2 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 154280 steps/s (collection: 0.502s, learning 0.135s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.64s
                        Total time: 320.24s
                               ETA: 583 mins 46.8 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 156765 steps/s (collection: 0.500s, learning 0.128s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.63s
                        Total time: 320.87s
                               ETA: 583 mins 37.4 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 141952 steps/s (collection: 0.568s, learning 0.124s)
               Value function loss: 0.0389
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0047
          Mean episode rew_dof_acc: -0.0096
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.69s
                        Total time: 321.56s
                               ETA: 583 mins 35.1 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 145723 steps/s (collection: 0.544s, learning 0.131s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.67s
                        Total time: 322.23s
                               ETA: 583 mins 30.9 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 161151 steps/s (collection: 0.486s, learning 0.124s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.61s
                        Total time: 322.84s
                               ETA: 583 mins 19.8 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 142347 steps/s (collection: 0.555s, learning 0.136s)
               Value function loss: 0.0410
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.69s
                        Total time: 323.53s
                               ETA: 583 mins 17.3 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 151972 steps/s (collection: 0.524s, learning 0.123s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0056
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.65s
                        Total time: 324.18s
                               ETA: 583 mins 10.2 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 144771 steps/s (collection: 0.544s, learning 0.135s)
               Value function loss: 0.0370
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.68s
                        Total time: 324.86s
                               ETA: 583 mins 6.6 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 143826 steps/s (collection: 0.535s, learning 0.148s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.68s
                        Total time: 325.54s
                               ETA: 583 mins 3.4 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 152458 steps/s (collection: 0.503s, learning 0.142s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.64s
                        Total time: 326.19s
                               ETA: 582 mins 56.1 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 154652 steps/s (collection: 0.495s, learning 0.140s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0388
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.64s
                        Total time: 326.82s
                               ETA: 582 mins 47.9 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 139719 steps/s (collection: 0.562s, learning 0.141s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0400
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.70s
                        Total time: 327.53s
                               ETA: 582 mins 46.9 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 157414 steps/s (collection: 0.483s, learning 0.141s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.62s
                        Total time: 328.15s
                               ETA: 582 mins 37.6 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 159828 steps/s (collection: 0.479s, learning 0.136s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.62s
                        Total time: 328.77s
                               ETA: 582 mins 27.2 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 151743 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0433
                    Surrogate loss: 0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.65s
                        Total time: 329.41s
                               ETA: 582 mins 20.4 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 146584 steps/s (collection: 0.549s, learning 0.122s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.67s
                        Total time: 330.08s
                               ETA: 582 mins 16.0 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 132595 steps/s (collection: 0.609s, learning 0.132s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.74s
                        Total time: 330.83s
                               ETA: 582 mins 19.1 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 154665 steps/s (collection: 0.511s, learning 0.124s)
               Value function loss: 0.0473
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.64s
                        Total time: 331.46s
                               ETA: 582 mins 11.1 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 155057 steps/s (collection: 0.512s, learning 0.122s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.63s
                        Total time: 332.10s
                               ETA: 582 mins 2.9 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 154411 steps/s (collection: 0.513s, learning 0.124s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.64s
                        Total time: 332.73s
                               ETA: 581 mins 55.0 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 156844 steps/s (collection: 0.503s, learning 0.124s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.63s
                        Total time: 333.36s
                               ETA: 581 mins 46.1 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 134361 steps/s (collection: 0.581s, learning 0.151s)
               Value function loss: 0.0433
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.73s
                        Total time: 334.09s
                               ETA: 581 mins 48.2 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 154688 steps/s (collection: 0.511s, learning 0.124s)
               Value function loss: 0.0446
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0097
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0400
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.64s
                        Total time: 334.73s
                               ETA: 581 mins 40.3 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 159494 steps/s (collection: 0.492s, learning 0.124s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0400
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.62s
                        Total time: 335.34s
                               ETA: 581 mins 30.4 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 148408 steps/s (collection: 0.531s, learning 0.131s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.66s
                        Total time: 336.00s
                               ETA: 581 mins 25.3 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 155425 steps/s (collection: 0.491s, learning 0.141s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.82
                Mean reward (task): -3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.63s
                        Total time: 336.64s
                               ETA: 581 mins 17.1 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 150215 steps/s (collection: 0.510s, learning 0.145s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.65s
                        Total time: 337.29s
                               ETA: 581 mins 11.3 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 147967 steps/s (collection: 0.522s, learning 0.142s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.66s
                        Total time: 337.96s
                               ETA: 581 mins 6.5 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 144091 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0098
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0057
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.68s
                        Total time: 338.64s
                               ETA: 581 mins 3.5 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 149409 steps/s (collection: 0.534s, learning 0.124s)
               Value function loss: 0.0380
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.66s
                        Total time: 339.30s
                               ETA: 580 mins 58.1 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 147556 steps/s (collection: 0.541s, learning 0.125s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.67s
                        Total time: 339.96s
                               ETA: 580 mins 53.5 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 154144 steps/s (collection: 0.516s, learning 0.122s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.64s
                        Total time: 340.60s
                               ETA: 580 mins 46.0 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 147400 steps/s (collection: 0.531s, learning 0.136s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.67s
                        Total time: 341.27s
                               ETA: 580 mins 41.6 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 160484 steps/s (collection: 0.491s, learning 0.122s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0405
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.61s
                        Total time: 341.88s
                               ETA: 580 mins 31.6 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 132993 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0058
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.74s
                        Total time: 342.62s
                               ETA: 580 mins 34.5 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 158940 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0452
                    Surrogate loss: 0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0405
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.62s
                        Total time: 343.24s
                               ETA: 580 mins 25.2 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 157888 steps/s (collection: 0.495s, learning 0.128s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.62s
                        Total time: 343.86s
                               ETA: 580 mins 16.3 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 152994 steps/s (collection: 0.508s, learning 0.134s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0408
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.64s
                        Total time: 344.50s
                               ETA: 580 mins 9.5 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 152909 steps/s (collection: 0.497s, learning 0.146s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0410
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.64s
                        Total time: 345.14s
                               ETA: 580 mins 2.7 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 152958 steps/s (collection: 0.506s, learning 0.137s)
               Value function loss: 0.0440
                    Surrogate loss: 0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.64s
                        Total time: 345.79s
                               ETA: 579 mins 55.9 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 158677 steps/s (collection: 0.481s, learning 0.139s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.62s
                        Total time: 346.41s
                               ETA: 579 mins 46.9 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 148492 steps/s (collection: 0.523s, learning 0.139s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0099
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.66s
                        Total time: 347.07s
                               ETA: 579 mins 42.1 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 146963 steps/s (collection: 0.534s, learning 0.134s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.67s
                        Total time: 347.74s
                               ETA: 579 mins 38.0 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 140245 steps/s (collection: 0.563s, learning 0.138s)
               Value function loss: 0.0511
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0102
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.70s
                        Total time: 348.44s
                               ETA: 579 mins 37.2 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 153089 steps/s (collection: 0.519s, learning 0.123s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0039
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.64s
                        Total time: 349.08s
                               ETA: 579 mins 30.4 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 150076 steps/s (collection: 0.525s, learning 0.130s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0037
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0406
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.66s
                        Total time: 349.74s
                               ETA: 579 mins 25.0 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 153207 steps/s (collection: 0.519s, learning 0.123s)
               Value function loss: 0.0461
                    Surrogate loss: 0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0048
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.64s
                        Total time: 350.38s
                               ETA: 579 mins 18.3 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 160220 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.54
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0100
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.61s
                        Total time: 350.99s
                               ETA: 579 mins 8.8 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 150593 steps/s (collection: 0.530s, learning 0.123s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0101
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0038
           Mean episode rew_no_fly: 0.0059
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0408
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.65s
                        Total time: 351.64s
                               ETA: 579 mins 3.3 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 150463 steps/s (collection: 0.531s, learning 0.123s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.65s
                        Total time: 352.30s
                               ETA: 578 mins 57.8 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 139244 steps/s (collection: 0.561s, learning 0.145s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0049
          Mean episode rew_dof_acc: -0.0103
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0060
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.71s
                        Total time: 353.00s
                               ETA: 578 mins 57.5 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 152290 steps/s (collection: 0.523s, learning 0.123s)
               Value function loss: 0.0444
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.92
                Mean reward (task): -3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.65s
                        Total time: 353.65s
                               ETA: 578 mins 51.3 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 151019 steps/s (collection: 0.527s, learning 0.124s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0104
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.65s
                        Total time: 354.30s
                               ETA: 578 mins 45.6 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 159468 steps/s (collection: 0.492s, learning 0.124s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0061
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.62s
                        Total time: 354.92s
                               ETA: 578 mins 36.6 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 146356 steps/s (collection: 0.537s, learning 0.134s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0436
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.67s
                        Total time: 355.59s
                               ETA: 578 mins 33.0 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 151993 steps/s (collection: 0.519s, learning 0.128s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.65s
                        Total time: 356.23s
                               ETA: 578 mins 26.9 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 136817 steps/s (collection: 0.574s, learning 0.145s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.72s
                        Total time: 356.95s
                               ETA: 578 mins 27.9 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 153813 steps/s (collection: 0.515s, learning 0.124s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0105
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0421
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.64s
                        Total time: 357.59s
                               ETA: 578 mins 21.2 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 156830 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0432
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.63s
                        Total time: 358.22s
                               ETA: 578 mins 13.3 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 157585 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0450
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.62s
                        Total time: 358.84s
                               ETA: 578 mins 5.1 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 157856 steps/s (collection: 0.500s, learning 0.123s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0455
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.62s
                        Total time: 359.47s
                               ETA: 577 mins 56.9 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 137558 steps/s (collection: 0.581s, learning 0.134s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0041
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.71s
                        Total time: 360.18s
                               ETA: 577 mins 57.5 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 140631 steps/s (collection: 0.556s, learning 0.143s)
               Value function loss: 0.0504
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0114
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.70s
                        Total time: 360.88s
                               ETA: 577 mins 56.7 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 144168 steps/s (collection: 0.557s, learning 0.125s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.85
                Mean reward (task): -3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.68s
                        Total time: 361.56s
                               ETA: 577 mins 54.1 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 143752 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0428
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.68s
                        Total time: 362.25s
                               ETA: 577 mins 51.8 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 149306 steps/s (collection: 0.536s, learning 0.122s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0441
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.66s
                        Total time: 362.90s
                               ETA: 577 mins 47.1 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 148126 steps/s (collection: 0.524s, learning 0.139s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0040
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.66s
                        Total time: 363.57s
                               ETA: 577 mins 42.9 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 159887 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0106
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0062
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0421
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.61s
                        Total time: 364.18s
                               ETA: 577 mins 34.0 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 159919 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.87
                Mean reward (task): -3.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0432
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.61s
                        Total time: 364.80s
                               ETA: 577 mins 25.2 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 153946 steps/s (collection: 0.502s, learning 0.137s)
               Value function loss: 0.0484
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.64s
                        Total time: 365.44s
                               ETA: 577 mins 18.6 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 152159 steps/s (collection: 0.517s, learning 0.129s)
               Value function loss: 0.0484
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0430
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.65s
                        Total time: 366.08s
                               ETA: 577 mins 12.8 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 149539 steps/s (collection: 0.533s, learning 0.125s)
               Value function loss: 0.0478
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0436
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.66s
                        Total time: 366.74s
                               ETA: 577 mins 8.1 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 154485 steps/s (collection: 0.514s, learning 0.123s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0116
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0063
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.64s
                        Total time: 367.38s
                               ETA: 577 mins 1.4 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 133194 steps/s (collection: 0.605s, learning 0.133s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.88
                Mean reward (task): -3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0109
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.74s
                        Total time: 368.11s
                               ETA: 577 mins 4.3 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 157104 steps/s (collection: 0.486s, learning 0.140s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0110
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0444
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.63s
                        Total time: 368.74s
                               ETA: 576 mins 56.7 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 139933 steps/s (collection: 0.575s, learning 0.127s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0448
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.70s
                        Total time: 369.44s
                               ETA: 576 mins 56.2 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 156037 steps/s (collection: 0.505s, learning 0.125s)
               Value function loss: 0.0452
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0110
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.63s
                        Total time: 370.07s
                               ETA: 576 mins 49.0 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 158444 steps/s (collection: 0.496s, learning 0.124s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0112
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.62s
                        Total time: 370.69s
                               ETA: 576 mins 40.9 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 147280 steps/s (collection: 0.546s, learning 0.121s)
               Value function loss: 0.0492
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0115
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0064
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.67s
                        Total time: 371.36s
                               ETA: 576 mins 37.3 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 149711 steps/s (collection: 0.535s, learning 0.122s)
               Value function loss: 0.0525
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.66s
                        Total time: 372.02s
                               ETA: 576 mins 32.6 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 143183 steps/s (collection: 0.553s, learning 0.133s)
               Value function loss: 0.0496
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0050
          Mean episode rew_dof_acc: -0.0107
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0065
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.69s
                        Total time: 372.70s
                               ETA: 576 mins 30.7 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 153542 steps/s (collection: 0.519s, learning 0.122s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.64s
                        Total time: 373.34s
                               ETA: 576 mins 24.5 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 143996 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0516
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.87
                Mean reward (task): -3.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0455
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.68s
                        Total time: 374.03s
                               ETA: 576 mins 22.3 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 159887 steps/s (collection: 0.493s, learning 0.122s)
               Value function loss: 0.0489
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0117
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0454
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.61s
                        Total time: 374.64s
                               ETA: 576 mins 13.8 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 149366 steps/s (collection: 0.536s, learning 0.122s)
               Value function loss: 0.0461
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0477
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.66s
                        Total time: 375.30s
                               ETA: 576 mins 9.4 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 160792 steps/s (collection: 0.487s, learning 0.124s)
               Value function loss: 0.0532
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.61s
                        Total time: 375.91s
                               ETA: 576 mins 0.6 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 137288 steps/s (collection: 0.594s, learning 0.122s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.72s
                        Total time: 376.63s
                               ETA: 576 mins 1.5 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 158933 steps/s (collection: 0.497s, learning 0.122s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.71
                Mean reward (task): -3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.62s
                        Total time: 377.24s
                               ETA: 575 mins 53.5 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 135421 steps/s (collection: 0.603s, learning 0.123s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.73s
                        Total time: 377.97s
                               ETA: 575 mins 55.3 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 144905 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.68s
                        Total time: 378.65s
                               ETA: 575 mins 52.7 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 160041 steps/s (collection: 0.492s, learning 0.122s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0110
   Mean episode rew_dof_pos_limits: -0.0118
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.61s
                        Total time: 379.26s
                               ETA: 575 mins 44.3 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 156489 steps/s (collection: 0.504s, learning 0.124s)
               Value function loss: 0.0527
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0066
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0457
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.63s
                        Total time: 379.89s
                               ETA: 575 mins 37.3 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 136412 steps/s (collection: 0.571s, learning 0.150s)
               Value function loss: 0.0516
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.72s
                        Total time: 380.61s
                               ETA: 575 mins 38.6 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 132891 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0111
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.74s
                        Total time: 381.35s
                               ETA: 575 mins 41.6 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 140204 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0471
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.70s
                        Total time: 382.05s
                               ETA: 575 mins 41.2 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 147961 steps/s (collection: 0.529s, learning 0.136s)
               Value function loss: 0.0551
                    Surrogate loss: 0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.90
                Mean reward (task): -3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0067
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0458
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.66s
                        Total time: 382.72s
                               ETA: 575 mins 37.4 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 135761 steps/s (collection: 0.595s, learning 0.129s)
               Value function loss: 0.0535
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0112
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.72s
                        Total time: 383.44s
                               ETA: 575 mins 39.0 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 151719 steps/s (collection: 0.504s, learning 0.144s)
               Value function loss: 0.0507
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.84
                Mean reward (task): -3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.65s
                        Total time: 384.09s
                               ETA: 575 mins 33.8 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 142788 steps/s (collection: 0.548s, learning 0.140s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.69s
                        Total time: 384.78s
                               ETA: 575 mins 32.2 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 155524 steps/s (collection: 0.510s, learning 0.123s)
               Value function loss: 0.0532
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0469
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.63s
                        Total time: 385.41s
                               ETA: 575 mins 25.6 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 138604 steps/s (collection: 0.582s, learning 0.127s)
               Value function loss: 0.0518
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.71s
                        Total time: 386.12s
                               ETA: 575 mins 25.9 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 158829 steps/s (collection: 0.495s, learning 0.124s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0051
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.62s
                        Total time: 386.74s
                               ETA: 575 mins 18.1 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 153929 steps/s (collection: 0.516s, learning 0.122s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.79
                Mean reward (task): -3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0114
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.64s
                        Total time: 387.38s
                               ETA: 575 mins 12.1 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 138987 steps/s (collection: 0.583s, learning 0.124s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.71s
                        Total time: 388.08s
                               ETA: 575 mins 12.2 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 159464 steps/s (collection: 0.495s, learning 0.122s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.89
                Mean reward (task): -3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0110
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.62s
                        Total time: 388.70s
                               ETA: 575 mins 4.3 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 146886 steps/s (collection: 0.548s, learning 0.121s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0120
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0492
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.67s
                        Total time: 389.37s
                               ETA: 575 mins 1.0 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 127422 steps/s (collection: 0.643s, learning 0.129s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0119
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0069
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0479
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.77s
                        Total time: 390.14s
                               ETA: 575 mins 6.9 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 147107 steps/s (collection: 0.527s, learning 0.142s)
               Value function loss: 0.0458
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0491
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.67s
                        Total time: 390.81s
                               ETA: 575 mins 3.5 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 130005 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0488
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.76s
                        Total time: 391.57s
                               ETA: 575 mins 8.0 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 137673 steps/s (collection: 0.580s, learning 0.135s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.71s
                        Total time: 392.28s
                               ETA: 575 mins 8.7 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 133428 steps/s (collection: 0.595s, learning 0.142s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0113
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0068
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.74s
                        Total time: 393.02s
                               ETA: 575 mins 11.4 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 127514 steps/s (collection: 0.625s, learning 0.146s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0507
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.77s
                        Total time: 393.79s
                               ETA: 575 mins 17.1 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 150795 steps/s (collection: 0.509s, learning 0.143s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.63
                Mean reward (task): -3.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.65s
                        Total time: 394.44s
                               ETA: 575 mins 12.3 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 153695 steps/s (collection: 0.499s, learning 0.141s)
               Value function loss: 0.0522
                    Surrogate loss: 0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.64s
                        Total time: 395.08s
                               ETA: 575 mins 6.5 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 143023 steps/s (collection: 0.564s, learning 0.124s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0489
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.69s
                        Total time: 395.77s
                               ETA: 575 mins 4.9 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 130727 steps/s (collection: 0.614s, learning 0.138s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0122
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0485
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.75s
                        Total time: 396.52s
                               ETA: 575 mins 8.9 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 135399 steps/s (collection: 0.597s, learning 0.129s)
               Value function loss: 0.0531
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.73s
                        Total time: 397.24s
                               ETA: 575 mins 10.6 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 152532 steps/s (collection: 0.508s, learning 0.136s)
               Value function loss: 0.0553
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.64s
                        Total time: 397.89s
                               ETA: 575 mins 5.3 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 142407 steps/s (collection: 0.569s, learning 0.121s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.69s
                        Total time: 398.58s
                               ETA: 575 mins 3.9 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 150133 steps/s (collection: 0.530s, learning 0.124s)
               Value function loss: 0.0543
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.86
                Mean reward (task): -3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0121
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0070
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0482
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.65s
                        Total time: 399.23s
                               ETA: 574 mins 59.5 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 154347 steps/s (collection: 0.514s, learning 0.123s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.64s
                        Total time: 399.87s
                               ETA: 574 mins 53.5 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 136736 steps/s (collection: 0.593s, learning 0.126s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.57
                Mean reward (task): -3.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0052
          Mean episode rew_dof_acc: -0.0108
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0043
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0496
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.72s
                        Total time: 400.59s
                               ETA: 574 mins 54.6 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 154934 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0519
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.65
                Mean reward (task): -3.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.63s
                        Total time: 401.22s
                               ETA: 574 mins 48.5 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 132490 steps/s (collection: 0.604s, learning 0.138s)
               Value function loss: 0.0516
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.74s
                        Total time: 401.97s
                               ETA: 574 mins 51.6 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 147150 steps/s (collection: 0.524s, learning 0.144s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.57
                Mean reward (task): -3.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0504
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.67s
                        Total time: 402.63s
                               ETA: 574 mins 48.3 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 141504 steps/s (collection: 0.573s, learning 0.122s)
               Value function loss: 0.0537
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.71
                Mean reward (task): -3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.69s
                        Total time: 403.33s
                               ETA: 574 mins 47.4 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 131284 steps/s (collection: 0.623s, learning 0.126s)
               Value function loss: 0.0517
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.62
                Mean reward (task): -3.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.75s
                        Total time: 404.08s
                               ETA: 574 mins 51.0 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 145637 steps/s (collection: 0.553s, learning 0.122s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0042
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.67s
                        Total time: 404.75s
                               ETA: 574 mins 48.4 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 142681 steps/s (collection: 0.564s, learning 0.125s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.69s
                        Total time: 405.44s
                               ETA: 574 mins 46.9 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 137977 steps/s (collection: 0.573s, learning 0.140s)
               Value function loss: 0.0542
                    Surrogate loss: 0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.71s
                        Total time: 406.15s
                               ETA: 574 mins 47.5 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 150627 steps/s (collection: 0.527s, learning 0.126s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0503
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.65s
                        Total time: 406.81s
                               ETA: 574 mins 42.9 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 159658 steps/s (collection: 0.491s, learning 0.125s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.80
                Mean reward (task): -3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.62s
                        Total time: 407.42s
                               ETA: 574 mins 35.3 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 143756 steps/s (collection: 0.554s, learning 0.130s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.68s
                        Total time: 408.11s
                               ETA: 574 mins 33.4 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 146195 steps/s (collection: 0.548s, learning 0.124s)
               Value function loss: 0.0544
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0071
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0496
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.67s
                        Total time: 408.78s
                               ETA: 574 mins 30.6 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 148205 steps/s (collection: 0.541s, learning 0.122s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.77
                Mean reward (task): -3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.66s
                        Total time: 409.44s
                               ETA: 574 mins 27.0 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 148621 steps/s (collection: 0.539s, learning 0.123s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.55
                Mean reward (task): -3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.66s
                        Total time: 410.10s
                               ETA: 574 mins 23.3 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 129201 steps/s (collection: 0.599s, learning 0.162s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.76s
                        Total time: 410.86s
                               ETA: 574 mins 27.9 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 149004 steps/s (collection: 0.530s, learning 0.130s)
               Value function loss: 0.0519
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0520
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.66s
                        Total time: 411.52s
                               ETA: 574 mins 24.0 s

################################################################################
                      Learning iteration 590/50000                      

                       Computation: 159542 steps/s (collection: 0.493s, learning 0.123s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.67
                Mean reward (task): -3.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0115
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0044
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.62s
                        Total time: 412.14s
                               ETA: 574 mins 16.5 s

################################################################################
                      Learning iteration 591/50000                      

                       Computation: 142499 steps/s (collection: 0.565s, learning 0.125s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.69s
                        Total time: 412.83s
                               ETA: 574 mins 15.2 s

################################################################################
                      Learning iteration 592/50000                      

                       Computation: 133545 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.74s
                        Total time: 413.57s
                               ETA: 574 mins 17.8 s

################################################################################
                      Learning iteration 593/50000                      

                       Computation: 141643 steps/s (collection: 0.573s, learning 0.121s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0511
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.69s
                        Total time: 414.26s
                               ETA: 574 mins 16.8 s

################################################################################
                      Learning iteration 594/50000                      

                       Computation: 138263 steps/s (collection: 0.585s, learning 0.126s)
               Value function loss: 0.0622
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0124
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.71s
                        Total time: 414.97s
                               ETA: 574 mins 17.2 s

################################################################################
                      Learning iteration 595/50000                      

                       Computation: 143522 steps/s (collection: 0.555s, learning 0.130s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0053
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0123
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0072
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0513
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.68s
                        Total time: 415.66s
                               ETA: 574 mins 15.5 s

################################################################################
                      Learning iteration 596/50000                      

                       Computation: 155008 steps/s (collection: 0.511s, learning 0.123s)
               Value function loss: 0.0581
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0045
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.63s
                        Total time: 416.29s
                               ETA: 574 mins 9.5 s

################################################################################
                      Learning iteration 597/50000                      

                       Computation: 150143 steps/s (collection: 0.523s, learning 0.132s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0046
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0516
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.65s
                        Total time: 416.94s
                               ETA: 574 mins 5.3 s

################################################################################
                      Learning iteration 598/50000                      

                       Computation: 149950 steps/s (collection: 0.525s, learning 0.131s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0117
   Mean episode rew_dof_pos_limits: -0.0125
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.66s
                        Total time: 417.60s
                               ETA: 574 mins 1.2 s

################################################################################
                      Learning iteration 599/50000                      

                       Computation: 151843 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0575
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.74
                Mean reward (task): -3.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0055
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0126
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.65s
                        Total time: 418.25s
                               ETA: 573 mins 56.4 s

################################################################################
                      Learning iteration 600/50000                      

                       Computation: 150407 steps/s (collection: 0.532s, learning 0.122s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.65s
                        Total time: 418.90s
                               ETA: 573 mins 52.1 s

################################################################################
                      Learning iteration 601/50000                      

                       Computation: 142345 steps/s (collection: 0.563s, learning 0.128s)
               Value function loss: 0.0553
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0054
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.69s
                        Total time: 419.59s
                               ETA: 573 mins 50.9 s

################################################################################
                      Learning iteration 602/50000                      

                       Computation: 145756 steps/s (collection: 0.553s, learning 0.122s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0127
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0074
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.67s
                        Total time: 420.27s
                               ETA: 573 mins 48.4 s

################################################################################
                      Learning iteration 603/50000                      

                       Computation: 158646 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0585
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0127
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0073
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.62s
                        Total time: 420.89s
                               ETA: 573 mins 41.3 s

################################################################################
                      Learning iteration 604/50000                      

                       Computation: 141612 steps/s (collection: 0.561s, learning 0.133s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.62
                Mean reward (task): -3.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0056
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0126
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0047
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0115
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0534
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.69s
                        Total time: 421.58s
                               ETA: 573 mins 40.4 s

################################################################################
                      Learning iteration 605/50000                      

                       Computation: 129005 steps/s (collection: 0.626s, learning 0.136s)
               Value function loss: 0.0603
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0128
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0537
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.76s
                        Total time: 422.34s
                               ETA: 573 mins 45.0 s

################################################################################
                      Learning iteration 606/50000                      

                       Computation: 133082 steps/s (collection: 0.597s, learning 0.141s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0129
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0537
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.74s
                        Total time: 423.08s
                               ETA: 573 mins 47.7 s

################################################################################
                      Learning iteration 607/50000                      

                       Computation: 144400 steps/s (collection: 0.549s, learning 0.132s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0129
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0531
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.68s
                        Total time: 423.76s
                               ETA: 573 mins 45.7 s

################################################################################
                      Learning iteration 608/50000                      

                       Computation: 147097 steps/s (collection: 0.545s, learning 0.123s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.83
                Mean reward (task): -3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0129
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0521
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.67s
                        Total time: 424.43s
                               ETA: 573 mins 42.7 s

################################################################################
                      Learning iteration 609/50000                      

                       Computation: 160729 steps/s (collection: 0.488s, learning 0.124s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0533
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.61s
                        Total time: 425.04s
                               ETA: 573 mins 35.1 s

################################################################################
                      Learning iteration 610/50000                      

                       Computation: 163631 steps/s (collection: 0.476s, learning 0.125s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0129
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0055
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0118
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0534
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.60s
                        Total time: 425.64s
                               ETA: 573 mins 26.6 s

################################################################################
                      Learning iteration 611/50000                      

                       Computation: 161285 steps/s (collection: 0.487s, learning 0.123s)
               Value function loss: 0.0657
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.67
                Mean reward (task): -3.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0075
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0117
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.61s
                        Total time: 426.25s
                               ETA: 573 mins 18.9 s

################################################################################
                      Learning iteration 612/50000                      

                       Computation: 162194 steps/s (collection: 0.483s, learning 0.123s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.71
                Mean reward (task): -3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0060
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.61s
                        Total time: 426.86s
                               ETA: 573 mins 10.9 s

################################################################################
                      Learning iteration 613/50000                      

                       Computation: 147692 steps/s (collection: 0.542s, learning 0.124s)
               Value function loss: 0.0581
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.67s
                        Total time: 427.52s
                               ETA: 573 mins 7.8 s

################################################################################
                      Learning iteration 614/50000                      

                       Computation: 147690 steps/s (collection: 0.540s, learning 0.126s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.75
                Mean reward (task): -3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.67s
                        Total time: 428.19s
                               ETA: 573 mins 4.6 s

################################################################################
                      Learning iteration 615/50000                      

                       Computation: 144696 steps/s (collection: 0.550s, learning 0.129s)
               Value function loss: 0.0602
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0048
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.68s
                        Total time: 428.87s
                               ETA: 573 mins 2.6 s

################################################################################
                      Learning iteration 616/50000                      

                       Computation: 149039 steps/s (collection: 0.537s, learning 0.122s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0049
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0549
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.66s
                        Total time: 429.53s
                               ETA: 572 mins 58.9 s

################################################################################
                      Learning iteration 617/50000                      

                       Computation: 136783 steps/s (collection: 0.585s, learning 0.134s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.72
                Mean reward (task): -3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0130
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0076
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0531
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.72s
                        Total time: 430.25s
                               ETA: 573 mins 0.0 s

################################################################################
                      Learning iteration 618/50000                      

                       Computation: 154603 steps/s (collection: 0.513s, learning 0.123s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0551
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.64s
                        Total time: 430.88s
                               ETA: 572 mins 54.5 s

################################################################################
                      Learning iteration 619/50000                      

                       Computation: 148996 steps/s (collection: 0.504s, learning 0.156s)
               Value function loss: 0.0667
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0556
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.66s
                        Total time: 431.54s
                               ETA: 572 mins 50.9 s

################################################################################
                      Learning iteration 620/50000                      

                       Computation: 140067 steps/s (collection: 0.577s, learning 0.124s)
               Value function loss: 0.0631
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.70
                Mean reward (task): -3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.70s
                        Total time: 432.24s
                               ETA: 572 mins 50.7 s

################################################################################
                      Learning iteration 621/50000                      

                       Computation: 144992 steps/s (collection: 0.545s, learning 0.133s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.68s
                        Total time: 432.92s
                               ETA: 572 mins 48.6 s

################################################################################
                      Learning iteration 622/50000                      

                       Computation: 162181 steps/s (collection: 0.484s, learning 0.122s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.57
                Mean reward (task): -3.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0055
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.61s
                        Total time: 433.53s
                               ETA: 572 mins 40.7 s

################################################################################
                      Learning iteration 623/50000                      

                       Computation: 143189 steps/s (collection: 0.554s, learning 0.132s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.81
                Mean reward (task): -3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0131
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0551
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.69s
                        Total time: 434.21s
                               ETA: 572 mins 39.3 s

################################################################################
                      Learning iteration 624/50000                      

                       Computation: 140833 steps/s (collection: 0.576s, learning 0.122s)
               Value function loss: 0.0704
                    Surrogate loss: 0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.51
                Mean reward (task): -3.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0054
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0120
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.70s
                        Total time: 434.91s
                               ETA: 572 mins 38.8 s

################################################################################
                      Learning iteration 625/50000                      

                       Computation: 150643 steps/s (collection: 0.523s, learning 0.130s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.66
                Mean reward (task): -3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0116
   Mean episode rew_dof_pos_limits: -0.0132
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0077
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0541
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.65s
                        Total time: 435.57s
                               ETA: 572 mins 34.7 s

################################################################################
                      Learning iteration 626/50000                      

                       Computation: 145862 steps/s (collection: 0.552s, learning 0.122s)
               Value function loss: 0.0640
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.49
                Mean reward (task): -3.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.67s
                        Total time: 436.24s
                               ETA: 572 mins 32.3 s

################################################################################
                      Learning iteration 627/50000                      

                       Computation: 132874 steps/s (collection: 0.591s, learning 0.149s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.51
                Mean reward (task): -3.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.74s
                        Total time: 436.98s
                               ETA: 572 mins 35.0 s

################################################################################
                      Learning iteration 628/50000                      

                       Computation: 140595 steps/s (collection: 0.531s, learning 0.168s)
               Value function loss: 0.0600
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.49
                Mean reward (task): -3.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.70s
                        Total time: 437.68s
                               ETA: 572 mins 34.6 s

################################################################################
                      Learning iteration 629/50000                      

                       Computation: 146831 steps/s (collection: 0.530s, learning 0.139s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0566
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.67s
                        Total time: 438.35s
                               ETA: 572 mins 31.8 s

################################################################################
                      Learning iteration 630/50000                      

                       Computation: 144471 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0628
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0135
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0054
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.68s
                        Total time: 439.03s
                               ETA: 572 mins 29.9 s

################################################################################
                      Learning iteration 631/50000                      

                       Computation: 160801 steps/s (collection: 0.488s, learning 0.124s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.64
                Mean reward (task): -3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0118
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0054
           Mean episode rew_no_fly: 0.0078
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.61s
                        Total time: 439.64s
                               ETA: 572 mins 22.7 s

################################################################################
                      Learning iteration 632/50000                      

                       Computation: 142015 steps/s (collection: 0.560s, learning 0.132s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.76
                Mean reward (task): -3.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0135
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0566
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.69s
                        Total time: 440.33s
                               ETA: 572 mins 21.7 s

################################################################################
                      Learning iteration 633/50000                      

                       Computation: 133598 steps/s (collection: 0.612s, learning 0.123s)
               Value function loss: 0.0675
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.60
                Mean reward (task): -3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0136
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.74s
                        Total time: 441.07s
                               ETA: 572 mins 24.1 s

################################################################################
                      Learning iteration 634/50000                      

                       Computation: 134932 steps/s (collection: 0.598s, learning 0.131s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.63
                Mean reward (task): -3.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0057
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.73s
                        Total time: 441.80s
                               ETA: 572 mins 26.0 s

################################################################################
                      Learning iteration 635/50000                      

                       Computation: 145533 steps/s (collection: 0.552s, learning 0.123s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.91
                Mean reward (task): -3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0055
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0566
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.68s
                        Total time: 442.47s
                               ETA: 572 mins 23.7 s

################################################################################
                      Learning iteration 636/50000                      

                       Computation: 138103 steps/s (collection: 0.585s, learning 0.127s)
               Value function loss: 0.0680
                    Surrogate loss: 0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.78
                Mean reward (task): -3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0136
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0054
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.71s
                        Total time: 443.18s
                               ETA: 572 mins 24.3 s

################################################################################
                      Learning iteration 637/50000                      

                       Computation: 140433 steps/s (collection: 0.555s, learning 0.145s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.55
                Mean reward (task): -3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0135
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.70s
                        Total time: 443.88s
                               ETA: 572 mins 23.9 s

################################################################################
                      Learning iteration 638/50000                      

                       Computation: 154722 steps/s (collection: 0.494s, learning 0.141s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.53
                Mean reward (task): -3.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0135
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0052
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0575
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.64s
                        Total time: 444.52s
                               ETA: 572 mins 18.5 s

################################################################################
                      Learning iteration 639/50000                      

                       Computation: 153979 steps/s (collection: 0.515s, learning 0.123s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.58
                Mean reward (task): -3.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0120
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0051
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.64s
                        Total time: 445.16s
                               ETA: 572 mins 13.4 s

################################################################################
                      Learning iteration 640/50000                      

                       Computation: 145697 steps/s (collection: 0.547s, learning 0.128s)
               Value function loss: 0.0659
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.68
                Mean reward (task): -3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0134
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0050
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0123
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.67s
                        Total time: 445.83s
                               ETA: 572 mins 11.1 s

################################################################################
                      Learning iteration 641/50000                      

                       Computation: 133683 steps/s (collection: 0.574s, learning 0.162s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0121
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0054
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.74s
                        Total time: 446.57s
                               ETA: 572 mins 13.5 s

################################################################################
                      Learning iteration 642/50000                      

                       Computation: 17121 steps/s (collection: 5.602s, learning 0.139s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.69
                Mean reward (task): -3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0121
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 5.74s
                        Total time: 452.31s
                               ETA: 578 mins 40.1 s

################################################################################
                      Learning iteration 643/50000                      

                       Computation: 85388 steps/s (collection: 1.012s, learning 0.140s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.73
                Mean reward (task): -3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0124
   Mean episode rew_dof_pos_limits: -0.0133
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0059
           Mean episode rew_no_fly: 0.0079
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 1.15s
                        Total time: 453.46s
                               ETA: 579 mins 13.8 s

################################################################################
                      Learning iteration 644/50000                      

                       Computation: 97191 steps/s (collection: 0.872s, learning 0.140s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.65
                Mean reward (task): -3.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0060
          Mean episode rew_dof_acc: -0.0125
   Mean episode rew_dof_pos_limits: -0.0135
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0080
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.01s
                        Total time: 454.47s
                               ETA: 579 mins 36.6 s

################################################################################
                      Learning iteration 645/50000                      

                       Computation: 95256 steps/s (collection: 0.892s, learning 0.140s)
               Value function loss: 0.0683
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.59
                Mean reward (task): -3.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0058
          Mean episode rew_dof_acc: -0.0119
   Mean episode rew_dof_pos_limits: -0.0136
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0053
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0573
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 1.03s
                        Total time: 455.50s
                               ETA: 580 mins 0.9 s

################################################################################
                      Learning iteration 646/50000                      

                       Computation: 93114 steps/s (collection: 0.917s, learning 0.138s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.55
                Mean reward (task): -3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0060
          Mean episode rew_dof_acc: -0.0126
   Mean episode rew_dof_pos_limits: -0.0137
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0057
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0125
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 1.06s
                        Total time: 456.56s
                               ETA: 580 mins 26.9 s

################################################################################
                      Learning iteration 647/50000                      

                       Computation: 97456 steps/s (collection: 0.868s, learning 0.140s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): -3.48
                Mean reward (task): -3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0123
   Mean episode rew_dof_pos_limits: -0.0136
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0056
           Mean episode rew_no_fly: 0.0081
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0575
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 1.01s
                        Total time: 457.57s
                               ETA: 580 mins 49.3 s

################################################################################
                      Learning iteration 648/50000                      

                       Computation: 83289 steps/s (collection: 1.045s, learning 0.135s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.55
               Mean reward (total): -3.59
                Mean reward (task): -3.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0059
          Mean episode rew_dof_acc: -0.0122
   Mean episode rew_dof_pos_limits: -0.0140
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0055
           Mean episode rew_no_fly: 0.0083
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 1.18s
                        Total time: 458.75s
                               ETA: 581 mins 24.6 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/nvkvpjdo7r3495z0bk4pj
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    