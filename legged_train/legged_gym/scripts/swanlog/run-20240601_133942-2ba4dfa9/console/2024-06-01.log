swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_133942-2ba4dfa9
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yh_gym_his_Jun01_13-39-43 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/m5bcjzhzpwfup387fixtx
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 18.90 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 11919 steps/s (collection: 8.025s, learning 0.222s)
               Value function loss: 7.9008
                    Surrogate loss: 0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -6.53
                Mean reward (task): -6.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0033
       Mean episode rew_ang_vel_xy: -0.0168
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0516
           Mean episode rew_no_fly: 0.0006
      Mean episode rew_orientation: -0.0007
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1367
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 1.6325
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.25s
                        Total time: 8.25s
                               ETA: 6872 mins 36.8 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 96024 steps/s (collection: 0.892s, learning 0.132s)
               Value function loss: 7.8494
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -6.77
                Mean reward (task): -6.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0585
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1997
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 1.0156
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 1.02s
                        Total time: 9.27s
                               ETA: 3862 mins 47.1 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 143206 steps/s (collection: 0.544s, learning 0.142s)
               Value function loss: 6.7290
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -7.20
                Mean reward (task): -7.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0172
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.5627
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.69s
                        Total time: 9.96s
                               ETA: 2765 mins 48.7 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 136667 steps/s (collection: 0.589s, learning 0.130s)
               Value function loss: 5.5870
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.83
                Mean reward (task): -6.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0403
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0582
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0175
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.2835
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.72s
                        Total time: 10.68s
                               ETA: 2224 mins 9.6 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 139209 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 5.0415
                    Surrogate loss: 0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -6.75
                Mean reward (task): -6.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0610
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.1385
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.71s
                        Total time: 11.38s
                               ETA: 1896 mins 58.6 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 135657 steps/s (collection: 0.591s, learning 0.134s)
               Value function loss: 3.7359
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -7.33
                Mean reward (task): -7.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0579
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0179
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0661
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.72s
                        Total time: 12.11s
                               ETA: 1681 mins 25.1 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 143921 steps/s (collection: 0.562s, learning 0.121s)
               Value function loss: 3.1141
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -7.01
                Mean reward (task): -7.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0584
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0182
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.68s
                        Total time: 12.79s
                               ETA: 1522 mins 29.4 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 137202 steps/s (collection: 0.590s, learning 0.127s)
               Value function loss: 2.6934
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.78
                Mean reward (task): -6.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0592
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0182
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.72s
                        Total time: 13.51s
                               ETA: 1406 mins 46.6 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 142705 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 2.2862
                    Surrogate loss: 0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.80
                Mean reward (task): -6.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0185
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.69s
                        Total time: 14.20s
                               ETA: 1314 mins 13.0 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 137181 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 1.9404
                    Surrogate loss: 0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -6.91
                Mean reward (task): -6.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0589
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0185
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.72s
                        Total time: 14.91s
                               ETA: 1242 mins 28.6 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 123420 steps/s (collection: 0.655s, learning 0.142s)
               Value function loss: 1.7234
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.79
                Mean reward (task): -6.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0588
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0186
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.80s
                        Total time: 15.71s
                               ETA: 1189 mins 49.8 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 119165 steps/s (collection: 0.701s, learning 0.124s)
               Value function loss: 1.5022
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.45
                Mean reward (task): -6.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0575
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0189
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.82s
                        Total time: 16.53s
                               ETA: 1147 mins 55.8 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 139521 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 1.1250
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -6.56
                Mean reward (task): -6.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0565
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0192
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.70s
                        Total time: 17.24s
                               ETA: 1104 mins 45.7 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 140008 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 1.0772
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.98
                Mean reward (task): -6.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0194
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.70s
                        Total time: 17.94s
                               ETA: 1067 mins 36.7 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 129501 steps/s (collection: 0.615s, learning 0.144s)
               Value function loss: 1.0141
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.93
                Mean reward (task): -6.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0195
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.76s
                        Total time: 18.70s
                               ETA: 1038 mins 34.7 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 105924 steps/s (collection: 0.753s, learning 0.175s)
               Value function loss: 0.7770
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -6.49
                Mean reward (task): -6.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0415
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0528
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0197
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.93s
                        Total time: 19.63s
                               ETA: 1021 mins 58.1 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 119240 steps/s (collection: 0.692s, learning 0.132s)
               Value function loss: 0.8236
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.59
                Mean reward (task): -6.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0424
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0199
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.82s
                        Total time: 20.45s
                               ETA: 1002 mins 14.0 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 119261 steps/s (collection: 0.664s, learning 0.160s)
               Value function loss: 0.8307
                    Surrogate loss: 0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.69
                Mean reward (task): -6.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0526
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0199
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.82s
                        Total time: 21.28s
                               ETA: 984 mins 41.0 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 118571 steps/s (collection: 0.704s, learning 0.126s)
               Value function loss: 0.7183
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.64
                Mean reward (task): -6.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0197
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.83s
                        Total time: 22.11s
                               ETA: 969 mins 11.3 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 136009 steps/s (collection: 0.601s, learning 0.122s)
               Value function loss: 0.7329
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.23
                Mean reward (task): -6.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0516
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0201
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.72s
                        Total time: 22.83s
                               ETA: 950 mins 48.9 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 137311 steps/s (collection: 0.585s, learning 0.131s)
               Value function loss: 0.5972
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.47
                Mean reward (task): -6.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0527
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0199
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.72s
                        Total time: 23.54s
                               ETA: 933 mins 55.1 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 129738 steps/s (collection: 0.626s, learning 0.132s)
               Value function loss: 0.6487
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.80
                Mean reward (task): -6.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0200
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.76s
                        Total time: 24.30s
                               ETA: 920 mins 8.3 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 138970 steps/s (collection: 0.582s, learning 0.126s)
               Value function loss: 0.5691
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.64
                Mean reward (task): -6.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0494
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0199
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.71s
                        Total time: 25.01s
                               ETA: 905 mins 44.0 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 136567 steps/s (collection: 0.590s, learning 0.130s)
               Value function loss: 0.5717
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.49
                Mean reward (task): -6.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0480
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0197
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.72s
                        Total time: 25.73s
                               ETA: 892 mins 57.5 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 131246 steps/s (collection: 0.606s, learning 0.143s)
               Value function loss: 0.5807
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -6.40
                Mean reward (task): -6.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0448
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0199
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.75s
                        Total time: 26.48s
                               ETA: 882 mins 10.7 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 118582 steps/s (collection: 0.692s, learning 0.137s)
               Value function loss: 0.4847
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -6.44
                Mean reward (task): -6.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0187
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0444
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0195
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.83s
                        Total time: 27.31s
                               ETA: 874 mins 47.3 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 125551 steps/s (collection: 0.653s, learning 0.130s)
               Value function loss: 0.5371
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -6.35
                Mean reward (task): -6.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0188
          Mean episode rew_dof_acc: -0.0379
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0443
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0192
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.78s
                        Total time: 28.09s
                               ETA: 866 mins 31.5 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 120173 steps/s (collection: 0.673s, learning 0.145s)
               Value function loss: 0.5265
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -6.35
                Mean reward (task): -6.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0185
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0429
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0193
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.82s
                        Total time: 28.91s
                               ETA: 859 mins 53.6 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 146649 steps/s (collection: 0.546s, learning 0.125s)
               Value function loss: 0.5105
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -6.30
                Mean reward (task): -6.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0186
          Mean episode rew_dof_acc: -0.0376
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0436
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0191
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.67s
                        Total time: 29.58s
                               ETA: 849 mins 28.6 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 136395 steps/s (collection: 0.583s, learning 0.137s)
               Value function loss: 0.3873
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.12
                Mean reward (task): -6.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0184
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0388
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0188
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.72s
                        Total time: 30.30s
                               ETA: 841 mins 9.2 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 136681 steps/s (collection: 0.590s, learning 0.129s)
               Value function loss: 0.5159
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.48
                Mean reward (task): -6.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0181
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0392
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0185
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.72s
                        Total time: 31.02s
                               ETA: 833 mins 19.5 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 116179 steps/s (collection: 0.716s, learning 0.130s)
               Value function loss: 0.3964
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.31
                Mean reward (task): -6.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0181
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0387
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0183
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.85s
                        Total time: 31.86s
                               ETA: 829 mins 17.3 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 139054 steps/s (collection: 0.584s, learning 0.122s)
               Value function loss: 0.4228
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.43
                Mean reward (task): -6.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0179
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0377
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0184
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.71s
                        Total time: 32.57s
                               ETA: 821 mins 59.0 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 140455 steps/s (collection: 0.573s, learning 0.127s)
               Value function loss: 0.3800
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -6.05
                Mean reward (task): -6.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0178
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0180
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.70s
                        Total time: 33.27s
                               ETA: 814 mins 56.1 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 133120 steps/s (collection: 0.610s, learning 0.128s)
               Value function loss: 0.4091
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -5.81
                Mean reward (task): -5.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0176
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0343
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0178
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.74s
                        Total time: 34.01s
                               ETA: 809 mins 12.3 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 122511 steps/s (collection: 0.680s, learning 0.123s)
               Value function loss: 0.3932
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -5.91
                Mean reward (task): -5.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0177
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0350
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0177
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.80s
                        Total time: 34.81s
                               ETA: 805 mins 16.4 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 140100 steps/s (collection: 0.581s, learning 0.121s)
               Value function loss: 0.3005
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.02
                Mean reward (task): -6.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0176
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0335
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0178
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.70s
                        Total time: 35.51s
                               ETA: 799 mins 17.1 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 143012 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.2817
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.81
                Mean reward (task): -5.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0174
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0323
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0170
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.69s
                        Total time: 36.20s
                               ETA: 793 mins 17.9 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 138912 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.2570
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.81
                Mean reward (task): -5.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0176
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0308
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0173
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.71s
                        Total time: 36.91s
                               ETA: 788 mins 3.1 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 131679 steps/s (collection: 0.625s, learning 0.122s)
               Value function loss: 0.2562
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -6.18
                Mean reward (task): -6.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0168
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.75s
                        Total time: 37.66s
                               ETA: 783 mins 52.5 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 136699 steps/s (collection: 0.589s, learning 0.130s)
               Value function loss: 0.2550
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.97
                Mean reward (task): -5.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0311
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0168
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.72s
                        Total time: 38.37s
                               ETA: 779 mins 20.8 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 127238 steps/s (collection: 0.643s, learning 0.130s)
               Value function loss: 0.2853
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.97
                Mean reward (task): -5.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0171
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0312
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0166
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.77s
                        Total time: 39.15s
                               ETA: 776 mins 5.5 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 130723 steps/s (collection: 0.629s, learning 0.123s)
               Value function loss: 0.2322
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.86
                Mean reward (task): -5.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0173
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0301
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0163
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.75s
                        Total time: 39.90s
                               ETA: 772 mins 35.4 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 131631 steps/s (collection: 0.625s, learning 0.122s)
               Value function loss: 0.2222
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.89
                Mean reward (task): -5.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0294
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0157
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.75s
                        Total time: 40.65s
                               ETA: 769 mins 8.8 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 143478 steps/s (collection: 0.559s, learning 0.126s)
               Value function loss: 0.2280
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.87
                Mean reward (task): -5.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0170
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0154
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.69s
                        Total time: 41.33s
                               ETA: 764 mins 43.0 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 140296 steps/s (collection: 0.572s, learning 0.129s)
               Value function loss: 0.1900
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.89
                Mean reward (task): -5.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0172
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0152
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.70s
                        Total time: 42.03s
                               ETA: 760 mins 45.6 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 128745 steps/s (collection: 0.637s, learning 0.127s)
               Value function loss: 0.1734
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.64
                Mean reward (task): -5.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0170
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0150
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.76s
                        Total time: 42.80s
                               ETA: 758 mins 5.1 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 131576 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.1814
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.74
                Mean reward (task): -5.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0170
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0146
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.75s
                        Total time: 43.54s
                               ETA: 755 mins 14.1 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 127421 steps/s (collection: 0.630s, learning 0.141s)
               Value function loss: 0.1671
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.66
                Mean reward (task): -5.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0171
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0144
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.77s
                        Total time: 44.31s
                               ETA: 752 mins 54.9 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/m5bcjzhzpwfup387fixtx
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    