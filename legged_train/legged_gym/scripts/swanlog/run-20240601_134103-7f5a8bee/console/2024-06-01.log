swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240601_134103-7f5a8bee
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yh_gym_his_Jun01_13-41-04 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/tvwywjir0h2o3dvr5dz9x
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.30 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 39257 steps/s (collection: 2.326s, learning 0.178s)
               Value function loss: 3.2718
                    Surrogate loss: 0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -3.94
                Mean reward (task): -3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0033
       Mean episode rew_ang_vel_xy: -0.0166
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0006
      Mean episode rew_orientation: -0.0007
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0001
      Mean episode rew_termination: -0.1356
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 1.6315
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.50s
                        Total time: 2.50s
                               ETA: 2086 mins 43.5 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 157712 steps/s (collection: 0.499s, learning 0.124s)
               Value function loss: 3.7385
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0169
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1996
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 1.0144
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.62s
                        Total time: 3.13s
                               ETA: 1303 mins 3.0 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 155963 steps/s (collection: 0.507s, learning 0.124s)
               Value function loss: 3.2770
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0622
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0171
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.5455
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.63s
                        Total time: 3.76s
                               ETA: 1043 mins 45.5 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 136147 steps/s (collection: 0.579s, learning 0.143s)
               Value function loss: 2.2593
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0609
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0175
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.2706
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.72s
                        Total time: 4.48s
                               ETA: 933 mins 13.2 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 150740 steps/s (collection: 0.529s, learning 0.123s)
               Value function loss: 1.4905
                    Surrogate loss: 0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0594
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0178
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.1278
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.65s
                        Total time: 5.13s
                               ETA: 855 mins 14.5 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 136593 steps/s (collection: 0.581s, learning 0.138s)
               Value function loss: 1.1488
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0618
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0180
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0637
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.72s
                        Total time: 5.85s
                               ETA: 812 mins 38.0 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 146142 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.9052
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0590
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0186
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.67s
                        Total time: 6.52s
                               ETA: 776 mins 35.9 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 143811 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.8375
                    Surrogate loss: 0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0186
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.68s
                        Total time: 7.21s
                               ETA: 750 mins 42.2 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 134203 steps/s (collection: 0.601s, learning 0.131s)
               Value function loss: 0.5883
                    Surrogate loss: 0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0422
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0588
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0192
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.73s
                        Total time: 7.94s
                               ETA: 735 mins 5.5 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 144021 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.4047
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0209
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0594
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0192
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.68s
                        Total time: 8.62s
                               ETA: 718 mins 26.4 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 137122 steps/s (collection: 0.593s, learning 0.124s)
               Value function loss: 0.2734
                    Surrogate loss: 0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0430
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0635
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0194
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.72s
                        Total time: 9.34s
                               ETA: 707 mins 24.9 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 141841 steps/s (collection: 0.571s, learning 0.122s)
               Value function loss: 0.2179
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0633
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0197
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.69s
                        Total time: 10.03s
                               ETA: 696 mins 34.1 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 141037 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.1640
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0205
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.70s
                        Total time: 10.73s
                               ETA: 687 mins 38.5 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 134159 steps/s (collection: 0.610s, learning 0.123s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0208
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.73s
                        Total time: 11.46s
                               ETA: 682 mins 7.0 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 134798 steps/s (collection: 0.606s, learning 0.124s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0593
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0210
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.73s
                        Total time: 12.19s
                               ETA: 677 mins 8.0 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 131757 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.0350
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0207
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0577
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0216
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.75s
                        Total time: 12.94s
                               ETA: 673 mins 38.8 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 143158 steps/s (collection: 0.557s, learning 0.130s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0567
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0217
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.69s
                        Total time: 13.62s
                               ETA: 667 mins 39.5 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 136834 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0462
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0562
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0224
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.72s
                        Total time: 14.34s
                               ETA: 663 mins 48.1 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 147415 steps/s (collection: 0.545s, learning 0.122s)
               Value function loss: 0.0157
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0464
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0227
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.67s
                        Total time: 15.01s
                               ETA: 658 mins 5.4 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 138310 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0484
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0233
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.71s
                        Total time: 15.72s
                               ETA: 654 mins 46.5 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 139777 steps/s (collection: 0.580s, learning 0.124s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0481
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0560
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0236
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.70s
                        Total time: 16.42s
                               ETA: 651 mins 28.8 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 144336 steps/s (collection: 0.558s, learning 0.123s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0493
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0555
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0241
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.68s
                        Total time: 17.10s
                               ETA: 647 mins 38.6 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 141034 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0493
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0528
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0247
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.70s
                        Total time: 17.80s
                               ETA: 644 mins 42.9 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 133467 steps/s (collection: 0.601s, learning 0.135s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0503
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0251
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.74s
                        Total time: 18.54s
                               ETA: 643 mins 24.1 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 143339 steps/s (collection: 0.564s, learning 0.121s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0514
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0570
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0254
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.69s
                        Total time: 19.22s
                               ETA: 640 mins 30.2 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 126417 steps/s (collection: 0.652s, learning 0.126s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0511
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0011
        Mean episode rew_lin_vel_z: -0.0526
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0260
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.78s
                        Total time: 20.00s
                               ETA: 640 mins 46.0 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 132360 steps/s (collection: 0.619s, learning 0.123s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0517
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0551
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0266
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.74s
                        Total time: 20.74s
                               ETA: 639 mins 56.0 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 132301 steps/s (collection: 0.620s, learning 0.123s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0532
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0522
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0269
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.74s
                        Total time: 21.49s
                               ETA: 639 mins 10.1 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 137024 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0537
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0273
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.72s
                        Total time: 22.21s
                               ETA: 637 mins 43.2 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 143182 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0539
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0556
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0277
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.69s
                        Total time: 22.89s
                               ETA: 635 mins 30.6 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 121003 steps/s (collection: 0.685s, learning 0.127s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0548
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0283
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.81s
                        Total time: 23.70s
                               ETA: 636 mins 49.4 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 141138 steps/s (collection: 0.572s, learning 0.124s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0545
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0538
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0288
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.70s
                        Total time: 24.40s
                               ETA: 635 mins 2.3 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 133135 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0554
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0526
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0293
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.74s
                        Total time: 25.14s
                               ETA: 634 mins 24.9 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 120954 steps/s (collection: 0.683s, learning 0.130s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0562
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0293
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.81s
                        Total time: 25.95s
                               ETA: 635 mins 39.0 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 127685 steps/s (collection: 0.648s, learning 0.122s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0189
          Mean episode rew_dof_acc: -0.0565
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0527
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0295
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.77s
                        Total time: 26.72s
                               ETA: 635 mins 47.7 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 141645 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0563
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0507
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0303
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.69s
                        Total time: 27.42s
                               ETA: 634 mins 10.5 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 138258 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0570
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0303
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.71s
                        Total time: 28.13s
                               ETA: 633 mins 1.5 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 144414 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0574
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0012
        Mean episode rew_lin_vel_z: -0.0525
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0311
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.68s
                        Total time: 28.81s
                               ETA: 631 mins 16.3 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 136461 steps/s (collection: 0.575s, learning 0.146s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0586
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0315
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.72s
                        Total time: 29.53s
                               ETA: 630 mins 27.2 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 136766 steps/s (collection: 0.572s, learning 0.147s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0583
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0484
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0318
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.72s
                        Total time: 30.25s
                               ETA: 629 mins 38.6 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 131988 steps/s (collection: 0.606s, learning 0.138s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0190
          Mean episode rew_dof_acc: -0.0589
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0499
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0323
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.74s
                        Total time: 30.99s
                               ETA: 629 mins 24.0 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 125518 steps/s (collection: 0.658s, learning 0.125s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0183
          Mean episode rew_dof_acc: -0.0587
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0490
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0324
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.78s
                        Total time: 31.77s
                               ETA: 629 mins 55.7 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 130373 steps/s (collection: 0.621s, learning 0.133s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0609
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0517
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0328
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.75s
                        Total time: 32.53s
                               ETA: 629 mins 52.0 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 131179 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0116
       Mean episode rew_ang_vel_xy: -0.0196
          Mean episode rew_dof_acc: -0.0602
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0335
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.75s
                        Total time: 33.28s
                               ETA: 629 mins 43.2 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 142631 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0610
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0515
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0341
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.69s
                        Total time: 33.97s
                               ETA: 628 mins 28.0 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 137997 steps/s (collection: 0.581s, learning 0.132s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0615
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0518
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0012
       Mean episode rew_smoothness: -0.0339
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.71s
                        Total time: 34.68s
                               ETA: 627 mins 41.1 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 144485 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0615
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0543
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0354
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.68s
                        Total time: 35.36s
                               ETA: 626 mins 22.2 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 135929 steps/s (collection: 0.600s, learning 0.124s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0194
          Mean episode rew_dof_acc: -0.0612
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0354
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.72s
                        Total time: 36.08s
                               ETA: 625 mins 51.1 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 144319 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0618
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0359
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.68s
                        Total time: 36.76s
                               ETA: 624 mins 38.4 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 134830 steps/s (collection: 0.607s, learning 0.122s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0195
          Mean episode rew_dof_acc: -0.0615
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0362
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.73s
                        Total time: 37.49s
                               ETA: 624 mins 16.5 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 134021 steps/s (collection: 0.606s, learning 0.128s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0197
          Mean episode rew_dof_acc: -0.0630
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0536
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0364
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.73s
                        Total time: 38.23s
                               ETA: 623 mins 59.7 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 138372 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0193
          Mean episode rew_dof_acc: -0.0632
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0519
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0369
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.71s
                        Total time: 38.94s
                               ETA: 623 mins 21.4 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 144950 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0130
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0628
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0013
        Mean episode rew_lin_vel_z: -0.0507
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0375
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.68s
                        Total time: 39.62s
                               ETA: 622 mins 14.1 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 129026 steps/s (collection: 0.639s, learning 0.123s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0133
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0638
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0520
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0383
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.76s
                        Total time: 40.38s
                               ETA: 622 mins 26.7 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 139368 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0648
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0535
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0386
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.71s
                        Total time: 41.08s
                               ETA: 621 mins 47.5 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 140025 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0644
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0387
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.70s
                        Total time: 41.78s
                               ETA: 621 mins 6.7 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 139643 steps/s (collection: 0.574s, learning 0.129s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0198
          Mean episode rew_dof_acc: -0.0653
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0530
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0392
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.70s
                        Total time: 42.49s
                               ETA: 620 mins 29.0 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 138893 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.96
                Mean reward (task): -3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0140
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0653
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0399
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.71s
                        Total time: 43.20s
                               ETA: 619 mins 55.8 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 139780 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0141
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0644
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0495
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0402
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.70s
                        Total time: 43.90s
                               ETA: 619 mins 19.9 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 146222 steps/s (collection: 0.550s, learning 0.122s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0206
          Mean episode rew_dof_acc: -0.0648
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0525
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0404
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.67s
                        Total time: 44.57s
                               ETA: 618 mins 19.4 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 144726 steps/s (collection: 0.556s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0146
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0664
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0533
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0414
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.68s
                        Total time: 45.25s
                               ETA: 617 mins 26.6 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 146853 steps/s (collection: 0.547s, learning 0.122s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0145
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0665
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0013
       Mean episode rew_smoothness: -0.0411
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.67s
                        Total time: 45.92s
                               ETA: 616 mins 27.5 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 137274 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0107
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0148
       Mean episode rew_ang_vel_xy: -0.0201
          Mean episode rew_dof_acc: -0.0665
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0421
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.72s
                        Total time: 46.64s
                               ETA: 616 mins 7.3 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 145665 steps/s (collection: 0.551s, learning 0.124s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0146
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0662
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0531
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0411
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.67s
                        Total time: 47.31s
                               ETA: 615 mins 15.6 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 142930 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0113
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0149
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0666
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0014
        Mean episode rew_lin_vel_z: -0.0524
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0422
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.69s
                        Total time: 48.00s
                               ETA: 614 mins 35.3 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 138669 steps/s (collection: 0.579s, learning 0.130s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0151
       Mean episode rew_ang_vel_xy: -0.0207
          Mean episode rew_dof_acc: -0.0676
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0549
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0429
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.71s
                        Total time: 48.71s
                               ETA: 614 mins 12.2 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 144277 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0203
          Mean episode rew_dof_acc: -0.0671
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0432
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.68s
                        Total time: 49.39s
                               ETA: 613 mins 29.2 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 133261 steps/s (collection: 0.615s, learning 0.122s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0154
       Mean episode rew_ang_vel_xy: -0.0204
          Mean episode rew_dof_acc: -0.0673
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0535
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0438
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.74s
                        Total time: 50.13s
                               ETA: 613 mins 28.9 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 122718 steps/s (collection: 0.651s, learning 0.150s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0200
          Mean episode rew_dof_acc: -0.0674
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0440
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.80s
                        Total time: 50.93s
                               ETA: 614 mins 14.4 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 132753 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0687
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0515
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0452
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.74s
                        Total time: 51.67s
                               ETA: 614 mins 15.4 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 142076 steps/s (collection: 0.569s, learning 0.123s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0694
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0454
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.69s
                        Total time: 52.36s
                               ETA: 613 mins 42.1 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 132941 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0694
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0463
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.74s
                        Total time: 53.10s
                               ETA: 613 mins 42.8 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 132665 steps/s (collection: 0.608s, learning 0.133s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0694
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0555
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0463
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.74s
                        Total time: 53.84s
                               ETA: 613 mins 44.4 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 126842 steps/s (collection: 0.636s, learning 0.139s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0166
       Mean episode rew_ang_vel_xy: -0.0205
          Mean episode rew_dof_acc: -0.0702
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0472
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.78s
                        Total time: 54.62s
                               ETA: 614 mins 8.9 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 131445 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0166
       Mean episode rew_ang_vel_xy: -0.0202
          Mean episode rew_dof_acc: -0.0691
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0517
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0473
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.75s
                        Total time: 55.36s
                               ETA: 614 mins 14.7 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 146039 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0168
       Mean episode rew_ang_vel_xy: -0.0207
          Mean episode rew_dof_acc: -0.0712
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0478
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.67s
                        Total time: 56.04s
                               ETA: 613 mins 31.3 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 135934 steps/s (collection: 0.595s, learning 0.128s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0707
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0534
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0014
       Mean episode rew_smoothness: -0.0484
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.72s
                        Total time: 56.76s
                               ETA: 613 mins 21.3 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 136332 steps/s (collection: 0.574s, learning 0.147s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0199
          Mean episode rew_dof_acc: -0.0704
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0491
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.72s
                        Total time: 57.48s
                               ETA: 613 mins 10.3 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 133397 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0175
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0708
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0517
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0495
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.74s
                        Total time: 58.22s
                               ETA: 613 mins 9.6 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 143699 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0122
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0177
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0716
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0539
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0499
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.68s
                        Total time: 58.90s
                               ETA: 612 mins 35.9 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 142465 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0177
       Mean episode rew_ang_vel_xy: -0.0208
          Mean episode rew_dof_acc: -0.0716
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0015
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0498
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.69s
                        Total time: 59.59s
                               ETA: 612 mins 6.6 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 133959 steps/s (collection: 0.609s, learning 0.125s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0180
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0725
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0545
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0507
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.73s
                        Total time: 60.33s
                               ETA: 612 mins 4.7 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 131876 steps/s (collection: 0.624s, learning 0.122s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0182
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0734
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0514
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.75s
                        Total time: 61.07s
                               ETA: 612 mins 9.9 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 145602 steps/s (collection: 0.551s, learning 0.124s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0185
       Mean episode rew_ang_vel_xy: -0.0210
          Mean episode rew_dof_acc: -0.0736
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0526
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0526
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.68s
                        Total time: 61.75s
                               ETA: 611 mins 33.1 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 133635 steps/s (collection: 0.611s, learning 0.125s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0186
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0745
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0529
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.74s
                        Total time: 62.48s
                               ETA: 611 mins 32.7 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 134086 steps/s (collection: 0.590s, learning 0.144s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0188
       Mean episode rew_ang_vel_xy: -0.0211
          Mean episode rew_dof_acc: -0.0737
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0559
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0533
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.73s
                        Total time: 63.22s
                               ETA: 611 mins 30.8 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 132504 steps/s (collection: 0.619s, learning 0.123s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0192
       Mean episode rew_ang_vel_xy: -0.0213
          Mean episode rew_dof_acc: -0.0743
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0545
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0539
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.74s
                        Total time: 63.96s
                               ETA: 611 mins 34.0 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 129590 steps/s (collection: 0.612s, learning 0.146s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0742
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0542
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.76s
                        Total time: 64.72s
                               ETA: 611 mins 46.5 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 128229 steps/s (collection: 0.631s, learning 0.136s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0748
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0550
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.77s
                        Total time: 65.48s
                               ETA: 612 mins 3.3 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 140499 steps/s (collection: 0.575s, learning 0.124s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0196
       Mean episode rew_ang_vel_xy: -0.0212
          Mean episode rew_dof_acc: -0.0761
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0553
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0015
       Mean episode rew_smoothness: -0.0552
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.70s
                        Total time: 66.18s
                               ETA: 611 mins 42.5 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 139191 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0199
       Mean episode rew_ang_vel_xy: -0.0217
          Mean episode rew_dof_acc: -0.0752
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0540
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0564
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.71s
                        Total time: 66.89s
                               ETA: 611 mins 25.8 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 146458 steps/s (collection: 0.549s, learning 0.123s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0201
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0751
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0553
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0567
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.67s
                        Total time: 67.56s
                               ETA: 610 mins 50.5 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 124578 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -3.97
                Mean reward (task): -3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0202
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0747
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0572
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.79s
                        Total time: 68.35s
                               ETA: 611 mins 19.1 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 136593 steps/s (collection: 0.598s, learning 0.122s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0206
       Mean episode rew_ang_vel_xy: -0.0214
          Mean episode rew_dof_acc: -0.0755
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0565
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0582
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.72s
                        Total time: 69.07s
                               ETA: 611 mins 10.3 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 149386 steps/s (collection: 0.537s, learning 0.121s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0204
       Mean episode rew_ang_vel_xy: -0.0215
          Mean episode rew_dof_acc: -0.0759
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0016
        Mean episode rew_lin_vel_z: -0.0558
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0580
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.66s
                        Total time: 69.73s
                               ETA: 610 mins 29.3 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 130243 steps/s (collection: 0.608s, learning 0.147s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0209
       Mean episode rew_ang_vel_xy: -0.0216
          Mean episode rew_dof_acc: -0.0771
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0588
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.75s
                        Total time: 70.48s
                               ETA: 610 mins 39.3 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 146113 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0219
          Mean episode rew_dof_acc: -0.0777
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0568
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0594
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.67s
                        Total time: 71.15s
                               ETA: 610 mins 7.0 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 141171 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0213
       Mean episode rew_ang_vel_xy: -0.0221
          Mean episode rew_dof_acc: -0.0777
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0588
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0604
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.70s
                        Total time: 71.85s
                               ETA: 609 mins 47.3 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 142893 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0219
       Mean episode rew_ang_vel_xy: -0.0221
          Mean episode rew_dof_acc: -0.0801
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0621
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.69s
                        Total time: 72.54s
                               ETA: 609 mins 23.8 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 148783 steps/s (collection: 0.538s, learning 0.122s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0224
          Mean episode rew_dof_acc: -0.0788
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0590
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0016
       Mean episode rew_smoothness: -0.0607
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.66s
                        Total time: 73.20s
                               ETA: 608 mins 47.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 123300 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0221
       Mean episode rew_ang_vel_xy: -0.0225
          Mean episode rew_dof_acc: -0.0810
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0603
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0622
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.80s
                        Total time: 74.00s
                               ETA: 609 mins 18.7 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 141525 steps/s (collection: 0.572s, learning 0.123s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0223
       Mean episode rew_ang_vel_xy: -0.0222
          Mean episode rew_dof_acc: -0.0784
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0590
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0622
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.69s
                        Total time: 74.69s
                               ETA: 608 mins 59.3 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 143792 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0222
       Mean episode rew_ang_vel_xy: -0.0219
          Mean episode rew_dof_acc: -0.0789
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0576
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0628
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.68s
                        Total time: 75.37s
                               ETA: 608 mins 35.1 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 149051 steps/s (collection: 0.537s, learning 0.122s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0223
          Mean episode rew_dof_acc: -0.0805
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0607
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0637
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.66s
                        Total time: 76.03s
                               ETA: 607 mins 59.7 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 139110 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0819
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0614
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0646
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.71s
                        Total time: 76.74s
                               ETA: 607 mins 47.3 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 141334 steps/s (collection: 0.573s, learning 0.122s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0119
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0227
          Mean episode rew_dof_acc: -0.0813
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0593
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0656
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.70s
                        Total time: 77.44s
                               ETA: 607 mins 29.9 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 143062 steps/s (collection: 0.565s, learning 0.122s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0118
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0810
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0607
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0658
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.69s
                        Total time: 78.12s
                               ETA: 607 mins 9.0 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 148468 steps/s (collection: 0.539s, learning 0.123s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0226
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0598
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0663
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.66s
                        Total time: 78.79s
                               ETA: 606 mins 36.8 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 134293 steps/s (collection: 0.599s, learning 0.133s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0237
       Mean episode rew_ang_vel_xy: -0.0228
          Mean episode rew_dof_acc: -0.0821
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0602
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0666
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.73s
                        Total time: 79.52s
                               ETA: 606 mins 37.3 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 139199 steps/s (collection: 0.584s, learning 0.122s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0229
          Mean episode rew_dof_acc: -0.0811
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0017
        Mean episode rew_lin_vel_z: -0.0619
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0667
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.71s
                        Total time: 80.22s
                               ETA: 606 mins 26.0 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 137580 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0230
          Mean episode rew_dof_acc: -0.0823
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0616
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0676
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.71s
                        Total time: 80.94s
                               ETA: 606 mins 18.6 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 149599 steps/s (collection: 0.533s, learning 0.124s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0236
          Mean episode rew_dof_acc: -0.0838
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0622
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0017
       Mean episode rew_smoothness: -0.0678
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.66s
                        Total time: 81.60s
                               ETA: 605 mins 45.7 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 142490 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0244
       Mean episode rew_ang_vel_xy: -0.0230
          Mean episode rew_dof_acc: -0.0824
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0619
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0679
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.69s
                        Total time: 82.29s
                               ETA: 605 mins 28.0 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 148430 steps/s (collection: 0.538s, learning 0.124s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0249
       Mean episode rew_ang_vel_xy: -0.0237
          Mean episode rew_dof_acc: -0.0838
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0623
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0689
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.66s
                        Total time: 82.95s
                               ETA: 604 mins 58.4 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 147584 steps/s (collection: 0.544s, learning 0.122s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0254
       Mean episode rew_ang_vel_xy: -0.0233
          Mean episode rew_dof_acc: -0.0851
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0661
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0699
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.67s
                        Total time: 83.61s
                               ETA: 604 mins 31.0 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 130079 steps/s (collection: 0.633s, learning 0.122s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0231
          Mean episode rew_dof_acc: -0.0830
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0628
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0697
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.76s
                        Total time: 84.37s
                               ETA: 604 mins 42.6 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 125240 steps/s (collection: 0.662s, learning 0.122s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.98
                Mean reward (task): -3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0256
       Mean episode rew_ang_vel_xy: -0.0232
          Mean episode rew_dof_acc: -0.0835
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0636
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0709
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.78s
                        Total time: 85.15s
                               ETA: 605 mins 6.4 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 132438 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0260
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0852
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0638
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0725
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.74s
                        Total time: 85.90s
                               ETA: 605 mins 11.8 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 143169 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.19
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0264
       Mean episode rew_ang_vel_xy: -0.0242
          Mean episode rew_dof_acc: -0.0863
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0671
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0729
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.69s
                        Total time: 86.58s
                               ETA: 604 mins 53.7 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 146270 steps/s (collection: 0.549s, learning 0.123s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0264
       Mean episode rew_ang_vel_xy: -0.0239
          Mean episode rew_dof_acc: -0.0850
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0623
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0731
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.67s
                        Total time: 87.26s
                               ETA: 604 mins 29.9 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 142938 steps/s (collection: 0.553s, learning 0.135s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0867
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0654
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0742
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.69s
                        Total time: 87.94s
                               ETA: 604 mins 13.0 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 139138 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0243
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0652
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0748
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.71s
                        Total time: 88.65s
                               ETA: 604 mins 3.9 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 143816 steps/s (collection: 0.561s, learning 0.123s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0851
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0636
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
       Mean episode rew_smoothness: -0.0749
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.68s
                        Total time: 89.33s
                               ETA: 603 mins 45.7 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 128398 steps/s (collection: 0.617s, learning 0.149s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0880
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0629
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0764
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.77s
                        Total time: 90.10s
                               ETA: 604 mins 0.8 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 134777 steps/s (collection: 0.600s, learning 0.129s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.20
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0237
          Mean episode rew_dof_acc: -0.0866
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0631
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0771
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.73s
                        Total time: 90.83s
                               ETA: 604 mins 1.2 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 145922 steps/s (collection: 0.551s, learning 0.123s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0240
          Mean episode rew_dof_acc: -0.0877
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0664
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0779
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.67s
                        Total time: 91.50s
                               ETA: 603 mins 39.5 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 142956 steps/s (collection: 0.566s, learning 0.122s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0280
       Mean episode rew_ang_vel_xy: -0.0248
          Mean episode rew_dof_acc: -0.0904
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0686
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0787
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.69s
                        Total time: 92.19s
                               ETA: 603 mins 23.7 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 123567 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0241
          Mean episode rew_dof_acc: -0.0859
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0018
        Mean episode rew_lin_vel_z: -0.0646
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0787
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.80s
                        Total time: 92.99s
                               ETA: 603 mins 50.1 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 141739 steps/s (collection: 0.567s, learning 0.126s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.21
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0881
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0665
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0788
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.69s
                        Total time: 93.68s
                               ETA: 603 mins 36.6 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 146624 steps/s (collection: 0.546s, learning 0.124s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0892
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0680
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0795
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.67s
                        Total time: 94.35s
                               ETA: 603 mins 14.5 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 124276 steps/s (collection: 0.657s, learning 0.134s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0244
          Mean episode rew_dof_acc: -0.0887
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0650
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0793
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.79s
                        Total time: 95.14s
                               ETA: 603 mins 38.6 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 145417 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0249
          Mean episode rew_dof_acc: -0.0894
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0689
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0799
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.68s
                        Total time: 95.82s
                               ETA: 603 mins 18.9 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 137844 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0246
          Mean episode rew_dof_acc: -0.0891
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0684
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0808
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.71s
                        Total time: 96.53s
                               ETA: 603 mins 13.4 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 144109 steps/s (collection: 0.545s, learning 0.137s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.22
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0894
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0666
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0802
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.68s
                        Total time: 97.21s
                               ETA: 602 mins 56.5 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 140902 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0871
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0656
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0811
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.70s
                        Total time: 97.91s
                               ETA: 602 mins 45.5 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 140124 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0927
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0670
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0851
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.70s
                        Total time: 98.61s
                               ETA: 602 mins 36.0 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 143063 steps/s (collection: 0.565s, learning 0.122s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0303
       Mean episode rew_ang_vel_xy: -0.0245
          Mean episode rew_dof_acc: -0.0895
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0683
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0838
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.69s
                        Total time: 99.30s
                               ETA: 602 mins 21.5 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 135303 steps/s (collection: 0.603s, learning 0.124s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0913
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0708
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0851
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.73s
                        Total time: 100.02s
                               ETA: 602 mins 21.4 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 139451 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0303
       Mean episode rew_ang_vel_xy: -0.0247
          Mean episode rew_dof_acc: -0.0893
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0677
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
       Mean episode rew_smoothness: -0.0838
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.70s
                        Total time: 100.73s
                               ETA: 602 mins 13.6 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 140265 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0251
          Mean episode rew_dof_acc: -0.0903
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0681
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0859
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.70s
                        Total time: 101.43s
                               ETA: 602 mins 4.3 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 141136 steps/s (collection: 0.575s, learning 0.121s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.23
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0314
       Mean episode rew_ang_vel_xy: -0.0252
          Mean episode rew_dof_acc: -0.0915
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0682
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0867
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.70s
                        Total time: 102.13s
                               ETA: 601 mins 53.7 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 133226 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0318
       Mean episode rew_ang_vel_xy: -0.0256
          Mean episode rew_dof_acc: -0.0932
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0683
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0877
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.74s
                        Total time: 102.86s
                               ETA: 601 mins 57.8 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 146157 steps/s (collection: 0.550s, learning 0.122s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0319
       Mean episode rew_ang_vel_xy: -0.0253
          Mean episode rew_dof_acc: -0.0923
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0019
        Mean episode rew_lin_vel_z: -0.0690
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0882
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.67s
                        Total time: 103.54s
                               ETA: 601 mins 39.0 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 129978 steps/s (collection: 0.629s, learning 0.127s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0939
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0689
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0020
       Mean episode rew_smoothness: -0.0881
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.76s
                        Total time: 104.29s
                               ETA: 601 mins 49.4 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 143618 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.24
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0330
       Mean episode rew_ang_vel_xy: -0.0259
          Mean episode rew_dof_acc: -0.0942
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0703
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0910
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.68s
                        Total time: 104.98s
                               ETA: 601 mins 35.0 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 125618 steps/s (collection: 0.653s, learning 0.129s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0329
       Mean episode rew_ang_vel_xy: -0.0264
          Mean episode rew_dof_acc: -0.0929
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0711
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0904
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.78s
                        Total time: 105.76s
                               ETA: 601 mins 54.3 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 138801 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0336
       Mean episode rew_ang_vel_xy: -0.0258
          Mean episode rew_dof_acc: -0.0934
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0692
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0925
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.71s
                        Total time: 106.47s
                               ETA: 601 mins 48.1 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 139827 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0938
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0704
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0917
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.70s
                        Total time: 107.17s
                               ETA: 601 mins 40.2 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 147351 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0337
       Mean episode rew_ang_vel_xy: -0.0254
          Mean episode rew_dof_acc: -0.0929
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0703
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0926
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.67s
                        Total time: 107.84s
                               ETA: 601 mins 20.4 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 138379 steps/s (collection: 0.577s, learning 0.133s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0340
       Mean episode rew_ang_vel_xy: -0.0261
          Mean episode rew_dof_acc: -0.0933
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0716
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0933
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.71s
                        Total time: 108.55s
                               ETA: 601 mins 15.3 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 139456 steps/s (collection: 0.582s, learning 0.122s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.25
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0347
       Mean episode rew_ang_vel_xy: -0.0259
          Mean episode rew_dof_acc: -0.0958
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0706
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0949
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.70s
                        Total time: 109.25s
                               ETA: 601 mins 8.4 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 136529 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0344
       Mean episode rew_ang_vel_xy: -0.0260
          Mean episode rew_dof_acc: -0.0938
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0714
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0947
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.72s
                        Total time: 109.97s
                               ETA: 601 mins 6.5 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 145876 steps/s (collection: 0.550s, learning 0.124s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0347
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0945
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0721
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0021
       Mean episode rew_smoothness: -0.0958
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.67s
                        Total time: 110.65s
                               ETA: 600 mins 49.6 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 138635 steps/s (collection: 0.587s, learning 0.122s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0353
       Mean episode rew_ang_vel_xy: -0.0267
          Mean episode rew_dof_acc: -0.0967
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0743
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0969
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.71s
                        Total time: 111.36s
                               ETA: 600 mins 44.3 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 143989 steps/s (collection: 0.560s, learning 0.123s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0360
       Mean episode rew_ang_vel_xy: -0.0260
          Mean episode rew_dof_acc: -0.0962
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0687
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0990
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.68s
                        Total time: 112.04s
                               ETA: 600 mins 30.6 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 146254 steps/s (collection: 0.549s, learning 0.123s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0361
       Mean episode rew_ang_vel_xy: -0.0262
          Mean episode rew_dof_acc: -0.0963
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0739
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.0996
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.67s
                        Total time: 112.71s
                               ETA: 600 mins 13.7 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 145305 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.26
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0365
       Mean episode rew_ang_vel_xy: -0.0270
          Mean episode rew_dof_acc: -0.0997
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0732
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1007
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.68s
                        Total time: 113.39s
                               ETA: 599 mins 58.3 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 145417 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0366
       Mean episode rew_ang_vel_xy: -0.0265
          Mean episode rew_dof_acc: -0.0963
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0738
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1017
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.68s
                        Total time: 114.06s
                               ETA: 599 mins 43.0 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 147200 steps/s (collection: 0.545s, learning 0.123s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0368
       Mean episode rew_ang_vel_xy: -0.0266
          Mean episode rew_dof_acc: -0.0978
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0731
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1019
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.67s
                        Total time: 114.73s
                               ETA: 599 mins 25.4 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 146906 steps/s (collection: 0.545s, learning 0.124s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0375
       Mean episode rew_ang_vel_xy: -0.0268
          Mean episode rew_dof_acc: -0.0986
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0759
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1036
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.67s
                        Total time: 115.40s
                               ETA: 599 mins 8.3 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 135724 steps/s (collection: 0.601s, learning 0.123s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0375
       Mean episode rew_ang_vel_xy: -0.0271
          Mean episode rew_dof_acc: -0.1002
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0783
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1041
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.72s
                        Total time: 116.13s
                               ETA: 599 mins 8.5 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 139583 steps/s (collection: 0.563s, learning 0.142s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0374
       Mean episode rew_ang_vel_xy: -0.0264
          Mean episode rew_dof_acc: -0.0983
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0723
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1040
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.70s
                        Total time: 116.83s
                               ETA: 599 mins 2.6 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 148006 steps/s (collection: 0.542s, learning 0.123s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.27
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0376
       Mean episode rew_ang_vel_xy: -0.0265
          Mean episode rew_dof_acc: -0.0978
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0715
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1048
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.66s
                        Total time: 117.49s
                               ETA: 598 mins 44.4 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 138763 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0383
       Mean episode rew_ang_vel_xy: -0.0266
          Mean episode rew_dof_acc: -0.1007
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0723
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1063
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.71s
                        Total time: 118.20s
                               ETA: 598 mins 39.9 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 135011 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0382
       Mean episode rew_ang_vel_xy: -0.0266
          Mean episode rew_dof_acc: -0.0975
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0020
        Mean episode rew_lin_vel_z: -0.0727
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0022
       Mean episode rew_smoothness: -0.1058
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.73s
                        Total time: 118.93s
                               ETA: 598 mins 41.4 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 140672 steps/s (collection: 0.566s, learning 0.133s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.28
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0387
       Mean episode rew_ang_vel_xy: -0.0269
          Mean episode rew_dof_acc: -0.1011
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0746
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1068
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.70s
                        Total time: 119.63s
                               ETA: 598 mins 34.1 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 132259 steps/s (collection: 0.620s, learning 0.123s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0390
       Mean episode rew_ang_vel_xy: -0.0272
          Mean episode rew_dof_acc: -0.0997
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0764
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.74s
                        Total time: 120.37s
                               ETA: 598 mins 40.2 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 131863 steps/s (collection: 0.624s, learning 0.122s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0393
       Mean episode rew_ang_vel_xy: -0.0276
          Mean episode rew_dof_acc: -0.1015
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0744
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1093
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.75s
                        Total time: 121.12s
                               ETA: 598 mins 46.8 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 146496 steps/s (collection: 0.548s, learning 0.123s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0404
       Mean episode rew_ang_vel_xy: -0.0276
          Mean episode rew_dof_acc: -0.1018
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0757
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1125
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.67s
                        Total time: 121.79s
                               ETA: 598 mins 31.3 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 143348 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0404
       Mean episode rew_ang_vel_xy: -0.0268
          Mean episode rew_dof_acc: -0.1002
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0703
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1127
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.69s
                        Total time: 122.48s
                               ETA: 598 mins 20.4 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 142839 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0127
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0407
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.1018
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0762
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1132
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.69s
                        Total time: 123.16s
                               ETA: 598 mins 10.3 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 137799 steps/s (collection: 0.575s, learning 0.138s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.29
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0416
       Mean episode rew_ang_vel_xy: -0.0280
          Mean episode rew_dof_acc: -0.1050
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0776
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1153
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.71s
                        Total time: 123.88s
                               ETA: 598 mins 7.6 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 135219 steps/s (collection: 0.589s, learning 0.138s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0418
       Mean episode rew_ang_vel_xy: -0.0278
          Mean episode rew_dof_acc: -0.1039
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0752
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1154
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.73s
                        Total time: 124.60s
                               ETA: 598 mins 8.8 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 131577 steps/s (collection: 0.615s, learning 0.132s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0407
       Mean episode rew_ang_vel_xy: -0.0274
          Mean episode rew_dof_acc: -0.1017
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0749
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1131
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.75s
                        Total time: 125.35s
                               ETA: 598 mins 15.8 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 137259 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0420
       Mean episode rew_ang_vel_xy: -0.0282
          Mean episode rew_dof_acc: -0.1054
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0791
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1165
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.72s
                        Total time: 126.07s
                               ETA: 598 mins 13.8 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 140172 steps/s (collection: 0.567s, learning 0.134s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0117
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0424
       Mean episode rew_ang_vel_xy: -0.0281
          Mean episode rew_dof_acc: -0.1046
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0793
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1180
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.70s
                        Total time: 126.77s
                               ETA: 598 mins 7.7 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 129955 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.30
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0425
       Mean episode rew_ang_vel_xy: -0.0271
          Mean episode rew_dof_acc: -0.1030
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0021
        Mean episode rew_lin_vel_z: -0.0739
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1183
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.76s
                        Total time: 127.52s
                               ETA: 598 mins 17.2 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 125049 steps/s (collection: 0.655s, learning 0.131s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0434
       Mean episode rew_ang_vel_xy: -0.0286
          Mean episode rew_dof_acc: -0.1045
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0802
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1204
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.79s
                        Total time: 128.31s
                               ETA: 598 mins 34.8 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 137290 steps/s (collection: 0.587s, learning 0.129s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0126
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0430
       Mean episode rew_ang_vel_xy: -0.0280
          Mean episode rew_dof_acc: -0.1034
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0767
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1185
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.72s
                        Total time: 129.03s
                               ETA: 598 mins 32.8 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 118516 steps/s (collection: 0.686s, learning 0.144s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0436
       Mean episode rew_ang_vel_xy: -0.0293
          Mean episode rew_dof_acc: -0.1077
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0797
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1203
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.83s
                        Total time: 129.86s
                               ETA: 599 mins 2.1 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 132834 steps/s (collection: 0.607s, learning 0.133s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.31
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0435
       Mean episode rew_ang_vel_xy: -0.0280
          Mean episode rew_dof_acc: -0.1035
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0790
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1194
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.74s
                        Total time: 130.60s
                               ETA: 599 mins 6.5 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 139097 steps/s (collection: 0.576s, learning 0.131s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0449
       Mean episode rew_ang_vel_xy: -0.0287
          Mean episode rew_dof_acc: -0.1084
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0798
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1243
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.71s
                        Total time: 131.30s
                               ETA: 599 mins 1.7 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 124901 steps/s (collection: 0.649s, learning 0.138s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0445
       Mean episode rew_ang_vel_xy: -0.0288
          Mean episode rew_dof_acc: -0.1068
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0820
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1227
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.79s
                        Total time: 132.09s
                               ETA: 599 mins 18.9 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 136318 steps/s (collection: 0.598s, learning 0.123s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0448
       Mean episode rew_ang_vel_xy: -0.0288
          Mean episode rew_dof_acc: -0.1075
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0817
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0023
       Mean episode rew_smoothness: -0.1241
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.72s
                        Total time: 132.81s
                               ETA: 599 mins 18.0 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 129749 steps/s (collection: 0.636s, learning 0.122s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0452
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1054
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0829
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1258
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.76s
                        Total time: 133.57s
                               ETA: 599 mins 26.9 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 141749 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.32
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0465
       Mean episode rew_ang_vel_xy: -0.0289
          Mean episode rew_dof_acc: -0.1094
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0800
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1291
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.69s
                        Total time: 134.26s
                               ETA: 599 mins 18.6 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 128638 steps/s (collection: 0.615s, learning 0.149s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0125
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0472
       Mean episode rew_ang_vel_xy: -0.0285
          Mean episode rew_dof_acc: -0.1113
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0850
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1314
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.76s
                        Total time: 135.03s
                               ETA: 599 mins 29.1 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 124000 steps/s (collection: 0.661s, learning 0.132s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0471
       Mean episode rew_ang_vel_xy: -0.0284
          Mean episode rew_dof_acc: -0.1094
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0806
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1308
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.79s
                        Total time: 135.82s
                               ETA: 599 mins 47.1 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 132562 steps/s (collection: 0.611s, learning 0.131s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0475
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1091
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0838
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1313
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.74s
                        Total time: 136.56s
                               ETA: 599 mins 51.4 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 130725 steps/s (collection: 0.628s, learning 0.124s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0476
       Mean episode rew_ang_vel_xy: -0.0286
          Mean episode rew_dof_acc: -0.1077
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0804
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1325
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.75s
                        Total time: 137.31s
                               ETA: 599 mins 58.4 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 127064 steps/s (collection: 0.640s, learning 0.134s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0472
       Mean episode rew_ang_vel_xy: -0.0290
          Mean episode rew_dof_acc: -0.1089
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0810
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1313
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.77s
                        Total time: 138.09s
                               ETA: 600 mins 11.0 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 145687 steps/s (collection: 0.552s, learning 0.122s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.33
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0479
       Mean episode rew_ang_vel_xy: -0.0288
          Mean episode rew_dof_acc: -0.1120
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0844
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1322
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.67s
                        Total time: 138.76s
                               ETA: 599 mins 57.8 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 137785 steps/s (collection: 0.584s, learning 0.129s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0485
       Mean episode rew_ang_vel_xy: -0.0288
          Mean episode rew_dof_acc: -0.1100
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0789
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1347
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.71s
                        Total time: 139.48s
                               ETA: 599 mins 54.7 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 139492 steps/s (collection: 0.581s, learning 0.124s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0491
       Mean episode rew_ang_vel_xy: -0.0287
          Mean episode rew_dof_acc: -0.1107
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0797
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1362
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.70s
                        Total time: 140.18s
                               ETA: 599 mins 49.3 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 137312 steps/s (collection: 0.587s, learning 0.129s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0500
       Mean episode rew_ang_vel_xy: -0.0300
          Mean episode rew_dof_acc: -0.1103
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0818
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1377
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0223
        Mean episode terrain_level: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.72s
                        Total time: 140.90s
                               ETA: 599 mins 46.9 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 142610 steps/s (collection: 0.568s, learning 0.121s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0485
       Mean episode rew_ang_vel_xy: -0.0300
          Mean episode rew_dof_acc: -0.1116
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0022
        Mean episode rew_lin_vel_z: -0.0834
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1342
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.69s
                        Total time: 141.58s
                               ETA: 599 mins 37.7 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 149582 steps/s (collection: 0.534s, learning 0.123s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0499
       Mean episode rew_ang_vel_xy: -0.0297
          Mean episode rew_dof_acc: -0.1118
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0831
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1384
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.66s
                        Total time: 142.24s
                               ETA: 599 mins 20.5 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 129919 steps/s (collection: 0.629s, learning 0.127s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.34
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0504
       Mean episode rew_ang_vel_xy: -0.0300
          Mean episode rew_dof_acc: -0.1116
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0841
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1395
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.76s
                        Total time: 143.00s
                               ETA: 599 mins 28.5 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 127626 steps/s (collection: 0.644s, learning 0.127s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0496
       Mean episode rew_ang_vel_xy: -0.0294
          Mean episode rew_dof_acc: -0.1111
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0845
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0024
       Mean episode rew_smoothness: -0.1380
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.77s
                        Total time: 143.77s
                               ETA: 599 mins 39.8 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 133792 steps/s (collection: 0.607s, learning 0.128s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0511
       Mean episode rew_ang_vel_xy: -0.0295
          Mean episode rew_dof_acc: -0.1130
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0834
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1425
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.73s
                        Total time: 144.50s
                               ETA: 599 mins 42.2 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 125047 steps/s (collection: 0.658s, learning 0.128s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0527
       Mean episode rew_ang_vel_xy: -0.0298
          Mean episode rew_dof_acc: -0.1158
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0847
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1469
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0226
        Mean episode terrain_level: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.79s
                        Total time: 145.29s
                               ETA: 599 mins 57.2 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 124836 steps/s (collection: 0.650s, learning 0.137s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0522
       Mean episode rew_ang_vel_xy: -0.0311
          Mean episode rew_dof_acc: -0.1150
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0874
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1441
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.79s
                        Total time: 146.08s
                               ETA: 600 mins 12.4 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 129723 steps/s (collection: 0.633s, learning 0.125s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0524
       Mean episode rew_ang_vel_xy: -0.0304
          Mean episode rew_dof_acc: -0.1140
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0875
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1455
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.76s
                        Total time: 146.84s
                               ETA: 600 mins 20.2 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 130913 steps/s (collection: 0.616s, learning 0.135s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.35
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0533
       Mean episode rew_ang_vel_xy: -0.0309
          Mean episode rew_dof_acc: -0.1174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0892
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1482
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.75s
                        Total time: 147.59s
                               ETA: 600 mins 26.2 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 142016 steps/s (collection: 0.565s, learning 0.128s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0532
       Mean episode rew_ang_vel_xy: -0.0304
          Mean episode rew_dof_acc: -0.1150
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0886
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1480
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.69s
                        Total time: 148.28s
                               ETA: 600 mins 17.9 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 137750 steps/s (collection: 0.588s, learning 0.125s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0533
       Mean episode rew_ang_vel_xy: -0.0304
          Mean episode rew_dof_acc: -0.1150
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0868
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1486
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0219
        Mean episode terrain_level: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.71s
                        Total time: 148.99s
                               ETA: 600 mins 14.8 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 129780 steps/s (collection: 0.618s, learning 0.140s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0538
       Mean episode rew_ang_vel_xy: -0.0311
          Mean episode rew_dof_acc: -0.1155
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0914
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1501
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.76s
                        Total time: 149.75s
                               ETA: 600 mins 22.3 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 112188 steps/s (collection: 0.732s, learning 0.145s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.36
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0540
       Mean episode rew_ang_vel_xy: -0.0305
          Mean episode rew_dof_acc: -0.1165
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0883
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1487
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.88s
                        Total time: 150.63s
                               ETA: 600 mins 58.2 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 131323 steps/s (collection: 0.617s, learning 0.132s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0566
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1212
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0854
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1575
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.75s
                        Total time: 151.37s
                               ETA: 601 mins 3.3 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 131636 steps/s (collection: 0.616s, learning 0.131s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0562
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1179
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0837
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1547
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0232
        Mean episode terrain_level: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.75s
                        Total time: 152.12s
                               ETA: 601 mins 7.9 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 135986 steps/s (collection: 0.591s, learning 0.132s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0552
       Mean episode rew_ang_vel_xy: -0.0305
          Mean episode rew_dof_acc: -0.1177
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0855
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0025
       Mean episode rew_smoothness: -0.1529
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.72s
                        Total time: 152.84s
                               ETA: 601 mins 6.8 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 127473 steps/s (collection: 0.634s, learning 0.137s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0132
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0563
       Mean episode rew_ang_vel_xy: -0.0299
          Mean episode rew_dof_acc: -0.1148
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0832
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1556
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.77s
                        Total time: 153.62s
                               ETA: 601 mins 17.1 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 128179 steps/s (collection: 0.632s, learning 0.135s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.37
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0574
       Mean episode rew_ang_vel_xy: -0.0311
          Mean episode rew_dof_acc: -0.1216
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0895
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1592
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.77s
                        Total time: 154.38s
                               ETA: 601 mins 26.2 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 127110 steps/s (collection: 0.643s, learning 0.131s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0564
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1184
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0023
        Mean episode rew_lin_vel_z: -0.0834
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1583
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.77s
                        Total time: 155.16s
                               ETA: 601 mins 36.8 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 129248 steps/s (collection: 0.622s, learning 0.139s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0575
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1205
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0877
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1592
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.76s
                        Total time: 155.92s
                               ETA: 601 mins 44.3 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 118456 steps/s (collection: 0.697s, learning 0.133s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0577
       Mean episode rew_ang_vel_xy: -0.0319
          Mean episode rew_dof_acc: -0.1190
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0873
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1599
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.83s
                        Total time: 156.75s
                               ETA: 602 mins 7.7 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 135385 steps/s (collection: 0.597s, learning 0.129s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0582
       Mean episode rew_ang_vel_xy: -0.0306
          Mean episode rew_dof_acc: -0.1197
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0870
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1621
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.73s
                        Total time: 157.47s
                               ETA: 602 mins 7.1 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 126565 steps/s (collection: 0.636s, learning 0.140s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0585
       Mean episode rew_ang_vel_xy: -0.0315
          Mean episode rew_dof_acc: -0.1219
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0890
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1621
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.78s
                        Total time: 158.25s
                               ETA: 602 mins 18.0 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 130008 steps/s (collection: 0.615s, learning 0.141s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0589
       Mean episode rew_ang_vel_xy: -0.0310
          Mean episode rew_dof_acc: -0.1193
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0831
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1634
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.76s
                        Total time: 159.00s
                               ETA: 602 mins 24.2 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 126357 steps/s (collection: 0.644s, learning 0.134s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0592
       Mean episode rew_ang_vel_xy: -0.0317
          Mean episode rew_dof_acc: -0.1225
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0883
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1631
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.78s
                        Total time: 159.78s
                               ETA: 602 mins 35.2 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 133570 steps/s (collection: 0.593s, learning 0.143s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.38
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0591
       Mean episode rew_ang_vel_xy: -0.0320
          Mean episode rew_dof_acc: -0.1229
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0922
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1635
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.74s
                        Total time: 160.52s
                               ETA: 602 mins 36.7 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 130047 steps/s (collection: 0.616s, learning 0.140s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0608
       Mean episode rew_ang_vel_xy: -0.0321
          Mean episode rew_dof_acc: -0.1222
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0881
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1677
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0233
        Mean episode terrain_level: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.76s
                        Total time: 161.27s
                               ETA: 602 mins 42.6 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 123171 steps/s (collection: 0.664s, learning 0.134s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0600
       Mean episode rew_ang_vel_xy: -0.0316
          Mean episode rew_dof_acc: -0.1238
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0865
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1640
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.80s
                        Total time: 162.07s
                               ETA: 602 mins 57.8 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 141688 steps/s (collection: 0.564s, learning 0.129s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0601
       Mean episode rew_ang_vel_xy: -0.0316
          Mean episode rew_dof_acc: -0.1215
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0856
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1669
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.69s
                        Total time: 162.77s
                               ETA: 602 mins 49.8 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 128587 steps/s (collection: 0.635s, learning 0.130s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0616
       Mean episode rew_ang_vel_xy: -0.0323
          Mean episode rew_dof_acc: -0.1236
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0903
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1705
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.76s
                        Total time: 163.53s
                               ETA: 602 mins 57.4 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 134443 steps/s (collection: 0.595s, learning 0.136s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0624
       Mean episode rew_ang_vel_xy: -0.0317
          Mean episode rew_dof_acc: -0.1266
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0884
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1737
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.73s
                        Total time: 164.26s
                               ETA: 602 mins 57.7 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 129561 steps/s (collection: 0.613s, learning 0.146s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0620
       Mean episode rew_ang_vel_xy: -0.0324
          Mean episode rew_dof_acc: -0.1233
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0902
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1715
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.76s
                        Total time: 165.02s
                               ETA: 603 mins 3.9 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 126229 steps/s (collection: 0.642s, learning 0.137s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.39
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0614
       Mean episode rew_ang_vel_xy: -0.0314
          Mean episode rew_dof_acc: -0.1228
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0864
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0026
       Mean episode rew_smoothness: -0.1692
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.78s
                        Total time: 165.80s
                               ETA: 603 mins 14.5 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 139157 steps/s (collection: 0.576s, learning 0.130s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0622
       Mean episode rew_ang_vel_xy: -0.0317
          Mean episode rew_dof_acc: -0.1237
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1718
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0231
        Mean episode terrain_level: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.71s
                        Total time: 166.51s
                               ETA: 603 mins 9.3 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 135888 steps/s (collection: 0.591s, learning 0.132s)
               Value function loss: 0.0435
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0635
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.1239
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0024
        Mean episode rew_lin_vel_z: -0.0889
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1743
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.72s
                        Total time: 167.23s
                               ETA: 603 mins 7.7 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 123836 steps/s (collection: 0.643s, learning 0.151s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.40
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0633
       Mean episode rew_ang_vel_xy: -0.0319
          Mean episode rew_dof_acc: -0.1257
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0905
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1730
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0227
        Mean episode terrain_level: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.79s
                        Total time: 168.02s
                               ETA: 603 mins 21.4 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 135450 steps/s (collection: 0.595s, learning 0.131s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0647
       Mean episode rew_ang_vel_xy: -0.0330
          Mean episode rew_dof_acc: -0.1275
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0920
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1785
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.73s
                        Total time: 168.75s
                               ETA: 603 mins 20.3 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 132694 steps/s (collection: 0.609s, learning 0.132s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0666
       Mean episode rew_ang_vel_xy: -0.0321
          Mean episode rew_dof_acc: -0.1273
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0896
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1841
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.74s
                        Total time: 169.49s
                               ETA: 603 mins 22.5 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 138210 steps/s (collection: 0.581s, learning 0.131s)
               Value function loss: 0.0449
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0661
       Mean episode rew_ang_vel_xy: -0.0329
          Mean episode rew_dof_acc: -0.1261
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0914
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1815
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.71s
                        Total time: 170.20s
                               ETA: 603 mins 18.3 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 123657 steps/s (collection: 0.662s, learning 0.133s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0660
       Mean episode rew_ang_vel_xy: -0.0322
          Mean episode rew_dof_acc: -0.1245
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1811
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.79s
                        Total time: 171.00s
                               ETA: 603 mins 31.9 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 140252 steps/s (collection: 0.569s, learning 0.132s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0669
       Mean episode rew_ang_vel_xy: -0.0329
          Mean episode rew_dof_acc: -0.1294
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0953
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1833
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.70s
                        Total time: 171.70s
                               ETA: 603 mins 25.5 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 130831 steps/s (collection: 0.617s, learning 0.135s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0159
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0668
       Mean episode rew_ang_vel_xy: -0.0323
          Mean episode rew_dof_acc: -0.1303
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0931
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1850
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.75s
                        Total time: 172.45s
                               ETA: 603 mins 29.8 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 128129 steps/s (collection: 0.633s, learning 0.134s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.41
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0673
       Mean episode rew_ang_vel_xy: -0.0325
          Mean episode rew_dof_acc: -0.1260
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1853
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.77s
                        Total time: 173.22s
                               ETA: 603 mins 37.4 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 130937 steps/s (collection: 0.620s, learning 0.131s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0676
       Mean episode rew_ang_vel_xy: -0.0325
          Mean episode rew_dof_acc: -0.1277
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0901
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.1864
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.75s
                        Total time: 173.97s
                               ETA: 603 mins 41.4 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 132823 steps/s (collection: 0.598s, learning 0.142s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0672
       Mean episode rew_ang_vel_xy: -0.0327
          Mean episode rew_dof_acc: -0.1279
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0981
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1852
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.74s
                        Total time: 174.71s
                               ETA: 603 mins 43.2 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 119919 steps/s (collection: 0.669s, learning 0.151s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0697
       Mean episode rew_ang_vel_xy: -0.0331
          Mean episode rew_dof_acc: -0.1297
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0900
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.1926
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.82s
                        Total time: 175.53s
                               ETA: 604 mins 1.5 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 124379 steps/s (collection: 0.649s, learning 0.141s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0709
       Mean episode rew_ang_vel_xy: -0.0330
          Mean episode rew_dof_acc: -0.1308
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0921
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.1957
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.79s
                        Total time: 176.32s
                               ETA: 604 mins 13.5 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 135445 steps/s (collection: 0.595s, learning 0.131s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.42
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0714
       Mean episode rew_ang_vel_xy: -0.0337
          Mean episode rew_dof_acc: -0.1320
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0987
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.1979
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.73s
                        Total time: 177.04s
                               ETA: 604 mins 12.2 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 122266 steps/s (collection: 0.653s, learning 0.151s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0695
       Mean episode rew_ang_vel_xy: -0.0334
          Mean episode rew_dof_acc: -0.1290
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0945
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1930
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.80s
                        Total time: 177.85s
                               ETA: 604 mins 26.8 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 123588 steps/s (collection: 0.644s, learning 0.152s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0708
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1321
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0961
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1960
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0228
        Mean episode terrain_level: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.80s
                        Total time: 178.64s
                               ETA: 604 mins 39.6 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 134816 steps/s (collection: 0.604s, learning 0.125s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0702
       Mean episode rew_ang_vel_xy: -0.0333
          Mean episode rew_dof_acc: -0.1293
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0025
        Mean episode rew_lin_vel_z: -0.0958
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1936
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.73s
                        Total time: 179.37s
                               ETA: 604 mins 38.9 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 131683 steps/s (collection: 0.618s, learning 0.129s)
               Value function loss: 0.0481
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.43
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0716
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.1346
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1010
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.2001
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.75s
                        Total time: 180.12s
                               ETA: 604 mins 41.7 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 132009 steps/s (collection: 0.623s, learning 0.121s)
               Value function loss: 0.0479
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0711
       Mean episode rew_ang_vel_xy: -0.0331
          Mean episode rew_dof_acc: -0.1315
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0950
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.1988
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.74s
                        Total time: 180.86s
                               ETA: 604 mins 44.0 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 135762 steps/s (collection: 0.601s, learning 0.123s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0716
       Mean episode rew_ang_vel_xy: -0.0328
          Mean episode rew_dof_acc: -0.1313
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0933
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0027
       Mean episode rew_smoothness: -0.1994
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.72s
                        Total time: 181.59s
                               ETA: 604 mins 42.3 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 114086 steps/s (collection: 0.722s, learning 0.139s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0742
       Mean episode rew_ang_vel_xy: -0.0335
          Mean episode rew_dof_acc: -0.1340
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0935
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2076
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.86s
                        Total time: 182.45s
                               ETA: 605 mins 7.9 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 138719 steps/s (collection: 0.574s, learning 0.135s)
               Value function loss: 0.0487
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0736
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.1302
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0996
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2067
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.71s
                        Total time: 183.16s
                               ETA: 605 mins 3.0 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 125158 steps/s (collection: 0.659s, learning 0.127s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0726
       Mean episode rew_ang_vel_xy: -0.0335
          Mean episode rew_dof_acc: -0.1334
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0965
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2026
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.79s
                        Total time: 183.94s
                               ETA: 605 mins 13.2 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 145018 steps/s (collection: 0.548s, learning 0.130s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.44
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0752
       Mean episode rew_ang_vel_xy: -0.0353
          Mean episode rew_dof_acc: -0.1341
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0963
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2105
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.68s
                        Total time: 184.62s
                               ETA: 605 mins 2.3 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 119721 steps/s (collection: 0.669s, learning 0.152s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0743
       Mean episode rew_ang_vel_xy: -0.0336
          Mean episode rew_dof_acc: -0.1336
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0962
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2081
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0234
        Mean episode terrain_level: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.82s
                        Total time: 185.44s
                               ETA: 605 mins 19.4 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 145667 steps/s (collection: 0.548s, learning 0.127s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0754
       Mean episode rew_ang_vel_xy: -0.0338
          Mean episode rew_dof_acc: -0.1330
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0975
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2107
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.67s
                        Total time: 186.12s
                               ETA: 605 mins 7.9 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 137538 steps/s (collection: 0.593s, learning 0.121s)
               Value function loss: 0.0507
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0751
       Mean episode rew_ang_vel_xy: -0.0338
          Mean episode rew_dof_acc: -0.1367
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0974
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.71s
                        Total time: 186.83s
                               ETA: 605 mins 4.3 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 124527 steps/s (collection: 0.662s, learning 0.127s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.45
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0744
       Mean episode rew_ang_vel_xy: -0.0340
          Mean episode rew_dof_acc: -0.1332
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0981
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2075
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0226
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.79s
                        Total time: 187.62s
                               ETA: 605 mins 15.1 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 133758 steps/s (collection: 0.612s, learning 0.123s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0755
       Mean episode rew_ang_vel_xy: -0.0336
          Mean episode rew_dof_acc: -0.1315
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0964
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2127
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.73s
                        Total time: 188.36s
                               ETA: 605 mins 15.3 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 148057 steps/s (collection: 0.530s, learning 0.134s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0770
       Mean episode rew_ang_vel_xy: -0.0343
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0999
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2131
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.66s
                        Total time: 189.02s
                               ETA: 605 mins 1.9 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 138714 steps/s (collection: 0.587s, learning 0.121s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0778
       Mean episode rew_ang_vel_xy: -0.0357
          Mean episode rew_dof_acc: -0.1386
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1048
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2158
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.71s
                        Total time: 189.73s
                               ETA: 604 mins 57.1 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 138403 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0789
       Mean episode rew_ang_vel_xy: -0.0347
          Mean episode rew_dof_acc: -0.1384
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0997
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2219
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.71s
                        Total time: 190.44s
                               ETA: 604 mins 52.7 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 130253 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0781
       Mean episode rew_ang_vel_xy: -0.0350
          Mean episode rew_dof_acc: -0.1374
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1032
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2178
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.75s
                        Total time: 191.19s
                               ETA: 604 mins 56.7 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 150603 steps/s (collection: 0.530s, learning 0.123s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0764
       Mean episode rew_ang_vel_xy: -0.0346
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.0972
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2121
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.65s
                        Total time: 191.85s
                               ETA: 604 mins 41.4 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 147237 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0499
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0344
          Mean episode rew_dof_acc: -0.1349
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0026
        Mean episode rew_lin_vel_z: -0.1001
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2196
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.67s
                        Total time: 192.51s
                               ETA: 604 mins 29.0 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 148085 steps/s (collection: 0.542s, learning 0.122s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0348
          Mean episode rew_dof_acc: -0.1402
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1023
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2248
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.66s
                        Total time: 193.18s
                               ETA: 604 mins 16.0 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 132083 steps/s (collection: 0.602s, learning 0.142s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0792
       Mean episode rew_ang_vel_xy: -0.0349
          Mean episode rew_dof_acc: -0.1373
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1015
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2206
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.74s
                        Total time: 193.92s
                               ETA: 604 mins 18.1 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 133102 steps/s (collection: 0.598s, learning 0.140s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.46
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0803
       Mean episode rew_ang_vel_xy: -0.0351
          Mean episode rew_dof_acc: -0.1352
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0961
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2232
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.74s
                        Total time: 194.66s
                               ETA: 604 mins 19.2 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 126274 steps/s (collection: 0.635s, learning 0.143s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0787
       Mean episode rew_ang_vel_xy: -0.0357
          Mean episode rew_dof_acc: -0.1365
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1038
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2205
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.78s
                        Total time: 195.44s
                               ETA: 604 mins 27.6 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 129394 steps/s (collection: 0.612s, learning 0.148s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0790
       Mean episode rew_ang_vel_xy: -0.0349
          Mean episode rew_dof_acc: -0.1384
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1016
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2206
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.76s
                        Total time: 196.20s
                               ETA: 604 mins 32.5 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 137804 steps/s (collection: 0.572s, learning 0.141s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0347
          Mean episode rew_dof_acc: -0.1397
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.0999
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2343
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.71s
                        Total time: 196.91s
                               ETA: 604 mins 28.9 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 135910 steps/s (collection: 0.588s, learning 0.135s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0844
       Mean episode rew_ang_vel_xy: -0.0356
          Mean episode rew_dof_acc: -0.1445
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1022
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2374
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.72s
                        Total time: 197.63s
                               ETA: 604 mins 27.0 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 143631 steps/s (collection: 0.560s, learning 0.124s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.47
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0344
          Mean episode rew_dof_acc: -0.1408
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1025
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2284
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.68s
                        Total time: 198.32s
                               ETA: 604 mins 18.1 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 136529 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0850
       Mean episode rew_ang_vel_xy: -0.0360
          Mean episode rew_dof_acc: -0.1452
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1031
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2385
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.72s
                        Total time: 199.04s
                               ETA: 604 mins 15.7 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 137989 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0852
       Mean episode rew_ang_vel_xy: -0.0366
          Mean episode rew_dof_acc: -0.1433
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1012
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2420
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.71s
                        Total time: 199.75s
                               ETA: 604 mins 12.0 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 140854 steps/s (collection: 0.574s, learning 0.124s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0838
       Mean episode rew_ang_vel_xy: -0.0353
          Mean episode rew_dof_acc: -0.1428
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1048
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2379
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.70s
                        Total time: 200.45s
                               ETA: 604 mins 5.6 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 132072 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0856
       Mean episode rew_ang_vel_xy: -0.0355
          Mean episode rew_dof_acc: -0.1432
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1047
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2441
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.74s
                        Total time: 201.19s
                               ETA: 604 mins 7.7 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 125927 steps/s (collection: 0.649s, learning 0.132s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0124
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.48
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0849
       Mean episode rew_ang_vel_xy: -0.0358
          Mean episode rew_dof_acc: -0.1430
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1057
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2376
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.78s
                        Total time: 201.97s
                               ETA: 604 mins 16.2 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 139112 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0843
       Mean episode rew_ang_vel_xy: -0.0358
          Mean episode rew_dof_acc: -0.1394
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1018
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0028
       Mean episode rew_smoothness: -0.2379
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.71s
                        Total time: 202.68s
                               ETA: 604 mins 11.4 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 135436 steps/s (collection: 0.601s, learning 0.125s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.49
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0861
       Mean episode rew_ang_vel_xy: -0.0359
          Mean episode rew_dof_acc: -0.1436
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1082
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2461
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.73s
                        Total time: 203.41s
                               ETA: 604 mins 10.1 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 136095 steps/s (collection: 0.593s, learning 0.129s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0866
       Mean episode rew_ang_vel_xy: -0.0362
          Mean episode rew_dof_acc: -0.1440
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1065
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2457
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.72s
                        Total time: 204.13s
                               ETA: 604 mins 8.2 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 128876 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0890
       Mean episode rew_ang_vel_xy: -0.0360
          Mean episode rew_dof_acc: -0.1450
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1071
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2528
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.76s
                        Total time: 204.89s
                               ETA: 604 mins 13.5 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 144977 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0869
       Mean episode rew_ang_vel_xy: -0.0364
          Mean episode rew_dof_acc: -0.1432
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2476
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.68s
                        Total time: 205.57s
                               ETA: 604 mins 3.7 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 132020 steps/s (collection: 0.622s, learning 0.122s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0883
       Mean episode rew_ang_vel_xy: -0.0362
          Mean episode rew_dof_acc: -0.1420
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1055
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2513
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.74s
                        Total time: 206.31s
                               ETA: 604 mins 5.7 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 133311 steps/s (collection: 0.602s, learning 0.135s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.50
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0869
       Mean episode rew_ang_vel_xy: -0.0359
          Mean episode rew_dof_acc: -0.1426
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1053
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2440
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.74s
                        Total time: 207.05s
                               ETA: 604 mins 6.5 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 129542 steps/s (collection: 0.613s, learning 0.146s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0881
       Mean episode rew_ang_vel_xy: -0.0356
          Mean episode rew_dof_acc: -0.1409
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1034
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2492
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.76s
                        Total time: 207.81s
                               ETA: 604 mins 10.9 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 128589 steps/s (collection: 0.641s, learning 0.124s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0881
       Mean episode rew_ang_vel_xy: -0.0361
          Mean episode rew_dof_acc: -0.1408
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1038
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2484
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.76s
                        Total time: 208.58s
                               ETA: 604 mins 16.4 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 138838 steps/s (collection: 0.562s, learning 0.146s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0901
       Mean episode rew_ang_vel_xy: -0.0367
          Mean episode rew_dof_acc: -0.1445
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1035
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2546
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.71s
                        Total time: 209.28s
                               ETA: 604 mins 11.9 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 128028 steps/s (collection: 0.642s, learning 0.125s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0906
       Mean episode rew_ang_vel_xy: -0.0366
          Mean episode rew_dof_acc: -0.1448
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1057
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2557
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.77s
                        Total time: 210.05s
                               ETA: 604 mins 17.9 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 130692 steps/s (collection: 0.626s, learning 0.126s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.51
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0903
       Mean episode rew_ang_vel_xy: -0.0349
          Mean episode rew_dof_acc: -0.1411
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0027
        Mean episode rew_lin_vel_z: -0.1012
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2571
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.75s
                        Total time: 210.80s
                               ETA: 604 mins 21.1 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 137460 steps/s (collection: 0.590s, learning 0.125s)
               Value function loss: 0.0553
                    Surrogate loss: -0.0121
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0925
       Mean episode rew_ang_vel_xy: -0.0363
          Mean episode rew_dof_acc: -0.1479
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1008
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2617
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0241
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.72s
                        Total time: 211.52s
                               ETA: 604 mins 17.9 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 133045 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0943
       Mean episode rew_ang_vel_xy: -0.0364
          Mean episode rew_dof_acc: -0.1483
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1055
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2728
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0249
        Mean episode terrain_level: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.74s
                        Total time: 212.26s
                               ETA: 604 mins 18.8 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 141419 steps/s (collection: 0.571s, learning 0.124s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0913
       Mean episode rew_ang_vel_xy: -0.0359
          Mean episode rew_dof_acc: -0.1444
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1022
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2595
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0238
        Mean episode terrain_level: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.70s
                        Total time: 212.95s
                               ETA: 604 mins 12.2 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 143792 steps/s (collection: 0.558s, learning 0.125s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0916
       Mean episode rew_ang_vel_xy: -0.0365
          Mean episode rew_dof_acc: -0.1470
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2626
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.68s
                        Total time: 213.64s
                               ETA: 604 mins 3.8 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 134330 steps/s (collection: 0.602s, learning 0.130s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0982
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1531
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1090
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.2847
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.73s
                        Total time: 214.37s
                               ETA: 604 mins 3.5 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 136308 steps/s (collection: 0.594s, learning 0.128s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.52
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0947
       Mean episode rew_ang_vel_xy: -0.0368
          Mean episode rew_dof_acc: -0.1480
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1053
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2685
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.72s
                        Total time: 215.09s
                               ETA: 604 mins 1.4 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 141033 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0578
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0951
       Mean episode rew_ang_vel_xy: -0.0360
          Mean episode rew_dof_acc: -0.1475
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1036
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2719
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0249
        Mean episode terrain_level: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.70s
                        Total time: 215.79s
                               ETA: 603 mins 55.3 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 141087 steps/s (collection: 0.571s, learning 0.126s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0947
       Mean episode rew_ang_vel_xy: -0.0368
          Mean episode rew_dof_acc: -0.1490
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1029
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2678
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0250
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.70s
                        Total time: 216.48s
                               ETA: 603 mins 49.2 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 149174 steps/s (collection: 0.536s, learning 0.123s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0974
       Mean episode rew_ang_vel_xy: -0.0365
          Mean episode rew_dof_acc: -0.1501
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2796
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.66s
                        Total time: 217.14s
                               ETA: 603 mins 36.8 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 149317 steps/s (collection: 0.535s, learning 0.123s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0136
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1000
       Mean episode rew_ang_vel_xy: -0.0364
          Mean episode rew_dof_acc: -0.1531
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1031
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.2876
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.66s
                        Total time: 217.80s
                               ETA: 603 mins 24.4 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 131952 steps/s (collection: 0.605s, learning 0.140s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.53
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1004
       Mean episode rew_ang_vel_xy: -0.0380
          Mean episode rew_dof_acc: -0.1565
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1102
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2881
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.74s
                        Total time: 218.55s
                               ETA: 603 mins 26.4 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 131590 steps/s (collection: 0.619s, learning 0.128s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1011
       Mean episode rew_ang_vel_xy: -0.0359
          Mean episode rew_dof_acc: -0.1511
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.2934
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.75s
                        Total time: 219.29s
                               ETA: 603 mins 28.7 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 142448 steps/s (collection: 0.566s, learning 0.124s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0165
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0973
       Mean episode rew_ang_vel_xy: -0.0374
          Mean episode rew_dof_acc: -0.1519
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1027
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2772
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0284
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.69s
                        Total time: 219.98s
                               ETA: 603 mins 21.7 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 141481 steps/s (collection: 0.570s, learning 0.124s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0967
       Mean episode rew_ang_vel_xy: -0.0363
          Mean episode rew_dof_acc: -0.1483
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1043
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2791
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0269
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.69s
                        Total time: 220.68s
                               ETA: 603 mins 15.4 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 148430 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0130
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0964
       Mean episode rew_ang_vel_xy: -0.0364
          Mean episode rew_dof_acc: -0.1484
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1024
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2771
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.66s
                        Total time: 221.34s
                               ETA: 603 mins 3.9 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 133565 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0145
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.54
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1027
       Mean episode rew_ang_vel_xy: -0.0377
          Mean episode rew_dof_acc: -0.1564
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1047
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2974
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0264
        Mean episode terrain_level: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.74s
                        Total time: 222.08s
                               ETA: 603 mins 4.5 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 138013 steps/s (collection: 0.568s, learning 0.144s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1021
       Mean episode rew_ang_vel_xy: -0.0382
          Mean episode rew_dof_acc: -0.1538
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1094
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2949
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.71s
                        Total time: 222.79s
                               ETA: 603 mins 1.2 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 139268 steps/s (collection: 0.580s, learning 0.126s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0989
       Mean episode rew_ang_vel_xy: -0.0372
          Mean episode rew_dof_acc: -0.1535
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1128
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.2856
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0266
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.71s
                        Total time: 223.49s
                               ETA: 602 mins 56.8 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 136967 steps/s (collection: 0.589s, learning 0.129s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1009
       Mean episode rew_ang_vel_xy: -0.0366
          Mean episode rew_dof_acc: -0.1526
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1067
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2908
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0249
        Mean episode terrain_level: 0.0297
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.72s
                        Total time: 224.21s
                               ETA: 602 mins 54.5 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 131021 steps/s (collection: 0.629s, learning 0.122s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -3.99
                Mean reward (task): -3.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1036
       Mean episode rew_ang_vel_xy: -0.0376
          Mean episode rew_dof_acc: -0.1535
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1052
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3011
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.75s
                        Total time: 224.96s
                               ETA: 602 mins 57.3 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 127300 steps/s (collection: 0.631s, learning 0.141s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0140
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1010
       Mean episode rew_ang_vel_xy: -0.0377
          Mean episode rew_dof_acc: -0.1538
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1067
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2930
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0248
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.77s
                        Total time: 225.73s
                               ETA: 603 mins 3.7 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 126679 steps/s (collection: 0.625s, learning 0.151s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.55
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1011
       Mean episode rew_ang_vel_xy: -0.0371
          Mean episode rew_dof_acc: -0.1514
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0028
        Mean episode rew_lin_vel_z: -0.1101
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.2928
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0237
        Mean episode terrain_level: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.78s
                        Total time: 226.51s
                               ETA: 603 mins 10.6 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 148348 steps/s (collection: 0.537s, learning 0.125s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1051
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1562
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1072
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3069
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.66s
                        Total time: 227.17s
                               ETA: 602 mins 59.4 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 125305 steps/s (collection: 0.638s, learning 0.146s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1060
       Mean episode rew_ang_vel_xy: -0.0373
          Mean episode rew_dof_acc: -0.1545
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1024
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3055
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.78s
                        Total time: 227.96s
                               ETA: 603 mins 7.6 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 126646 steps/s (collection: 0.634s, learning 0.142s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1054
       Mean episode rew_ang_vel_xy: -0.0375
          Mean episode rew_dof_acc: -0.1567
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1048
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3079
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0239
        Mean episode terrain_level: 0.0271
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.78s
                        Total time: 228.73s
                               ETA: 603 mins 14.5 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 141424 steps/s (collection: 0.558s, learning 0.137s)
               Value function loss: 0.0607
                    Surrogate loss: -0.0156
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1071
       Mean episode rew_ang_vel_xy: -0.0378
          Mean episode rew_dof_acc: -0.1582
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1063
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3108
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.70s
                        Total time: 229.43s
                               ETA: 603 mins 8.5 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 137196 steps/s (collection: 0.559s, learning 0.158s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.56
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1093
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1608
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1087
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3169
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0266
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.72s
                        Total time: 230.14s
                               ETA: 603 mins 5.9 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 136318 steps/s (collection: 0.584s, learning 0.137s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1053
       Mean episode rew_ang_vel_xy: -0.0370
          Mean episode rew_dof_acc: -0.1545
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3092
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.72s
                        Total time: 230.87s
                               ETA: 603 mins 4.0 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 116331 steps/s (collection: 0.698s, learning 0.147s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0151
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1101
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1608
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1083
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3215
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.85s
                        Total time: 231.71s
                               ETA: 603 mins 21.6 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 142117 steps/s (collection: 0.552s, learning 0.140s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0138
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1068
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1561
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0029
       Mean episode rew_smoothness: -0.3137
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0261
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.69s
                        Total time: 232.40s
                               ETA: 603 mins 15.1 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 136741 steps/s (collection: 0.568s, learning 0.151s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0143
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1097
       Mean episode rew_ang_vel_xy: -0.0382
          Mean episode rew_dof_acc: -0.1592
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1053
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3212
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0293
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.72s
                        Total time: 233.12s
                               ETA: 603 mins 12.9 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 144094 steps/s (collection: 0.544s, learning 0.138s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1107
       Mean episode rew_ang_vel_xy: -0.0381
          Mean episode rew_dof_acc: -0.1601
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1067
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3265
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.68s
                        Total time: 233.80s
                               ETA: 603 mins 5.0 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 129259 steps/s (collection: 0.624s, learning 0.137s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1092
       Mean episode rew_ang_vel_xy: -0.0379
          Mean episode rew_dof_acc: -0.1590
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1093
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3187
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0270
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.76s
                        Total time: 234.56s
                               ETA: 603 mins 9.2 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 136127 steps/s (collection: 0.599s, learning 0.123s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.57
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1126
       Mean episode rew_ang_vel_xy: -0.0376
          Mean episode rew_dof_acc: -0.1603
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1087
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3343
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.72s
                        Total time: 235.29s
                               ETA: 603 mins 7.5 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 140206 steps/s (collection: 0.566s, learning 0.135s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1123
       Mean episode rew_ang_vel_xy: -0.0386
          Mean episode rew_dof_acc: -0.1601
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1072
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3325
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.70s
                        Total time: 235.99s
                               ETA: 603 mins 2.6 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 144508 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1096
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1591
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1038
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3222
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0289
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.68s
                        Total time: 236.67s
                               ETA: 602 mins 54.5 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 132733 steps/s (collection: 0.598s, learning 0.143s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1082
       Mean episode rew_ang_vel_xy: -0.0378
          Mean episode rew_dof_acc: -0.1563
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1068
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3185
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.74s
                        Total time: 237.41s
                               ETA: 602 mins 55.7 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 141931 steps/s (collection: 0.562s, learning 0.131s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0160
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1114
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1608
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1072
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3329
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.69s
                        Total time: 238.10s
                               ETA: 602 mins 49.5 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 142042 steps/s (collection: 0.560s, learning 0.132s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1116
       Mean episode rew_ang_vel_xy: -0.0385
          Mean episode rew_dof_acc: -0.1609
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1072
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3296
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.69s
                        Total time: 238.79s
                               ETA: 602 mins 43.3 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 125015 steps/s (collection: 0.658s, learning 0.128s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.58
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1109
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1580
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1097
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3262
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.79s
                        Total time: 239.58s
                               ETA: 602 mins 51.4 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 121295 steps/s (collection: 0.670s, learning 0.141s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1134
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1590
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3349
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.81s
                        Total time: 240.39s
                               ETA: 603 mins 3.1 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 139525 steps/s (collection: 0.565s, learning 0.139s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0133
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1173
       Mean episode rew_ang_vel_xy: -0.0382
          Mean episode rew_dof_acc: -0.1619
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1079
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3453
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0285
        Mean episode terrain_level: 0.0315
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.70s
                        Total time: 241.09s
                               ETA: 602 mins 58.7 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 139488 steps/s (collection: 0.563s, learning 0.142s)
               Value function loss: 0.0639
                    Surrogate loss: -0.0161
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1147
       Mean episode rew_ang_vel_xy: -0.0391
          Mean episode rew_dof_acc: -0.1627
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1102
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3406
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0264
        Mean episode terrain_level: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.70s
                        Total time: 241.80s
                               ETA: 602 mins 54.5 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 136272 steps/s (collection: 0.581s, learning 0.140s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1140
       Mean episode rew_ang_vel_xy: -0.0380
          Mean episode rew_dof_acc: -0.1625
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1104
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3344
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.72s
                        Total time: 242.52s
                               ETA: 602 mins 52.7 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 135605 steps/s (collection: 0.576s, learning 0.149s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0144
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.59
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1138
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1603
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1098
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3385
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0270
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.72s
                        Total time: 243.25s
                               ETA: 602 mins 51.5 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 134531 steps/s (collection: 0.603s, learning 0.128s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0147
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1142
       Mean episode rew_ang_vel_xy: -0.0382
          Mean episode rew_dof_acc: -0.1638
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1091
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3364
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0272
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.73s
                        Total time: 243.98s
                               ETA: 602 mins 51.1 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 138591 steps/s (collection: 0.583s, learning 0.127s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1143
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.1620
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1102
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3358
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.71s
                        Total time: 244.69s
                               ETA: 602 mins 47.6 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 138987 steps/s (collection: 0.571s, learning 0.136s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0135
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1150
       Mean episode rew_ang_vel_xy: -0.0383
          Mean episode rew_dof_acc: -0.1573
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0029
        Mean episode rew_lin_vel_z: -0.1004
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0030
       Mean episode rew_smoothness: -0.3418
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.71s
                        Total time: 245.39s
                               ETA: 602 mins 43.8 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 119447 steps/s (collection: 0.672s, learning 0.150s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0153
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1161
       Mean episode rew_ang_vel_xy: -0.0392
          Mean episode rew_dof_acc: -0.1638
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1106
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3432
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.82s
                        Total time: 246.22s
                               ETA: 602 mins 57.0 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 141353 steps/s (collection: 0.556s, learning 0.139s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.60
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1200
       Mean episode rew_ang_vel_xy: -0.0384
          Mean episode rew_dof_acc: -0.1663
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1078
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3577
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.70s
                        Total time: 246.91s
                               ETA: 602 mins 51.4 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 135115 steps/s (collection: 0.589s, learning 0.139s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0152
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1203
       Mean episode rew_ang_vel_xy: -0.0391
          Mean episode rew_dof_acc: -0.1690
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1099
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3590
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0350
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.73s
                        Total time: 247.64s
                               ETA: 602 mins 50.6 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 141223 steps/s (collection: 0.566s, learning 0.130s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1189
       Mean episode rew_ang_vel_xy: -0.0401
          Mean episode rew_dof_acc: -0.1657
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1130
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3524
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0263
        Mean episode terrain_level: 0.0352
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.70s
                        Total time: 248.33s
                               ETA: 602 mins 45.1 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 133259 steps/s (collection: 0.591s, learning 0.147s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0158
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1215
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1666
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1122
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3650
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0276
        Mean episode terrain_level: 0.0340
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.74s
                        Total time: 249.07s
                               ETA: 602 mins 45.8 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 132446 steps/s (collection: 0.621s, learning 0.121s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.61
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1189
       Mean episode rew_ang_vel_xy: -0.0394
          Mean episode rew_dof_acc: -0.1647
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1090
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3509
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.74s
                        Total time: 249.81s
                               ETA: 602 mins 47.1 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 135997 steps/s (collection: 0.587s, learning 0.136s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0129
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1177
       Mean episode rew_ang_vel_xy: -0.0394
          Mean episode rew_dof_acc: -0.1616
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1099
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3537
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.72s
                        Total time: 250.54s
                               ETA: 602 mins 45.6 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 142130 steps/s (collection: 0.561s, learning 0.131s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0146
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1226
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.1726
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1081
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3657
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.69s
                        Total time: 251.23s
                               ETA: 602 mins 39.5 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 130963 steps/s (collection: 0.614s, learning 0.136s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0157
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1172
       Mean episode rew_ang_vel_xy: -0.0390
          Mean episode rew_dof_acc: -0.1613
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0030
        Mean episode rew_lin_vel_z: -0.1083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0031
       Mean episode rew_smoothness: -0.3515
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0270
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.75s
                        Total time: 251.98s
                               ETA: 602 mins 42.0 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 138728 steps/s (collection: 0.568s, learning 0.140s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0123
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1260
       Mean episode rew_ang_vel_xy: -0.0400
          Mean episode rew_dof_acc: -0.1721
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1107
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3784
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.71s
                        Total time: 252.69s
                               ETA: 602 mins 38.5 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 145227 steps/s (collection: 0.535s, learning 0.142s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0162
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1243
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.1700
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3711
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.68s
                        Total time: 253.37s
                               ETA: 602 mins 30.4 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 141121 steps/s (collection: 0.557s, learning 0.140s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0141
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.62
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1238
       Mean episode rew_ang_vel_xy: -0.0402
          Mean episode rew_dof_acc: -0.1699
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1147
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3680
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.70s
                        Total time: 254.06s
                               ETA: 602 mins 25.2 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 142065 steps/s (collection: 0.551s, learning 0.141s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0148
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1245
       Mean episode rew_ang_vel_xy: -0.0387
          Mean episode rew_dof_acc: -0.1646
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1036
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3718
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0281
        Mean episode terrain_level: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.69s
                        Total time: 254.75s
                               ETA: 602 mins 19.4 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 138631 steps/s (collection: 0.568s, learning 0.141s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0131
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1246
       Mean episode rew_ang_vel_xy: -0.0403
          Mean episode rew_dof_acc: -0.1709
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1062
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3762
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0270
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.71s
                        Total time: 255.46s
                               ETA: 602 mins 16.0 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 142169 steps/s (collection: 0.547s, learning 0.145s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1251
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1685
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1034
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3727
      Mean episode rew_stand_still: -0.0015
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.69s
                        Total time: 256.15s
                               ETA: 602 mins 10.2 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 143382 steps/s (collection: 0.549s, learning 0.137s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1267
       Mean episode rew_ang_vel_xy: -0.0388
          Mean episode rew_dof_acc: -0.1671
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1060
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3819
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0285
        Mean episode terrain_level: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.69s
                        Total time: 256.84s
                               ETA: 602 mins 3.5 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 139139 steps/s (collection: 0.562s, learning 0.145s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0142
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1257
       Mean episode rew_ang_vel_xy: -0.0393
          Mean episode rew_dof_acc: -0.1688
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1033
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3717
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.71s
                        Total time: 257.55s
                               ETA: 601 mins 59.8 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 136571 steps/s (collection: 0.579s, learning 0.140s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0149
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1257
       Mean episode rew_ang_vel_xy: -0.0395
          Mean episode rew_dof_acc: -0.1672
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1098
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3766
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0273
        Mean episode terrain_level: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.72s
                        Total time: 258.27s
                               ETA: 601 mins 58.0 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 143714 steps/s (collection: 0.544s, learning 0.140s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0139
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.63
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1251
       Mean episode rew_ang_vel_xy: -0.0389
          Mean episode rew_dof_acc: -0.1641
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1051
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0032
       Mean episode rew_smoothness: -0.3754
      Mean episode rew_stand_still: -0.0012
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0254
        Mean episode terrain_level: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.68s
                        Total time: 258.95s
                               ETA: 601 mins 51.2 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 138015 steps/s (collection: 0.566s, learning 0.146s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0150
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1298
       Mean episode rew_ang_vel_xy: -0.0402
          Mean episode rew_dof_acc: -0.1703
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1042
           Mean episode rew_no_fly: 0.0045
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3874
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.71s
                        Total time: 259.66s
                               ETA: 601 mins 48.4 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 137428 steps/s (collection: 0.573s, learning 0.142s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0112
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1299
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1708
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1075
           Mean episode rew_no_fly: 0.0046
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3854
      Mean episode rew_stand_still: -0.0014
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0279
        Mean episode terrain_level: 0.0349
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.72s
                        Total time: 260.38s
                               ETA: 601 mins 46.0 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 136875 steps/s (collection: 0.578s, learning 0.140s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0155
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1311
       Mean episode rew_ang_vel_xy: -0.0396
          Mean episode rew_dof_acc: -0.1689
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0032
        Mean episode rew_lin_vel_z: -0.1034
           Mean episode rew_no_fly: 0.0043
      Mean episode rew_orientation: -0.0034
       Mean episode rew_smoothness: -0.3924
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0306
        Mean episode terrain_level: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.72s
                        Total time: 261.10s
                               ETA: 601 mins 44.0 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 129945 steps/s (collection: 0.609s, learning 0.147s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0154
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.64
               Mean reward (total): -4.00
                Mean reward (task): -4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.1276
       Mean episode rew_ang_vel_xy: -0.0390
          Mean episode rew_dof_acc: -0.1671
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0031
        Mean episode rew_lin_vel_z: -0.1090
           Mean episode rew_no_fly: 0.0044
      Mean episode rew_orientation: -0.0033
       Mean episode rew_smoothness: -0.3778
      Mean episode rew_stand_still: -0.0013
      Mean episode rew_termination: -0.1999
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0351
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.76s
                        Total time: 261.85s
                               ETA: 601 mins 47.3 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/tvwywjir0h2o3dvr5dz9x
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    