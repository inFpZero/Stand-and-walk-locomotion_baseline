swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240531_180517-377711d6
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run test_May31_18-05-17 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/4zo16fpi3g53rwu0j2f6k
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.81 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 20824 steps/s (collection: 4.456s, learning 0.264s)
               Value function loss: 0.0708
                    Surrogate loss: 0.0172
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0023
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0013
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0009
       Mean episode rew_smoothness: -0.0062
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0016
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 1.6658
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 4.72s
                        Total time: 4.72s
                               ETA: 3933 mins 48.1 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 144983 steps/s (collection: 0.553s, learning 0.125s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0251
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 1.1084
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.68s
                        Total time: 5.40s
                               ETA: 2249 mins 22.3 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 149904 steps/s (collection: 0.532s, learning 0.124s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0249
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0018
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.6642
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.66s
                        Total time: 6.05s
                               ETA: 1681 mins 42.2 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 128860 steps/s (collection: 0.624s, learning 0.139s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0252
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.3623
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.76s
                        Total time: 6.82s
                               ETA: 1420 mins 10.5 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 126964 steps/s (collection: 0.647s, learning 0.127s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.02
                Mean reward (task): 0.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0255
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.1890
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.77s
                        Total time: 7.59s
                               ETA: 1265 mins 9.1 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 134012 steps/s (collection: 0.609s, learning 0.124s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.02
                Mean reward (task): 0.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0258
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0578
           Mean episode rew_no_fly: 0.0018
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0995
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.73s
                        Total time: 8.33s
                               ETA: 1156 mins 8.5 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 114399 steps/s (collection: 0.734s, learning 0.126s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0249
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0541
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.86s
                        Total time: 9.18s
                               ETA: 1093 mins 14.7 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 133623 steps/s (collection: 0.613s, learning 0.122s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0246
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0562
           Mean episode rew_no_fly: 0.0018
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.74s
                        Total time: 9.92s
                               ETA: 1033 mins 11.6 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 122426 steps/s (collection: 0.675s, learning 0.128s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0255
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0571
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.80s
                        Total time: 10.72s
                               ETA: 992 mins 42.7 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 121332 steps/s (collection: 0.666s, learning 0.144s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0253
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.81s
                        Total time: 11.53s
                               ETA: 960 mins 55.6 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 133318 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0596
           Mean episode rew_no_fly: 0.0019
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.74s
                        Total time: 12.27s
                               ETA: 929 mins 24.1 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 136008 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0249
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0405
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0577
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.72s
                        Total time: 12.99s
                               ETA: 902 mins 7.0 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 131347 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.75s
                        Total time: 13.74s
                               ETA: 880 mins 40.3 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 112456 steps/s (collection: 0.736s, learning 0.138s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0243
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0561
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.87s
                        Total time: 14.62s
                               ETA: 869 mins 46.2 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 128971 steps/s (collection: 0.629s, learning 0.134s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.03
                Mean reward (task): 0.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0247
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0023
        Mean episode rew_lin_vel_z: -0.0544
           Mean episode rew_no_fly: 0.0020
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.76s
                        Total time: 15.38s
                               ETA: 854 mins 6.1 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 118627 steps/s (collection: 0.694s, learning 0.135s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.02
                Mean reward (task): 0.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0246
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0568
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.83s
                        Total time: 16.21s
                               ETA: 843 mins 51.1 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 130379 steps/s (collection: 0.606s, learning 0.148s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0250
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0576
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.75s
                        Total time: 16.96s
                               ETA: 831 mins 8.7 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 133746 steps/s (collection: 0.613s, learning 0.122s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0243
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0403
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0554
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.74s
                        Total time: 17.70s
                               ETA: 818 mins 58.3 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 125993 steps/s (collection: 0.637s, learning 0.144s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0242
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0581
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.78s
                        Total time: 18.48s
                               ETA: 810 mins 3.6 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 134143 steps/s (collection: 0.608s, learning 0.125s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0240
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0544
           Mean episode rew_no_fly: 0.0021
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.73s
                        Total time: 19.21s
                               ETA: 800 mins 3.9 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 129210 steps/s (collection: 0.640s, learning 0.121s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0240
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0415
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0545
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.76s
                        Total time: 19.97s
                               ETA: 792 mins 7.8 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 119744 steps/s (collection: 0.686s, learning 0.135s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0237
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0537
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.82s
                        Total time: 20.79s
                               ETA: 787 mins 11.5 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 122724 steps/s (collection: 0.658s, learning 0.143s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0238
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0560
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.80s
                        Total time: 21.59s
                               ETA: 781 mins 57.6 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 125539 steps/s (collection: 0.659s, learning 0.124s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0239
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0547
           Mean episode rew_no_fly: 0.0022
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.78s
                        Total time: 22.37s
                               ETA: 776 mins 32.5 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 127446 steps/s (collection: 0.636s, learning 0.135s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0239
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0537
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.77s
                        Total time: 23.15s
                               ETA: 771 mins 9.8 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 135191 steps/s (collection: 0.598s, learning 0.129s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0239
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0550
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.73s
                        Total time: 23.87s
                               ETA: 764 mins 47.0 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 127975 steps/s (collection: 0.646s, learning 0.122s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.77s
                        Total time: 24.64s
                               ETA: 760 mins 8.3 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 118531 steps/s (collection: 0.703s, learning 0.126s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0421
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0519
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.83s
                        Total time: 25.47s
                               ETA: 757 mins 38.7 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 135091 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.04
                Mean reward (task): 0.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0539
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.73s
                        Total time: 26.20s
                               ETA: 752 mins 24.2 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 133538 steps/s (collection: 0.593s, learning 0.143s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0026
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.74s
                        Total time: 26.93s
                               ETA: 747 mins 44.8 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 125769 steps/s (collection: 0.644s, learning 0.138s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0492
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.78s
                        Total time: 27.72s
                               ETA: 744 mins 36.6 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 134200 steps/s (collection: 0.611s, learning 0.121s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0023
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.73s
                        Total time: 28.45s
                               ETA: 740 mins 23.4 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 125507 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0513
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.78s
                        Total time: 29.23s
                               ETA: 737 mins 42.4 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 135719 steps/s (collection: 0.601s, learning 0.123s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0229
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.72s
                        Total time: 29.96s
                               ETA: 733 mins 44.1 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 110477 steps/s (collection: 0.737s, learning 0.153s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0510
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.89s
                        Total time: 30.85s
                               ETA: 733 mins 55.7 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 119829 steps/s (collection: 0.671s, learning 0.150s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0235
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0516
           Mean episode rew_no_fly: 0.0024
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0128
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.82s
                        Total time: 31.67s
                               ETA: 732 mins 30.3 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 127317 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0486
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.77s
                        Total time: 32.44s
                               ETA: 730 mins 4.2 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 123523 steps/s (collection: 0.660s, learning 0.136s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0501
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.80s
                        Total time: 33.23s
                               ETA: 728 mins 17.0 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 131864 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0434
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0493
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.75s
                        Total time: 33.98s
                               ETA: 725 mins 30.7 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 129187 steps/s (collection: 0.638s, learning 0.123s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0446
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0505
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.76s
                        Total time: 34.74s
                               ETA: 723 mins 12.1 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 136468 steps/s (collection: 0.598s, learning 0.123s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.72s
                        Total time: 35.46s
                               ETA: 720 mins 10.6 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 116503 steps/s (collection: 0.699s, learning 0.145s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0465
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.84s
                        Total time: 36.30s
                               ETA: 719 mins 44.7 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 128186 steps/s (collection: 0.643s, learning 0.124s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0233
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0443
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.77s
                        Total time: 37.07s
                               ETA: 717 mins 50.5 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 121728 steps/s (collection: 0.683s, learning 0.124s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0478
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.81s
                        Total time: 37.88s
                               ETA: 716 mins 47.7 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 133015 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0229
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0491
           Mean episode rew_no_fly: 0.0025
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.74s
                        Total time: 38.62s
                               ETA: 714 mins 31.5 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 116541 steps/s (collection: 0.707s, learning 0.137s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.05
                Mean reward (task): 0.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0481
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.84s
                        Total time: 39.46s
                               ETA: 714 mins 14.7 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 133406 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0229
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0451
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0487
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.74s
                        Total time: 40.20s
                               ETA: 712 mins 5.3 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 134933 steps/s (collection: 0.606s, learning 0.122s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.73s
                        Total time: 40.93s
                               ETA: 709 mins 52.5 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 136556 steps/s (collection: 0.598s, learning 0.122s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0477
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.72s
                        Total time: 41.65s
                               ETA: 707 mins 36.3 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 140120 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0512
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.70s
                        Total time: 42.35s
                               ETA: 705 mins 7.2 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 134839 steps/s (collection: 0.606s, learning 0.123s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0450
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.73s
                        Total time: 43.08s
                               ETA: 703 mins 10.9 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 130950 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0223
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0444
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0452
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.75s
                        Total time: 43.83s
                               ETA: 701 mins 39.8 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 128954 steps/s (collection: 0.630s, learning 0.132s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): 0.06
                Mean reward (task): 0.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0446
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0026
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.76s
                        Total time: 44.59s
                               ETA: 700 mins 23.0 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 129604 steps/s (collection: 0.632s, learning 0.126s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0463
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.76s
                        Total time: 45.35s
                               ETA: 699 mins 5.6 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 135092 steps/s (collection: 0.595s, learning 0.133s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0468
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.73s
                        Total time: 46.08s
                               ETA: 697 mins 22.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 136985 steps/s (collection: 0.590s, learning 0.128s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0231
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0480
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.72s
                        Total time: 46.79s
                               ETA: 695 mins 34.9 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 125622 steps/s (collection: 0.648s, learning 0.135s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0459
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.78s
                        Total time: 47.58s
                               ETA: 694 mins 47.6 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 120956 steps/s (collection: 0.661s, learning 0.152s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0232
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0475
           Mean episode rew_no_fly: 0.0027
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0161
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.81s
                        Total time: 48.39s
                               ETA: 694 mins 27.8 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 132562 steps/s (collection: 0.599s, learning 0.142s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0444
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.74s
                        Total time: 49.13s
                               ETA: 693 mins 8.5 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 126396 steps/s (collection: 0.652s, learning 0.125s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0446
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.78s
                        Total time: 49.91s
                               ETA: 692 mins 21.9 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 134301 steps/s (collection: 0.609s, learning 0.123s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0460
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.73s
                        Total time: 50.64s
                               ETA: 690 mins 59.3 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 122061 steps/s (collection: 0.674s, learning 0.131s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0442
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.81s
                        Total time: 51.45s
                               ETA: 690 mins 38.5 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 134950 steps/s (collection: 0.606s, learning 0.122s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): 0.08
                Mean reward (task): 0.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0456
           Mean episode rew_no_fly: 0.0028
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.73s
                        Total time: 52.17s
                               ETA: 689 mins 17.3 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 125151 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0449
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.79s
                        Total time: 52.96s
                               ETA: 688 mins 43.2 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 116796 steps/s (collection: 0.712s, learning 0.130s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.07
                Mean reward (task): 0.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0458
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0447
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.84s
                        Total time: 53.80s
                               ETA: 688 mins 53.2 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 133925 steps/s (collection: 0.587s, learning 0.147s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): 0.09
                Mean reward (task): 0.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0452
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.73s
                        Total time: 54.54s
                               ETA: 687 mins 41.5 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 138653 steps/s (collection: 0.583s, learning 0.126s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0447
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.71s
                        Total time: 55.25s
                               ETA: 686 mins 13.3 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 139142 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0443
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0420
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.71s
                        Total time: 55.95s
                               ETA: 684 mins 45.7 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 134880 steps/s (collection: 0.579s, learning 0.150s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0416
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.73s
                        Total time: 56.68s
                               ETA: 683 mins 36.9 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 130340 steps/s (collection: 0.632s, learning 0.122s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0436
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.75s
                        Total time: 57.43s
                               ETA: 682 mins 48.1 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 125278 steps/s (collection: 0.661s, learning 0.124s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0461
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0442
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.78s
                        Total time: 58.22s
                               ETA: 682 mins 22.1 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 136739 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0416
           Mean episode rew_no_fly: 0.0029
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.72s
                        Total time: 58.94s
                               ETA: 681 mins 11.2 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 127903 steps/s (collection: 0.625s, learning 0.143s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0422
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.77s
                        Total time: 59.71s
                               ETA: 680 mins 36.2 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 124547 steps/s (collection: 0.649s, learning 0.140s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0420
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.79s
                        Total time: 60.50s
                               ETA: 680 mins 16.1 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 137220 steps/s (collection: 0.586s, learning 0.130s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0426
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.72s
                        Total time: 61.21s
                               ETA: 679 mins 7.9 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 128248 steps/s (collection: 0.629s, learning 0.138s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0459
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0429
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.77s
                        Total time: 61.98s
                               ETA: 678 mins 34.5 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 132610 steps/s (collection: 0.618s, learning 0.123s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0464
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0448
           Mean episode rew_no_fly: 0.0030
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.74s
                        Total time: 62.72s
                               ETA: 677 mins 45.6 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 132912 steps/s (collection: 0.618s, learning 0.122s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0440
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.74s
                        Total time: 63.46s
                               ETA: 676 mins 56.8 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 125668 steps/s (collection: 0.641s, learning 0.142s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0427
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.78s
                        Total time: 64.24s
                               ETA: 676 mins 36.2 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 122335 steps/s (collection: 0.651s, learning 0.152s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0464
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0413
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.80s
                        Total time: 65.05s
                               ETA: 676 mins 29.3 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 138754 steps/s (collection: 0.579s, learning 0.129s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0403
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.71s
                        Total time: 65.75s
                               ETA: 675 mins 24.1 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 136209 steps/s (collection: 0.595s, learning 0.127s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.10
                Mean reward (task): 0.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0407
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.72s
                        Total time: 66.48s
                               ETA: 674 mins 28.4 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 133828 steps/s (collection: 0.605s, learning 0.130s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0417
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.73s
                        Total time: 67.21s
                               ETA: 673 mins 41.8 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 138750 steps/s (collection: 0.569s, learning 0.139s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0465
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0442
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.71s
                        Total time: 67.92s
                               ETA: 672 mins 40.8 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 126719 steps/s (collection: 0.620s, learning 0.155s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0466
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0435
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.78s
                        Total time: 68.69s
                               ETA: 672 mins 20.8 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 119824 steps/s (collection: 0.695s, learning 0.125s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0465
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0435
           Mean episode rew_no_fly: 0.0031
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.82s
                        Total time: 69.52s
                               ETA: 672 mins 27.1 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 133850 steps/s (collection: 0.611s, learning 0.123s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0462
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0431
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.73s
                        Total time: 70.25s
                               ETA: 671 mins 43.9 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 128761 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0439
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.76s
                        Total time: 71.01s
                               ETA: 671 mins 18.1 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 113560 steps/s (collection: 0.726s, learning 0.140s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0460
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0423
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.87s
                        Total time: 71.88s
                               ETA: 671 mins 50.2 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 120178 steps/s (collection: 0.677s, learning 0.141s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0458
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0415
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.82s
                        Total time: 72.70s
                               ETA: 671 mins 55.1 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 140717 steps/s (collection: 0.576s, learning 0.122s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0036
        Mean episode rew_lin_vel_z: -0.0408
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.70s
                        Total time: 73.40s
                               ETA: 670 mins 54.5 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 141599 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0412
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.69s
                        Total time: 74.09s
                               ETA: 669 mins 52.7 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 127707 steps/s (collection: 0.645s, learning 0.124s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0420
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.77s
                        Total time: 74.86s
                               ETA: 669 mins 32.8 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 132255 steps/s (collection: 0.600s, learning 0.144s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0428
           Mean episode rew_no_fly: 0.0032
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.74s
                        Total time: 75.60s
                               ETA: 668 mins 59.3 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 125354 steps/s (collection: 0.635s, learning 0.149s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0458
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0410
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.78s
                        Total time: 76.39s
                               ETA: 668 mins 48.0 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 121455 steps/s (collection: 0.668s, learning 0.142s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0420
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.81s
                        Total time: 77.20s
                               ETA: 668 mins 49.9 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 139984 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): 0.12
                Mean reward (task): 0.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0455
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0417
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.70s
                        Total time: 77.90s
                               ETA: 667 mins 56.7 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 141942 steps/s (collection: 0.571s, learning 0.122s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0475
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0439
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.69s
                        Total time: 78.59s
                               ETA: 666 mins 59.6 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 130117 steps/s (collection: 0.634s, learning 0.122s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0408
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.76s
                        Total time: 79.35s
                               ETA: 666 mins 35.4 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 127890 steps/s (collection: 0.648s, learning 0.121s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0453
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0410
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.77s
                        Total time: 80.12s
                               ETA: 666 mins 18.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 139608 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0426
           Mean episode rew_no_fly: 0.0033
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.70s
                        Total time: 80.82s
                               ETA: 665 mins 29.5 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 138410 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0382
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.71s
                        Total time: 81.53s
                               ETA: 664 mins 44.7 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 145046 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0418
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.68s
                        Total time: 82.21s
                               ETA: 663 mins 45.0 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 130674 steps/s (collection: 0.614s, learning 0.138s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0386
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0222
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.75s
                        Total time: 82.96s
                               ETA: 663 mins 22.2 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 140105 steps/s (collection: 0.578s, learning 0.124s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0378
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.70s
                        Total time: 83.66s
                               ETA: 662 mins 35.8 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 137643 steps/s (collection: 0.587s, learning 0.127s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0458
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0415
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.71s
                        Total time: 84.38s
                               ETA: 661 mins 56.1 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 126346 steps/s (collection: 0.650s, learning 0.128s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.11
                Mean reward (task): 0.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0451
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0414
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0225
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.78s
                        Total time: 85.15s
                               ETA: 661 mins 46.9 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 130927 steps/s (collection: 0.619s, learning 0.131s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0456
   Mean episode rew_dof_pos_limits: -0.0038
        Mean episode rew_lin_vel_z: -0.0403
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.75s
                        Total time: 85.90s
                               ETA: 661 mins 25.4 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 130761 steps/s (collection: 0.612s, learning 0.140s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0399
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0230
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.75s
                        Total time: 86.66s
                               ETA: 661 mins 4.6 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 138488 steps/s (collection: 0.579s, learning 0.131s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): 0.13
                Mean reward (task): 0.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0457
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0424
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.71s
                        Total time: 87.37s
                               ETA: 660 mins 25.2 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 138024 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.16
                Mean reward (task): 0.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0454
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0418
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.71s
                        Total time: 88.08s
                               ETA: 659 mins 47.5 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 136696 steps/s (collection: 0.594s, learning 0.125s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0442
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0401
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0236
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.72s
                        Total time: 88.80s
                               ETA: 659 mins 13.6 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 120423 steps/s (collection: 0.687s, learning 0.129s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.14
                Mean reward (task): 0.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0390
           Mean episode rew_no_fly: 0.0034
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0235
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.82s
                        Total time: 89.61s
                               ETA: 659 mins 23.2 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 141994 steps/s (collection: 0.562s, learning 0.130s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0451
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0383
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0242
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.69s
                        Total time: 90.31s
                               ETA: 658 mins 38.3 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 142602 steps/s (collection: 0.568s, learning 0.122s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0452
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0405
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.69s
                        Total time: 91.00s
                               ETA: 657 mins 52.9 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 130544 steps/s (collection: 0.624s, learning 0.129s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0402
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.75s
                        Total time: 91.75s
                               ETA: 657 mins 35.7 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 136016 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0448
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0403
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.72s
                        Total time: 92.47s
                               ETA: 657 mins 5.8 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 134988 steps/s (collection: 0.604s, learning 0.124s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0394
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0240
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.73s
                        Total time: 93.20s
                               ETA: 656 mins 38.8 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 138339 steps/s (collection: 0.581s, learning 0.130s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0449
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0372
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0241
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.71s
                        Total time: 93.91s
                               ETA: 656 mins 4.8 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 133394 steps/s (collection: 0.597s, learning 0.140s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0395
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.74s
                        Total time: 94.65s
                               ETA: 655 mins 42.3 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 120465 steps/s (collection: 0.660s, learning 0.156s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0364
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0243
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.82s
                        Total time: 95.46s
                               ETA: 655 mins 52.7 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 132368 steps/s (collection: 0.617s, learning 0.125s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0372
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0244
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.74s
                        Total time: 96.21s
                               ETA: 655 mins 33.0 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 121586 steps/s (collection: 0.667s, learning 0.142s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0378
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.81s
                        Total time: 97.01s
                               ETA: 655 mins 40.3 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 137586 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0361
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0245
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.71s
                        Total time: 97.73s
                               ETA: 655 mins 9.7 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 135053 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.15
                Mean reward (task): 0.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0040
        Mean episode rew_lin_vel_z: -0.0379
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0246
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.73s
                        Total time: 98.46s
                               ETA: 654 mins 44.8 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 141065 steps/s (collection: 0.572s, learning 0.125s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0435
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0381
           Mean episode rew_no_fly: 0.0035
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0247
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.70s
                        Total time: 99.15s
                               ETA: 654 mins 8.1 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 123622 steps/s (collection: 0.661s, learning 0.134s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0444
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0385
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.80s
                        Total time: 99.95s
                               ETA: 654 mins 10.6 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 125489 steps/s (collection: 0.660s, learning 0.123s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0374
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0248
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.78s
                        Total time: 100.73s
                               ETA: 654 mins 8.4 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 124709 steps/s (collection: 0.652s, learning 0.136s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0391
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0251
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.79s
                        Total time: 101.52s
                               ETA: 654 mins 8.1 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 133955 steps/s (collection: 0.591s, learning 0.143s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0386
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.73s
                        Total time: 102.25s
                               ETA: 653 mins 46.9 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 128259 steps/s (collection: 0.644s, learning 0.123s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0448
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0378
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0252
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.77s
                        Total time: 103.02s
                               ETA: 653 mins 38.5 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 134353 steps/s (collection: 0.588s, learning 0.144s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.18
                Mean reward (task): 0.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0440
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0379
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0255
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.73s
                        Total time: 103.75s
                               ETA: 653 mins 17.0 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 135656 steps/s (collection: 0.601s, learning 0.124s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0362
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0259
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.72s
                        Total time: 104.48s
                               ETA: 652 mins 53.2 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 138346 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0372
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0253
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.71s
                        Total time: 105.19s
                               ETA: 652 mins 24.5 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 115549 steps/s (collection: 0.712s, learning 0.138s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.19
                Mean reward (task): 0.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0388
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0262
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.85s
                        Total time: 106.04s
                               ETA: 652 mins 48.1 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 139785 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): 0.22
                Mean reward (task): 0.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0377
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0257
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.70s
                        Total time: 106.74s
                               ETA: 652 mins 17.1 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 142602 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0378
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.69s
                        Total time: 107.43s
                               ETA: 651 mins 41.6 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 129697 steps/s (collection: 0.615s, learning 0.143s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0445
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0386
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0256
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.76s
                        Total time: 108.19s
                               ETA: 651 mins 31.3 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 124460 steps/s (collection: 0.649s, learning 0.141s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0398
           Mean episode rew_no_fly: 0.0036
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0258
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.79s
                        Total time: 108.98s
                               ETA: 651 mins 32.6 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 136453 steps/s (collection: 0.595s, learning 0.125s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.20
                Mean reward (task): 0.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0367
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0260
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.72s
                        Total time: 109.70s
                               ETA: 651 mins 9.2 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 129986 steps/s (collection: 0.634s, learning 0.122s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.17
                Mean reward (task): 0.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0437
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0371
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0261
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.76s
                        Total time: 110.46s
                               ETA: 650 mins 58.8 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 132785 steps/s (collection: 0.619s, learning 0.121s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0110
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0438
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0358
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0266
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.74s
                        Total time: 111.20s
                               ETA: 650 mins 42.9 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 144034 steps/s (collection: 0.561s, learning 0.121s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0108
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0447
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0384
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0268
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.68s
                        Total time: 111.88s
                               ETA: 650 mins 7.0 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 139898 steps/s (collection: 0.576s, learning 0.126s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.21
                Mean reward (task): 0.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0349
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0267
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.70s
                        Total time: 112.58s
                               ETA: 649 mins 38.7 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 138199 steps/s (collection: 0.578s, learning 0.133s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0369
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.71s
                        Total time: 113.29s
                               ETA: 649 mins 13.6 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 140482 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0361
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.70s
                        Total time: 113.99s
                               ETA: 648 mins 45.0 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 131556 steps/s (collection: 0.614s, learning 0.134s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0441
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0378
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0269
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.75s
                        Total time: 114.74s
                               ETA: 648 mins 32.9 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 128138 steps/s (collection: 0.622s, learning 0.146s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0434
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0360
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0270
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.77s
                        Total time: 115.51s
                               ETA: 648 mins 27.6 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 133723 steps/s (collection: 0.609s, learning 0.126s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0432
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0357
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.74s
                        Total time: 116.24s
                               ETA: 648 mins 11.6 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 134606 steps/s (collection: 0.585s, learning 0.146s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0341
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.73s
                        Total time: 116.97s
                               ETA: 647 mins 54.3 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 140721 steps/s (collection: 0.577s, learning 0.122s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0343
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.70s
                        Total time: 117.67s
                               ETA: 647 mins 26.7 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 126808 steps/s (collection: 0.649s, learning 0.127s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0328
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0271
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.78s
                        Total time: 118.45s
                               ETA: 647 mins 24.6 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 137295 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0354
           Mean episode rew_no_fly: 0.0037
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0272
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.72s
                        Total time: 119.16s
                               ETA: 647 mins 3.2 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 144129 steps/s (collection: 0.560s, learning 0.122s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.24
                Mean reward (task): 0.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0337
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.68s
                        Total time: 119.84s
                               ETA: 646 mins 31.1 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 138659 steps/s (collection: 0.568s, learning 0.141s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0429
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0358
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.71s
                        Total time: 120.55s
                               ETA: 646 mins 8.1 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 129489 steps/s (collection: 0.636s, learning 0.123s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0436
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0350
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0275
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.76s
                        Total time: 121.31s
                               ETA: 646 mins 1.3 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 129394 steps/s (collection: 0.622s, learning 0.138s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0356
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.76s
                        Total time: 122.07s
                               ETA: 645 mins 54.9 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 120204 steps/s (collection: 0.673s, learning 0.145s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0424
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0351
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0278
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.82s
                        Total time: 122.89s
                               ETA: 646 mins 6.8 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 136511 steps/s (collection: 0.587s, learning 0.133s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0342
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0280
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.72s
                        Total time: 123.61s
                               ETA: 645 mins 47.9 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 118536 steps/s (collection: 0.687s, learning 0.143s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0363
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0274
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.83s
                        Total time: 124.44s
                               ETA: 646 mins 3.3 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 129830 steps/s (collection: 0.631s, learning 0.126s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0431
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0280
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.76s
                        Total time: 125.20s
                               ETA: 645 mins 56.2 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 118091 steps/s (collection: 0.695s, learning 0.138s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.25
                Mean reward (task): 0.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0423
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0345
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.83s
                        Total time: 126.03s
                               ETA: 646 mins 12.3 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 133693 steps/s (collection: 0.597s, learning 0.138s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0427
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0336
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0280
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.74s
                        Total time: 126.76s
                               ETA: 645 mins 58.5 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 142091 steps/s (collection: 0.568s, learning 0.123s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0348
           Mean episode rew_no_fly: 0.0038
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0283
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.69s
                        Total time: 127.46s
                               ETA: 645 mins 31.6 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 133245 steps/s (collection: 0.588s, learning 0.149s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0330
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.74s
                        Total time: 128.19s
                               ETA: 645 mins 18.9 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 124110 steps/s (collection: 0.658s, learning 0.134s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0424
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0332
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0284
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.79s
                        Total time: 128.98s
                               ETA: 645 mins 22.7 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 120188 steps/s (collection: 0.680s, learning 0.138s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0376
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.82s
                        Total time: 129.80s
                               ETA: 645 mins 34.1 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 127498 steps/s (collection: 0.613s, learning 0.158s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0292
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.77s
                        Total time: 130.57s
                               ETA: 645 mins 31.5 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 119277 steps/s (collection: 0.680s, learning 0.144s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0426
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0359
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.82s
                        Total time: 131.40s
                               ETA: 645 mins 44.5 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 114923 steps/s (collection: 0.692s, learning 0.163s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0344
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0289
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.86s
                        Total time: 132.25s
                               ETA: 646 mins 6.6 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 135774 steps/s (collection: 0.587s, learning 0.137s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0362
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.72s
                        Total time: 132.98s
                               ETA: 645 mins 50.1 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 143582 steps/s (collection: 0.560s, learning 0.125s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.23
                Mean reward (task): 0.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0425
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0350
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0287
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.68s
                        Total time: 133.66s
                               ETA: 645 mins 22.4 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 131900 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0330
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0293
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.75s
                        Total time: 134.41s
                               ETA: 645 mins 12.4 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 139783 steps/s (collection: 0.581s, learning 0.122s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0349
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.70s
                        Total time: 135.11s
                               ETA: 644 mins 50.6 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 141275 steps/s (collection: 0.574s, learning 0.122s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0342
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.70s
                        Total time: 135.81s
                               ETA: 644 mins 26.8 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 135334 steps/s (collection: 0.597s, learning 0.129s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0416
   Mean episode rew_dof_pos_limits: -0.0045
        Mean episode rew_lin_vel_z: -0.0325
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.73s
                        Total time: 136.53s
                               ETA: 644 mins 12.0 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 136307 steps/s (collection: 0.588s, learning 0.134s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0411
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0337
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0297
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.72s
                        Total time: 137.25s
                               ETA: 643 mins 55.9 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 138814 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0342
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0299
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.71s
                        Total time: 137.96s
                               ETA: 643 mins 36.2 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 138238 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0349
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.71s
                        Total time: 138.67s
                               ETA: 643 mins 17.7 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 142800 steps/s (collection: 0.565s, learning 0.124s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0421
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0299
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.69s
                        Total time: 139.36s
                               ETA: 642 mins 53.0 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 142612 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0418
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0357
           Mean episode rew_no_fly: 0.0039
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0291
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.69s
                        Total time: 140.05s
                               ETA: 642 mins 28.9 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 137488 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0354
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.72s
                        Total time: 140.77s
                               ETA: 642 mins 12.0 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 136069 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.26
                Mean reward (task): 0.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0419
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0352
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.72s
                        Total time: 141.49s
                               ETA: 641 mins 57.3 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 139380 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0359
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0303
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.71s
                        Total time: 142.19s
                               ETA: 641 mins 38.2 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 133107 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0428
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0355
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0299
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.74s
                        Total time: 142.93s
                               ETA: 641 mins 28.2 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 140436 steps/s (collection: 0.576s, learning 0.123s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0336
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.70s
                        Total time: 143.63s
                               ETA: 641 mins 8.0 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 143662 steps/s (collection: 0.563s, learning 0.121s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.29
                Mean reward (task): 0.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0420
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0343
           Mean episode rew_no_fly: 0.0040
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0302
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.68s
                        Total time: 144.32s
                               ETA: 640 mins 43.8 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 141970 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.69s
                        Total time: 145.01s
                               ETA: 640 mins 22.0 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 141903 steps/s (collection: 0.569s, learning 0.124s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.65
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0313
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.69s
                        Total time: 145.70s
                               ETA: 640 mins 0.5 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 120182 steps/s (collection: 0.676s, learning 0.142s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0414
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0319
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.82s
                        Total time: 146.52s
                               ETA: 640 mins 12.1 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 129846 steps/s (collection: 0.631s, learning 0.126s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0417
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0321
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0306
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.76s
                        Total time: 147.28s
                               ETA: 640 mins 7.6 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 139807 steps/s (collection: 0.581s, learning 0.122s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0327
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0309
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.70s
                        Total time: 147.98s
                               ETA: 639 mins 49.2 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 144593 steps/s (collection: 0.557s, learning 0.123s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0048
        Mean episode rew_lin_vel_z: -0.0334
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.68s
                        Total time: 148.66s
                               ETA: 639 mins 25.0 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 143995 steps/s (collection: 0.559s, learning 0.124s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0320
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0311
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.68s
                        Total time: 149.34s
                               ETA: 639 mins 1.8 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 133893 steps/s (collection: 0.592s, learning 0.142s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0309
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.73s
                        Total time: 150.08s
                               ETA: 638 mins 51.9 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 124812 steps/s (collection: 0.649s, learning 0.139s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.27
                Mean reward (task): 0.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0337
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0308
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.79s
                        Total time: 150.86s
                               ETA: 638 mins 55.7 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 134080 steps/s (collection: 0.605s, learning 0.128s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.73s
                        Total time: 151.60s
                               ETA: 638 mins 45.7 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 139834 steps/s (collection: 0.582s, learning 0.121s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.70s
                        Total time: 152.30s
                               ETA: 638 mins 28.2 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 131078 steps/s (collection: 0.619s, learning 0.131s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0312
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.75s
                        Total time: 153.05s
                               ETA: 638 mins 22.6 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 138912 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.64
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0310
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.71s
                        Total time: 153.76s
                               ETA: 638 mins 6.5 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 120544 steps/s (collection: 0.691s, learning 0.124s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0378
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0316
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.82s
                        Total time: 154.57s
                               ETA: 638 mins 17.3 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 122888 steps/s (collection: 0.670s, learning 0.130s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0308
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0319
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.80s
                        Total time: 155.37s
                               ETA: 638 mins 24.2 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 136822 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0311
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0317
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.72s
                        Total time: 156.09s
                               ETA: 638 mins 11.0 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 132771 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0050
        Mean episode rew_lin_vel_z: -0.0327
           Mean episode rew_no_fly: 0.0041
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0318
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.74s
                        Total time: 156.83s
                               ETA: 638 mins 3.3 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 116279 steps/s (collection: 0.717s, learning 0.128s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0335
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0315
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.85s
                        Total time: 157.68s
                               ETA: 638 mins 21.1 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 123971 steps/s (collection: 0.666s, learning 0.127s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.79s
                        Total time: 158.47s
                               ETA: 638 mins 26.1 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 117745 steps/s (collection: 0.656s, learning 0.179s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0098
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0316
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.83s
                        Total time: 159.31s
                               ETA: 638 mins 41.1 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 126721 steps/s (collection: 0.648s, learning 0.127s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0322
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.78s
                        Total time: 160.08s
                               ETA: 638 mins 41.8 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 121531 steps/s (collection: 0.675s, learning 0.134s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0396
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.81s
                        Total time: 160.89s
                               ETA: 638 mins 50.4 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 114593 steps/s (collection: 0.696s, learning 0.162s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.32
                Mean reward (task): 0.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0294
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.86s
                        Total time: 161.75s
                               ETA: 639 mins 10.5 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 120102 steps/s (collection: 0.663s, learning 0.155s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0326
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.82s
                        Total time: 162.57s
                               ETA: 639 mins 21.1 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 120094 steps/s (collection: 0.675s, learning 0.144s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0308
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0323
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.82s
                        Total time: 163.39s
                               ETA: 639 mins 31.6 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 123007 steps/s (collection: 0.650s, learning 0.150s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0323
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.80s
                        Total time: 164.18s
                               ETA: 639 mins 37.5 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 123701 steps/s (collection: 0.657s, learning 0.138s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0405
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0293
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.79s
                        Total time: 164.98s
                               ETA: 639 mins 42.3 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 121995 steps/s (collection: 0.657s, learning 0.149s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.81s
                        Total time: 165.78s
                               ETA: 639 mins 49.6 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 140562 steps/s (collection: 0.563s, learning 0.136s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0323
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0321
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.70s
                        Total time: 166.48s
                               ETA: 639 mins 32.3 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 124575 steps/s (collection: 0.665s, learning 0.124s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0042
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.79s
                        Total time: 167.27s
                               ETA: 639 mins 35.7 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 122398 steps/s (collection: 0.671s, learning 0.132s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.80s
                        Total time: 168.08s
                               ETA: 639 mins 42.4 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 123648 steps/s (collection: 0.673s, learning 0.122s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.63
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.80s
                        Total time: 168.87s
                               ETA: 639 mins 47.0 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 142758 steps/s (collection: 0.566s, learning 0.123s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0319
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.69s
                        Total time: 169.56s
                               ETA: 639 mins 27.6 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 127936 steps/s (collection: 0.645s, learning 0.123s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.77s
                        Total time: 170.33s
                               ETA: 639 mins 26.3 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 130854 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.75s
                        Total time: 171.08s
                               ETA: 639 mins 21.2 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 121698 steps/s (collection: 0.659s, learning 0.149s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0322
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.81s
                        Total time: 171.89s
                               ETA: 639 mins 28.7 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 134797 steps/s (collection: 0.585s, learning 0.144s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0099
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.73s
                        Total time: 172.62s
                               ETA: 639 mins 18.7 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 130656 steps/s (collection: 0.616s, learning 0.137s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.30
                Mean reward (task): 0.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0338
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0329
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.75s
                        Total time: 173.37s
                               ETA: 639 mins 13.9 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 133329 steps/s (collection: 0.606s, learning 0.132s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0308
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.74s
                        Total time: 174.11s
                               ETA: 639 mins 5.8 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 133942 steps/s (collection: 0.602s, learning 0.132s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0310
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0332
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.73s
                        Total time: 174.84s
                               ETA: 638 mins 57.0 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 138054 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0338
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.71s
                        Total time: 175.55s
                               ETA: 638 mins 43.6 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 144285 steps/s (collection: 0.560s, learning 0.121s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0328
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.68s
                        Total time: 176.23s
                               ETA: 638 mins 23.5 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 122951 steps/s (collection: 0.676s, learning 0.124s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0307
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.80s
                        Total time: 177.03s
                               ETA: 638 mins 29.2 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 128276 steps/s (collection: 0.644s, learning 0.122s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.77s
                        Total time: 177.80s
                               ETA: 638 mins 27.7 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 114971 steps/s (collection: 0.711s, learning 0.144s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0340
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0330
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.86s
                        Total time: 178.65s
                               ETA: 638 mins 45.3 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 133989 steps/s (collection: 0.581s, learning 0.153s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0329
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.73s
                        Total time: 179.39s
                               ETA: 638 mins 36.7 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 115301 steps/s (collection: 0.686s, learning 0.167s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0324
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0327
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.85s
                        Total time: 180.24s
                               ETA: 638 mins 53.6 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 123106 steps/s (collection: 0.650s, learning 0.149s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0332
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.80s
                        Total time: 181.04s
                               ETA: 638 mins 58.8 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 134809 steps/s (collection: 0.606s, learning 0.123s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.28
                Mean reward (task): 0.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0043
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0325
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.73s
                        Total time: 181.77s
                               ETA: 638 mins 49.3 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 139525 steps/s (collection: 0.582s, learning 0.122s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0337
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.70s
                        Total time: 182.47s
                               ETA: 638 mins 34.8 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 137132 steps/s (collection: 0.580s, learning 0.137s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.72s
                        Total time: 183.19s
                               ETA: 638 mins 22.9 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 125150 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.79s
                        Total time: 183.98s
                               ETA: 638 mins 25.4 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 127355 steps/s (collection: 0.648s, learning 0.124s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.31
                Mean reward (task): 0.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0396
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0358
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0334
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.77s
                        Total time: 184.75s
                               ETA: 638 mins 25.1 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 119649 steps/s (collection: 0.676s, learning 0.145s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0393
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0309
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.82s
                        Total time: 185.57s
                               ETA: 638 mins 35.0 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 117544 steps/s (collection: 0.696s, learning 0.140s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0333
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.84s
                        Total time: 186.41s
                               ETA: 638 mins 47.9 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 137868 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.71s
                        Total time: 187.12s
                               ETA: 638 mins 35.4 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 133017 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.33
                Mean reward (task): 0.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0330
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.74s
                        Total time: 187.86s
                               ETA: 638 mins 28.3 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 136108 steps/s (collection: 0.599s, learning 0.124s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0326
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.72s
                        Total time: 188.58s
                               ETA: 638 mins 17.8 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 122525 steps/s (collection: 0.651s, learning 0.151s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.34
                Mean reward (task): 0.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.80s
                        Total time: 189.38s
                               ETA: 638 mins 23.7 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 127810 steps/s (collection: 0.646s, learning 0.123s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0395
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0309
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0336
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.77s
                        Total time: 190.15s
                               ETA: 638 mins 22.8 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 129026 steps/s (collection: 0.639s, learning 0.123s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.62
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0399
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0338
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0345
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.76s
                        Total time: 190.91s
                               ETA: 638 mins 20.4 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 126388 steps/s (collection: 0.638s, learning 0.140s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0044
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0340
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.78s
                        Total time: 191.69s
                               ETA: 638 mins 21.2 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 128954 steps/s (collection: 0.636s, learning 0.126s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0339
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.76s
                        Total time: 192.45s
                               ETA: 638 mins 18.9 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 138392 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0314
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.71s
                        Total time: 193.16s
                               ETA: 638 mins 6.4 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 125415 steps/s (collection: 0.641s, learning 0.142s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0308
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.78s
                        Total time: 193.95s
                               ETA: 638 mins 8.4 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 137509 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0400
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0333
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.71s
                        Total time: 194.66s
                               ETA: 637 mins 56.9 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 132525 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0384
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.74s
                        Total time: 195.40s
                               ETA: 637 mins 50.7 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 140842 steps/s (collection: 0.560s, learning 0.138s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0294
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.70s
                        Total time: 196.10s
                               ETA: 637 mins 36.0 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 120312 steps/s (collection: 0.647s, learning 0.170s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0331
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0352
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.82s
                        Total time: 196.92s
                               ETA: 637 mins 44.6 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 140152 steps/s (collection: 0.568s, learning 0.134s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0293
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.70s
                        Total time: 197.62s
                               ETA: 637 mins 30.7 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 140375 steps/s (collection: 0.577s, learning 0.124s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0303
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0357
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.70s
                        Total time: 198.32s
                               ETA: 637 mins 16.7 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 144288 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0384
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0325
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0350
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.68s
                        Total time: 199.00s
                               ETA: 636 mins 59.1 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 136896 steps/s (collection: 0.597s, learning 0.121s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0384
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0303
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0346
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.72s
                        Total time: 199.72s
                               ETA: 636 mins 48.8 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 140911 steps/s (collection: 0.575s, learning 0.122s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.70s
                        Total time: 200.42s
                               ETA: 636 mins 34.6 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 117558 steps/s (collection: 0.672s, learning 0.165s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0374
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0309
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0351
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.84s
                        Total time: 201.25s
                               ETA: 636 mins 46.8 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 134379 steps/s (collection: 0.574s, learning 0.157s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.38
                Mean reward (task): 0.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0317
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0351
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.73s
                        Total time: 201.99s
                               ETA: 636 mins 39.1 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 134798 steps/s (collection: 0.607s, learning 0.123s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0384
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0337
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0100
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0347
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.73s
                        Total time: 202.71s
                               ETA: 636 mins 31.0 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 121822 steps/s (collection: 0.670s, learning 0.137s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0392
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0317
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.81s
                        Total time: 203.52s
                               ETA: 636 mins 37.6 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 137756 steps/s (collection: 0.592s, learning 0.122s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0298
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.71s
                        Total time: 204.24s
                               ETA: 636 mins 26.6 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 130466 steps/s (collection: 0.613s, learning 0.141s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0349
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.75s
                        Total time: 204.99s
                               ETA: 636 mins 23.2 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 124663 steps/s (collection: 0.634s, learning 0.155s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0379
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0307
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0101
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0342
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.79s
                        Total time: 205.78s
                               ETA: 636 mins 26.3 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 133743 steps/s (collection: 0.594s, learning 0.141s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.61
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.74s
                        Total time: 206.51s
                               ETA: 636 mins 19.5 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 132671 steps/s (collection: 0.600s, learning 0.141s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0356
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.74s
                        Total time: 207.25s
                               ETA: 636 mins 13.8 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 139495 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0315
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0355
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.70s
                        Total time: 207.96s
                               ETA: 636 mins 1.4 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 125196 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0354
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.79s
                        Total time: 208.74s
                               ETA: 636 mins 3.9 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 115025 steps/s (collection: 0.707s, learning 0.148s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0394
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0323
           Mean episode rew_no_fly: 0.0045
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.85s
                        Total time: 209.60s
                               ETA: 636 mins 19.1 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 121228 steps/s (collection: 0.669s, learning 0.142s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0360
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.81s
                        Total time: 210.41s
                               ETA: 636 mins 26.1 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 119685 steps/s (collection: 0.683s, learning 0.138s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0379
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0357
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.82s
                        Total time: 211.23s
                               ETA: 636 mins 35.0 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 120562 steps/s (collection: 0.692s, learning 0.123s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0378
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0349
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.82s
                        Total time: 212.05s
                               ETA: 636 mins 42.8 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 141320 steps/s (collection: 0.573s, learning 0.122s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0374
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0295
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0359
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.70s
                        Total time: 212.74s
                               ETA: 636 mins 28.9 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 131428 steps/s (collection: 0.624s, learning 0.124s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.39
                Mean reward (task): 0.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0316
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.75s
                        Total time: 213.49s
                               ETA: 636 mins 24.6 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 138363 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.71s
                        Total time: 214.20s
                               ETA: 636 mins 13.6 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 124153 steps/s (collection: 0.670s, learning 0.122s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.35
                Mean reward (task): 0.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0320
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0357
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.79s
                        Total time: 214.99s
                               ETA: 636 mins 17.1 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 135034 steps/s (collection: 0.595s, learning 0.133s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.36
                Mean reward (task): 0.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0301
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.73s
                        Total time: 215.72s
                               ETA: 636 mins 9.3 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 137393 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0372
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.72s
                        Total time: 216.43s
                               ETA: 635 mins 59.3 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 140250 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.70s
                        Total time: 217.14s
                               ETA: 635 mins 46.8 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 139732 steps/s (collection: 0.582s, learning 0.122s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0371
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0358
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.70s
                        Total time: 217.84s
                               ETA: 635 mins 34.9 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 114901 steps/s (collection: 0.717s, learning 0.138s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0376
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.86s
                        Total time: 218.69s
                               ETA: 635 mins 49.6 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 127699 steps/s (collection: 0.645s, learning 0.125s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0361
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.77s
                        Total time: 219.46s
                               ETA: 635 mins 49.2 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 117727 steps/s (collection: 0.700s, learning 0.135s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.37
                Mean reward (task): 0.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.84s
                        Total time: 220.30s
                               ETA: 636 mins 0.2 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 124828 steps/s (collection: 0.659s, learning 0.129s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.79s
                        Total time: 221.09s
                               ETA: 636 mins 2.9 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 134694 steps/s (collection: 0.607s, learning 0.123s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0364
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.73s
                        Total time: 221.82s
                               ETA: 635 mins 55.6 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 130158 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0378
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.76s
                        Total time: 222.57s
                               ETA: 635 mins 52.7 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 127665 steps/s (collection: 0.648s, learning 0.122s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0362
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.77s
                        Total time: 223.34s
                               ETA: 635 mins 52.4 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 138561 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0376
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.71s
                        Total time: 224.05s
                               ETA: 635 mins 41.7 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 130856 steps/s (collection: 0.625s, learning 0.126s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0295
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0371
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.75s
                        Total time: 224.80s
                               ETA: 635 mins 38.2 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 129312 steps/s (collection: 0.619s, learning 0.141s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0385
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0313
           Mean episode rew_no_fly: 0.0046
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0356
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.76s
                        Total time: 225.56s
                               ETA: 635 mins 36.3 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 119709 steps/s (collection: 0.680s, learning 0.142s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.82s
                        Total time: 226.38s
                               ETA: 635 mins 44.6 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 137748 steps/s (collection: 0.573s, learning 0.141s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0102
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0369
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.71s
                        Total time: 227.10s
                               ETA: 635 mins 34.8 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 131890 steps/s (collection: 0.586s, learning 0.159s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.75s
                        Total time: 227.84s
                               ETA: 635 mins 30.4 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 127576 steps/s (collection: 0.637s, learning 0.134s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0303
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0368
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.77s
                        Total time: 228.61s
                               ETA: 635 mins 30.2 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 121939 steps/s (collection: 0.671s, learning 0.135s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.81s
                        Total time: 229.42s
                               ETA: 635 mins 35.9 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 136888 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.72s
                        Total time: 230.14s
                               ETA: 635 mins 27.0 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 137451 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.40
                Mean reward (task): 0.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0103
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0374
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.72s
                        Total time: 230.85s
                               ETA: 635 mins 17.6 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 117799 steps/s (collection: 0.686s, learning 0.149s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0296
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0372
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.83s
                        Total time: 231.69s
                               ETA: 635 mins 28.0 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 138781 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0296
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0375
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.71s
                        Total time: 232.40s
                               ETA: 635 mins 17.6 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 121303 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.81s
                        Total time: 233.21s
                               ETA: 635 mins 23.9 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 131030 steps/s (collection: 0.607s, learning 0.143s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0375
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.75s
                        Total time: 233.96s
                               ETA: 635 mins 20.4 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 123879 steps/s (collection: 0.659s, learning 0.134s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0047
       Mean episode rew_smoothness: -0.0104
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0367
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.79s
                        Total time: 234.75s
                               ETA: 635 mins 23.9 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 125863 steps/s (collection: 0.642s, learning 0.139s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0373
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.78s
                        Total time: 235.53s
                               ETA: 635 mins 25.4 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 127416 steps/s (collection: 0.649s, learning 0.123s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0384
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.77s
                        Total time: 236.30s
                               ETA: 635 mins 25.3 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 128312 steps/s (collection: 0.618s, learning 0.148s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0371
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0371
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.77s
                        Total time: 237.07s
                               ETA: 635 mins 24.4 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 137154 steps/s (collection: 0.593s, learning 0.123s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0383
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.72s
                        Total time: 237.79s
                               ETA: 635 mins 15.5 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 139319 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0294
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0382
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.71s
                        Total time: 238.49s
                               ETA: 635 mins 4.9 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 117123 steps/s (collection: 0.688s, learning 0.151s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0377
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.84s
                        Total time: 239.33s
                               ETA: 635 mins 15.7 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 110719 steps/s (collection: 0.741s, learning 0.147s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0374
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0379
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.89s
                        Total time: 240.22s
                               ETA: 635 mins 34.1 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 115443 steps/s (collection: 0.708s, learning 0.144s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0390
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.85s
                        Total time: 241.07s
                               ETA: 635 mins 46.7 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 131289 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0296
           Mean episode rew_no_fly: 0.0048
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0382
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.75s
                        Total time: 241.82s
                               ETA: 635 mins 42.9 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 137998 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0387
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.71s
                        Total time: 242.53s
                               ETA: 635 mins 33.4 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 129796 steps/s (collection: 0.634s, learning 0.123s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0275
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0381
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.76s
                        Total time: 243.29s
                               ETA: 635 mins 31.1 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 136597 steps/s (collection: 0.594s, learning 0.126s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.43
                Mean reward (task): 0.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0057
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0105
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.72s
                        Total time: 244.01s
                               ETA: 635 mins 22.8 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 135031 steps/s (collection: 0.603s, learning 0.125s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0371
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0298
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.73s
                        Total time: 244.74s
                               ETA: 635 mins 15.9 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 129156 steps/s (collection: 0.638s, learning 0.123s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0371
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0284
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0384
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.76s
                        Total time: 245.50s
                               ETA: 635 mins 14.2 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 133446 steps/s (collection: 0.614s, learning 0.122s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0284
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0106
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.74s
                        Total time: 246.23s
                               ETA: 635 mins 8.7 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 138496 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0375
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0303
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0385
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.71s
                        Total time: 246.94s
                               ETA: 634 mins 59.1 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 117905 steps/s (collection: 0.692s, learning 0.142s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.42
                Mean reward (task): 0.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.83s
                        Total time: 247.78s
                               ETA: 635 mins 8.7 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 137364 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0281
           Mean episode rew_no_fly: 0.0049
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0389
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.72s
                        Total time: 248.49s
                               ETA: 634 mins 60.0 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 123576 steps/s (collection: 0.672s, learning 0.124s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.41
                Mean reward (task): 0.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.80s
                        Total time: 249.29s
                               ETA: 635 mins 3.6 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 130360 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.75s
                        Total time: 250.04s
                               ETA: 635 mins 0.8 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 124382 steps/s (collection: 0.653s, learning 0.137s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0296
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0402
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.79s
                        Total time: 250.83s
                               ETA: 635 mins 3.6 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 131978 steps/s (collection: 0.621s, learning 0.124s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0372
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0297
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.74s
                        Total time: 251.58s
                               ETA: 634 mins 59.5 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 131013 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.75s
                        Total time: 252.33s
                               ETA: 634 mins 56.2 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 138508 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0392
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.71s
                        Total time: 253.04s
                               ETA: 634 mins 46.8 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 139250 steps/s (collection: 0.584s, learning 0.122s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0397
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.71s
                        Total time: 253.74s
                               ETA: 634 mins 36.9 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 127557 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0394
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.77s
                        Total time: 254.51s
                               ETA: 634 mins 36.8 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 129161 steps/s (collection: 0.623s, learning 0.138s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0399
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.76s
                        Total time: 255.28s
                               ETA: 634 mins 35.2 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 129153 steps/s (collection: 0.607s, learning 0.154s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0393
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.76s
                        Total time: 256.04s
                               ETA: 634 mins 33.6 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 129367 steps/s (collection: 0.618s, learning 0.142s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0306
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.76s
                        Total time: 256.80s
                               ETA: 634 mins 31.9 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 129271 steps/s (collection: 0.622s, learning 0.138s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.76s
                        Total time: 257.56s
                               ETA: 634 mins 30.2 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 120215 steps/s (collection: 0.675s, learning 0.142s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0058
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0398
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.82s
                        Total time: 258.37s
                               ETA: 634 mins 37.0 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 138756 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0396
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.71s
                        Total time: 259.08s
                               ETA: 634 mins 27.6 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 121820 steps/s (collection: 0.661s, learning 0.146s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0406
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.81s
                        Total time: 259.89s
                               ETA: 634 mins 32.8 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 118069 steps/s (collection: 0.708s, learning 0.124s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0293
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0391
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.83s
                        Total time: 260.72s
                               ETA: 634 mins 41.7 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 123753 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.79s
                        Total time: 261.52s
                               ETA: 634 mins 44.9 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 138825 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0372
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.71s
                        Total time: 262.23s
                               ETA: 634 mins 35.6 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 133438 steps/s (collection: 0.597s, learning 0.139s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0374
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0299
           Mean episode rew_no_fly: 0.0050
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.74s
                        Total time: 262.96s
                               ETA: 634 mins 30.5 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 132730 steps/s (collection: 0.615s, learning 0.126s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0275
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0406
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.74s
                        Total time: 263.70s
                               ETA: 634 mins 26.0 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 135767 steps/s (collection: 0.600s, learning 0.124s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0400
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.72s
                        Total time: 264.43s
                               ETA: 634 mins 19.1 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 124872 steps/s (collection: 0.644s, learning 0.144s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.44
                Mean reward (task): 0.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0415
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.79s
                        Total time: 265.21s
                               ETA: 634 mins 21.3 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 129809 steps/s (collection: 0.629s, learning 0.129s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.49
                Mean reward (task): 0.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.76s
                        Total time: 265.97s
                               ETA: 634 mins 19.2 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 134700 steps/s (collection: 0.604s, learning 0.126s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0284
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0401
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.73s
                        Total time: 266.70s
                               ETA: 634 mins 13.2 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 114228 steps/s (collection: 0.722s, learning 0.139s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.86s
                        Total time: 267.56s
                               ETA: 634 mins 25.8 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 133208 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0051
       Mean episode rew_smoothness: -0.0107
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0404
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.74s
                        Total time: 268.30s
                               ETA: 634 mins 21.0 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 140154 steps/s (collection: 0.579s, learning 0.122s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.46
                Mean reward (task): 0.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0403
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.70s
                        Total time: 269.00s
                               ETA: 634 mins 11.0 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 128748 steps/s (collection: 0.639s, learning 0.125s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0409
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.76s
                        Total time: 269.76s
                               ETA: 634 mins 9.8 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 121539 steps/s (collection: 0.686s, learning 0.123s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0281
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0408
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.81s
                        Total time: 270.57s
                               ETA: 634 mins 15.0 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 128282 steps/s (collection: 0.639s, learning 0.127s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0420
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.77s
                        Total time: 271.34s
                               ETA: 634 mins 14.3 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 131464 steps/s (collection: 0.626s, learning 0.122s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.45
                Mean reward (task): 0.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0283
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.75s
                        Total time: 272.09s
                               ETA: 634 mins 10.9 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 131512 steps/s (collection: 0.614s, learning 0.133s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.75s
                        Total time: 272.84s
                               ETA: 634 mins 7.5 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 137447 steps/s (collection: 0.593s, learning 0.122s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.72s
                        Total time: 273.55s
                               ETA: 633 mins 59.6 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 122669 steps/s (collection: 0.654s, learning 0.147s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0411
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.80s
                        Total time: 274.35s
                               ETA: 634 mins 3.7 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 120078 steps/s (collection: 0.682s, learning 0.137s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0429
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.82s
                        Total time: 275.17s
                               ETA: 634 mins 10.2 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 126602 steps/s (collection: 0.640s, learning 0.136s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.78s
                        Total time: 275.95s
                               ETA: 634 mins 10.8 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 119472 steps/s (collection: 0.687s, learning 0.136s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0419
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.82s
                        Total time: 276.77s
                               ETA: 634 mins 17.7 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 139516 steps/s (collection: 0.581s, learning 0.124s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.70s
                        Total time: 277.47s
                               ETA: 634 mins 8.5 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 123339 steps/s (collection: 0.655s, learning 0.142s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0300
           Mean episode rew_no_fly: 0.0052
       Mean episode rew_smoothness: -0.0108
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0413
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.80s
                        Total time: 278.27s
                               ETA: 634 mins 11.9 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 131808 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0281
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.75s
                        Total time: 279.02s
                               ETA: 634 mins 8.3 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 128463 steps/s (collection: 0.636s, learning 0.129s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0381
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0305
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.77s
                        Total time: 279.78s
                               ETA: 634 mins 7.3 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 113090 steps/s (collection: 0.736s, learning 0.134s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0110
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0425
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.87s
                        Total time: 280.65s
                               ETA: 634 mins 20.5 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 133844 steps/s (collection: 0.592s, learning 0.143s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0298
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0428
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.73s
                        Total time: 281.39s
                               ETA: 634 mins 15.3 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 118717 steps/s (collection: 0.686s, learning 0.142s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.83s
                        Total time: 282.21s
                               ETA: 634 mins 22.8 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 134711 steps/s (collection: 0.589s, learning 0.140s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.73s
                        Total time: 282.94s
                               ETA: 634 mins 17.1 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 120455 steps/s (collection: 0.670s, learning 0.146s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0109
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0414
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.82s
                        Total time: 283.76s
                               ETA: 634 mins 22.9 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 109293 steps/s (collection: 0.731s, learning 0.168s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.48
                Mean reward (task): 0.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0426
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.90s
                        Total time: 284.66s
                               ETA: 634 mins 39.9 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 126786 steps/s (collection: 0.639s, learning 0.137s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.78s
                        Total time: 285.43s
                               ETA: 634 mins 40.2 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 132633 steps/s (collection: 0.599s, learning 0.142s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0283
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.74s
                        Total time: 286.18s
                               ETA: 634 mins 36.0 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 130230 steps/s (collection: 0.625s, learning 0.130s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0431
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.75s
                        Total time: 286.93s
                               ETA: 634 mins 33.6 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 117364 steps/s (collection: 0.697s, learning 0.141s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0433
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.84s
                        Total time: 287.77s
                               ETA: 634 mins 42.1 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 119358 steps/s (collection: 0.687s, learning 0.136s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0418
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.82s
                        Total time: 288.59s
                               ETA: 634 mins 48.8 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 134641 steps/s (collection: 0.601s, learning 0.129s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0419
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.73s
                        Total time: 289.32s
                               ETA: 634 mins 43.1 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 136999 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0421
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.72s
                        Total time: 290.04s
                               ETA: 634 mins 35.7 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 130494 steps/s (collection: 0.630s, learning 0.123s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.47
                Mean reward (task): 0.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0416
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.75s
                        Total time: 290.79s
                               ETA: 634 mins 33.1 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 132828 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0284
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0421
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.74s
                        Total time: 291.53s
                               ETA: 634 mins 28.8 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 127166 steps/s (collection: 0.651s, learning 0.122s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0424
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.77s
                        Total time: 292.31s
                               ETA: 634 mins 28.8 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 115934 steps/s (collection: 0.720s, learning 0.127s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0376
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0423
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.85s
                        Total time: 293.15s
                               ETA: 634 mins 38.5 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 136427 steps/s (collection: 0.598s, learning 0.123s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.72s
                        Total time: 293.87s
                               ETA: 634 mins 31.7 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 112285 steps/s (collection: 0.734s, learning 0.141s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0102
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0367
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0429
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.88s
                        Total time: 294.75s
                               ETA: 634 mins 44.9 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 125639 steps/s (collection: 0.633s, learning 0.150s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.78s
                        Total time: 295.53s
                               ETA: 634 mins 46.1 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 128746 steps/s (collection: 0.629s, learning 0.134s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0285
           Mean episode rew_no_fly: 0.0053
       Mean episode rew_smoothness: -0.0111
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0426
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.76s
                        Total time: 296.30s
                               ETA: 634 mins 44.8 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 131625 steps/s (collection: 0.626s, learning 0.121s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.75s
                        Total time: 297.04s
                               ETA: 634 mins 41.3 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 128501 steps/s (collection: 0.643s, learning 0.122s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.77s
                        Total time: 297.81s
                               ETA: 634 mins 40.2 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 135899 steps/s (collection: 0.593s, learning 0.130s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0435
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.72s
                        Total time: 298.53s
                               ETA: 634 mins 33.8 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 115495 steps/s (collection: 0.719s, learning 0.132s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.85s
                        Total time: 299.38s
                               ETA: 634 mins 43.7 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 132103 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.74s
                        Total time: 300.13s
                               ETA: 634 mins 40.0 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 137800 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.71s
                        Total time: 300.84s
                               ETA: 634 mins 32.4 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 122346 steps/s (collection: 0.674s, learning 0.130s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.80s
                        Total time: 301.64s
                               ETA: 634 mins 36.1 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 122133 steps/s (collection: 0.681s, learning 0.124s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0112
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0434
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.80s
                        Total time: 302.45s
                               ETA: 634 mins 40.1 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 116842 steps/s (collection: 0.702s, learning 0.139s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0442
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.84s
                        Total time: 303.29s
                               ETA: 634 mins 48.6 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 134407 steps/s (collection: 0.608s, learning 0.124s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.73s
                        Total time: 304.02s
                               ETA: 634 mins 43.2 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 135307 steps/s (collection: 0.604s, learning 0.122s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0441
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.73s
                        Total time: 304.75s
                               ETA: 634 mins 37.3 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 124323 steps/s (collection: 0.664s, learning 0.127s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0283
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.79s
                        Total time: 305.54s
                               ETA: 634 mins 39.4 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 131172 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.75s
                        Total time: 306.29s
                               ETA: 634 mins 36.4 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 117666 steps/s (collection: 0.707s, learning 0.128s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.52
                Mean reward (task): 0.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0295
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0442
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.84s
                        Total time: 307.12s
                               ETA: 634 mins 44.0 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 135508 steps/s (collection: 0.600s, learning 0.125s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0432
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.73s
                        Total time: 307.85s
                               ETA: 634 mins 38.0 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 135881 steps/s (collection: 0.602s, learning 0.121s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0451
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.72s
                        Total time: 308.57s
                               ETA: 634 mins 31.8 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 128491 steps/s (collection: 0.626s, learning 0.139s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.51
                Mean reward (task): 0.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0298
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0437
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.77s
                        Total time: 309.34s
                               ETA: 634 mins 30.7 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 127306 steps/s (collection: 0.650s, learning 0.122s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0427
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.77s
                        Total time: 310.11s
                               ETA: 634 mins 30.5 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 127050 steps/s (collection: 0.648s, learning 0.126s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 42.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0054
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0440
        Mean episode terrain_level: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.77s
                        Total time: 310.88s
                               ETA: 634 mins 30.5 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 113136 steps/s (collection: 0.736s, learning 0.133s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.87s
                        Total time: 311.75s
                               ETA: 634 mins 42.1 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 120800 steps/s (collection: 0.682s, learning 0.132s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0292
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0439
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.81s
                        Total time: 312.57s
                               ETA: 634 mins 46.9 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 119898 steps/s (collection: 0.692s, learning 0.128s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0451
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.82s
                        Total time: 313.39s
                               ETA: 634 mins 52.5 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 124896 steps/s (collection: 0.666s, learning 0.122s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0104
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0287
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0440
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.79s
                        Total time: 314.17s
                               ETA: 634 mins 54.0 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 117499 steps/s (collection: 0.709s, learning 0.128s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0282
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.84s
                        Total time: 315.01s
                               ETA: 635 mins 1.5 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 109462 steps/s (collection: 0.775s, learning 0.123s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.90s
                        Total time: 315.91s
                               ETA: 635 mins 16.4 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 117226 steps/s (collection: 0.704s, learning 0.135s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0113
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0440
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.84s
                        Total time: 316.75s
                               ETA: 635 mins 24.0 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 121104 steps/s (collection: 0.680s, learning 0.132s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0451
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.81s
                        Total time: 317.56s
                               ETA: 635 mins 28.4 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 127235 steps/s (collection: 0.626s, learning 0.146s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.77s
                        Total time: 318.33s
                               ETA: 635 mins 28.1 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 136102 steps/s (collection: 0.599s, learning 0.124s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.72s
                        Total time: 319.05s
                               ETA: 635 mins 21.8 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 21486 steps/s (collection: 4.420s, learning 0.155s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0446
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 4.58s
                        Total time: 323.63s
                               ETA: 642 mins 54.7 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 71819 steps/s (collection: 1.234s, learning 0.134s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0446
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.37s
                        Total time: 325.00s
                               ETA: 644 mins 4.2 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 129938 steps/s (collection: 0.633s, learning 0.124s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0458
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.76s
                        Total time: 325.75s
                               ETA: 644 mins 0.7 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 131717 steps/s (collection: 0.601s, learning 0.146s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.75s
                        Total time: 326.50s
                               ETA: 643 mins 56.0 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 120033 steps/s (collection: 0.696s, learning 0.123s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.50
                Mean reward (task): 0.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0274
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0449
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.82s
                        Total time: 327.32s
                               ETA: 643 mins 59.9 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 112860 steps/s (collection: 0.744s, learning 0.127s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0443
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.87s
                        Total time: 328.19s
                               ETA: 644 mins 9.9 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 133103 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0452
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.74s
                        Total time: 328.93s
                               ETA: 644 mins 4.3 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 126163 steps/s (collection: 0.651s, learning 0.128s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.78s
                        Total time: 329.71s
                               ETA: 644 mins 3.5 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 117168 steps/s (collection: 0.716s, learning 0.123s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0448
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.84s
                        Total time: 330.55s
                               ETA: 644 mins 9.7 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 132775 steps/s (collection: 0.616s, learning 0.125s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0466
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.74s
                        Total time: 331.29s
                               ETA: 644 mins 4.3 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 129568 steps/s (collection: 0.634s, learning 0.125s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.76s
                        Total time: 332.04s
                               ETA: 644 mins 1.1 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 131527 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.53
                Mean reward (task): 0.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 43.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0055
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0454
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.75s
                        Total time: 332.79s
                               ETA: 643 mins 56.6 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 117255 steps/s (collection: 0.697s, learning 0.142s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0460
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.84s
                        Total time: 333.63s
                               ETA: 644 mins 2.7 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 128757 steps/s (collection: 0.641s, learning 0.122s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0453
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.76s
                        Total time: 334.39s
                               ETA: 644 mins 0.1 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 123336 steps/s (collection: 0.665s, learning 0.132s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0456
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.80s
                        Total time: 335.19s
                               ETA: 644 mins 1.3 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 122342 steps/s (collection: 0.672s, learning 0.132s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0138
 Mean episode rew_tracking_lin_vel: 0.0456
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.80s
                        Total time: 335.99s
                               ETA: 644 mins 3.3 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 127401 steps/s (collection: 0.647s, learning 0.124s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0463
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.77s
                        Total time: 336.77s
                               ETA: 644 mins 1.6 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 121215 steps/s (collection: 0.674s, learning 0.137s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0466
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.81s
                        Total time: 337.58s
                               ETA: 644 mins 4.4 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 116429 steps/s (collection: 0.699s, learning 0.145s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0056
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0451
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.84s
                        Total time: 338.42s
                               ETA: 644 mins 11.0 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 118414 steps/s (collection: 0.691s, learning 0.140s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0459
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.83s
                        Total time: 339.25s
                               ETA: 644 mins 16.0 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 119214 steps/s (collection: 0.660s, learning 0.165s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.82s
                        Total time: 340.08s
                               ETA: 644 mins 20.3 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 112793 steps/s (collection: 0.705s, learning 0.166s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0140
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.87s
                        Total time: 340.95s
                               ETA: 644 mins 29.9 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 127687 steps/s (collection: 0.630s, learning 0.140s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0464
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.77s
                        Total time: 341.72s
                               ETA: 644 mins 28.0 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 127274 steps/s (collection: 0.632s, learning 0.141s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.77s
                        Total time: 342.49s
                               ETA: 644 mins 26.3 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 119033 steps/s (collection: 0.661s, learning 0.165s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.83s
                        Total time: 343.32s
                               ETA: 644 mins 30.7 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 115827 steps/s (collection: 0.688s, learning 0.161s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0275
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0139
 Mean episode rew_tracking_lin_vel: 0.0462
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.85s
                        Total time: 344.16s
                               ETA: 644 mins 37.6 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 130584 steps/s (collection: 0.615s, learning 0.137s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.75s
                        Total time: 344.92s
                               ETA: 644 mins 33.7 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 118238 steps/s (collection: 0.693s, learning 0.138s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.83s
                        Total time: 345.75s
                               ETA: 644 mins 38.6 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 127042 steps/s (collection: 0.630s, learning 0.144s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.77s
                        Total time: 346.52s
                               ETA: 644 mins 37.1 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 111520 steps/s (collection: 0.724s, learning 0.158s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0143
 Mean episode rew_tracking_lin_vel: 0.0470
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.88s
                        Total time: 347.40s
                               ETA: 644 mins 47.6 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 124122 steps/s (collection: 0.632s, learning 0.160s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.55
                Mean reward (task): 0.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.79s
                        Total time: 348.20s
                               ETA: 644 mins 48.0 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 123403 steps/s (collection: 0.671s, learning 0.125s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0057
       Mean episode rew_smoothness: -0.0114
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0457
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.80s
                        Total time: 348.99s
                               ETA: 644 mins 49.0 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 118732 steps/s (collection: 0.693s, learning 0.135s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.56
                Mean reward (task): 0.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0471
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.83s
                        Total time: 349.82s
                               ETA: 644 mins 53.5 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 128941 steps/s (collection: 0.638s, learning 0.125s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.76s
                        Total time: 350.58s
                               ETA: 644 mins 50.6 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 122141 steps/s (collection: 0.655s, learning 0.150s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.54
                Mean reward (task): 0.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0279
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0478
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.80s
                        Total time: 351.39s
                               ETA: 644 mins 52.5 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 112193 steps/s (collection: 0.728s, learning 0.148s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0293
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0142
 Mean episode rew_tracking_lin_vel: 0.0468
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.88s
                        Total time: 352.26s
                               ETA: 645 mins 2.2 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 124278 steps/s (collection: 0.662s, learning 0.129s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0467
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.79s
                        Total time: 353.06s
                               ETA: 645 mins 2.5 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 125025 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0488
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.79s
                        Total time: 353.84s
                               ETA: 645 mins 2.3 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 137016 steps/s (collection: 0.593s, learning 0.124s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.72s
                        Total time: 354.56s
                               ETA: 644 mins 54.6 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 122601 steps/s (collection: 0.656s, learning 0.146s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.80s
                        Total time: 355.36s
                               ETA: 644 mins 56.1 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 126838 steps/s (collection: 0.642s, learning 0.133s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0281
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0141
 Mean episode rew_tracking_lin_vel: 0.0461
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.78s
                        Total time: 356.14s
                               ETA: 644 mins 54.6 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 118273 steps/s (collection: 0.704s, learning 0.128s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0473
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.83s
                        Total time: 356.97s
                               ETA: 644 mins 59.3 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 129430 steps/s (collection: 0.632s, learning 0.128s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0465
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.76s
                        Total time: 357.73s
                               ETA: 644 mins 56.2 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 124375 steps/s (collection: 0.658s, learning 0.133s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.79s
                        Total time: 358.52s
                               ETA: 644 mins 56.4 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 116684 steps/s (collection: 0.696s, learning 0.146s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0487
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.84s
                        Total time: 359.36s
                               ETA: 645 mins 2.2 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 126477 steps/s (collection: 0.637s, learning 0.141s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0476
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.78s
                        Total time: 360.14s
                               ETA: 645 mins 1.0 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 125782 steps/s (collection: 0.639s, learning 0.142s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0276
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.78s
                        Total time: 360.92s
                               ETA: 645 mins 0.3 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 121167 steps/s (collection: 0.661s, learning 0.151s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0480
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.81s
                        Total time: 361.73s
                               ETA: 645 mins 2.7 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 124061 steps/s (collection: 0.669s, learning 0.124s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0486
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.79s
                        Total time: 362.52s
                               ETA: 645 mins 3.1 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 130615 steps/s (collection: 0.625s, learning 0.128s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.75s
                        Total time: 363.27s
                               ETA: 644 mins 59.3 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 118121 steps/s (collection: 0.665s, learning 0.167s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.58
                Mean reward (task): 0.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0478
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.83s
                        Total time: 364.11s
                               ETA: 645 mins 3.9 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 133148 steps/s (collection: 0.597s, learning 0.141s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0484
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.74s
                        Total time: 364.85s
                               ETA: 644 mins 58.6 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 125436 steps/s (collection: 0.637s, learning 0.146s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0147
 Mean episode rew_tracking_lin_vel: 0.0474
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.78s
                        Total time: 365.63s
                               ETA: 644 mins 58.0 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 120105 steps/s (collection: 0.678s, learning 0.140s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0275
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0144
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.82s
                        Total time: 366.45s
                               ETA: 645 mins 1.2 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 121993 steps/s (collection: 0.661s, learning 0.145s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.57
                Mean reward (task): 0.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0058
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0475
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.81s
                        Total time: 367.25s
                               ETA: 645 mins 3.0 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 128928 steps/s (collection: 0.629s, learning 0.134s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0064
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0145
 Mean episode rew_tracking_lin_vel: 0.0479
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.76s
                        Total time: 368.02s
                               ETA: 645 mins 0.2 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 116995 steps/s (collection: 0.717s, learning 0.123s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.84s
                        Total time: 368.86s
                               ETA: 645 mins 5.6 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 125135 steps/s (collection: 0.655s, learning 0.131s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0492
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.79s
                        Total time: 369.64s
                               ETA: 645 mins 5.3 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 113048 steps/s (collection: 0.730s, learning 0.140s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 44.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0280
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0146
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.87s
                        Total time: 370.51s
                               ETA: 645 mins 13.7 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 126647 steps/s (collection: 0.645s, learning 0.132s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.60
                Mean reward (task): 0.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0483
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.78s
                        Total time: 371.29s
                               ETA: 645 mins 12.3 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 111576 steps/s (collection: 0.734s, learning 0.147s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0482
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.88s
                        Total time: 372.17s
                               ETA: 645 mins 21.9 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 130028 steps/s (collection: 0.621s, learning 0.135s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0486
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.76s
                        Total time: 372.92s
                               ETA: 645 mins 18.4 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 124790 steps/s (collection: 0.653s, learning 0.135s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0492
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.79s
                        Total time: 373.71s
                               ETA: 645 mins 18.3 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 125469 steps/s (collection: 0.660s, learning 0.124s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0059
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0481
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.78s
                        Total time: 374.50s
                               ETA: 645 mins 17.7 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 122215 steps/s (collection: 0.638s, learning 0.166s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.80s
                        Total time: 375.30s
                               ETA: 645 mins 19.2 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 119384 steps/s (collection: 0.675s, learning 0.149s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.82s
                        Total time: 376.12s
                               ETA: 645 mins 22.7 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 124406 steps/s (collection: 0.668s, learning 0.123s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0291
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0494
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.79s
                        Total time: 376.91s
                               ETA: 645 mins 22.8 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 125479 steps/s (collection: 0.643s, learning 0.140s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0491
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.78s
                        Total time: 377.70s
                               ETA: 645 mins 22.1 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 135451 steps/s (collection: 0.602s, learning 0.124s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.73s
                        Total time: 378.42s
                               ETA: 645 mins 15.6 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 121438 steps/s (collection: 0.651s, learning 0.159s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0148
 Mean episode rew_tracking_lin_vel: 0.0482
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.81s
                        Total time: 379.23s
                               ETA: 645 mins 17.6 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 135653 steps/s (collection: 0.600s, learning 0.125s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0501
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.72s
                        Total time: 379.96s
                               ETA: 645 mins 11.0 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 132228 steps/s (collection: 0.619s, learning 0.124s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0275
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0497
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.74s
                        Total time: 380.70s
                               ETA: 645 mins 6.3 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 121885 steps/s (collection: 0.674s, learning 0.132s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0487
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.81s
                        Total time: 381.51s
                               ETA: 645 mins 8.1 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 120010 steps/s (collection: 0.693s, learning 0.126s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0496
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.82s
                        Total time: 382.33s
                               ETA: 645 mins 11.1 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 122545 steps/s (collection: 0.673s, learning 0.129s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.80s
                        Total time: 383.13s
                               ETA: 645 mins 12.3 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 125845 steps/s (collection: 0.658s, learning 0.123s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.59
                Mean reward (task): 0.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0500
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.78s
                        Total time: 383.91s
                               ETA: 645 mins 11.5 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 117252 steps/s (collection: 0.701s, learning 0.138s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.84s
                        Total time: 384.75s
                               ETA: 645 mins 16.4 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 120132 steps/s (collection: 0.694s, learning 0.124s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.82s
                        Total time: 385.57s
                               ETA: 645 mins 19.3 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 111755 steps/s (collection: 0.741s, learning 0.139s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.88s
                        Total time: 386.45s
                               ETA: 645 mins 28.2 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 119291 steps/s (collection: 0.693s, learning 0.131s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0501
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.82s
                        Total time: 387.27s
                               ETA: 645 mins 31.6 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 130299 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.75s
                        Total time: 388.02s
                               ETA: 645 mins 28.1 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 125738 steps/s (collection: 0.645s, learning 0.137s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.78s
                        Total time: 388.81s
                               ETA: 645 mins 27.2 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 124512 steps/s (collection: 0.648s, learning 0.142s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0495
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.79s
                        Total time: 389.60s
                               ETA: 645 mins 27.2 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 119171 steps/s (collection: 0.666s, learning 0.158s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.82s
                        Total time: 390.42s
                               ETA: 645 mins 30.6 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 128961 steps/s (collection: 0.622s, learning 0.141s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.76s
                        Total time: 391.18s
                               ETA: 645 mins 27.8 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 120253 steps/s (collection: 0.644s, learning 0.174s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0065
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.82s
                        Total time: 392.00s
                               ETA: 645 mins 30.5 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 133164 steps/s (collection: 0.600s, learning 0.138s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0149
 Mean episode rew_tracking_lin_vel: 0.0498
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.74s
                        Total time: 392.74s
                               ETA: 645 mins 25.4 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 110919 steps/s (collection: 0.736s, learning 0.151s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0502
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.89s
                        Total time: 393.62s
                               ETA: 645 mins 34.8 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 101299 steps/s (collection: 0.825s, learning 0.145s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0267
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.97s
                        Total time: 394.59s
                               ETA: 645 mins 52.5 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 121230 steps/s (collection: 0.672s, learning 0.139s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0115
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.81s
                        Total time: 395.41s
                               ETA: 645 mins 54.5 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 127654 steps/s (collection: 0.647s, learning 0.124s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0060
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0499
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.77s
                        Total time: 396.18s
                               ETA: 645 mins 52.4 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 127968 steps/s (collection: 0.636s, learning 0.132s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 45.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0152
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.77s
                        Total time: 396.94s
                               ETA: 645 mins 50.2 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 123919 steps/s (collection: 0.650s, learning 0.144s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0513
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.79s
                        Total time: 397.74s
                               ETA: 645 mins 50.4 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 130303 steps/s (collection: 0.631s, learning 0.123s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0061
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.75s
                        Total time: 398.49s
                               ETA: 645 mins 46.9 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 121446 steps/s (collection: 0.667s, learning 0.142s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.81s
                        Total time: 399.30s
                               ETA: 645 mins 48.7 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 129719 steps/s (collection: 0.614s, learning 0.143s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0507
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.76s
                        Total time: 400.06s
                               ETA: 645 mins 45.4 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 119224 steps/s (collection: 0.663s, learning 0.162s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0154
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.82s
                        Total time: 400.88s
                               ETA: 645 mins 48.7 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 132232 steps/s (collection: 0.605s, learning 0.139s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0151
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.74s
                        Total time: 401.63s
                               ETA: 645 mins 44.1 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 128457 steps/s (collection: 0.641s, learning 0.124s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0503
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.77s
                        Total time: 402.39s
                               ETA: 645 mins 41.6 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 137262 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0510
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.72s
                        Total time: 403.11s
                               ETA: 645 mins 34.4 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 113700 steps/s (collection: 0.727s, learning 0.138s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.86s
                        Total time: 403.97s
                               ETA: 645 mins 41.5 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 124343 steps/s (collection: 0.650s, learning 0.141s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.79s
                        Total time: 404.76s
                               ETA: 645 mins 41.4 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 123648 steps/s (collection: 0.662s, learning 0.133s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0516
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.80s
                        Total time: 405.56s
                               ETA: 645 mins 41.8 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 124536 steps/s (collection: 0.649s, learning 0.140s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.79s
                        Total time: 406.35s
                               ETA: 645 mins 41.6 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 117209 steps/s (collection: 0.676s, learning 0.163s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0506
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.84s
                        Total time: 407.19s
                               ETA: 645 mins 46.2 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 119588 steps/s (collection: 0.681s, learning 0.141s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.82s
                        Total time: 408.01s
                               ETA: 645 mins 49.1 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 134150 steps/s (collection: 0.606s, learning 0.127s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0116
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0504
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.73s
                        Total time: 408.74s
                               ETA: 645 mins 43.5 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 123198 steps/s (collection: 0.668s, learning 0.130s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0062
       Mean episode rew_smoothness: -0.0117
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0507
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.80s
                        Total time: 409.54s
                               ETA: 645 mins 44.1 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 112825 steps/s (collection: 0.735s, learning 0.137s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0530
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.87s
                        Total time: 410.41s
                               ETA: 645 mins 51.7 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 115545 steps/s (collection: 0.715s, learning 0.136s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.85s
                        Total time: 411.26s
                               ETA: 645 mins 57.3 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 132820 steps/s (collection: 0.602s, learning 0.138s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.61
                Mean reward (task): 0.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0118
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.74s
                        Total time: 412.00s
                               ETA: 645 mins 52.4 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 121329 steps/s (collection: 0.679s, learning 0.132s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.81s
                        Total time: 412.81s
                               ETA: 645 mins 54.2 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 112129 steps/s (collection: 0.730s, learning 0.147s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.88s
                        Total time: 413.69s
                               ETA: 646 mins 2.1 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 129187 steps/s (collection: 0.631s, learning 0.130s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0119
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0153
 Mean episode rew_tracking_lin_vel: 0.0508
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.76s
                        Total time: 414.45s
                               ETA: 645 mins 59.2 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 135693 steps/s (collection: 0.600s, learning 0.125s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0524
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.72s
                        Total time: 415.17s
                               ETA: 645 mins 53.0 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 136322 steps/s (collection: 0.592s, learning 0.129s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.72s
                        Total time: 415.89s
                               ETA: 645 mins 46.4 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 122139 steps/s (collection: 0.667s, learning 0.138s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0505
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.80s
                        Total time: 416.70s
                               ETA: 645 mins 47.6 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 131215 steps/s (collection: 0.623s, learning 0.126s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.65
                Mean reward (task): 0.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.75s
                        Total time: 417.45s
                               ETA: 645 mins 43.7 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 115527 steps/s (collection: 0.706s, learning 0.144s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0515
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.85s
                        Total time: 418.30s
                               ETA: 645 mins 49.1 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 120046 steps/s (collection: 0.680s, learning 0.139s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0520
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.82s
                        Total time: 419.12s
                               ETA: 645 mins 51.6 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 135492 steps/s (collection: 0.603s, learning 0.122s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0157
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.73s
                        Total time: 419.84s
                               ETA: 645 mins 45.5 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 129646 steps/s (collection: 0.612s, learning 0.146s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0516
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.76s
                        Total time: 420.60s
                               ETA: 645 mins 42.4 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 125269 steps/s (collection: 0.653s, learning 0.132s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0155
 Mean episode rew_tracking_lin_vel: 0.0513
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.78s
                        Total time: 421.39s
                               ETA: 645 mins 41.8 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 123604 steps/s (collection: 0.658s, learning 0.137s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0527
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.80s
                        Total time: 422.18s
                               ETA: 645 mins 42.1 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 132687 steps/s (collection: 0.617s, learning 0.124s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0514
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.74s
                        Total time: 422.92s
                               ETA: 645 mins 37.4 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 125614 steps/s (collection: 0.638s, learning 0.145s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.78s
                        Total time: 423.71s
                               ETA: 645 mins 36.6 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 131303 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0529
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.75s
                        Total time: 424.45s
                               ETA: 645 mins 32.7 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 133181 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.64
                Mean reward (task): 0.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0156
 Mean episode rew_tracking_lin_vel: 0.0520
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.74s
                        Total time: 425.19s
                               ETA: 645 mins 27.8 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 117423 steps/s (collection: 0.692s, learning 0.145s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0526
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.84s
                        Total time: 426.03s
                               ETA: 645 mins 31.9 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 114653 steps/s (collection: 0.719s, learning 0.138s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0158
 Mean episode rew_tracking_lin_vel: 0.0523
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.86s
                        Total time: 426.89s
                               ETA: 645 mins 37.9 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 130711 steps/s (collection: 0.629s, learning 0.123s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.75s
                        Total time: 427.64s
                               ETA: 645 mins 34.3 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 115563 steps/s (collection: 0.728s, learning 0.123s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.85s
                        Total time: 428.49s
                               ETA: 645 mins 39.6 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 123687 steps/s (collection: 0.652s, learning 0.142s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.79s
                        Total time: 429.28s
                               ETA: 645 mins 39.8 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 138092 steps/s (collection: 0.589s, learning 0.122s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.62
                Mean reward (task): 0.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.71s
                        Total time: 430.00s
                               ETA: 645 mins 32.6 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 112526 steps/s (collection: 0.723s, learning 0.151s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0121
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0516
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.87s
                        Total time: 430.87s
                               ETA: 645 mins 40.0 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 120738 steps/s (collection: 0.688s, learning 0.126s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.81s
                        Total time: 431.68s
                               ETA: 645 mins 41.9 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 124357 steps/s (collection: 0.661s, learning 0.130s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0518
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.79s
                        Total time: 432.48s
                               ETA: 645 mins 41.8 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 131664 steps/s (collection: 0.605s, learning 0.142s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.67
                Mean reward (task): 0.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.75s
                        Total time: 433.22s
                               ETA: 645 mins 37.7 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 108086 steps/s (collection: 0.769s, learning 0.140s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.91s
                        Total time: 434.13s
                               ETA: 645 mins 48.2 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 116828 steps/s (collection: 0.686s, learning 0.156s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0163
 Mean episode rew_tracking_lin_vel: 0.0542
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.84s
                        Total time: 434.97s
                               ETA: 645 mins 52.5 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 123477 steps/s (collection: 0.658s, learning 0.138s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0123
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0519
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.80s
                        Total time: 435.77s
                               ETA: 645 mins 52.9 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 123427 steps/s (collection: 0.663s, learning 0.134s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0063
       Mean episode rew_smoothness: -0.0120
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.80s
                        Total time: 436.57s
                               ETA: 645 mins 53.2 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 135022 steps/s (collection: 0.604s, learning 0.124s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0161
 Mean episode rew_tracking_lin_vel: 0.0521
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.73s
                        Total time: 437.29s
                               ETA: 645 mins 47.5 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 132853 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0122
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0509
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.74s
                        Total time: 438.03s
                               ETA: 645 mins 42.8 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 126292 steps/s (collection: 0.657s, learning 0.122s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0536
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.78s
                        Total time: 438.81s
                               ETA: 645 mins 41.6 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 125799 steps/s (collection: 0.654s, learning 0.127s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0533
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.78s
                        Total time: 439.59s
                               ETA: 645 mins 40.6 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 119505 steps/s (collection: 0.682s, learning 0.141s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.82s
                        Total time: 440.42s
                               ETA: 645 mins 43.3 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 116918 steps/s (collection: 0.683s, learning 0.157s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0160
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.84s
                        Total time: 441.26s
                               ETA: 645 mins 47.5 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 122935 steps/s (collection: 0.629s, learning 0.171s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.63
                Mean reward (task): 0.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0261
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0159
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.80s
                        Total time: 442.06s
                               ETA: 645 mins 48.1 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 113559 steps/s (collection: 0.741s, learning 0.124s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.69
                Mean reward (task): 0.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.87s
                        Total time: 442.92s
                               ETA: 645 mins 54.5 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 127022 steps/s (collection: 0.641s, learning 0.133s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0530
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.77s
                        Total time: 443.70s
                               ETA: 645 mins 52.8 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 128715 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.76s
                        Total time: 444.46s
                               ETA: 645 mins 50.3 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 134412 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0539
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.73s
                        Total time: 445.19s
                               ETA: 645 mins 44.9 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 122739 steps/s (collection: 0.676s, learning 0.125s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0064
       Mean episode rew_smoothness: -0.0124
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0162
 Mean episode rew_tracking_lin_vel: 0.0525
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.80s
                        Total time: 445.99s
                               ETA: 645 mins 45.6 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 128660 steps/s (collection: 0.618s, learning 0.146s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0096
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.76s
                        Total time: 446.76s
                               ETA: 645 mins 43.1 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 116090 steps/s (collection: 0.678s, learning 0.169s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0097
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0544
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.85s
                        Total time: 447.60s
                               ETA: 645 mins 47.8 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 125340 steps/s (collection: 0.629s, learning 0.155s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.68
                Mean reward (task): 0.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0535
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.78s
                        Total time: 448.39s
                               ETA: 645 mins 47.0 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 119630 steps/s (collection: 0.679s, learning 0.142s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0125
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0530
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.82s
                        Total time: 449.21s
                               ETA: 645 mins 49.5 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 116473 steps/s (collection: 0.702s, learning 0.142s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0551
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.84s
                        Total time: 450.05s
                               ETA: 645 mins 53.9 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 117802 steps/s (collection: 0.667s, learning 0.167s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0537
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.83s
                        Total time: 450.89s
                               ETA: 645 mins 57.5 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 117931 steps/s (collection: 0.685s, learning 0.148s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0551
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.83s
                        Total time: 451.72s
                               ETA: 646 mins 0.9 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 130438 steps/s (collection: 0.617s, learning 0.137s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.66
                Mean reward (task): 0.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.75s
                        Total time: 452.47s
                               ETA: 645 mins 57.5 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 112138 steps/s (collection: 0.728s, learning 0.149s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.88s
                        Total time: 453.35s
                               ETA: 646 mins 4.6 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 118698 steps/s (collection: 0.686s, learning 0.142s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.83s
                        Total time: 454.18s
                               ETA: 646 mins 7.6 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 121888 steps/s (collection: 0.667s, learning 0.140s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0065
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.81s
                        Total time: 454.99s
                               ETA: 646 mins 8.7 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 122134 steps/s (collection: 0.664s, learning 0.140s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0548
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.80s
                        Total time: 455.79s
                               ETA: 646 mins 9.6 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 138121 steps/s (collection: 0.583s, learning 0.129s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.71s
                        Total time: 456.50s
                               ETA: 646 mins 2.7 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 139161 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0068
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0538
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.71s
                        Total time: 457.21s
                               ETA: 645 mins 55.3 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 137101 steps/s (collection: 0.594s, learning 0.123s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0066
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0545
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.72s
                        Total time: 457.93s
                               ETA: 645 mins 48.8 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 139962 steps/s (collection: 0.581s, learning 0.122s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.70s
                        Total time: 458.63s
                               ETA: 645 mins 41.1 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 139226 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0557
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.71s
                        Total time: 459.33s
                               ETA: 645 mins 33.8 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 138597 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.71s
                        Total time: 460.04s
                               ETA: 645 mins 26.7 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 136793 steps/s (collection: 0.594s, learning 0.125s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0554
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.72s
                        Total time: 460.76s
                               ETA: 645 mins 20.5 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 140120 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0555
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.70s
                        Total time: 461.46s
                               ETA: 645 mins 12.8 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 139185 steps/s (collection: 0.582s, learning 0.125s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0548
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.71s
                        Total time: 462.17s
                               ETA: 645 mins 5.5 s

################################################################################
                      Learning iteration 590/50000                      

                       Computation: 137775 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0564
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.71s
                        Total time: 462.88s
                               ETA: 644 mins 58.9 s

################################################################################
                      Learning iteration 591/50000                      

                       Computation: 139974 steps/s (collection: 0.574s, learning 0.128s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0067
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0166
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.70s
                        Total time: 463.59s
                               ETA: 644 mins 51.4 s

################################################################################
                      Learning iteration 592/50000                      

                       Computation: 138954 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0165
 Mean episode rew_tracking_lin_vel: 0.0548
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.71s
                        Total time: 464.29s
                               ETA: 644 mins 44.3 s

################################################################################
                      Learning iteration 593/50000                      

                       Computation: 138933 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.71s
                        Total time: 465.00s
                               ETA: 644 mins 37.2 s

################################################################################
                      Learning iteration 594/50000                      

                       Computation: 138143 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0565
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.71s
                        Total time: 465.71s
                               ETA: 644 mins 30.5 s

################################################################################
                      Learning iteration 595/50000                      

                       Computation: 137877 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.72
                Mean reward (task): 0.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.71s
                        Total time: 466.43s
                               ETA: 644 mins 24.0 s

################################################################################
                      Learning iteration 596/50000                      

                       Computation: 137357 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.72s
                        Total time: 467.14s
                               ETA: 644 mins 17.7 s

################################################################################
                      Learning iteration 597/50000                      

                       Computation: 136139 steps/s (collection: 0.597s, learning 0.125s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0556
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.72s
                        Total time: 467.86s
                               ETA: 644 mins 11.9 s

################################################################################
                      Learning iteration 598/50000                      

                       Computation: 137968 steps/s (collection: 0.588s, learning 0.124s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0167
 Mean episode rew_tracking_lin_vel: 0.0554
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.71s
                        Total time: 468.58s
                               ETA: 644 mins 5.3 s

################################################################################
                      Learning iteration 599/50000                      

                       Computation: 138881 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.71s
                        Total time: 469.28s
                               ETA: 643 mins 58.4 s

################################################################################
                      Learning iteration 600/50000                      

                       Computation: 137686 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0168
 Mean episode rew_tracking_lin_vel: 0.0553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.71s
                        Total time: 470.00s
                               ETA: 643 mins 52.1 s

################################################################################
                      Learning iteration 601/50000                      

                       Computation: 138769 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0559
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.71s
                        Total time: 470.71s
                               ETA: 643 mins 45.2 s

################################################################################
                      Learning iteration 602/50000                      

                       Computation: 140074 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0169
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.70s
                        Total time: 471.41s
                               ETA: 643 mins 37.9 s

################################################################################
                      Learning iteration 603/50000                      

                       Computation: 139216 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0552
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.71s
                        Total time: 472.11s
                               ETA: 643 mins 30.9 s

################################################################################
                      Learning iteration 604/50000                      

                       Computation: 136901 steps/s (collection: 0.593s, learning 0.125s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0164
 Mean episode rew_tracking_lin_vel: 0.0549
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.72s
                        Total time: 472.83s
                               ETA: 643 mins 24.9 s

################################################################################
                      Learning iteration 605/50000                      

                       Computation: 139165 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.71s
                        Total time: 473.54s
                               ETA: 643 mins 18.0 s

################################################################################
                      Learning iteration 606/50000                      

                       Computation: 138287 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0584
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.71s
                        Total time: 474.25s
                               ETA: 643 mins 11.5 s

################################################################################
                      Learning iteration 607/50000                      

                       Computation: 138926 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.71s
                        Total time: 474.96s
                               ETA: 643 mins 4.7 s

################################################################################
                      Learning iteration 608/50000                      

                       Computation: 134961 steps/s (collection: 0.604s, learning 0.125s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.73s
                        Total time: 475.68s
                               ETA: 642 mins 59.7 s

################################################################################
                      Learning iteration 609/50000                      

                       Computation: 136226 steps/s (collection: 0.597s, learning 0.124s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0127
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0560
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.72s
                        Total time: 476.41s
                               ETA: 642 mins 54.1 s

################################################################################
                      Learning iteration 610/50000                      

                       Computation: 137858 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.70
                Mean reward (task): 0.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0575
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.71s
                        Total time: 477.12s
                               ETA: 642 mins 47.8 s

################################################################################
                      Learning iteration 611/50000                      

                       Computation: 139091 steps/s (collection: 0.582s, learning 0.125s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0573
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.71s
                        Total time: 477.83s
                               ETA: 642 mins 41.1 s

################################################################################
                      Learning iteration 612/50000                      

                       Computation: 137708 steps/s (collection: 0.588s, learning 0.126s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0567
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.71s
                        Total time: 478.54s
                               ETA: 642 mins 34.9 s

################################################################################
                      Learning iteration 613/50000                      

                       Computation: 138410 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0094
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0126
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0561
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.71s
                        Total time: 479.25s
                               ETA: 642 mins 28.4 s

################################################################################
                      Learning iteration 614/50000                      

                       Computation: 138884 steps/s (collection: 0.587s, learning 0.121s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0068
       Mean episode rew_smoothness: -0.0128
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.71s
                        Total time: 479.96s
                               ETA: 642 mins 21.8 s

################################################################################
                      Learning iteration 615/50000                      

                       Computation: 139520 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.70s
                        Total time: 480.66s
                               ETA: 642 mins 15.0 s

################################################################################
                      Learning iteration 616/50000                      

                       Computation: 137310 steps/s (collection: 0.591s, learning 0.125s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 47.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0130
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.72s
                        Total time: 481.38s
                               ETA: 642 mins 9.0 s

################################################################################
                      Learning iteration 617/50000                      

                       Computation: 137588 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.71s
                        Total time: 482.09s
                               ETA: 642 mins 3.0 s

################################################################################
                      Learning iteration 618/50000                      

                       Computation: 139183 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0129
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.71s
                        Total time: 482.80s
                               ETA: 641 mins 56.3 s

################################################################################
                      Learning iteration 619/50000                      

                       Computation: 137786 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.71s
                        Total time: 483.51s
                               ETA: 641 mins 50.2 s

################################################################################
                      Learning iteration 620/50000                      

                       Computation: 137932 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.71
                Mean reward (task): 0.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0568
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.71s
                        Total time: 484.23s
                               ETA: 641 mins 44.1 s

################################################################################
                      Learning iteration 621/50000                      

                       Computation: 136723 steps/s (collection: 0.595s, learning 0.124s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.72s
                        Total time: 484.94s
                               ETA: 641 mins 38.5 s

################################################################################
                      Learning iteration 622/50000                      

                       Computation: 137290 steps/s (collection: 0.590s, learning 0.126s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.72s
                        Total time: 485.66s
                               ETA: 641 mins 32.7 s

################################################################################
                      Learning iteration 623/50000                      

                       Computation: 139358 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.71s
                        Total time: 486.37s
                               ETA: 641 mins 26.1 s

################################################################################
                      Learning iteration 624/50000                      

                       Computation: 139618 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0582
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.70s
                        Total time: 487.07s
                               ETA: 641 mins 19.3 s

################################################################################
                      Learning iteration 625/50000                      

                       Computation: 139166 steps/s (collection: 0.581s, learning 0.125s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0170
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.71s
                        Total time: 487.78s
                               ETA: 641 mins 12.8 s

################################################################################
                      Learning iteration 626/50000                      

                       Computation: 140817 steps/s (collection: 0.575s, learning 0.123s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0576
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.70s
                        Total time: 488.47s
                               ETA: 641 mins 5.6 s

################################################################################
                      Learning iteration 627/50000                      

                       Computation: 139531 steps/s (collection: 0.582s, learning 0.122s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0573
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.70s
                        Total time: 489.18s
                               ETA: 640 mins 59.0 s

################################################################################
                      Learning iteration 628/50000                      

                       Computation: 141662 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.69s
                        Total time: 489.87s
                               ETA: 640 mins 51.5 s

################################################################################
                      Learning iteration 629/50000                      

                       Computation: 139750 steps/s (collection: 0.579s, learning 0.124s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.70s
                        Total time: 490.58s
                               ETA: 640 mins 44.8 s

################################################################################
                      Learning iteration 630/50000                      

                       Computation: 138938 steps/s (collection: 0.585s, learning 0.123s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0131
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.71s
                        Total time: 491.28s
                               ETA: 640 mins 38.5 s

################################################################################
                      Learning iteration 631/50000                      

                       Computation: 139029 steps/s (collection: 0.583s, learning 0.124s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.71s
                        Total time: 491.99s
                               ETA: 640 mins 32.1 s

################################################################################
                      Learning iteration 632/50000                      

                       Computation: 137936 steps/s (collection: 0.588s, learning 0.125s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.71s
                        Total time: 492.70s
                               ETA: 640 mins 26.2 s

################################################################################
                      Learning iteration 633/50000                      

                       Computation: 133866 steps/s (collection: 0.592s, learning 0.142s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0595
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.73s
                        Total time: 493.44s
                               ETA: 640 mins 22.0 s

################################################################################
                      Learning iteration 634/50000                      

                       Computation: 134845 steps/s (collection: 0.606s, learning 0.123s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.73s
                        Total time: 494.17s
                               ETA: 640 mins 17.4 s

################################################################################
                      Learning iteration 635/50000                      

                       Computation: 137513 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.74
                Mean reward (task): 0.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0572
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.71s
                        Total time: 494.88s
                               ETA: 640 mins 11.7 s

################################################################################
                      Learning iteration 636/50000                      

                       Computation: 137800 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.71s
                        Total time: 495.60s
                               ETA: 640 mins 5.9 s

################################################################################
                      Learning iteration 637/50000                      

                       Computation: 124369 steps/s (collection: 0.643s, learning 0.148s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0578
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.79s
                        Total time: 496.39s
                               ETA: 640 mins 6.1 s

################################################################################
                      Learning iteration 638/50000                      

                       Computation: 133037 steps/s (collection: 0.613s, learning 0.126s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0585
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.74s
                        Total time: 497.12s
                               ETA: 640 mins 2.3 s

################################################################################
                      Learning iteration 639/50000                      

                       Computation: 130802 steps/s (collection: 0.614s, learning 0.137s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.75
                Mean reward (task): 0.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0578
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.75s
                        Total time: 497.88s
                               ETA: 639 mins 59.5 s

################################################################################
                      Learning iteration 640/50000                      

                       Computation: 139704 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.70s
                        Total time: 498.58s
                               ETA: 639 mins 53.0 s

################################################################################
                      Learning iteration 641/50000                      

                       Computation: 126390 steps/s (collection: 0.632s, learning 0.146s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.78s
                        Total time: 499.36s
                               ETA: 639 mins 52.2 s

################################################################################
                      Learning iteration 642/50000                      

                       Computation: 119925 steps/s (collection: 0.694s, learning 0.125s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.82s
                        Total time: 500.18s
                               ETA: 639 mins 54.6 s

################################################################################
                      Learning iteration 643/50000                      

                       Computation: 137654 steps/s (collection: 0.591s, learning 0.123s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0591
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.71s
                        Total time: 500.89s
                               ETA: 639 mins 49.0 s

################################################################################
                      Learning iteration 644/50000                      

                       Computation: 137896 steps/s (collection: 0.582s, learning 0.131s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0173
 Mean episode rew_tracking_lin_vel: 0.0577
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.71s
                        Total time: 501.60s
                               ETA: 639 mins 43.2 s

################################################################################
                      Learning iteration 645/50000                      

                       Computation: 115384 steps/s (collection: 0.685s, learning 0.167s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.85s
                        Total time: 502.46s
                               ETA: 639 mins 48.1 s

################################################################################
                      Learning iteration 646/50000                      

                       Computation: 135063 steps/s (collection: 0.587s, learning 0.141s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0594
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.73s
                        Total time: 503.18s
                               ETA: 639 mins 43.5 s

################################################################################
                      Learning iteration 647/50000                      

                       Computation: 111208 steps/s (collection: 0.718s, learning 0.165s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0069
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0171
 Mean episode rew_tracking_lin_vel: 0.0569
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.88s
                        Total time: 504.07s
                               ETA: 639 mins 50.9 s

################################################################################
                      Learning iteration 648/50000                      

                       Computation: 135940 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0597
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.72s
                        Total time: 504.79s
                               ETA: 639 mins 45.9 s

################################################################################
                      Learning iteration 649/50000                      

                       Computation: 132729 steps/s (collection: 0.617s, learning 0.124s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 46.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0132
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0590
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.74s
                        Total time: 505.53s
                               ETA: 639 mins 42.3 s

################################################################################
                      Learning iteration 650/50000                      

                       Computation: 140039 steps/s (collection: 0.581s, learning 0.121s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0133
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0584
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.70s
                        Total time: 506.23s
                               ETA: 639 mins 35.8 s

################################################################################
                      Learning iteration 651/50000                      

                       Computation: 133432 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0174
 Mean episode rew_tracking_lin_vel: 0.0581
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.74s
                        Total time: 506.97s
                               ETA: 639 mins 31.9 s

################################################################################
                      Learning iteration 652/50000                      

                       Computation: 138034 steps/s (collection: 0.590s, learning 0.123s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.71s
                        Total time: 507.68s
                               ETA: 639 mins 26.2 s

################################################################################
                      Learning iteration 653/50000                      

                       Computation: 135430 steps/s (collection: 0.604s, learning 0.122s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0069
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0579
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.73s
                        Total time: 508.41s
                               ETA: 639 mins 21.5 s

################################################################################
                      Learning iteration 654/50000                      

                       Computation: 136947 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0070
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0172
 Mean episode rew_tracking_lin_vel: 0.0571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.72s
                        Total time: 509.13s
                               ETA: 639 mins 16.3 s

################################################################################
                      Learning iteration 655/50000                      

                       Computation: 121294 steps/s (collection: 0.681s, learning 0.129s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0593
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.81s
                        Total time: 509.94s
                               ETA: 639 mins 18.0 s

################################################################################
                      Learning iteration 656/50000                      

                       Computation: 118753 steps/s (collection: 0.705s, learning 0.123s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0099
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0602
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.83s
                        Total time: 510.76s
                               ETA: 639 mins 21.0 s

################################################################################
                      Learning iteration 657/50000                      

                       Computation: 130944 steps/s (collection: 0.626s, learning 0.125s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0175
 Mean episode rew_tracking_lin_vel: 0.0583
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.75s
                        Total time: 511.52s
                               ETA: 639 mins 18.2 s

################################################################################
                      Learning iteration 658/50000                      

                       Computation: 120587 steps/s (collection: 0.676s, learning 0.139s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.82s
                        Total time: 512.33s
                               ETA: 639 mins 20.3 s

################################################################################
                      Learning iteration 659/50000                      

                       Computation: 121652 steps/s (collection: 0.676s, learning 0.132s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0134
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0177
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.81s
                        Total time: 513.14s
                               ETA: 639 mins 21.8 s

################################################################################
                      Learning iteration 660/50000                      

                       Computation: 129478 steps/s (collection: 0.609s, learning 0.151s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.76s
                        Total time: 513.90s
                               ETA: 639 mins 19.7 s

################################################################################
                      Learning iteration 661/50000                      

                       Computation: 128681 steps/s (collection: 0.631s, learning 0.133s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0596
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.76s
                        Total time: 514.66s
                               ETA: 639 mins 17.9 s

################################################################################
                      Learning iteration 662/50000                      

                       Computation: 119647 steps/s (collection: 0.699s, learning 0.122s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0178
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.82s
                        Total time: 515.48s
                               ETA: 639 mins 20.4 s

################################################################################
                      Learning iteration 663/50000                      

                       Computation: 138002 steps/s (collection: 0.588s, learning 0.124s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0176
 Mean episode rew_tracking_lin_vel: 0.0588
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.71s
                        Total time: 516.20s
                               ETA: 639 mins 14.8 s

################################################################################
                      Learning iteration 664/50000                      

                       Computation: 131190 steps/s (collection: 0.627s, learning 0.122s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.75s
                        Total time: 516.95s
                               ETA: 639 mins 11.9 s

################################################################################
                      Learning iteration 665/50000                      

                       Computation: 115174 steps/s (collection: 0.708s, learning 0.145s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.77
                Mean reward (task): 0.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0605
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.85s
                        Total time: 517.80s
                               ETA: 639 mins 16.8 s

################################################################################
                      Learning iteration 666/50000                      

                       Computation: 128421 steps/s (collection: 0.627s, learning 0.138s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0070
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0071
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0588
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 0.77s
                        Total time: 518.56s
                               ETA: 639 mins 15.1 s

################################################################################
                      Learning iteration 667/50000                      

                       Computation: 133377 steps/s (collection: 0.594s, learning 0.143s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 0.74s
                        Total time: 519.30s
                               ETA: 639 mins 11.3 s

################################################################################
                      Learning iteration 668/50000                      

                       Computation: 133696 steps/s (collection: 0.596s, learning 0.140s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 0.74s
                        Total time: 520.04s
                               ETA: 639 mins 7.5 s

################################################################################
                      Learning iteration 669/50000                      

                       Computation: 121584 steps/s (collection: 0.685s, learning 0.124s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0586
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 0.81s
                        Total time: 520.85s
                               ETA: 639 mins 9.0 s

################################################################################
                      Learning iteration 670/50000                      

                       Computation: 123578 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0588
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 0.80s
                        Total time: 521.64s
                               ETA: 639 mins 9.5 s

################################################################################
                      Learning iteration 671/50000                      

                       Computation: 138372 steps/s (collection: 0.588s, learning 0.123s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0615
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 0.71s
                        Total time: 522.35s
                               ETA: 639 mins 3.8 s

################################################################################
                      Learning iteration 672/50000                      

                       Computation: 134429 steps/s (collection: 0.608s, learning 0.124s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 0.73s
                        Total time: 523.08s
                               ETA: 638 mins 59.7 s

################################################################################
                      Learning iteration 673/50000                      

                       Computation: 135913 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 0.72s
                        Total time: 523.81s
                               ETA: 638 mins 55.0 s

################################################################################
                      Learning iteration 674/50000                      

                       Computation: 140950 steps/s (collection: 0.573s, learning 0.125s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.73
                Mean reward (task): 0.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0624
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 0.70s
                        Total time: 524.50s
                               ETA: 638 mins 48.3 s

################################################################################
                      Learning iteration 675/50000                      

                       Computation: 124755 steps/s (collection: 0.649s, learning 0.139s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0179
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 0.79s
                        Total time: 525.29s
                               ETA: 638 mins 48.4 s

################################################################################
                      Learning iteration 676/50000                      

                       Computation: 125297 steps/s (collection: 0.663s, learning 0.122s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0598
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.78s
                        Total time: 526.08s
                               ETA: 638 mins 48.1 s

################################################################################
                      Learning iteration 677/50000                      

                       Computation: 121274 steps/s (collection: 0.689s, learning 0.122s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.81s
                        Total time: 526.89s
                               ETA: 638 mins 49.8 s

################################################################################
                      Learning iteration 678/50000                      

                       Computation: 140053 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0180
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.70s
                        Total time: 527.59s
                               ETA: 638 mins 43.6 s

################################################################################
                      Learning iteration 679/50000                      

                       Computation: 139866 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0607
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.70s
                        Total time: 528.29s
                               ETA: 638 mins 37.4 s

################################################################################
                      Learning iteration 680/50000                      

                       Computation: 137891 steps/s (collection: 0.575s, learning 0.138s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0621
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.71s
                        Total time: 529.00s
                               ETA: 638 mins 32.0 s

################################################################################
                      Learning iteration 681/50000                      

                       Computation: 115724 steps/s (collection: 0.702s, learning 0.147s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 49.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.85s
                        Total time: 529.85s
                               ETA: 638 mins 36.5 s

################################################################################
                      Learning iteration 682/50000                      

                       Computation: 138653 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0103
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0072
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.71s
                        Total time: 530.56s
                               ETA: 638 mins 30.8 s

################################################################################
                      Learning iteration 683/50000                      

                       Computation: 124576 steps/s (collection: 0.655s, learning 0.134s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0599
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.79s
                        Total time: 531.35s
                               ETA: 638 mins 30.9 s

################################################################################
                      Learning iteration 684/50000                      

                       Computation: 131693 steps/s (collection: 0.620s, learning 0.126s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0608
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.75s
                        Total time: 532.10s
                               ETA: 638 mins 27.9 s

################################################################################
                      Learning iteration 685/50000                      

                       Computation: 133184 steps/s (collection: 0.616s, learning 0.122s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.74s
                        Total time: 532.84s
                               ETA: 638 mins 24.4 s

################################################################################
                      Learning iteration 686/50000                      

                       Computation: 122204 steps/s (collection: 0.672s, learning 0.132s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.80s
                        Total time: 533.64s
                               ETA: 638 mins 25.6 s

################################################################################
                      Learning iteration 687/50000                      

                       Computation: 121573 steps/s (collection: 0.659s, learning 0.149s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0182
 Mean episode rew_tracking_lin_vel: 0.0615
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.81s
                        Total time: 534.45s
                               ETA: 638 mins 27.1 s

################################################################################
                      Learning iteration 688/50000                      

                       Computation: 128143 steps/s (collection: 0.628s, learning 0.139s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0609
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.77s
                        Total time: 535.22s
                               ETA: 638 mins 25.6 s

################################################################################
                      Learning iteration 689/50000                      

                       Computation: 125134 steps/s (collection: 0.648s, learning 0.137s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0073
       Mean episode rew_smoothness: -0.0135
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0181
 Mean episode rew_tracking_lin_vel: 0.0601
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.79s
                        Total time: 536.00s
                               ETA: 638 mins 25.5 s

################################################################################
                      Learning iteration 690/50000                      

                       Computation: 137418 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.83
                Mean reward (task): 0.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0183
 Mean episode rew_tracking_lin_vel: 0.0604
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.72s
                        Total time: 536.72s
                               ETA: 638 mins 20.3 s

################################################################################
                      Learning iteration 691/50000                      

                       Computation: 124840 steps/s (collection: 0.641s, learning 0.147s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0619
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.79s
                        Total time: 537.50s
                               ETA: 638 mins 20.3 s

################################################################################
                      Learning iteration 692/50000                      

                       Computation: 128022 steps/s (collection: 0.627s, learning 0.141s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0613
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.77s
                        Total time: 538.27s
                               ETA: 638 mins 18.9 s

################################################################################
                      Learning iteration 693/50000                      

                       Computation: 125543 steps/s (collection: 0.641s, learning 0.142s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0614
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.78s
                        Total time: 539.06s
                               ETA: 638 mins 18.6 s

################################################################################
                      Learning iteration 694/50000                      

                       Computation: 131595 steps/s (collection: 0.598s, learning 0.149s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0137
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.75s
                        Total time: 539.80s
                               ETA: 638 mins 15.7 s

################################################################################
                      Learning iteration 695/50000                      

                       Computation: 137915 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.76
                Mean reward (task): 0.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0074
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0616
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.71s
                        Total time: 540.52s
                               ETA: 638 mins 10.4 s

################################################################################
                      Learning iteration 696/50000                      

                       Computation: 131337 steps/s (collection: 0.625s, learning 0.124s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0072
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.75s
                        Total time: 541.26s
                               ETA: 638 mins 7.6 s

################################################################################
                      Learning iteration 697/50000                      

                       Computation: 133952 steps/s (collection: 0.610s, learning 0.124s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0625
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.73s
                        Total time: 542.00s
                               ETA: 638 mins 3.8 s

################################################################################
                      Learning iteration 698/50000                      

                       Computation: 127151 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.77s
                        Total time: 542.77s
                               ETA: 638 mins 2.8 s

################################################################################
                      Learning iteration 699/50000                      

                       Computation: 139727 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0187
 Mean episode rew_tracking_lin_vel: 0.0624
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.70s
                        Total time: 543.47s
                               ETA: 637 mins 56.9 s

################################################################################
                      Learning iteration 700/50000                      

                       Computation: 123279 steps/s (collection: 0.675s, learning 0.122s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0635
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.80s
                        Total time: 544.27s
                               ETA: 637 mins 57.6 s

################################################################################
                      Learning iteration 701/50000                      

                       Computation: 123696 steps/s (collection: 0.663s, learning 0.132s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0185
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.79s
                        Total time: 545.07s
                               ETA: 637 mins 58.1 s

################################################################################
                      Learning iteration 702/50000                      

                       Computation: 119348 steps/s (collection: 0.684s, learning 0.139s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 48.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0623
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.82s
                        Total time: 545.89s
                               ETA: 638 mins 0.6 s

################################################################################
                      Learning iteration 703/50000                      

                       Computation: 120246 steps/s (collection: 0.696s, learning 0.122s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0632
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.82s
                        Total time: 546.71s
                               ETA: 638 mins 2.7 s

################################################################################
                      Learning iteration 704/50000                      

                       Computation: 135081 steps/s (collection: 0.604s, learning 0.124s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.73s
                        Total time: 547.44s
                               ETA: 637 mins 58.5 s

################################################################################
                      Learning iteration 705/50000                      

                       Computation: 132523 steps/s (collection: 0.614s, learning 0.128s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0637
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.74s
                        Total time: 548.18s
                               ETA: 637 mins 55.3 s

################################################################################
                      Learning iteration 706/50000                      

                       Computation: 126371 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0075
       Mean episode rew_smoothness: -0.0136
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0184
 Mean episode rew_tracking_lin_vel: 0.0615
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.78s
                        Total time: 548.95s
                               ETA: 637 mins 54.7 s

################################################################################
                      Learning iteration 707/50000                      

                       Computation: 138413 steps/s (collection: 0.581s, learning 0.129s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0634
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.71s
                        Total time: 549.67s
                               ETA: 637 mins 49.3 s

################################################################################
                      Learning iteration 708/50000                      

                       Computation: 130972 steps/s (collection: 0.599s, learning 0.151s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.75s
                        Total time: 550.42s
                               ETA: 637 mins 46.7 s

################################################################################
                      Learning iteration 709/50000                      

                       Computation: 120027 steps/s (collection: 0.665s, learning 0.154s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0631
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.82s
                        Total time: 551.23s
                               ETA: 637 mins 48.9 s

################################################################################
                      Learning iteration 710/50000                      

                       Computation: 121886 steps/s (collection: 0.653s, learning 0.154s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.85
                Mean reward (task): 0.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0627
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.81s
                        Total time: 552.04s
                               ETA: 637 mins 50.2 s

################################################################################
                      Learning iteration 711/50000                      

                       Computation: 132030 steps/s (collection: 0.589s, learning 0.156s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.82
                Mean reward (task): 0.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0631
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.74s
                        Total time: 552.79s
                               ETA: 637 mins 47.2 s

################################################################################
                      Learning iteration 712/50000                      

                       Computation: 134658 steps/s (collection: 0.590s, learning 0.140s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0640
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.73s
                        Total time: 553.52s
                               ETA: 637 mins 43.2 s

################################################################################
                      Learning iteration 713/50000                      

                       Computation: 124771 steps/s (collection: 0.661s, learning 0.127s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.98
                Mean reward (task): 0.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.79s
                        Total time: 554.30s
                               ETA: 637 mins 43.3 s

################################################################################
                      Learning iteration 714/50000                      

                       Computation: 129614 steps/s (collection: 0.623s, learning 0.135s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0653
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.76s
                        Total time: 555.06s
                               ETA: 637 mins 41.2 s

################################################################################
                      Learning iteration 715/50000                      

                       Computation: 139142 steps/s (collection: 0.582s, learning 0.124s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.84
                Mean reward (task): 0.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0074
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0076
       Mean episode rew_smoothness: -0.0138
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0186
 Mean episode rew_tracking_lin_vel: 0.0629
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.71s
                        Total time: 555.77s
                               ETA: 637 mins 35.7 s

################################################################################
                      Learning iteration 716/50000                      

                       Computation: 134323 steps/s (collection: 0.609s, learning 0.123s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.78
                Mean reward (task): 0.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0645
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.73s
                        Total time: 556.50s
                               ETA: 637 mins 31.8 s

################################################################################
                      Learning iteration 717/50000                      

                       Computation: 139024 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.71s
                        Total time: 557.21s
                               ETA: 637 mins 26.3 s

################################################################################
                      Learning iteration 718/50000                      

                       Computation: 139618 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.07
                Mean reward (task): 1.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0642
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.70s
                        Total time: 557.91s
                               ETA: 637 mins 20.6 s

################################################################################
                      Learning iteration 719/50000                      

                       Computation: 140565 steps/s (collection: 0.576s, learning 0.124s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0638
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.70s
                        Total time: 558.61s
                               ETA: 637 mins 14.6 s

################################################################################
                      Learning iteration 720/50000                      

                       Computation: 128856 steps/s (collection: 0.639s, learning 0.124s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0628
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.76s
                        Total time: 559.37s
                               ETA: 637 mins 12.9 s

################################################################################
                      Learning iteration 721/50000                      

                       Computation: 140292 steps/s (collection: 0.577s, learning 0.124s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0641
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.70s
                        Total time: 560.07s
                               ETA: 637 mins 7.0 s

################################################################################
                      Learning iteration 722/50000                      

                       Computation: 140517 steps/s (collection: 0.574s, learning 0.125s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.80
                Mean reward (task): 0.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0139
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0641
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.70s
                        Total time: 560.77s
                               ETA: 637 mins 1.1 s

################################################################################
                      Learning iteration 723/50000                      

                       Computation: 139511 steps/s (collection: 0.582s, learning 0.123s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.70s
                        Total time: 561.48s
                               ETA: 636 mins 55.5 s

################################################################################
                      Learning iteration 724/50000                      

                       Computation: 136904 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0189
 Mean episode rew_tracking_lin_vel: 0.0654
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.72s
                        Total time: 562.20s
                               ETA: 636 mins 50.8 s

################################################################################
                      Learning iteration 725/50000                      

                       Computation: 140099 steps/s (collection: 0.577s, learning 0.125s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.94
                Mean reward (task): 0.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.70s
                        Total time: 562.90s
                               ETA: 636 mins 45.0 s

################################################################################
                      Learning iteration 726/50000                      

                       Computation: 139337 steps/s (collection: 0.581s, learning 0.124s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.71s
                        Total time: 563.60s
                               ETA: 636 mins 39.5 s

################################################################################
                      Learning iteration 727/50000                      

                       Computation: 138613 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.90
                Mean reward (task): 0.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0140
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0190
 Mean episode rew_tracking_lin_vel: 0.0648
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.71s
                        Total time: 564.31s
                               ETA: 636 mins 34.2 s

################################################################################
                      Learning iteration 728/50000                      

                       Computation: 140175 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.70s
                        Total time: 565.01s
                               ETA: 636 mins 28.5 s

################################################################################
                      Learning iteration 729/50000                      

                       Computation: 132812 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0670
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.74s
                        Total time: 565.75s
                               ETA: 636 mins 25.3 s

################################################################################
                      Learning iteration 730/50000                      

                       Computation: 127433 steps/s (collection: 0.638s, learning 0.133s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.77s
                        Total time: 566.53s
                               ETA: 636 mins 24.3 s

################################################################################
                      Learning iteration 731/50000                      

                       Computation: 123209 steps/s (collection: 0.661s, learning 0.137s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0673
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.80s
                        Total time: 567.32s
                               ETA: 636 mins 25.1 s

################################################################################
                      Learning iteration 732/50000                      

                       Computation: 112786 steps/s (collection: 0.727s, learning 0.145s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.87s
                        Total time: 568.20s
                               ETA: 636 mins 30.8 s

################################################################################
                      Learning iteration 733/50000                      

                       Computation: 131665 steps/s (collection: 0.606s, learning 0.140s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0661
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.75s
                        Total time: 568.94s
                               ETA: 636 mins 28.1 s

################################################################################
                      Learning iteration 734/50000                      

                       Computation: 135831 steps/s (collection: 0.582s, learning 0.142s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0657
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.72s
                        Total time: 569.67s
                               ETA: 636 mins 23.9 s

################################################################################
                      Learning iteration 735/50000                      

                       Computation: 135675 steps/s (collection: 0.582s, learning 0.142s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0655
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.72s
                        Total time: 570.39s
                               ETA: 636 mins 19.7 s

################################################################################
                      Learning iteration 736/50000                      

                       Computation: 118285 steps/s (collection: 0.695s, learning 0.136s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.01
                Mean reward (task): 1.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.83s
                        Total time: 571.22s
                               ETA: 636 mins 22.7 s

################################################################################
                      Learning iteration 737/50000                      

                       Computation: 135173 steps/s (collection: 0.592s, learning 0.136s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.93
                Mean reward (task): 0.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0665
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.73s
                        Total time: 571.95s
                               ETA: 636 mins 18.7 s

################################################################################
                      Learning iteration 738/50000                      

                       Computation: 124970 steps/s (collection: 0.637s, learning 0.150s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0676
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.79s
                        Total time: 572.74s
                               ETA: 636 mins 18.7 s

################################################################################
                      Learning iteration 739/50000                      

                       Computation: 131315 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.81
                Mean reward (task): 0.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0077
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0636
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.75s
                        Total time: 573.48s
                               ETA: 636 mins 16.2 s

################################################################################
                      Learning iteration 740/50000                      

                       Computation: 119308 steps/s (collection: 0.701s, learning 0.123s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0650
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.82s
                        Total time: 574.31s
                               ETA: 636 mins 18.7 s

################################################################################
                      Learning iteration 741/50000                      

                       Computation: 132063 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0192
 Mean episode rew_tracking_lin_vel: 0.0645
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.74s
                        Total time: 575.05s
                               ETA: 636 mins 15.9 s

################################################################################
                      Learning iteration 742/50000                      

                       Computation: 133381 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0660
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.74s
                        Total time: 575.79s
                               ETA: 636 mins 12.6 s

################################################################################
                      Learning iteration 743/50000                      

                       Computation: 137983 steps/s (collection: 0.588s, learning 0.125s)
               Value function loss: 0.0256
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0652
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.71s
                        Total time: 576.50s
                               ETA: 636 mins 7.7 s

################################################################################
                      Learning iteration 744/50000                      

                       Computation: 117986 steps/s (collection: 0.695s, learning 0.138s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.79
                Mean reward (task): 0.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 50.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0078
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0649
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.83s
                        Total time: 577.33s
                               ETA: 636 mins 10.8 s

################################################################################
                      Learning iteration 745/50000                      

                       Computation: 133661 steps/s (collection: 0.601s, learning 0.134s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.74s
                        Total time: 578.07s
                               ETA: 636 mins 7.4 s

################################################################################
                      Learning iteration 746/50000                      

                       Computation: 131499 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0656
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.75s
                        Total time: 578.82s
                               ETA: 636 mins 4.8 s

################################################################################
                      Learning iteration 747/50000                      

                       Computation: 120863 steps/s (collection: 0.672s, learning 0.141s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.86
                Mean reward (task): 0.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.81s
                        Total time: 579.63s
                               ETA: 636 mins 6.6 s

################################################################################
                      Learning iteration 748/50000                      

                       Computation: 129411 steps/s (collection: 0.635s, learning 0.124s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0676
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.76s
                        Total time: 580.39s
                               ETA: 636 mins 4.8 s

################################################################################
                      Learning iteration 749/50000                      

                       Computation: 138732 steps/s (collection: 0.584s, learning 0.124s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.87
                Mean reward (task): 0.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0680
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.71s
                        Total time: 581.10s
                               ETA: 635 mins 59.6 s

################################################################################
                      Learning iteration 750/50000                      

                       Computation: 134731 steps/s (collection: 0.594s, learning 0.135s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 51.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0076
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0079
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0191
 Mean episode rew_tracking_lin_vel: 0.0658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.73s
                        Total time: 581.83s
                               ETA: 635 mins 55.9 s

################################################################################
                      Learning iteration 751/50000                      

                       Computation: 126344 steps/s (collection: 0.639s, learning 0.139s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0077
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0669
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.78s
                        Total time: 582.61s
                               ETA: 635 mins 55.4 s

################################################################################
                      Learning iteration 752/50000                      

                       Computation: 118150 steps/s (collection: 0.689s, learning 0.143s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.98
                Mean reward (task): 0.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0078
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0677
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.83s
                        Total time: 583.44s
                               ETA: 635 mins 58.3 s

################################################################################
                      Learning iteration 753/50000                      

                       Computation: 136962 steps/s (collection: 0.580s, learning 0.137s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.18
                Mean reward (task): 1.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0682
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.72s
                        Total time: 584.16s
                               ETA: 635 mins 53.8 s

################################################################################
                      Learning iteration 754/50000                      

                       Computation: 133811 steps/s (collection: 0.612s, learning 0.122s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.73s
                        Total time: 584.89s
                               ETA: 635 mins 50.4 s

################################################################################
                      Learning iteration 755/50000                      

                       Computation: 136942 steps/s (collection: 0.583s, learning 0.135s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0671
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.72s
                        Total time: 585.61s
                               ETA: 635 mins 46.0 s

################################################################################
                      Learning iteration 756/50000                      

                       Computation: 122104 steps/s (collection: 0.684s, learning 0.121s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0673
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.81s
                        Total time: 586.41s
                               ETA: 635 mins 47.2 s

################################################################################
                      Learning iteration 757/50000                      

                       Computation: 116871 steps/s (collection: 0.714s, learning 0.127s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.84s
                        Total time: 587.26s
                               ETA: 635 mins 50.7 s

################################################################################
                      Learning iteration 758/50000                      

                       Computation: 133646 steps/s (collection: 0.608s, learning 0.128s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.98
                Mean reward (task): 0.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0681
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.74s
                        Total time: 587.99s
                               ETA: 635 mins 47.4 s

################################################################################
                      Learning iteration 759/50000                      

                       Computation: 139572 steps/s (collection: 0.581s, learning 0.123s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0687
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.70s
                        Total time: 588.70s
                               ETA: 635 mins 42.1 s

################################################################################
                      Learning iteration 760/50000                      

                       Computation: 123128 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0666
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.80s
                        Total time: 589.49s
                               ETA: 635 mins 42.8 s

################################################################################
                      Learning iteration 761/50000                      

                       Computation: 132714 steps/s (collection: 0.600s, learning 0.141s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0677
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.74s
                        Total time: 590.23s
                               ETA: 635 mins 39.9 s

################################################################################
                      Learning iteration 762/50000                      

                       Computation: 117393 steps/s (collection: 0.713s, learning 0.125s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.09
                Mean reward (task): 1.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.84s
                        Total time: 591.07s
                               ETA: 635 mins 43.1 s

################################################################################
                      Learning iteration 763/50000                      

                       Computation: 113430 steps/s (collection: 0.722s, learning 0.144s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0672
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.87s
                        Total time: 591.94s
                               ETA: 635 mins 48.3 s

################################################################################
                      Learning iteration 764/50000                      

                       Computation: 130802 steps/s (collection: 0.629s, learning 0.122s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0682
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.75s
                        Total time: 592.69s
                               ETA: 635 mins 46.0 s

################################################################################
                      Learning iteration 765/50000                      

                       Computation: 127393 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0692
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.77s
                        Total time: 593.46s
                               ETA: 635 mins 45.0 s

################################################################################
                      Learning iteration 766/50000                      

                       Computation: 123677 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.01
                Mean reward (task): 1.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0699
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.79s
                        Total time: 594.26s
                               ETA: 635 mins 45.6 s

################################################################################
                      Learning iteration 767/50000                      

                       Computation: 123252 steps/s (collection: 0.661s, learning 0.137s)
               Value function loss: 0.0278
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0690
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.80s
                        Total time: 595.05s
                               ETA: 635 mins 46.2 s

################################################################################
                      Learning iteration 768/50000                      

                       Computation: 139461 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0688
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.70s
                        Total time: 595.76s
                               ETA: 635 mins 41.0 s

################################################################################
                      Learning iteration 769/50000                      

                       Computation: 131283 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0676
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.75s
                        Total time: 596.51s
                               ETA: 635 mins 38.6 s

################################################################################
                      Learning iteration 770/50000                      

                       Computation: 127374 steps/s (collection: 0.649s, learning 0.123s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0687
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.77s
                        Total time: 597.28s
                               ETA: 635 mins 37.6 s

################################################################################
                      Learning iteration 771/50000                      

                       Computation: 125402 steps/s (collection: 0.632s, learning 0.152s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0080
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0195
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.78s
                        Total time: 598.06s
                               ETA: 635 mins 37.4 s

################################################################################
                      Learning iteration 772/50000                      

                       Computation: 131343 steps/s (collection: 0.625s, learning 0.124s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0683
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.75s
                        Total time: 598.81s
                               ETA: 635 mins 35.0 s

################################################################################
                      Learning iteration 773/50000                      

                       Computation: 142430 steps/s (collection: 0.569s, learning 0.121s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0079
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0196
 Mean episode rew_tracking_lin_vel: 0.0685
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.69s
                        Total time: 599.50s
                               ETA: 635 mins 28.8 s

################################################################################
                      Learning iteration 774/50000                      

                       Computation: 139960 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0194
 Mean episode rew_tracking_lin_vel: 0.0704
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.70s
                        Total time: 600.20s
                               ETA: 635 mins 23.5 s

################################################################################
                      Learning iteration 775/50000                      

                       Computation: 122765 steps/s (collection: 0.676s, learning 0.125s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.89
                Mean reward (task): 0.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0081
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0141
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0675
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.80s
                        Total time: 601.01s
                               ETA: 635 mins 24.3 s

################################################################################
                      Learning iteration 776/50000                      

                       Computation: 130443 steps/s (collection: 0.624s, learning 0.130s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0705
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.75s
                        Total time: 601.76s
                               ETA: 635 mins 22.3 s

################################################################################
                      Learning iteration 777/50000                      

                       Computation: 139368 steps/s (collection: 0.581s, learning 0.124s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.07
                Mean reward (task): 1.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0081
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0193
 Mean episode rew_tracking_lin_vel: 0.0674
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.71s
                        Total time: 602.46s
                               ETA: 635 mins 17.1 s

################################################################################
                      Learning iteration 778/50000                      

                       Computation: 128965 steps/s (collection: 0.621s, learning 0.141s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0082
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0694
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.76s
                        Total time: 603.23s
                               ETA: 635 mins 15.6 s

################################################################################
                      Learning iteration 779/50000                      

                       Computation: 119402 steps/s (collection: 0.698s, learning 0.125s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0709
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.82s
                        Total time: 604.05s
                               ETA: 635 mins 17.9 s

################################################################################
                      Learning iteration 780/50000                      

                       Computation: 132327 steps/s (collection: 0.619s, learning 0.124s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0700
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.74s
                        Total time: 604.79s
                               ETA: 635 mins 15.1 s

################################################################################
                      Learning iteration 781/50000                      

                       Computation: 117329 steps/s (collection: 0.695s, learning 0.143s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0686
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.84s
                        Total time: 605.63s
                               ETA: 635 mins 18.3 s

################################################################################
                      Learning iteration 782/50000                      

                       Computation: 126739 steps/s (collection: 0.637s, learning 0.138s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0716
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.78s
                        Total time: 606.41s
                               ETA: 635 mins 17.6 s

################################################################################
                      Learning iteration 783/50000                      

                       Computation: 125728 steps/s (collection: 0.622s, learning 0.160s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0198
 Mean episode rew_tracking_lin_vel: 0.0686
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.78s
                        Total time: 607.19s
                               ETA: 635 mins 17.3 s

################################################################################
                      Learning iteration 784/50000                      

                       Computation: 141159 steps/s (collection: 0.573s, learning 0.123s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0698
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.70s
                        Total time: 607.88s
                               ETA: 635 mins 11.7 s

################################################################################
                      Learning iteration 785/50000                      

                       Computation: 134704 steps/s (collection: 0.604s, learning 0.126s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.22
                Mean reward (task): 1.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0707
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.73s
                        Total time: 608.61s
                               ETA: 635 mins 8.1 s

################################################################################
                      Learning iteration 786/50000                      

                       Computation: 132958 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0695
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.74s
                        Total time: 609.35s
                               ETA: 635 mins 5.1 s

################################################################################
                      Learning iteration 787/50000                      

                       Computation: 131133 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.01
                Mean reward (task): 1.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0712
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.75s
                        Total time: 610.10s
                               ETA: 635 mins 2.8 s

################################################################################
                      Learning iteration 788/50000                      

                       Computation: 122518 steps/s (collection: 0.664s, learning 0.138s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0729
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.80s
                        Total time: 610.91s
                               ETA: 635 mins 3.8 s

################################################################################
                      Learning iteration 789/50000                      

                       Computation: 136048 steps/s (collection: 0.599s, learning 0.123s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0719
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.72s
                        Total time: 611.63s
                               ETA: 634 mins 59.8 s

################################################################################
                      Learning iteration 790/50000                      

                       Computation: 132055 steps/s (collection: 0.622s, learning 0.122s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0729
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.74s
                        Total time: 612.37s
                               ETA: 634 mins 57.2 s

################################################################################
                      Learning iteration 791/50000                      

                       Computation: 121587 steps/s (collection: 0.685s, learning 0.124s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0187
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0202
 Mean episode rew_tracking_lin_vel: 0.0708
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.81s
                        Total time: 613.18s
                               ETA: 634 mins 58.5 s

################################################################################
                      Learning iteration 792/50000                      

                       Computation: 140389 steps/s (collection: 0.576s, learning 0.125s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0197
 Mean episode rew_tracking_lin_vel: 0.0697
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.70s
                        Total time: 613.88s
                               ETA: 634 mins 53.2 s

################################################################################
                      Learning iteration 793/50000                      

                       Computation: 134125 steps/s (collection: 0.605s, learning 0.128s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0705
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.73s
                        Total time: 614.61s
                               ETA: 634 mins 49.8 s

################################################################################
                      Learning iteration 794/50000                      

                       Computation: 122403 steps/s (collection: 0.680s, learning 0.123s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.21
                Mean reward (task): 1.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0710
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.80s
                        Total time: 615.42s
                               ETA: 634 mins 50.9 s

################################################################################
                      Learning iteration 795/50000                      

                       Computation: 135482 steps/s (collection: 0.601s, learning 0.124s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.17
                Mean reward (task): 1.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0199
 Mean episode rew_tracking_lin_vel: 0.0710
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.73s
                        Total time: 616.14s
                               ETA: 634 mins 47.1 s

################################################################################
                      Learning iteration 796/50000                      

                       Computation: 123858 steps/s (collection: 0.656s, learning 0.138s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.97
                Mean reward (task): 0.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0725
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.79s
                        Total time: 616.94s
                               ETA: 634 mins 47.5 s

################################################################################
                      Learning iteration 797/50000                      

                       Computation: 134051 steps/s (collection: 0.609s, learning 0.124s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.25
                Mean reward (task): 1.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0201
 Mean episode rew_tracking_lin_vel: 0.0713
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.73s
                        Total time: 617.67s
                               ETA: 634 mins 44.2 s

################################################################################
                      Learning iteration 798/50000                      

                       Computation: 123215 steps/s (collection: 0.666s, learning 0.132s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.13
                Mean reward (task): 1.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0710
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.80s
                        Total time: 618.47s
                               ETA: 634 mins 44.9 s

################################################################################
                      Learning iteration 799/50000                      

                       Computation: 132159 steps/s (collection: 0.619s, learning 0.125s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.07
                Mean reward (task): 1.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0734
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.74s
                        Total time: 619.21s
                               ETA: 634 mins 42.3 s

################################################################################
                      Learning iteration 800/50000                      

                       Computation: 141654 steps/s (collection: 0.570s, learning 0.124s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.13
                Mean reward (task): 1.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0740
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.69s
                        Total time: 619.91s
                               ETA: 634 mins 36.6 s

################################################################################
                      Learning iteration 801/50000                      

                       Computation: 138336 steps/s (collection: 0.588s, learning 0.122s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.95
                Mean reward (task): 0.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0144
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0692
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.71s
                        Total time: 620.62s
                               ETA: 634 mins 31.9 s

################################################################################
                      Learning iteration 802/50000                      

                       Computation: 139473 steps/s (collection: 0.573s, learning 0.132s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0718
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.70s
                        Total time: 621.32s
                               ETA: 634 mins 26.9 s

################################################################################
                      Learning iteration 803/50000                      

                       Computation: 117935 steps/s (collection: 0.693s, learning 0.140s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0713
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.83s
                        Total time: 622.15s
                               ETA: 634 mins 29.8 s

################################################################################
                      Learning iteration 804/50000                      

                       Computation: 133804 steps/s (collection: 0.573s, learning 0.162s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0729
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.73s
                        Total time: 622.89s
                               ETA: 634 mins 26.7 s

################################################################################
                      Learning iteration 805/50000                      

                       Computation: 120765 steps/s (collection: 0.648s, learning 0.166s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.03
                Mean reward (task): 1.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0716
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.81s
                        Total time: 623.70s
                               ETA: 634 mins 28.3 s

################################################################################
                      Learning iteration 806/50000                      

                       Computation: 129735 steps/s (collection: 0.618s, learning 0.140s)
               Value function loss: 0.0313
                    Surrogate loss: 0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0722
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.76s
                        Total time: 624.46s
                               ETA: 634 mins 26.6 s

################################################################################
                      Learning iteration 807/50000                      

                       Computation: 122977 steps/s (collection: 0.677s, learning 0.122s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.96
                Mean reward (task): 0.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0082
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0083
       Mean episode rew_smoothness: -0.0142
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0200
 Mean episode rew_tracking_lin_vel: 0.0696
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.80s
                        Total time: 625.26s
                               ETA: 634 mins 27.4 s

################################################################################
                      Learning iteration 808/50000                      

                       Computation: 125004 steps/s (collection: 0.640s, learning 0.146s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.07
                Mean reward (task): 1.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0190
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0143
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0715
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.79s
                        Total time: 626.05s
                               ETA: 634 mins 27.4 s

################################################################################
                      Learning iteration 809/50000                      

                       Computation: 129059 steps/s (collection: 0.621s, learning 0.140s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0712
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.76s
                        Total time: 626.81s
                               ETA: 634 mins 25.9 s

################################################################################
                      Learning iteration 810/50000                      

                       Computation: 125806 steps/s (collection: 0.639s, learning 0.142s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0735
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.78s
                        Total time: 627.59s
                               ETA: 634 mins 25.5 s

################################################################################
                      Learning iteration 811/50000                      

                       Computation: 137444 steps/s (collection: 0.575s, learning 0.140s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0084
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0714
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.72s
                        Total time: 628.31s
                               ETA: 634 mins 21.2 s

################################################################################
                      Learning iteration 812/50000                      

                       Computation: 116474 steps/s (collection: 0.685s, learning 0.159s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 0.92
                Mean reward (task): 0.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 53.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0085
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0146
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0732
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.84s
                        Total time: 629.15s
                               ETA: 634 mins 24.7 s

################################################################################
                      Learning iteration 813/50000                      

                       Computation: 137855 steps/s (collection: 0.588s, learning 0.125s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.21
                Mean reward (task): 1.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0742
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.71s
                        Total time: 629.86s
                               ETA: 634 mins 20.2 s

################################################################################
                      Learning iteration 814/50000                      

                       Computation: 141274 steps/s (collection: 0.574s, learning 0.122s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0204
 Mean episode rew_tracking_lin_vel: 0.0722
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.70s
                        Total time: 630.56s
                               ETA: 634 mins 14.8 s

################################################################################
                      Learning iteration 815/50000                      

                       Computation: 126046 steps/s (collection: 0.647s, learning 0.133s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.88
                Mean reward (task): 0.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0145
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0203
 Mean episode rew_tracking_lin_vel: 0.0719
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.78s
                        Total time: 631.34s
                               ETA: 634 mins 14.4 s

################################################################################
                      Learning iteration 816/50000                      

                       Computation: 133711 steps/s (collection: 0.612s, learning 0.123s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.24
                Mean reward (task): 1.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0085
       Mean episode rew_smoothness: -0.0147
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0732
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.74s
                        Total time: 632.07s
                               ETA: 634 mins 11.3 s

################################################################################
                      Learning iteration 817/50000                      

                       Computation: 133747 steps/s (collection: 0.599s, learning 0.136s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 54.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0101
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0731
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.73s
                        Total time: 632.81s
                               ETA: 634 mins 8.2 s

################################################################################
                      Learning iteration 818/50000                      

                       Computation: 137227 steps/s (collection: 0.574s, learning 0.142s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.09
                Mean reward (task): 1.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0736
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.72s
                        Total time: 633.52s
                               ETA: 634 mins 4.0 s

################################################################################
                      Learning iteration 819/50000                      

                       Computation: 112045 steps/s (collection: 0.720s, learning 0.158s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.99
                Mean reward (task): 0.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0086
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0724
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.88s
                        Total time: 634.40s
                               ETA: 634 mins 9.4 s

################################################################################
                      Learning iteration 820/50000                      

                       Computation: 131358 steps/s (collection: 0.608s, learning 0.140s)
               Value function loss: 0.0315
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.27
                Mean reward (task): 1.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0088
       Mean episode rew_smoothness: -0.0149
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0750
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.75s
                        Total time: 635.15s
                               ETA: 634 mins 7.1 s

################################################################################
                      Learning iteration 821/50000                      

                       Computation: 121222 steps/s (collection: 0.653s, learning 0.158s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0739
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.81s
                        Total time: 635.96s
                               ETA: 634 mins 8.6 s

################################################################################
                      Learning iteration 822/50000                      

                       Computation: 139468 steps/s (collection: 0.562s, learning 0.143s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0088
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0748
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.70s
                        Total time: 636.67s
                               ETA: 634 mins 3.7 s

################################################################################
                      Learning iteration 823/50000                      

                       Computation: 132698 steps/s (collection: 0.594s, learning 0.147s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.14
                Mean reward (task): 1.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0102
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0729
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.74s
                        Total time: 637.41s
                               ETA: 634 mins 1.0 s

################################################################################
                      Learning iteration 824/50000                      

                       Computation: 122901 steps/s (collection: 0.664s, learning 0.136s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 0.91
                Mean reward (task): 0.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 52.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0148
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0737
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.80s
                        Total time: 638.21s
                               ETA: 634 mins 1.8 s

################################################################################
                      Learning iteration 825/50000                      

                       Computation: 125623 steps/s (collection: 0.659s, learning 0.123s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.43
                Mean reward (task): 1.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0090
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0765
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.78s
                        Total time: 638.99s
                               ETA: 634 mins 1.5 s

################################################################################
                      Learning iteration 826/50000                      

                       Computation: 129529 steps/s (collection: 0.618s, learning 0.141s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0748
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.76s
                        Total time: 639.75s
                               ETA: 633 mins 59.9 s

################################################################################
                      Learning iteration 827/50000                      

                       Computation: 142773 steps/s (collection: 0.566s, learning 0.122s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0088
       Mean episode rew_smoothness: -0.0152
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0207
 Mean episode rew_tracking_lin_vel: 0.0755
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.69s
                        Total time: 640.44s
                               ETA: 633 mins 54.1 s

################################################################################
                      Learning iteration 828/50000                      

                       Computation: 129661 steps/s (collection: 0.635s, learning 0.124s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.56
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0088
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0208
 Mean episode rew_tracking_lin_vel: 0.0749
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.76s
                        Total time: 641.19s
                               ETA: 633 mins 52.4 s

################################################################################
                      Learning iteration 829/50000                      

                       Computation: 139576 steps/s (collection: 0.580s, learning 0.124s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.00
                Mean reward (task): 1.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 55.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0104
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0087
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0087
       Mean episode rew_smoothness: -0.0150
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0732
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.70s
                        Total time: 641.90s
                               ETA: 633 mins 47.5 s

################################################################################
                      Learning iteration 830/50000                      

                       Computation: 134444 steps/s (collection: 0.608s, learning 0.123s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.36
                Mean reward (task): 1.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0761
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.73s
                        Total time: 642.63s
                               ETA: 633 mins 44.2 s

################################################################################
                      Learning iteration 831/50000                      

                       Computation: 141354 steps/s (collection: 0.572s, learning 0.124s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0774
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.70s
                        Total time: 643.33s
                               ETA: 633 mins 38.9 s

################################################################################
                      Learning iteration 832/50000                      

                       Computation: 140961 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0103
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0151
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0767
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.70s
                        Total time: 644.02s
                               ETA: 633 mins 33.6 s

################################################################################
                      Learning iteration 833/50000                      

                       Computation: 129481 steps/s (collection: 0.634s, learning 0.125s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.18
                Mean reward (task): 1.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0090
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0769
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.76s
                        Total time: 644.78s
                               ETA: 633 mins 32.0 s

################################################################################
                      Learning iteration 834/50000                      

                       Computation: 137770 steps/s (collection: 0.590s, learning 0.124s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0206
 Mean episode rew_tracking_lin_vel: 0.0752
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.71s
                        Total time: 645.50s
                               ETA: 633 mins 27.7 s

################################################################################
                      Learning iteration 835/50000                      

                       Computation: 128910 steps/s (collection: 0.620s, learning 0.143s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.02
                Mean reward (task): 1.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 56.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0091
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0770
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.76s
                        Total time: 646.26s
                               ETA: 633 mins 26.3 s

################################################################################
                      Learning iteration 836/50000                      

                       Computation: 141453 steps/s (collection: 0.571s, learning 0.124s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0105
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0088
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0741
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.69s
                        Total time: 646.95s
                               ETA: 633 mins 21.0 s

################################################################################
                      Learning iteration 837/50000                      

                       Computation: 124153 steps/s (collection: 0.665s, learning 0.127s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.33
                Mean reward (task): 1.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0188
           Mean episode rew_no_fly: 0.0090
       Mean episode rew_smoothness: -0.0154
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0761
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.79s
                        Total time: 647.75s
                               ETA: 633 mins 21.3 s

################################################################################
                      Learning iteration 838/50000                      

                       Computation: 120774 steps/s (collection: 0.661s, learning 0.153s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.07
                Mean reward (task): 1.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0091
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0762
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.81s
                        Total time: 648.56s
                               ETA: 633 mins 22.9 s

################################################################################
                      Learning iteration 839/50000                      

                       Computation: 143007 steps/s (collection: 0.564s, learning 0.123s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.20
                Mean reward (task): 1.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0090
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0775
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.69s
                        Total time: 649.25s
                               ETA: 633 mins 17.2 s

################################################################################
                      Learning iteration 840/50000                      

                       Computation: 141602 steps/s (collection: 0.570s, learning 0.124s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0773
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.69s
                        Total time: 649.94s
                               ETA: 633 mins 11.8 s

################################################################################
                      Learning iteration 841/50000                      

                       Computation: 134654 steps/s (collection: 0.603s, learning 0.127s)
               Value function loss: 0.0374
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0106
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0089
       Mean episode rew_smoothness: -0.0153
          Mean episode rew_torques: -0.0020
 Mean episode rew_tracking_ang_vel: 0.0209
 Mean episode rew_tracking_lin_vel: 0.0774
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.73s
                        Total time: 650.67s
                               ETA: 633 mins 8.5 s

################################################################################
                      Learning iteration 842/50000                      

                       Computation: 126373 steps/s (collection: 0.646s, learning 0.132s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.09
                Mean reward (task): 1.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0094
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0160
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0827
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.78s
                        Total time: 651.45s
                               ETA: 633 mins 8.0 s

################################################################################
                      Learning iteration 843/50000                      

                       Computation: 120578 steps/s (collection: 0.668s, learning 0.147s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0092
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0091
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0210
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.82s
                        Total time: 652.26s
                               ETA: 633 mins 9.7 s

################################################################################
                      Learning iteration 844/50000                      

                       Computation: 128522 steps/s (collection: 0.641s, learning 0.124s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0090
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0205
 Mean episode rew_tracking_lin_vel: 0.0757
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.76s
                        Total time: 653.03s
                               ETA: 633 mins 8.5 s

################################################################################
                      Learning iteration 845/50000                      

                       Computation: 131660 steps/s (collection: 0.586s, learning 0.160s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.33
                Mean reward (task): 1.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0810
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.75s
                        Total time: 653.78s
                               ETA: 633 mins 6.2 s

################################################################################
                      Learning iteration 846/50000                      

                       Computation: 116028 steps/s (collection: 0.696s, learning 0.151s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0160
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0796
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.85s
                        Total time: 654.62s
                               ETA: 633 mins 9.8 s

################################################################################
                      Learning iteration 847/50000                      

                       Computation: 129139 steps/s (collection: 0.599s, learning 0.163s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0108
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.0793
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.76s
                        Total time: 655.38s
                               ETA: 633 mins 8.3 s

################################################################################
                      Learning iteration 848/50000                      

                       Computation: 121431 steps/s (collection: 0.676s, learning 0.134s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.33
                Mean reward (task): 1.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0212
 Mean episode rew_tracking_lin_vel: 0.0803
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.81s
                        Total time: 656.19s
                               ETA: 633 mins 9.7 s

################################################################################
                      Learning iteration 849/50000                      

                       Computation: 134307 steps/s (collection: 0.602s, learning 0.130s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.11
                Mean reward (task): 1.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0156
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.0784
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.73s
                        Total time: 656.93s
                               ETA: 633 mins 6.5 s

################################################################################
                      Learning iteration 850/50000                      

                       Computation: 128585 steps/s (collection: 0.635s, learning 0.129s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.04
                Mean reward (task): 1.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0095
       Mean episode rew_smoothness: -0.0161
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0817
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.76s
                        Total time: 657.69s
                               ETA: 633 mins 5.3 s

################################################################################
                      Learning iteration 851/50000                      

                       Computation: 122613 steps/s (collection: 0.657s, learning 0.145s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.29
                Mean reward (task): 1.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0091
       Mean episode rew_smoothness: -0.0157
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0211
 Mean episode rew_tracking_lin_vel: 0.0767
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.80s
                        Total time: 658.49s
                               ETA: 633 mins 6.2 s

################################################################################
                      Learning iteration 852/50000                      

                       Computation: 114215 steps/s (collection: 0.709s, learning 0.152s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.09
                Mean reward (task): 1.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0107
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0094
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0155
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0213
 Mean episode rew_tracking_lin_vel: 0.0766
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.86s
                        Total time: 659.35s
                               ETA: 633 mins 10.5 s

################################################################################
                      Learning iteration 853/50000                      

                       Computation: 120129 steps/s (collection: 0.692s, learning 0.127s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.05
                Mean reward (task): 1.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0790
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.82s
                        Total time: 660.17s
                               ETA: 633 mins 12.3 s

################################################################################
                      Learning iteration 854/50000                      

                       Computation: 111823 steps/s (collection: 0.745s, learning 0.135s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0095
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0813
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.88s
                        Total time: 661.05s
                               ETA: 633 mins 17.6 s

################################################################################
                      Learning iteration 855/50000                      

                       Computation: 138154 steps/s (collection: 0.580s, learning 0.132s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.43
                Mean reward (task): 1.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0092
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0021
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0789
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.71s
                        Total time: 661.76s
                               ETA: 633 mins 13.3 s

################################################################################
                      Learning iteration 856/50000                      

                       Computation: 140586 steps/s (collection: 0.561s, learning 0.138s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.19
                Mean reward (task): 1.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0214
 Mean episode rew_tracking_lin_vel: 0.0798
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.70s
                        Total time: 662.46s
                               ETA: 633 mins 8.3 s

################################################################################
                      Learning iteration 857/50000                      

                       Computation: 117885 steps/s (collection: 0.694s, learning 0.140s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.23
                Mean reward (task): 1.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0161
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0807
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.83s
                        Total time: 663.29s
                               ETA: 633 mins 11.0 s

################################################################################
                      Learning iteration 858/50000                      

                       Computation: 130254 steps/s (collection: 0.599s, learning 0.156s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.31
                Mean reward (task): 1.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0094
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0160
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0802
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.75s
                        Total time: 664.05s
                               ETA: 633 mins 9.2 s

################################################################################
                      Learning iteration 859/50000                      

                       Computation: 129445 steps/s (collection: 0.624s, learning 0.135s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0794
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.76s
                        Total time: 664.81s
                               ETA: 633 mins 7.6 s

################################################################################
                      Learning iteration 860/50000                      

                       Computation: 142780 steps/s (collection: 0.565s, learning 0.124s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.06
                Mean reward (task): 1.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0094
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0782
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.69s
                        Total time: 665.50s
                               ETA: 633 mins 2.0 s

################################################################################
                      Learning iteration 861/50000                      

                       Computation: 141239 steps/s (collection: 0.562s, learning 0.134s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.20
                Mean reward (task): 1.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0160
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0793
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.70s
                        Total time: 666.19s
                               ETA: 632 mins 56.9 s

################################################################################
                      Learning iteration 862/50000                      

                       Computation: 127863 steps/s (collection: 0.645s, learning 0.124s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0161
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0824
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.77s
                        Total time: 666.96s
                               ETA: 632 mins 55.9 s

################################################################################
                      Learning iteration 863/50000                      

                       Computation: 142988 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.19
                Mean reward (task): 1.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0109
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0094
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0786
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.69s
                        Total time: 667.65s
                               ETA: 632 mins 50.2 s

################################################################################
                      Learning iteration 864/50000                      

                       Computation: 130915 steps/s (collection: 0.628s, learning 0.122s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.23
                Mean reward (task): 1.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0163
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0825
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.75s
                        Total time: 668.40s
                               ETA: 632 mins 48.2 s

################################################################################
                      Learning iteration 865/50000                      

                       Computation: 142240 steps/s (collection: 0.567s, learning 0.124s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0097
       Mean episode rew_smoothness: -0.0164
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0838
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.69s
                        Total time: 669.09s
                               ETA: 632 mins 42.8 s

################################################################################
                      Learning iteration 866/50000                      

                       Computation: 127498 steps/s (collection: 0.620s, learning 0.151s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.27
                Mean reward (task): 1.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0112
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0828
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.77s
                        Total time: 669.86s
                               ETA: 632 mins 42.0 s

################################################################################
                      Learning iteration 867/50000                      

                       Computation: 133700 steps/s (collection: 0.599s, learning 0.136s)
               Value function loss: 0.0375
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.14
                Mean reward (task): 1.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0099
        Mean episode rew_lin_vel_z: -0.0188
           Mean episode rew_no_fly: 0.0097
       Mean episode rew_smoothness: -0.0164
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0838
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.74s
                        Total time: 670.60s
                               ETA: 632 mins 39.1 s

################################################################################
                      Learning iteration 868/50000                      

                       Computation: 121939 steps/s (collection: 0.675s, learning 0.131s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 57.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0800
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.81s
                        Total time: 671.40s
                               ETA: 632 mins 40.2 s

################################################################################
                      Learning iteration 869/50000                      

                       Computation: 142805 steps/s (collection: 0.564s, learning 0.124s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.17
                Mean reward (task): 1.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0096
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0814
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.69s
                        Total time: 672.09s
                               ETA: 632 mins 34.7 s

################################################################################
                      Learning iteration 870/50000                      

                       Computation: 142586 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 60.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0098
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0094
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0216
 Mean episode rew_tracking_lin_vel: 0.0809
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.69s
                        Total time: 672.78s
                               ETA: 632 mins 29.2 s

################################################################################
                      Learning iteration 871/50000                      

                       Computation: 130126 steps/s (collection: 0.631s, learning 0.125s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.20
                Mean reward (task): 1.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0097
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0093
       Mean episode rew_smoothness: -0.0158
          Mean episode rew_torques: -0.0022
 Mean episode rew_tracking_ang_vel: 0.0217
 Mean episode rew_tracking_lin_vel: 0.0799
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.76s
                        Total time: 673.54s
                               ETA: 632 mins 27.5 s

################################################################################
                      Learning iteration 872/50000                      

                       Computation: 141774 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0098
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0161
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0220
 Mean episode rew_tracking_lin_vel: 0.0822
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.69s
                        Total time: 674.23s
                               ETA: 632 mins 22.3 s

################################################################################
                      Learning iteration 873/50000                      

                       Computation: 134006 steps/s (collection: 0.611s, learning 0.122s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.10
                Mean reward (task): 1.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0110
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0098
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0095
       Mean episode rew_smoothness: -0.0159
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0807
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.73s
                        Total time: 674.96s
                               ETA: 632 mins 19.3 s

################################################################################
                      Learning iteration 874/50000                      

                       Computation: 141041 steps/s (collection: 0.576s, learning 0.121s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.27
                Mean reward (task): 1.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0102
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0098
       Mean episode rew_smoothness: -0.0165
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0835
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.70s
                        Total time: 675.66s
                               ETA: 632 mins 14.3 s

################################################################################
                      Learning iteration 875/50000                      

                       Computation: 143139 steps/s (collection: 0.565s, learning 0.122s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.21
                Mean reward (task): 1.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0101
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0164
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0858
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.69s
                        Total time: 676.35s
                               ETA: 632 mins 8.8 s

################################################################################
                      Learning iteration 876/50000                      

                       Computation: 118322 steps/s (collection: 0.686s, learning 0.145s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.34
                Mean reward (task): 1.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0099
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0842
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.83s
                        Total time: 677.18s
                               ETA: 632 mins 11.3 s

################################################################################
                      Learning iteration 877/50000                      

                       Computation: 142510 steps/s (collection: 0.566s, learning 0.124s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.24
                Mean reward (task): 1.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0103
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0101
       Mean episode rew_smoothness: -0.0166
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0872
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.69s
                        Total time: 677.87s
                               ETA: 632 mins 5.9 s

################################################################################
                      Learning iteration 878/50000                      

                       Computation: 124235 steps/s (collection: 0.648s, learning 0.144s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.24
                Mean reward (task): 1.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0111
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0099
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0097
       Mean episode rew_smoothness: -0.0161
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0836
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.79s
                        Total time: 678.66s
                               ETA: 632 mins 6.2 s

################################################################################
                      Learning iteration 879/50000                      

                       Computation: 122555 steps/s (collection: 0.671s, learning 0.131s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.41
                Mean reward (task): 1.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0101
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0098
       Mean episode rew_smoothness: -0.0164
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0847
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.80s
                        Total time: 679.46s
                               ETA: 632 mins 7.1 s

################################################################################
                      Learning iteration 880/50000                      

                       Computation: 116673 steps/s (collection: 0.687s, learning 0.156s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.53
                Mean reward (task): 1.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0103
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0101
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0875
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.84s
                        Total time: 680.30s
                               ETA: 632 mins 10.3 s

################################################################################
                      Learning iteration 881/50000                      

                       Computation: 139435 steps/s (collection: 0.565s, learning 0.140s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0103
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0165
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0851
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.71s
                        Total time: 681.01s
                               ETA: 632 mins 5.7 s

################################################################################
                      Learning iteration 882/50000                      

                       Computation: 121072 steps/s (collection: 0.661s, learning 0.151s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0114
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0102
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0163
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0833
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.81s
                        Total time: 681.82s
                               ETA: 632 mins 7.2 s

################################################################################
                      Learning iteration 883/50000                      

                       Computation: 134547 steps/s (collection: 0.597s, learning 0.134s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.38
                Mean reward (task): 1.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0103
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0100
       Mean episode rew_smoothness: -0.0164
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.73s
                        Total time: 682.55s
                               ETA: 632 mins 4.1 s

################################################################################
                      Learning iteration 884/50000                      

                       Computation: 122485 steps/s (collection: 0.674s, learning 0.129s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.54
                Mean reward (task): 1.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0113
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0104
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0100
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0851
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.80s
                        Total time: 683.35s
                               ETA: 632 mins 5.0 s

################################################################################
                      Learning iteration 885/50000                      

                       Computation: 126286 steps/s (collection: 0.648s, learning 0.131s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.23
                Mean reward (task): 1.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0106
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0168
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0887
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.78s
                        Total time: 684.13s
                               ETA: 632 mins 4.6 s

################################################################################
                      Learning iteration 886/50000                      

                       Computation: 138500 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0101
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0096
       Mean episode rew_smoothness: -0.0162
          Mean episode rew_torques: -0.0023
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0819
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.71s
                        Total time: 684.84s
                               ETA: 632 mins 0.4 s

################################################################################
                      Learning iteration 887/50000                      

                       Computation: 124680 steps/s (collection: 0.655s, learning 0.134s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.43
                Mean reward (task): 1.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0105
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0168
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0890
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.79s
                        Total time: 685.63s
                               ETA: 632 mins 0.5 s

################################################################################
                      Learning iteration 888/50000                      

                       Computation: 139104 steps/s (collection: 0.575s, learning 0.131s)
               Value function loss: 0.0387
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.45
                Mean reward (task): 1.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0105
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0101
       Mean episode rew_smoothness: -0.0166
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0868
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.71s
                        Total time: 686.34s
                               ETA: 631 mins 56.1 s

################################################################################
                      Learning iteration 889/50000                      

                       Computation: 134232 steps/s (collection: 0.609s, learning 0.123s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.25
                Mean reward (task): 1.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0105
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0100
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0848
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.73s
                        Total time: 687.07s
                               ETA: 631 mins 53.2 s

################################################################################
                      Learning iteration 890/50000                      

                       Computation: 141509 steps/s (collection: 0.571s, learning 0.123s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.32
                Mean reward (task): 1.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0107
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0168
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0861
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.69s
                        Total time: 687.77s
                               ETA: 631 mins 48.1 s

################################################################################
                      Learning iteration 891/50000                      

                       Computation: 145497 steps/s (collection: 0.552s, learning 0.123s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0115
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0104
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0165
          Mean episode rew_torques: -0.0024
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0838
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.68s
                        Total time: 688.44s
                               ETA: 631 mins 42.1 s

################################################################################
                      Learning iteration 892/50000                      

                       Computation: 143139 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.41
                Mean reward (task): 1.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0109
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0105
       Mean episode rew_smoothness: -0.0173
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0886
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.69s
                        Total time: 689.13s
                               ETA: 631 mins 36.6 s

################################################################################
                      Learning iteration 893/50000                      

                       Computation: 145263 steps/s (collection: 0.553s, learning 0.124s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.41
                Mean reward (task): 1.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0105
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0169
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0876
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.68s
                        Total time: 689.80s
                               ETA: 631 mins 30.6 s

################################################################################
                      Learning iteration 894/50000                      

                       Computation: 135310 steps/s (collection: 0.598s, learning 0.128s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.52
                Mean reward (task): 1.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0108
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0104
       Mean episode rew_smoothness: -0.0172
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.0897
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.73s
                        Total time: 690.53s
                               ETA: 631 mins 27.4 s

################################################################################
                      Learning iteration 895/50000                      

                       Computation: 130217 steps/s (collection: 0.630s, learning 0.125s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0107
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0875
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.75s
                        Total time: 691.29s
                               ETA: 631 mins 25.7 s

################################################################################
                      Learning iteration 896/50000                      

                       Computation: 133849 steps/s (collection: 0.599s, learning 0.135s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.27
                Mean reward (task): 1.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0109
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0169
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0868
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.73s
                        Total time: 692.02s
                               ETA: 631 mins 22.9 s

################################################################################
                      Learning iteration 897/50000                      

                       Computation: 126903 steps/s (collection: 0.648s, learning 0.126s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.31
                Mean reward (task): 1.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0108
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0874
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.77s
                        Total time: 692.79s
                               ETA: 631 mins 22.3 s

################################################################################
                      Learning iteration 898/50000                      

                       Computation: 124754 steps/s (collection: 0.662s, learning 0.126s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.34
                Mean reward (task): 1.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0106
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0100
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0863
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.79s
                        Total time: 693.58s
                               ETA: 631 mins 22.4 s

################################################################################
                      Learning iteration 899/50000                      

                       Computation: 131542 steps/s (collection: 0.624s, learning 0.124s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 61.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0116
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0104
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0101
       Mean episode rew_smoothness: -0.0165
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0221
 Mean episode rew_tracking_lin_vel: 0.0862
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.75s
                        Total time: 694.33s
                               ETA: 631 mins 20.3 s

################################################################################
                      Learning iteration 900/50000                      

                       Computation: 144064 steps/s (collection: 0.558s, learning 0.124s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.15
                Mean reward (task): 1.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 58.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0107
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0101
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0871
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.68s
                        Total time: 695.01s
                               ETA: 631 mins 14.7 s

################################################################################
                      Learning iteration 901/50000                      

                       Computation: 143675 steps/s (collection: 0.560s, learning 0.124s)
               Value function loss: 0.0373
                    Surrogate loss: 0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0105
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0219
 Mean episode rew_tracking_lin_vel: 0.0861
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.68s
                        Total time: 695.70s
                               ETA: 631 mins 9.2 s

################################################################################
                      Learning iteration 902/50000                      

                       Computation: 132680 steps/s (collection: 0.595s, learning 0.146s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.54
                Mean reward (task): 1.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0111
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0104
       Mean episode rew_smoothness: -0.0170
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0226
 Mean episode rew_tracking_lin_vel: 0.0897
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.74s
                        Total time: 696.44s
                               ETA: 631 mins 6.8 s

################################################################################
                      Learning iteration 903/50000                      

                       Computation: 124869 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.18
                Mean reward (task): 1.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0117
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0106
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0166
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0855
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.79s
                        Total time: 697.22s
                               ETA: 631 mins 6.9 s

################################################################################
                      Learning iteration 904/50000                      

                       Computation: 131833 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0403
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 59.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0107
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0099
       Mean episode rew_smoothness: -0.0166
          Mean episode rew_torques: -0.0025
 Mean episode rew_tracking_ang_vel: 0.0222
 Mean episode rew_tracking_lin_vel: 0.0841
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.75s
                        Total time: 697.97s
                               ETA: 631 mins 4.7 s

################################################################################
                      Learning iteration 905/50000                      

                       Computation: 141832 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.18
                Mean reward (task): 1.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0121
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0112
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0105
       Mean episode rew_smoothness: -0.0173
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0913
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.69s
                        Total time: 698.66s
                               ETA: 630 mins 59.7 s

################################################################################
                      Learning iteration 906/50000                      

                       Computation: 138144 steps/s (collection: 0.588s, learning 0.124s)
               Value function loss: 0.0410
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.34
                Mean reward (task): 1.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0109
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0102
       Mean episode rew_smoothness: -0.0167
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0225
 Mean episode rew_tracking_lin_vel: 0.0873
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.71s
                        Total time: 699.38s
                               ETA: 630 mins 55.7 s

################################################################################
                      Learning iteration 907/50000                      

                       Computation: 138084 steps/s (collection: 0.590s, learning 0.122s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0113
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0106
       Mean episode rew_smoothness: -0.0173
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.0924
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.71s
                        Total time: 700.09s
                               ETA: 630 mins 51.7 s

################################################################################
                      Learning iteration 908/50000                      

                       Computation: 123994 steps/s (collection: 0.659s, learning 0.134s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.46
                Mean reward (task): 1.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0110
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0103
       Mean episode rew_smoothness: -0.0170
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0227
 Mean episode rew_tracking_lin_vel: 0.0893
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.79s
                        Total time: 700.88s
                               ETA: 630 mins 52.1 s

################################################################################
                      Learning iteration 909/50000                      

                       Computation: 142367 steps/s (collection: 0.552s, learning 0.138s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0118
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0112
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0103
       Mean episode rew_smoothness: -0.0168
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0895
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.69s
                        Total time: 701.57s
                               ETA: 630 mins 47.0 s

################################################################################
                      Learning iteration 910/50000                      

                       Computation: 129432 steps/s (collection: 0.606s, learning 0.153s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.42
                Mean reward (task): 1.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0112
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0106
       Mean episode rew_smoothness: -0.0172
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0926
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.76s
                        Total time: 702.33s
                               ETA: 630 mins 45.6 s

################################################################################
                      Learning iteration 911/50000                      

                       Computation: 123317 steps/s (collection: 0.652s, learning 0.145s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.12
                Mean reward (task): 1.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0112
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0105
       Mean episode rew_smoothness: -0.0172
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0229
 Mean episode rew_tracking_lin_vel: 0.0903
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.80s
                        Total time: 703.13s
                               ETA: 630 mins 46.3 s

################################################################################
                      Learning iteration 912/50000                      

                       Computation: 145581 steps/s (collection: 0.552s, learning 0.123s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.33
                Mean reward (task): 1.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0111
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0104
       Mean episode rew_smoothness: -0.0170
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0224
 Mean episode rew_tracking_lin_vel: 0.0882
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.68s
                        Total time: 703.80s
                               ETA: 630 mins 40.4 s

################################################################################
                      Learning iteration 913/50000                      

                       Computation: 130419 steps/s (collection: 0.628s, learning 0.126s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.44
                Mean reward (task): 1.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0113
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0108
       Mean episode rew_smoothness: -0.0175
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.0938
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.75s
                        Total time: 704.56s
                               ETA: 630 mins 38.7 s

################################################################################
                      Learning iteration 914/50000                      

                       Computation: 124248 steps/s (collection: 0.660s, learning 0.131s)
               Value function loss: 0.0446
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0119
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0111
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0105
       Mean episode rew_smoothness: -0.0170
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0892
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.79s
                        Total time: 705.35s
                               ETA: 630 mins 39.0 s

################################################################################
                      Learning iteration 915/50000                      

                       Computation: 131153 steps/s (collection: 0.599s, learning 0.150s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.47
                Mean reward (task): 1.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0113
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0106
       Mean episode rew_smoothness: -0.0175
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0913
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.75s
                        Total time: 706.10s
                               ETA: 630 mins 37.1 s

################################################################################
                      Learning iteration 916/50000                      

                       Computation: 124723 steps/s (collection: 0.635s, learning 0.153s)
               Value function loss: 0.0433
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.41
                Mean reward (task): 1.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0121
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0114
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0108
       Mean episode rew_smoothness: -0.0174
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0917
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.79s
                        Total time: 706.88s
                               ETA: 630 mins 37.2 s

################################################################################
                      Learning iteration 917/50000                      

                       Computation: 144727 steps/s (collection: 0.555s, learning 0.124s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.81
                Mean reward (task): 1.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0185
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.0990
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.68s
                        Total time: 707.56s
                               ETA: 630 mins 31.6 s

################################################################################
                      Learning iteration 918/50000                      

                       Computation: 126919 steps/s (collection: 0.652s, learning 0.122s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.30
                Mean reward (task): 1.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0116
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.0942
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.77s
                        Total time: 708.34s
                               ETA: 630 mins 31.0 s

################################################################################
                      Learning iteration 919/50000                      

                       Computation: 128878 steps/s (collection: 0.628s, learning 0.135s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.90
                Mean reward (task): 1.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0176
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.0944
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.76s
                        Total time: 709.10s
                               ETA: 630 mins 29.8 s

################################################################################
                      Learning iteration 920/50000                      

                       Computation: 139675 steps/s (collection: 0.559s, learning 0.145s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.32
                Mean reward (task): 1.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0175
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.0954
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.70s
                        Total time: 709.81s
                               ETA: 630 mins 25.4 s

################################################################################
                      Learning iteration 921/50000                      

                       Computation: 132958 steps/s (collection: 0.614s, learning 0.125s)
               Value function loss: 0.0410
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.62
                Mean reward (task): 1.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0118
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0109
       Mean episode rew_smoothness: -0.0176
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.0936
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.74s
                        Total time: 710.54s
                               ETA: 630 mins 23.0 s

################################################################################
                      Learning iteration 922/50000                      

                       Computation: 130055 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0408
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.39
                Mean reward (task): 1.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0118
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0176
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.0951
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.76s
                        Total time: 711.30s
                               ETA: 630 mins 21.5 s

################################################################################
                      Learning iteration 923/50000                      

                       Computation: 110863 steps/s (collection: 0.750s, learning 0.137s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.60
                Mean reward (task): 1.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0121
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0114
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0107
       Mean episode rew_smoothness: -0.0174
          Mean episode rew_torques: -0.0027
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0919
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.89s
                        Total time: 712.19s
                               ETA: 630 mins 26.8 s

################################################################################
                      Learning iteration 924/50000                      

                       Computation: 131173 steps/s (collection: 0.605s, learning 0.144s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.0982
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.75s
                        Total time: 712.94s
                               ETA: 630 mins 24.9 s

################################################################################
                      Learning iteration 925/50000                      

                       Computation: 141278 steps/s (collection: 0.556s, learning 0.139s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.47
                Mean reward (task): 1.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0121
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.0973
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.70s
                        Total time: 713.63s
                               ETA: 630 mins 20.2 s

################################################################################
                      Learning iteration 926/50000                      

                       Computation: 126832 steps/s (collection: 0.644s, learning 0.131s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.64
                Mean reward (task): 1.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0112
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.0969
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.78s
                        Total time: 714.41s
                               ETA: 630 mins 19.7 s

################################################################################
                      Learning iteration 927/50000                      

                       Computation: 131652 steps/s (collection: 0.626s, learning 0.121s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.16
                Mean reward (task): 1.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.1023
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.75s
                        Total time: 715.15s
                               ETA: 630 mins 17.6 s

################################################################################
                      Learning iteration 928/50000                      

                       Computation: 147144 steps/s (collection: 0.544s, learning 0.124s)
               Value function loss: 0.0479
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.56
                Mean reward (task): 1.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0180
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.0970
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.67s
                        Total time: 715.82s
                               ETA: 630 mins 11.4 s

################################################################################
                      Learning iteration 929/50000                      

                       Computation: 145211 steps/s (collection: 0.555s, learning 0.122s)
               Value function loss: 0.0465
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.48
                Mean reward (task): 1.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0121
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.0974
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.68s
                        Total time: 716.50s
                               ETA: 630 mins 5.7 s

################################################################################
                      Learning iteration 930/50000                      

                       Computation: 128343 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.27
                Mean reward (task): 1.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0123
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0108
       Mean episode rew_smoothness: -0.0174
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0927
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.77s
                        Total time: 717.27s
                               ETA: 630 mins 4.7 s

################################################################################
                      Learning iteration 931/50000                      

                       Computation: 137426 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.28
                Mean reward (task): 1.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0112
       Mean episode rew_smoothness: -0.0179
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.0969
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.72s
                        Total time: 717.98s
                               ETA: 630 mins 1.1 s

################################################################################
                      Learning iteration 932/50000                      

                       Computation: 132029 steps/s (collection: 0.618s, learning 0.127s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.29
                Mean reward (task): 1.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0122
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0108
       Mean episode rew_smoothness: -0.0173
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.0941
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.74s
                        Total time: 718.72s
                               ETA: 629 mins 58.9 s

################################################################################
                      Learning iteration 933/50000                      

                       Computation: 129063 steps/s (collection: 0.623s, learning 0.139s)
               Value function loss: 0.0486
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.40
                Mean reward (task): 1.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0121
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0108
       Mean episode rew_smoothness: -0.0171
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0960
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.76s
                        Total time: 719.49s
                               ETA: 629 mins 57.7 s

################################################################################
                      Learning iteration 934/50000                      

                       Computation: 119757 steps/s (collection: 0.695s, learning 0.126s)
               Value function loss: 0.0475
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.38
                Mean reward (task): 1.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0118
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0176
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.0978
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.82s
                        Total time: 720.31s
                               ETA: 629 mins 59.6 s

################################################################################
                      Learning iteration 935/50000                      

                       Computation: 128989 steps/s (collection: 0.633s, learning 0.129s)
               Value function loss: 0.0505
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.72
                Mean reward (task): 1.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0260
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1005
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.76s
                        Total time: 721.07s
                               ETA: 629 mins 58.4 s

################################################################################
                      Learning iteration 936/50000                      

                       Computation: 136677 steps/s (collection: 0.580s, learning 0.139s)
               Value function loss: 0.0524
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.31
                Mean reward (task): 1.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0268
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0184
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0175
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.0972
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.72s
                        Total time: 721.79s
                               ETA: 629 mins 54.9 s

################################################################################
                      Learning iteration 937/50000                      

                       Computation: 139861 steps/s (collection: 0.567s, learning 0.135s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.35
                Mean reward (task): 1.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0120
       Mean episode rew_ang_vel_xy: -0.0124
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0251
   Mean episode rew_dof_pos_limits: -0.0115
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0107
       Mean episode rew_smoothness: -0.0170
          Mean episode rew_torques: -0.0028
 Mean episode rew_tracking_ang_vel: 0.0228
 Mean episode rew_tracking_lin_vel: 0.0941
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.70s
                        Total time: 722.49s
                               ETA: 629 mins 50.6 s

################################################################################
                      Learning iteration 938/50000                      

                       Computation: 124070 steps/s (collection: 0.660s, learning 0.132s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.50
                Mean reward (task): 1.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1014
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.79s
                        Total time: 723.28s
                               ETA: 629 mins 51.0 s

################################################################################
                      Learning iteration 939/50000                      

                       Computation: 142308 steps/s (collection: 0.566s, learning 0.125s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.40
                Mean reward (task): 1.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0111
       Mean episode rew_smoothness: -0.0180
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.0984
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.69s
                        Total time: 723.97s
                               ETA: 629 mins 46.1 s

################################################################################
                      Learning iteration 940/50000                      

                       Computation: 147392 steps/s (collection: 0.543s, learning 0.124s)
               Value function loss: 0.0486
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.51
                Mean reward (task): 1.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.67s
                        Total time: 724.64s
                               ETA: 629 mins 39.9 s

################################################################################
                      Learning iteration 941/50000                      

                       Computation: 127965 steps/s (collection: 0.642s, learning 0.126s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.69
                Mean reward (task): 1.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.0999
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.77s
                        Total time: 725.41s
                               ETA: 629 mins 39.1 s

################################################################################
                      Learning iteration 942/50000                      

                       Computation: 140547 steps/s (collection: 0.575s, learning 0.124s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.67
                Mean reward (task): 1.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1005
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.70s
                        Total time: 726.11s
                               ETA: 629 mins 34.6 s

################################################################################
                      Learning iteration 943/50000                      

                       Computation: 143359 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.39
                Mean reward (task): 1.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.0985
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.69s
                        Total time: 726.80s
                               ETA: 629 mins 29.5 s

################################################################################
                      Learning iteration 944/50000                      

                       Computation: 142558 steps/s (collection: 0.553s, learning 0.136s)
               Value function loss: 0.0478
                    Surrogate loss: 0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.39
                Mean reward (task): 1.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0112
       Mean episode rew_smoothness: -0.0179
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0953
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.69s
                        Total time: 727.48s
                               ETA: 629 mins 24.5 s

################################################################################
                      Learning iteration 945/50000                      

                       Computation: 120288 steps/s (collection: 0.684s, learning 0.134s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.54
                Mean reward (task): 1.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0121
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0180
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.82s
                        Total time: 728.30s
                               ETA: 629 mins 26.2 s

################################################################################
                      Learning iteration 946/50000                      

                       Computation: 116722 steps/s (collection: 0.705s, learning 0.137s)
               Value function loss: 0.0463
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.75
                Mean reward (task): 1.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0268
   Mean episode rew_dof_pos_limits: -0.0119
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0112
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.0989
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.84s
                        Total time: 729.14s
                               ETA: 629 mins 29.2 s

################################################################################
                      Learning iteration 947/50000                      

                       Computation: 146857 steps/s (collection: 0.547s, learning 0.123s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.69
                Mean reward (task): 1.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0118
       Mean episode rew_smoothness: -0.0186
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.1050
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.67s
                        Total time: 729.81s
                               ETA: 629 mins 23.2 s

################################################################################
                      Learning iteration 948/50000                      

                       Computation: 138602 steps/s (collection: 0.585s, learning 0.124s)
               Value function loss: 0.0492
                    Surrogate loss: 0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.72
                Mean reward (task): 1.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0240
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.71s
                        Total time: 730.52s
                               ETA: 629 mins 19.3 s

################################################################################
                      Learning iteration 949/50000                      

                       Computation: 141310 steps/s (collection: 0.573s, learning 0.123s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.83
                Mean reward (task): 1.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0128
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0120
       Mean episode rew_smoothness: -0.0191
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.70s
                        Total time: 731.22s
                               ETA: 629 mins 14.7 s

################################################################################
                      Learning iteration 950/50000                      

                       Computation: 124298 steps/s (collection: 0.643s, learning 0.148s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.86
                Mean reward (task): 1.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1000
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.79s
                        Total time: 732.01s
                               ETA: 629 mins 15.1 s

################################################################################
                      Learning iteration 951/50000                      

                       Computation: 140486 steps/s (collection: 0.553s, learning 0.146s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.66
                Mean reward (task): 1.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0131
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0129
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0120
       Mean episode rew_smoothness: -0.0187
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0247
 Mean episode rew_tracking_lin_vel: 0.1048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.70s
                        Total time: 732.71s
                               ETA: 629 mins 10.7 s

################################################################################
                      Learning iteration 952/50000                      

                       Computation: 129982 steps/s (collection: 0.612s, learning 0.144s)
               Value function loss: 0.0489
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.69
                Mean reward (task): 1.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0131
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0188
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.1044
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.76s
                        Total time: 733.47s
                               ETA: 629 mins 9.2 s

################################################################################
                      Learning iteration 953/50000                      

                       Computation: 126797 steps/s (collection: 0.634s, learning 0.141s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.32
                Mean reward (task): 1.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 66.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1002
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.78s
                        Total time: 734.24s
                               ETA: 629 mins 8.7 s

################################################################################
                      Learning iteration 954/50000                      

                       Computation: 130797 steps/s (collection: 0.613s, learning 0.139s)
               Value function loss: 0.0498
                    Surrogate loss: 0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.30
                Mean reward (task): 1.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 63.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0184
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1046
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.75s
                        Total time: 734.99s
                               ETA: 629 mins 7.0 s

################################################################################
                      Learning iteration 955/50000                      

                       Computation: 121150 steps/s (collection: 0.663s, learning 0.148s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.52
                Mean reward (task): 1.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0184
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1039
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.81s
                        Total time: 735.80s
                               ETA: 629 mins 8.4 s

################################################################################
                      Learning iteration 956/50000                      

                       Computation: 121696 steps/s (collection: 0.678s, learning 0.129s)
               Value function loss: 0.0508
                    Surrogate loss: 0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.61
                Mean reward (task): 1.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1027
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.81s
                        Total time: 736.61s
                               ETA: 629 mins 9.6 s

################################################################################
                      Learning iteration 957/50000                      

                       Computation: 144194 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0501
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.47
                Mean reward (task): 1.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0180
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1001
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.68s
                        Total time: 737.29s
                               ETA: 629 mins 4.3 s

################################################################################
                      Learning iteration 958/50000                      

                       Computation: 141581 steps/s (collection: 0.551s, learning 0.143s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.08
                Mean reward (task): 1.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 62.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0125
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0179
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.0996
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.69s
                        Total time: 737.99s
                               ETA: 628 mins 59.7 s

################################################################################
                      Learning iteration 959/50000                      

                       Computation: 113641 steps/s (collection: 0.722s, learning 0.143s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.45
                Mean reward (task): 1.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1003
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.87s
                        Total time: 738.85s
                               ETA: 629 mins 3.8 s

################################################################################
                      Learning iteration 960/50000                      

                       Computation: 128701 steps/s (collection: 0.641s, learning 0.123s)
               Value function loss: 0.0502
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.67
                Mean reward (task): 1.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0254
   Mean episode rew_dof_pos_limits: -0.0124
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0179
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1008
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.76s
                        Total time: 739.62s
                               ETA: 629 mins 2.8 s

################################################################################
                      Learning iteration 961/50000                      

                       Computation: 135953 steps/s (collection: 0.595s, learning 0.128s)
               Value function loss: 0.0495
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 68.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0124
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1020
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.72s
                        Total time: 740.34s
                               ETA: 628 mins 59.6 s

################################################################################
                      Learning iteration 962/50000                      

                       Computation: 127600 steps/s (collection: 0.618s, learning 0.152s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0126
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0116
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1036
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.77s
                        Total time: 741.11s
                               ETA: 628 mins 58.9 s

################################################################################
                      Learning iteration 963/50000                      

                       Computation: 141719 steps/s (collection: 0.551s, learning 0.143s)
               Value function loss: 0.0514
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.77
                Mean reward (task): 1.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0252
   Mean episode rew_dof_pos_limits: -0.0121
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0111
       Mean episode rew_smoothness: -0.0177
          Mean episode rew_torques: -0.0029
 Mean episode rew_tracking_ang_vel: 0.0231
 Mean episode rew_tracking_lin_vel: 0.0970
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.69s
                        Total time: 741.80s
                               ETA: 628 mins 54.2 s

################################################################################
                      Learning iteration 964/50000                      

                       Computation: 123684 steps/s (collection: 0.659s, learning 0.136s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.52
                Mean reward (task): 1.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0124
       Mean episode rew_ang_vel_xy: -0.0125
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0251
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0112
       Mean episode rew_smoothness: -0.0177
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.0991
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.79s
                        Total time: 742.60s
                               ETA: 628 mins 54.8 s

################################################################################
                      Learning iteration 965/50000                      

                       Computation: 145890 steps/s (collection: 0.551s, learning 0.123s)
               Value function loss: 0.0510
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.50
                Mean reward (task): 1.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0126
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0124
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0235
 Mean episode rew_tracking_lin_vel: 0.1000
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.67s
                        Total time: 743.27s
                               ETA: 628 mins 49.1 s

################################################################################
                      Learning iteration 966/50000                      

                       Computation: 125452 steps/s (collection: 0.638s, learning 0.146s)
               Value function loss: 0.0540
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.93
                Mean reward (task): 1.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1016
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.78s
                        Total time: 744.06s
                               ETA: 628 mins 49.1 s

################################################################################
                      Learning iteration 967/50000                      

                       Computation: 123894 steps/s (collection: 0.652s, learning 0.141s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.47
                Mean reward (task): 1.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0131
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0118
       Mean episode rew_smoothness: -0.0187
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0244
 Mean episode rew_tracking_lin_vel: 0.1045
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.79s
                        Total time: 744.85s
                               ETA: 628 mins 49.5 s

################################################################################
                      Learning iteration 968/50000                      

                       Computation: 139187 steps/s (collection: 0.573s, learning 0.133s)
               Value function loss: 0.0498
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.66
                Mean reward (task): 1.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0116
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0239
 Mean episode rew_tracking_lin_vel: 0.1020
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.71s
                        Total time: 745.56s
                               ETA: 628 mins 45.6 s

################################################################################
                      Learning iteration 969/50000                      

                       Computation: 146576 steps/s (collection: 0.547s, learning 0.124s)
               Value function loss: 0.0492
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.30
                Mean reward (task): 1.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1021
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.67s
                        Total time: 746.23s
                               ETA: 628 mins 39.8 s

################################################################################
                      Learning iteration 970/50000                      

                       Computation: 128099 steps/s (collection: 0.610s, learning 0.157s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.39
                Mean reward (task): 1.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0131
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0187
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1038
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.77s
                        Total time: 746.99s
                               ETA: 628 mins 38.9 s

################################################################################
                      Learning iteration 971/50000                      

                       Computation: 141156 steps/s (collection: 0.557s, learning 0.139s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.36
                Mean reward (task): 1.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0260
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0116
       Mean episode rew_smoothness: -0.0183
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0234
 Mean episode rew_tracking_lin_vel: 0.1009
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.70s
                        Total time: 747.69s
                               ETA: 628 mins 34.5 s

################################################################################
                      Learning iteration 972/50000                      

                       Computation: 136158 steps/s (collection: 0.590s, learning 0.132s)
               Value function loss: 0.0501
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.63
                Mean reward (task): 1.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 71.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0124
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0194
           Mean episode rew_no_fly: 0.0115
       Mean episode rew_smoothness: -0.0182
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.1015
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.72s
                        Total time: 748.41s
                               ETA: 628 mins 31.3 s

################################################################################
                      Learning iteration 973/50000                      

                       Computation: 146186 steps/s (collection: 0.549s, learning 0.123s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.44
                Mean reward (task): 1.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0128
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0114
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.1011
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.67s
                        Total time: 749.08s
                               ETA: 628 mins 25.7 s

################################################################################
                      Learning iteration 974/50000                      

                       Computation: 131256 steps/s (collection: 0.623s, learning 0.126s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.59
                Mean reward (task): 1.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0129
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0123
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0113
       Mean episode rew_smoothness: -0.0181
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0230
 Mean episode rew_tracking_lin_vel: 0.0989
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.75s
                        Total time: 749.83s
                               ETA: 628 mins 23.9 s

################################################################################
                      Learning iteration 975/50000                      

                       Computation: 142514 steps/s (collection: 0.550s, learning 0.140s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.37
                Mean reward (task): 1.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0127
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0122
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0110
       Mean episode rew_smoothness: -0.0178
          Mean episode rew_torques: -0.0030
 Mean episode rew_tracking_ang_vel: 0.0223
 Mean episode rew_tracking_lin_vel: 0.0954
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.69s
                        Total time: 750.52s
                               ETA: 628 mins 19.2 s

################################################################################
                      Learning iteration 976/50000                      

                       Computation: 130500 steps/s (collection: 0.614s, learning 0.140s)
               Value function loss: 0.0497
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.51
                Mean reward (task): 1.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0134
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0121
       Mean episode rew_smoothness: -0.0191
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0247
 Mean episode rew_tracking_lin_vel: 0.1081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.75s
                        Total time: 751.28s
                               ETA: 628 mins 17.6 s

################################################################################
                      Learning iteration 977/50000                      

                       Computation: 123801 steps/s (collection: 0.648s, learning 0.146s)
               Value function loss: 0.0489
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.77
                Mean reward (task): 1.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0130
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0256
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0185
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.1031
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.79s
                        Total time: 752.07s
                               ETA: 628 mins 18.1 s

################################################################################
                      Learning iteration 978/50000                      

                       Computation: 121272 steps/s (collection: 0.667s, learning 0.144s)
               Value function loss: 0.0494
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.68
                Mean reward (task): 1.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0131
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0119
       Mean episode rew_smoothness: -0.0189
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0242
 Mean episode rew_tracking_lin_vel: 0.1051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.81s
                        Total time: 752.88s
                               ETA: 628 mins 19.4 s

################################################################################
                      Learning iteration 979/50000                      

                       Computation: 143253 steps/s (collection: 0.546s, learning 0.140s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.37
                Mean reward (task): 1.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 65.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0130
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0117
       Mean episode rew_smoothness: -0.0186
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0233
 Mean episode rew_tracking_lin_vel: 0.1016
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.69s
                        Total time: 753.57s
                               ETA: 628 mins 14.5 s

################################################################################
                      Learning iteration 980/50000                      

                       Computation: 138658 steps/s (collection: 0.556s, learning 0.153s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.34
                Mean reward (task): 1.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0130
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0127
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0116
       Mean episode rew_smoothness: -0.0184
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0232
 Mean episode rew_tracking_lin_vel: 0.1017
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.71s
                        Total time: 754.28s
                               ETA: 628 mins 10.7 s

################################################################################
                      Learning iteration 981/50000                      

                       Computation: 140213 steps/s (collection: 0.563s, learning 0.138s)
               Value function loss: 0.0465
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.57
                Mean reward (task): 1.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0131
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0118
       Mean episode rew_smoothness: -0.0188
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0237
 Mean episode rew_tracking_lin_vel: 0.1045
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.70s
                        Total time: 754.98s
                               ETA: 628 mins 6.6 s

################################################################################
                      Learning iteration 982/50000                      

                       Computation: 146163 steps/s (collection: 0.531s, learning 0.142s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.79
                Mean reward (task): 1.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0131
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0120
       Mean episode rew_smoothness: -0.0189
          Mean episode rew_torques: -0.0032
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1060
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.67s
                        Total time: 755.65s
                               ETA: 628 mins 1.0 s

################################################################################
                      Learning iteration 983/50000                      

                       Computation: 136852 steps/s (collection: 0.580s, learning 0.138s)
               Value function loss: 0.0498
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.00
                Mean reward (task): 2.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0138
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0134
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0124
       Mean episode rew_smoothness: -0.0198
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1118
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.72s
                        Total time: 756.37s
                               ETA: 627 mins 57.7 s

################################################################################
                      Learning iteration 984/50000                      

                       Computation: 149207 steps/s (collection: 0.535s, learning 0.124s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.75
                Mean reward (task): 1.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0131
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0129
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0118
       Mean episode rew_smoothness: -0.0187
          Mean episode rew_torques: -0.0031
 Mean episode rew_tracking_ang_vel: 0.0236
 Mean episode rew_tracking_lin_vel: 0.1024
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.66s
                        Total time: 757.03s
                               ETA: 627 mins 51.5 s

################################################################################
                      Learning iteration 985/50000                      

                       Computation: 133329 steps/s (collection: 0.600s, learning 0.138s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.74
                Mean reward (task): 1.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0135
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0124
       Mean episode rew_smoothness: -0.0197
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0253
 Mean episode rew_tracking_lin_vel: 0.1096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.74s
                        Total time: 757.76s
                               ETA: 627 mins 49.2 s

################################################################################
                      Learning iteration 986/50000                      

                       Computation: 119036 steps/s (collection: 0.675s, learning 0.151s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.80
                Mean reward (task): 1.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0132
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0120
       Mean episode rew_smoothness: -0.0188
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.83s
                        Total time: 758.59s
                               ETA: 627 mins 51.3 s

################################################################################
                      Learning iteration 987/50000                      

                       Computation: 144022 steps/s (collection: 0.558s, learning 0.125s)
               Value function loss: 0.0512
                    Surrogate loss: 0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.30
                Mean reward (task): 1.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 64.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0136
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0125
       Mean episode rew_smoothness: -0.0196
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.68s
                        Total time: 759.27s
                               ETA: 627 mins 46.2 s

################################################################################
                      Learning iteration 988/50000                      

                       Computation: 118760 steps/s (collection: 0.681s, learning 0.147s)
               Value function loss: 0.0496
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.76
                Mean reward (task): 1.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0135
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0122
       Mean episode rew_smoothness: -0.0192
          Mean episode rew_torques: -0.0033
 Mean episode rew_tracking_ang_vel: 0.0243
 Mean episode rew_tracking_lin_vel: 0.1084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.83s
                        Total time: 760.10s
                               ETA: 627 mins 48.4 s

################################################################################
                      Learning iteration 989/50000                      

                       Computation: 143769 steps/s (collection: 0.560s, learning 0.123s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.63
                Mean reward (task): 1.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0127
       Mean episode rew_smoothness: -0.0201
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.68s
                        Total time: 760.78s
                               ETA: 627 mins 43.4 s

################################################################################
                      Learning iteration 990/50000                      

                       Computation: 149289 steps/s (collection: 0.535s, learning 0.123s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0136
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0139
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0126
       Mean episode rew_smoothness: -0.0195
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.66s
                        Total time: 761.44s
                               ETA: 627 mins 37.2 s

################################################################################
                      Learning iteration 991/50000                      

                       Computation: 135561 steps/s (collection: 0.602s, learning 0.123s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.70
                Mean reward (task): 1.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0141
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0128
       Mean episode rew_smoothness: -0.0200
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.73s
                        Total time: 762.17s
                               ETA: 627 mins 34.3 s

################################################################################
                      Learning iteration 992/50000                      

                       Computation: 146341 steps/s (collection: 0.548s, learning 0.123s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.76
                Mean reward (task): 1.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0136
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0124
       Mean episode rew_smoothness: -0.0193
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0247
 Mean episode rew_tracking_lin_vel: 0.1100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.67s
                        Total time: 762.84s
                               ETA: 627 mins 28.8 s

################################################################################
                      Learning iteration 993/50000                      

                       Computation: 149968 steps/s (collection: 0.534s, learning 0.121s)
               Value function loss: 0.0559
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.90
                Mean reward (task): 1.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0137
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0124
       Mean episode rew_smoothness: -0.0194
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.1116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.66s
                        Total time: 763.50s
                               ETA: 627 mins 22.5 s

################################################################################
                      Learning iteration 994/50000                      

                       Computation: 146072 steps/s (collection: 0.551s, learning 0.122s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.75
                Mean reward (task): 1.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0138
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0126
       Mean episode rew_smoothness: -0.0199
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0253
 Mean episode rew_tracking_lin_vel: 0.1124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.67s
                        Total time: 764.17s
                               ETA: 627 mins 17.0 s

################################################################################
                      Learning iteration 995/50000                      

                       Computation: 125987 steps/s (collection: 0.656s, learning 0.125s)
               Value function loss: 0.0511
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.47
                Mean reward (task): 1.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0129
       Mean episode rew_smoothness: -0.0198
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0254
 Mean episode rew_tracking_lin_vel: 0.1161
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.78s
                        Total time: 764.95s
                               ETA: 627 mins 16.8 s

################################################################################
                      Learning iteration 996/50000                      

                       Computation: 146764 steps/s (collection: 0.545s, learning 0.125s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.41
                Mean reward (task): 1.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0138
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0125
       Mean episode rew_smoothness: -0.0198
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.1117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.67s
                        Total time: 765.62s
                               ETA: 627 mins 11.2 s

################################################################################
                      Learning iteration 997/50000                      

                       Computation: 129363 steps/s (collection: 0.601s, learning 0.159s)
               Value function loss: 0.0561
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.86
                Mean reward (task): 1.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0136
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0125
       Mean episode rew_smoothness: -0.0193
          Mean episode rew_torques: -0.0034
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.76s
                        Total time: 766.38s
                               ETA: 627 mins 10.1 s

################################################################################
                      Learning iteration 998/50000                      

                       Computation: 145989 steps/s (collection: 0.549s, learning 0.124s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.77
                Mean reward (task): 1.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0136
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0129
       Mean episode rew_smoothness: -0.0196
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0253
 Mean episode rew_tracking_lin_vel: 0.1166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.67s
                        Total time: 767.05s
                               ETA: 627 mins 4.7 s

################################################################################
                      Learning iteration 999/50000                      

                       Computation: 130463 steps/s (collection: 0.576s, learning 0.178s)
               Value function loss: 0.0574
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.07
                Mean reward (task): 2.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0131
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0143
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0130
       Mean episode rew_smoothness: -0.0199
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0254
 Mean episode rew_tracking_lin_vel: 0.1153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.75s
                        Total time: 767.80s
                               ETA: 627 mins 3.2 s

################################################################################
                     Learning iteration 1000/50000                      

                       Computation: 120866 steps/s (collection: 0.664s, learning 0.150s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.76
                Mean reward (task): 1.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0238
   Mean episode rew_dof_pos_limits: -0.0141
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0128
       Mean episode rew_smoothness: -0.0194
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0251
 Mean episode rew_tracking_lin_vel: 0.1168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 0.81s
                        Total time: 768.62s
                               ETA: 627 mins 4.7 s

################################################################################
                     Learning iteration 1001/50000                      

                       Computation: 138729 steps/s (collection: 0.584s, learning 0.125s)
               Value function loss: 0.0533
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.78
                Mean reward (task): 1.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0256
   Mean episode rew_dof_pos_limits: -0.0145
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0131
       Mean episode rew_smoothness: -0.0199
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 0.71s
                        Total time: 769.33s
                               ETA: 627 mins 1.0 s

################################################################################
                     Learning iteration 1002/50000                      

                       Computation: 121101 steps/s (collection: 0.672s, learning 0.140s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.68
                Mean reward (task): 1.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0133
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0246
   Mean episode rew_dof_pos_limits: -0.0138
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0123
       Mean episode rew_smoothness: -0.0191
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0238
 Mean episode rew_tracking_lin_vel: 0.1097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 0.81s
                        Total time: 770.14s
                               ETA: 627 mins 2.4 s

################################################################################
                     Learning iteration 1003/50000                      

                       Computation: 134335 steps/s (collection: 0.574s, learning 0.158s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.77
                Mean reward (task): 1.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 73.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0138
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0122
       Mean episode rew_smoothness: -0.0190
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0241
 Mean episode rew_tracking_lin_vel: 0.1090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 0.73s
                        Total time: 770.87s
                               ETA: 626 mins 59.9 s

################################################################################
                     Learning iteration 1004/50000                      

                       Computation: 141056 steps/s (collection: 0.556s, learning 0.140s)
               Value function loss: 0.0538
                    Surrogate loss: 0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.68
                Mean reward (task): 1.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 72.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0148
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0133
       Mean episode rew_smoothness: -0.0204
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0261
 Mean episode rew_tracking_lin_vel: 0.1219
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 0.70s
                        Total time: 771.57s
                               ETA: 626 mins 55.6 s

################################################################################
                     Learning iteration 1005/50000                      

                       Computation: 145292 steps/s (collection: 0.536s, learning 0.140s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.49
                Mean reward (task): 1.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 69.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0256
   Mean episode rew_dof_pos_limits: -0.0141
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0127
       Mean episode rew_smoothness: -0.0195
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0248
 Mean episode rew_tracking_lin_vel: 0.1136
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 0.68s
                        Total time: 772.24s
                               ETA: 626 mins 50.4 s

################################################################################
                     Learning iteration 1006/50000                      

                       Computation: 147145 steps/s (collection: 0.532s, learning 0.136s)
               Value function loss: 0.0518
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.78
                Mean reward (task): 1.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0132
       Mean episode rew_ang_vel_xy: -0.0127
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0246
   Mean episode rew_dof_pos_limits: -0.0139
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0122
       Mean episode rew_smoothness: -0.0190
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.1088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 0.67s
                        Total time: 772.91s
                               ETA: 626 mins 44.8 s

################################################################################
                     Learning iteration 1007/50000                      

                       Computation: 151061 steps/s (collection: 0.527s, learning 0.123s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.85
                Mean reward (task): 1.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0133
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0128
       Mean episode rew_smoothness: -0.0195
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0245
 Mean episode rew_tracking_lin_vel: 0.1154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 0.65s
                        Total time: 773.56s
                               ETA: 626 mins 38.4 s

################################################################################
                     Learning iteration 1008/50000                      

                       Computation: 148694 steps/s (collection: 0.538s, learning 0.123s)
               Value function loss: 0.0518
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.09
                Mean reward (task): 2.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0253
   Mean episode rew_dof_pos_limits: -0.0150
        Mean episode rew_lin_vel_z: -0.0192
           Mean episode rew_no_fly: 0.0134
       Mean episode rew_smoothness: -0.0201
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0259
 Mean episode rew_tracking_lin_vel: 0.1212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 0.66s
                        Total time: 774.22s
                               ETA: 626 mins 32.4 s

################################################################################
                     Learning iteration 1009/50000                      

                       Computation: 152218 steps/s (collection: 0.522s, learning 0.124s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.78
                Mean reward (task): 1.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0129
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0244
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0128
       Mean episode rew_smoothness: -0.0195
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1151
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.65s
                        Total time: 774.87s
                               ETA: 626 mins 25.8 s

################################################################################
                     Learning iteration 1010/50000                      

                       Computation: 151285 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.43
                Mean reward (task): 1.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0134
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0255
   Mean episode rew_dof_pos_limits: -0.0139
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0125
       Mean episode rew_smoothness: -0.0193
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0244
 Mean episode rew_tracking_lin_vel: 0.1106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.65s
                        Total time: 775.52s
                               ETA: 626 mins 19.3 s

################################################################################
                     Learning iteration 1011/50000                      

                       Computation: 148792 steps/s (collection: 0.537s, learning 0.124s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.58
                Mean reward (task): 1.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 70.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0135
       Mean episode rew_ang_vel_xy: -0.0128
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0242
   Mean episode rew_dof_pos_limits: -0.0145
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0130
       Mean episode rew_smoothness: -0.0196
          Mean episode rew_torques: -0.0036
 Mean episode rew_tracking_ang_vel: 0.0255
 Mean episode rew_tracking_lin_vel: 0.1159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.66s
                        Total time: 776.18s
                               ETA: 626 mins 13.4 s

################################################################################
                     Learning iteration 1012/50000                      

                       Computation: 148851 steps/s (collection: 0.536s, learning 0.125s)
               Value function loss: 0.0565
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.43
                Mean reward (task): 1.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 67.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0133
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0249
   Mean episode rew_dof_pos_limits: -0.0140
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0124
       Mean episode rew_smoothness: -0.0190
          Mean episode rew_torques: -0.0035
 Mean episode rew_tracking_ang_vel: 0.0244
 Mean episode rew_tracking_lin_vel: 0.1093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.66s
                        Total time: 776.84s
                               ETA: 626 mins 7.5 s

################################################################################
                     Learning iteration 1013/50000                      

                       Computation: 151056 steps/s (collection: 0.527s, learning 0.124s)
               Value function loss: 0.0542
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.97
                Mean reward (task): 1.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0138
       Mean episode rew_ang_vel_xy: -0.0130
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0251
   Mean episode rew_dof_pos_limits: -0.0145
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0129
       Mean episode rew_smoothness: -0.0198
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0246
 Mean episode rew_tracking_lin_vel: 0.1135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.65s
                        Total time: 777.49s
                               ETA: 626 mins 1.1 s

################################################################################
                     Learning iteration 1014/50000                      

                       Computation: 149484 steps/s (collection: 0.536s, learning 0.121s)
               Value function loss: 0.0564
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.83
                Mean reward (task): 1.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0141
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0152
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0136
       Mean episode rew_smoothness: -0.0205
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0266
 Mean episode rew_tracking_lin_vel: 0.1231
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.66s
                        Total time: 778.15s
                               ETA: 625 mins 55.1 s

################################################################################
                     Learning iteration 1015/50000                      

                       Computation: 150256 steps/s (collection: 0.532s, learning 0.122s)
               Value function loss: 0.0551
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.68
                Mean reward (task): 1.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0141
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0250
   Mean episode rew_dof_pos_limits: -0.0146
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0133
       Mean episode rew_smoothness: -0.0203
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0262
 Mean episode rew_tracking_lin_vel: 0.1179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.65s
                        Total time: 778.80s
                               ETA: 625 mins 48.9 s

################################################################################
                     Learning iteration 1016/50000                      

                       Computation: 146686 steps/s (collection: 0.532s, learning 0.138s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.98
                Mean reward (task): 1.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0139
       Mean episode rew_ang_vel_xy: -0.0133
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0255
   Mean episode rew_dof_pos_limits: -0.0149
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0134
       Mean episode rew_smoothness: -0.0202
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.67s
                        Total time: 779.47s
                               ETA: 625 mins 43.5 s

################################################################################
                     Learning iteration 1017/50000                      

                       Computation: 142232 steps/s (collection: 0.550s, learning 0.141s)
               Value function loss: 0.0563
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.60
                Mean reward (task): 1.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0151
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0136
       Mean episode rew_smoothness: -0.0207
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0263
 Mean episode rew_tracking_lin_vel: 0.1227
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.69s
                        Total time: 780.16s
                               ETA: 625 mins 39.1 s

################################################################################
                     Learning iteration 1018/50000                      

                       Computation: 146818 steps/s (collection: 0.529s, learning 0.140s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.83
                Mean reward (task): 1.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0141
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0147
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0131
       Mean episode rew_smoothness: -0.0204
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0254
 Mean episode rew_tracking_lin_vel: 0.1142
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.67s
                        Total time: 780.83s
                               ETA: 625 mins 33.7 s

################################################################################
                     Learning iteration 1019/50000                      

                       Computation: 151170 steps/s (collection: 0.525s, learning 0.125s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.00
                Mean reward (task): 2.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0140
       Mean episode rew_ang_vel_xy: -0.0132
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0256
   Mean episode rew_dof_pos_limits: -0.0147
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0135
       Mean episode rew_smoothness: -0.0203
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0258
 Mean episode rew_tracking_lin_vel: 0.1183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.65s
                        Total time: 781.48s
                               ETA: 625 mins 27.3 s

################################################################################
                     Learning iteration 1020/50000                      

                       Computation: 152922 steps/s (collection: 0.520s, learning 0.123s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.06
                Mean reward (task): 2.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0137
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0147
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0130
       Mean episode rew_smoothness: -0.0199
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0250
 Mean episode rew_tracking_lin_vel: 0.1145
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.64s
                        Total time: 782.13s
                               ETA: 625 mins 20.7 s

################################################################################
                     Learning iteration 1021/50000                      

                       Computation: 148992 steps/s (collection: 0.537s, learning 0.123s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.05
                Mean reward (task): 2.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0150
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0136
       Mean episode rew_smoothness: -0.0206
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0264
 Mean episode rew_tracking_lin_vel: 0.1220
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.66s
                        Total time: 782.79s
                               ETA: 625 mins 14.8 s

################################################################################
                     Learning iteration 1022/50000                      

                       Computation: 150610 steps/s (collection: 0.529s, learning 0.124s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.70
                Mean reward (task): 1.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 75.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0145
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0155
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0140
       Mean episode rew_smoothness: -0.0210
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0267
 Mean episode rew_tracking_lin_vel: 0.1266
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.65s
                        Total time: 783.44s
                               ETA: 625 mins 8.6 s

################################################################################
                     Learning iteration 1023/50000                      

                       Computation: 151094 steps/s (collection: 0.527s, learning 0.123s)
               Value function loss: 0.0647
                    Surrogate loss: 0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.85
                Mean reward (task): 1.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0144
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0152
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0140
       Mean episode rew_smoothness: -0.0210
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0271
 Mean episode rew_tracking_lin_vel: 0.1249
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.65s
                        Total time: 784.09s
                               ETA: 625 mins 2.3 s

################################################################################
                     Learning iteration 1024/50000                      

                       Computation: 150840 steps/s (collection: 0.528s, learning 0.124s)
               Value function loss: 0.0640
                    Surrogate loss: 0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.14
                Mean reward (task): 2.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0144
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0155
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0142
       Mean episode rew_smoothness: -0.0211
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0269
 Mean episode rew_tracking_lin_vel: 0.1308
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.65s
                        Total time: 784.74s
                               ETA: 624 mins 56.1 s

################################################################################
                     Learning iteration 1025/50000                      

                       Computation: 151473 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.01
                Mean reward (task): 2.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0144
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0153
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0139
       Mean episode rew_smoothness: -0.0209
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0262
 Mean episode rew_tracking_lin_vel: 0.1233
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.65s
                        Total time: 785.39s
                               ETA: 624 mins 49.8 s

################################################################################
                     Learning iteration 1026/50000                      

                       Computation: 148725 steps/s (collection: 0.539s, learning 0.122s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.78
                Mean reward (task): 1.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0141
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0145
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0131
       Mean episode rew_smoothness: -0.0203
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0252
 Mean episode rew_tracking_lin_vel: 0.1172
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.66s
                        Total time: 786.05s
                               ETA: 624 mins 44.0 s

################################################################################
                     Learning iteration 1027/50000                      

                       Computation: 149663 steps/s (collection: 0.533s, learning 0.124s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.59
                Mean reward (task): 1.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0143
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0260
   Mean episode rew_dof_pos_limits: -0.0151
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0135
       Mean episode rew_smoothness: -0.0205
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0263
 Mean episode rew_tracking_lin_vel: 0.1184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.66s
                        Total time: 786.71s
                               ETA: 624 mins 38.1 s

################################################################################
                     Learning iteration 1028/50000                      

                       Computation: 150703 steps/s (collection: 0.526s, learning 0.127s)
               Value function loss: 0.0637
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.51
                Mean reward (task): 2.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0144
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0149
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0135
       Mean episode rew_smoothness: -0.0208
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0266
 Mean episode rew_tracking_lin_vel: 0.1208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.65s
                        Total time: 787.36s
                               ETA: 624 mins 32.0 s

################################################################################
                     Learning iteration 1029/50000                      

                       Computation: 150191 steps/s (collection: 0.531s, learning 0.124s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.79
                Mean reward (task): 1.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0142
       Mean episode rew_ang_vel_xy: -0.0136
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0152
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0137
       Mean episode rew_smoothness: -0.0206
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0267
 Mean episode rew_tracking_lin_vel: 0.1213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.65s
                        Total time: 788.02s
                               ETA: 624 mins 25.9 s

################################################################################
                     Learning iteration 1030/50000                      

                       Computation: 150851 steps/s (collection: 0.526s, learning 0.125s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.18
                Mean reward (task): 2.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0143
       Mean episode rew_ang_vel_xy: -0.0134
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0150
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0137
       Mean episode rew_smoothness: -0.0207
          Mean episode rew_torques: -0.0038
 Mean episode rew_tracking_ang_vel: 0.0266
 Mean episode rew_tracking_lin_vel: 0.1212
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.65s
                        Total time: 788.67s
                               ETA: 624 mins 19.8 s

################################################################################
                     Learning iteration 1031/50000                      

                       Computation: 148805 steps/s (collection: 0.524s, learning 0.137s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.87
                Mean reward (task): 1.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0147
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0261
   Mean episode rew_dof_pos_limits: -0.0155
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0143
       Mean episode rew_smoothness: -0.0213
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0277
 Mean episode rew_tracking_lin_vel: 0.1255
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.66s
                        Total time: 789.33s
                               ETA: 624 mins 14.1 s

################################################################################
                     Learning iteration 1032/50000                      

                       Computation: 135659 steps/s (collection: 0.584s, learning 0.140s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.18
                Mean reward (task): 2.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0149
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0158
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0143
       Mean episode rew_smoothness: -0.0214
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0273
 Mean episode rew_tracking_lin_vel: 0.1261
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.72s
                        Total time: 790.05s
                               ETA: 624 mins 11.4 s

################################################################################
                     Learning iteration 1033/50000                      

                       Computation: 146050 steps/s (collection: 0.534s, learning 0.140s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.90
                Mean reward (task): 1.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0150
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0160
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0145
       Mean episode rew_smoothness: -0.0217
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0282
 Mean episode rew_tracking_lin_vel: 0.1271
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.67s
                        Total time: 790.73s
                               ETA: 624 mins 6.3 s

################################################################################
                     Learning iteration 1034/50000                      

                       Computation: 124104 steps/s (collection: 0.620s, learning 0.173s)
               Value function loss: 0.0606
                    Surrogate loss: 0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.98
                Mean reward (task): 1.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 85.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0151
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0159
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0143
       Mean episode rew_smoothness: -0.0217
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0273
 Mean episode rew_tracking_lin_vel: 0.1277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.79s
                        Total time: 791.52s
                               ETA: 624 mins 6.8 s

################################################################################
                     Learning iteration 1035/50000                      

                       Computation: 133667 steps/s (collection: 0.595s, learning 0.141s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.14
                Mean reward (task): 2.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0163
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0147
       Mean episode rew_smoothness: -0.0221
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0277
 Mean episode rew_tracking_lin_vel: 0.1291
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.74s
                        Total time: 792.25s
                               ETA: 624 mins 4.7 s

################################################################################
                     Learning iteration 1036/50000                      

                       Computation: 136985 steps/s (collection: 0.578s, learning 0.140s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.00
                Mean reward (task): 2.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0150
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0160
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0142
       Mean episode rew_smoothness: -0.0216
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0272
 Mean episode rew_tracking_lin_vel: 0.1253
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.72s
                        Total time: 792.97s
                               ETA: 624 mins 1.7 s

################################################################################
                     Learning iteration 1037/50000                      

                       Computation: 136989 steps/s (collection: 0.577s, learning 0.140s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.67
                Mean reward (task): 1.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 77.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0154
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0163
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0146
       Mean episode rew_smoothness: -0.0222
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0284
 Mean episode rew_tracking_lin_vel: 0.1296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.72s
                        Total time: 793.69s
                               ETA: 623 mins 58.7 s

################################################################################
                     Learning iteration 1038/50000                      

                       Computation: 130446 steps/s (collection: 0.630s, learning 0.123s)
               Value function loss: 0.0600
                    Surrogate loss: 0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.73
                Mean reward (task): 1.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 81.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0161
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0147
       Mean episode rew_smoothness: -0.0221
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0279
 Mean episode rew_tracking_lin_vel: 0.1314
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.75s
                        Total time: 794.44s
                               ETA: 623 mins 57.4 s

################################################################################
                     Learning iteration 1039/50000                      

                       Computation: 143982 steps/s (collection: 0.559s, learning 0.123s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.88
                Mean reward (task): 1.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0162
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0148
       Mean episode rew_smoothness: -0.0223
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0284
 Mean episode rew_tracking_lin_vel: 0.1328
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.68s
                        Total time: 795.12s
                               ETA: 623 mins 52.8 s

################################################################################
                     Learning iteration 1040/50000                      

                       Computation: 124114 steps/s (collection: 0.666s, learning 0.126s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.16
                Mean reward (task): 2.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0167
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0153
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0287
 Mean episode rew_tracking_lin_vel: 0.1349
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.79s
                        Total time: 795.92s
                               ETA: 623 mins 53.3 s

################################################################################
                     Learning iteration 1041/50000                      

                       Computation: 142749 steps/s (collection: 0.567s, learning 0.122s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.91
                Mean reward (task): 1.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0163
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0148
       Mean episode rew_smoothness: -0.0224
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0281
 Mean episode rew_tracking_lin_vel: 0.1312
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.69s
                        Total time: 796.61s
                               ETA: 623 mins 49.0 s

################################################################################
                     Learning iteration 1042/50000                      

                       Computation: 142362 steps/s (collection: 0.554s, learning 0.137s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0162
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0148
       Mean episode rew_smoothness: -0.0224
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0284
 Mean episode rew_tracking_lin_vel: 0.1346
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.69s
                        Total time: 797.30s
                               ETA: 623 mins 44.7 s

################################################################################
                     Learning iteration 1043/50000                      

                       Computation: 137775 steps/s (collection: 0.589s, learning 0.124s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.78
                Mean reward (task): 1.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0167
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0154
       Mean episode rew_smoothness: -0.0232
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0295
 Mean episode rew_tracking_lin_vel: 0.1371
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.71s
                        Total time: 798.01s
                               ETA: 623 mins 41.6 s

################################################################################
                     Learning iteration 1044/50000                      

                       Computation: 146958 steps/s (collection: 0.546s, learning 0.123s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.89
                Mean reward (task): 1.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0154
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0166
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0150
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0287
 Mean episode rew_tracking_lin_vel: 0.1348
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.67s
                        Total time: 798.68s
                               ETA: 623 mins 36.4 s

################################################################################
                     Learning iteration 1045/50000                      

                       Computation: 140073 steps/s (collection: 0.578s, learning 0.124s)
               Value function loss: 0.0585
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.29
                Mean reward (task): 2.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0169
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0155
       Mean episode rew_smoothness: -0.0231
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0297
 Mean episode rew_tracking_lin_vel: 0.1412
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.70s
                        Total time: 799.38s
                               ETA: 623 mins 32.7 s

################################################################################
                     Learning iteration 1046/50000                      

                       Computation: 129969 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.03
                Mean reward (task): 2.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0154
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0165
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0151
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0287
 Mean episode rew_tracking_lin_vel: 0.1344
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.76s
                        Total time: 800.14s
                               ETA: 623 mins 31.5 s

################################################################################
                     Learning iteration 1047/50000                      

                       Computation: 135664 steps/s (collection: 0.603s, learning 0.122s)
               Value function loss: 0.0619
                    Surrogate loss: 0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.55
                Mean reward (task): 2.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0156
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0165
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0151
       Mean episode rew_smoothness: -0.0227
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0290
 Mean episode rew_tracking_lin_vel: 0.1359
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.72s
                        Total time: 800.86s
                               ETA: 623 mins 28.9 s

################################################################################
                     Learning iteration 1048/50000                      

                       Computation: 146511 steps/s (collection: 0.530s, learning 0.141s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.72
                Mean reward (task): 1.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0149
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0157
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0142
       Mean episode rew_smoothness: -0.0214
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0271
 Mean episode rew_tracking_lin_vel: 0.1240
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.67s
                        Total time: 801.53s
                               ETA: 623 mins 23.8 s

################################################################################
                     Learning iteration 1049/50000                      

                       Computation: 145710 steps/s (collection: 0.553s, learning 0.122s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.61
                Mean reward (task): 1.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 76.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0150
       Mean episode rew_ang_vel_xy: -0.0139
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0158
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0143
       Mean episode rew_smoothness: -0.0217
          Mean episode rew_torques: -0.0039
 Mean episode rew_tracking_ang_vel: 0.0271
 Mean episode rew_tracking_lin_vel: 0.1247
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.67s
                        Total time: 802.21s
                               ETA: 623 mins 18.9 s

################################################################################
                     Learning iteration 1050/50000                      

                       Computation: 145554 steps/s (collection: 0.549s, learning 0.127s)
               Value function loss: 0.0604
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.60
                Mean reward (task): 1.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 78.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0163
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0149
       Mean episode rew_smoothness: -0.0228
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1320
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.68s
                        Total time: 802.88s
                               ETA: 623 mins 14.0 s

################################################################################
                     Learning iteration 1051/50000                      

                       Computation: 149223 steps/s (collection: 0.531s, learning 0.128s)
               Value function loss: 0.0595
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.13
                Mean reward (task): 2.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0169
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0155
       Mean episode rew_smoothness: -0.0233
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0293
 Mean episode rew_tracking_lin_vel: 0.1361
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.66s
                        Total time: 803.54s
                               ETA: 623 mins 8.3 s

################################################################################
                     Learning iteration 1052/50000                      

                       Computation: 129938 steps/s (collection: 0.622s, learning 0.134s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.10
                Mean reward (task): 2.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0169
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0154
       Mean episode rew_smoothness: -0.0227
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1344
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.76s
                        Total time: 804.30s
                               ETA: 623 mins 7.2 s

################################################################################
                     Learning iteration 1053/50000                      

                       Computation: 135709 steps/s (collection: 0.580s, learning 0.144s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.27
                Mean reward (task): 2.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0150
       Mean episode rew_smoothness: -0.0225
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0281
 Mean episode rew_tracking_lin_vel: 0.1296
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.72s
                        Total time: 805.02s
                               ETA: 623 mins 4.6 s

################################################################################
                     Learning iteration 1054/50000                      

                       Computation: 149959 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.67
                Mean reward (task): 1.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0165
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0151
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0293
 Mean episode rew_tracking_lin_vel: 0.1323
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.66s
                        Total time: 805.68s
                               ETA: 622 mins 58.8 s

################################################################################
                     Learning iteration 1055/50000                      

                       Computation: 121530 steps/s (collection: 0.652s, learning 0.157s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.69
                Mean reward (task): 1.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0153
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0161
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0146
       Mean episode rew_smoothness: -0.0221
          Mean episode rew_torques: -0.0040
 Mean episode rew_tracking_ang_vel: 0.0282
 Mean episode rew_tracking_lin_vel: 0.1256
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.81s
                        Total time: 806.49s
                               ETA: 623 mins 0.2 s

################################################################################
                     Learning iteration 1056/50000                      

                       Computation: 118866 steps/s (collection: 0.670s, learning 0.157s)
               Value function loss: 0.0637
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.30
                Mean reward (task): 2.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0158
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0167
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0153
       Mean episode rew_smoothness: -0.0228
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0290
 Mean episode rew_tracking_lin_vel: 0.1360
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.83s
                        Total time: 807.31s
                               ETA: 623 mins 2.3 s

################################################################################
                     Learning iteration 1057/50000                      

                       Computation: 118857 steps/s (collection: 0.666s, learning 0.161s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.79
                Mean reward (task): 1.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0168
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0152
       Mean episode rew_smoothness: -0.0232
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0291
 Mean episode rew_tracking_lin_vel: 0.1325
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.83s
                        Total time: 808.14s
                               ETA: 623 mins 4.5 s

################################################################################
                     Learning iteration 1058/50000                      

                       Computation: 133173 steps/s (collection: 0.601s, learning 0.137s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.69
                Mean reward (task): 1.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 79.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0150
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1333
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.74s
                        Total time: 808.88s
                               ETA: 623 mins 2.6 s

################################################################################
                     Learning iteration 1059/50000                      

                       Computation: 121412 steps/s (collection: 0.672s, learning 0.137s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.27
                Mean reward (task): 2.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0156
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0165
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0149
       Mean episode rew_smoothness: -0.0225
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0281
 Mean episode rew_tracking_lin_vel: 0.1285
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.81s
                        Total time: 809.69s
                               ETA: 623 mins 3.9 s

################################################################################
                     Learning iteration 1060/50000                      

                       Computation: 129432 steps/s (collection: 0.605s, learning 0.155s)
               Value function loss: 0.0640
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.70
                Mean reward (task): 1.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0168
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0167
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0314
 Mean episode rew_tracking_lin_vel: 0.1463
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.76s
                        Total time: 810.45s
                               ETA: 623 mins 2.9 s

################################################################################
                     Learning iteration 1061/50000                      

                       Computation: 151436 steps/s (collection: 0.528s, learning 0.121s)
               Value function loss: 0.0646
                    Surrogate loss: 0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.84
                Mean reward (task): 1.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0151
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0294
 Mean episode rew_tracking_lin_vel: 0.1288
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.65s
                        Total time: 811.10s
                               ETA: 622 mins 56.9 s

################################################################################
                     Learning iteration 1062/50000                      

                       Computation: 145952 steps/s (collection: 0.545s, learning 0.128s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.17
                Mean reward (task): 2.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0161
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0172
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0156
       Mean episode rew_smoothness: -0.0233
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0296
 Mean episode rew_tracking_lin_vel: 0.1369
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.67s
                        Total time: 811.77s
                               ETA: 622 mins 52.0 s

################################################################################
                     Learning iteration 1063/50000                      

                       Computation: 134280 steps/s (collection: 0.593s, learning 0.139s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.50
                Mean reward (task): 2.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0173
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0157
       Mean episode rew_smoothness: -0.0237
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0294
 Mean episode rew_tracking_lin_vel: 0.1372
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.73s
                        Total time: 812.50s
                               ETA: 622 mins 49.8 s

################################################################################
                     Learning iteration 1064/50000                      

                       Computation: 130658 steps/s (collection: 0.615s, learning 0.137s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.22
                Mean reward (task): 2.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0147
       Mean episode rew_smoothness: -0.0222
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0277
 Mean episode rew_tracking_lin_vel: 0.1276
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.75s
                        Total time: 813.25s
                               ETA: 622 mins 48.5 s

################################################################################
                     Learning iteration 1065/50000                      

                       Computation: 142193 steps/s (collection: 0.568s, learning 0.124s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.82
                Mean reward (task): 1.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0157
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0163
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0149
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0041
 Mean episode rew_tracking_ang_vel: 0.0282
 Mean episode rew_tracking_lin_vel: 0.1285
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.69s
                        Total time: 813.95s
                               ETA: 622 mins 44.4 s

################################################################################
                     Learning iteration 1066/50000                      

                       Computation: 128597 steps/s (collection: 0.625s, learning 0.139s)
               Value function loss: 0.0607
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.89
                Mean reward (task): 1.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0168
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0157
       Mean episode rew_smoothness: -0.0232
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0288
 Mean episode rew_tracking_lin_vel: 0.1366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.76s
                        Total time: 814.71s
                               ETA: 622 mins 43.7 s

################################################################################
                     Learning iteration 1067/50000                      

                       Computation: 134342 steps/s (collection: 0.578s, learning 0.154s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0162
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0175
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0159
       Mean episode rew_smoothness: -0.0233
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0297
 Mean episode rew_tracking_lin_vel: 0.1414
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.73s
                        Total time: 815.44s
                               ETA: 622 mins 41.5 s

################################################################################
                     Learning iteration 1068/50000                      

                       Computation: 151906 steps/s (collection: 0.524s, learning 0.123s)
               Value function loss: 0.0569
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.24
                Mean reward (task): 2.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0158
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0172
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0156
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0294
 Mean episode rew_tracking_lin_vel: 0.1346
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.65s
                        Total time: 816.09s
                               ETA: 622 mins 35.4 s

################################################################################
                     Learning iteration 1069/50000                      

                       Computation: 143476 steps/s (collection: 0.563s, learning 0.122s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.44
                Mean reward (task): 2.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0162
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0181
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0238
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1459
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.69s
                        Total time: 816.77s
                               ETA: 622 mins 31.0 s

################################################################################
                     Learning iteration 1070/50000                      

                       Computation: 127359 steps/s (collection: 0.631s, learning 0.141s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.53
                Mean reward (task): 2.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0249
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0317
 Mean episode rew_tracking_lin_vel: 0.1494
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.77s
                        Total time: 817.55s
                               ETA: 622 mins 30.6 s

################################################################################
                     Learning iteration 1071/50000                      

                       Computation: 150148 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0596
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.07
                Mean reward (task): 2.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0135
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0166
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0148
       Mean episode rew_smoothness: -0.0222
          Mean episode rew_torques: -0.0042
 Mean episode rew_tracking_ang_vel: 0.0285
 Mean episode rew_tracking_lin_vel: 0.1271
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.65s
                        Total time: 818.20s
                               ETA: 622 mins 24.9 s

################################################################################
                     Learning iteration 1072/50000                      

                       Computation: 128560 steps/s (collection: 0.630s, learning 0.134s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.96
                Mean reward (task): 1.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0156
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0268
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0151
       Mean episode rew_smoothness: -0.0226
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0284
 Mean episode rew_tracking_lin_vel: 0.1299
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.76s
                        Total time: 818.97s
                               ETA: 622 mins 24.2 s

################################################################################
                     Learning iteration 1073/50000                      

                       Computation: 149314 steps/s (collection: 0.527s, learning 0.131s)
               Value function loss: 0.0620
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.81
                Mean reward (task): 1.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 86.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0171
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0155
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0298
 Mean episode rew_tracking_lin_vel: 0.1348
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.66s
                        Total time: 819.62s
                               ETA: 622 mins 18.7 s

################################################################################
                     Learning iteration 1074/50000                      

                       Computation: 145086 steps/s (collection: 0.555s, learning 0.123s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.54
                Mean reward (task): 1.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 74.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0173
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0156
       Mean episode rew_smoothness: -0.0232
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0294
 Mean episode rew_tracking_lin_vel: 0.1380
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.68s
                        Total time: 820.30s
                               ETA: 622 mins 14.0 s

################################################################################
                     Learning iteration 1075/50000                      

                       Computation: 144809 steps/s (collection: 0.552s, learning 0.127s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.43
                Mean reward (task): 2.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0185
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0251
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0315
 Mean episode rew_tracking_lin_vel: 0.1527
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.68s
                        Total time: 820.98s
                               ETA: 622 mins 9.4 s

################################################################################
                     Learning iteration 1076/50000                      

                       Computation: 130843 steps/s (collection: 0.606s, learning 0.145s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.36
                Mean reward (task): 2.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0161
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0170
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0155
       Mean episode rew_smoothness: -0.0232
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0297
 Mean episode rew_tracking_lin_vel: 0.1364
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.75s
                        Total time: 821.73s
                               ETA: 622 mins 8.1 s

################################################################################
                     Learning iteration 1077/50000                      

                       Computation: 140132 steps/s (collection: 0.578s, learning 0.123s)
               Value function loss: 0.0608
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.17
                Mean reward (task): 2.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0186
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0252
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0319
 Mean episode rew_tracking_lin_vel: 0.1523
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.70s
                        Total time: 822.43s
                               ETA: 622 mins 4.6 s

################################################################################
                     Learning iteration 1078/50000                      

                       Computation: 132283 steps/s (collection: 0.602s, learning 0.141s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.04
                Mean reward (task): 2.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0170
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0154
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0044
 Mean episode rew_tracking_ang_vel: 0.0290
 Mean episode rew_tracking_lin_vel: 0.1365
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.74s
                        Total time: 823.18s
                               ETA: 622 mins 2.9 s

################################################################################
                     Learning iteration 1079/50000                      

                       Computation: 151639 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.04
                Mean reward (task): 2.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0181
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0165
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0306
 Mean episode rew_tracking_lin_vel: 0.1468
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.65s
                        Total time: 823.82s
                               ETA: 621 mins 57.0 s

################################################################################
                     Learning iteration 1080/50000                      

                       Computation: 127698 steps/s (collection: 0.627s, learning 0.143s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.44
                Mean reward (task): 2.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0169
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0149
       Mean episode rew_smoothness: -0.0220
          Mean episode rew_torques: -0.0045
 Mean episode rew_tracking_ang_vel: 0.0281
 Mean episode rew_tracking_lin_vel: 0.1301
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.77s
                        Total time: 824.59s
                               ETA: 621 mins 56.5 s

################################################################################
                     Learning iteration 1081/50000                      

                       Computation: 142878 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.75
                Mean reward (task): 1.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 84.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0155
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0165
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0150
       Mean episode rew_smoothness: -0.0224
          Mean episode rew_torques: -0.0043
 Mean episode rew_tracking_ang_vel: 0.0274
 Mean episode rew_tracking_lin_vel: 0.1301
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.69s
                        Total time: 825.28s
                               ETA: 621 mins 52.4 s

################################################################################
                     Learning iteration 1082/50000                      

                       Computation: 153588 steps/s (collection: 0.517s, learning 0.123s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.26
                Mean reward (task): 2.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0167
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0177
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0161
       Mean episode rew_smoothness: -0.0242
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0304
 Mean episode rew_tracking_lin_vel: 0.1435
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.64s
                        Total time: 825.92s
                               ETA: 621 mins 46.1 s

################################################################################
                     Learning iteration 1083/50000                      

                       Computation: 148457 steps/s (collection: 0.521s, learning 0.141s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.15
                Mean reward (task): 2.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0182
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0239
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0304
 Mean episode rew_tracking_lin_vel: 0.1473
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.66s
                        Total time: 826.58s
                               ETA: 621 mins 40.8 s

################################################################################
                     Learning iteration 1084/50000                      

                       Computation: 130498 steps/s (collection: 0.592s, learning 0.161s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.11
                Mean reward (task): 2.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0169
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0186
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0169
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0316
 Mean episode rew_tracking_lin_vel: 0.1494
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.75s
                        Total time: 827.34s
                               ETA: 621 mins 39.6 s

################################################################################
                     Learning iteration 1085/50000                      

                       Computation: 135975 steps/s (collection: 0.582s, learning 0.141s)
               Value function loss: 0.0646
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.34
                Mean reward (task): 2.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0163
       Mean episode rew_smoothness: -0.0239
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1461
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.72s
                        Total time: 828.06s
                               ETA: 621 mins 37.1 s

################################################################################
                     Learning iteration 1086/50000                      

                       Computation: 148761 steps/s (collection: 0.520s, learning 0.141s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0249
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0317
 Mean episode rew_tracking_lin_vel: 0.1538
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.66s
                        Total time: 828.72s
                               ETA: 621 mins 31.7 s

################################################################################
                     Learning iteration 1087/50000                      

                       Computation: 147546 steps/s (collection: 0.542s, learning 0.124s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.29
                Mean reward (task): 3.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0189
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0251
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0311
 Mean episode rew_tracking_lin_vel: 0.1596
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.67s
                        Total time: 829.39s
                               ETA: 621 mins 26.6 s

################################################################################
                     Learning iteration 1088/50000                      

                       Computation: 151246 steps/s (collection: 0.526s, learning 0.124s)
               Value function loss: 0.0637
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.51
                Mean reward (task): 3.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0181
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0245
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0310
 Mean episode rew_tracking_lin_vel: 0.1465
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.65s
                        Total time: 830.04s
                               ETA: 621 mins 20.8 s

################################################################################
                     Learning iteration 1089/50000                      

                       Computation: 120229 steps/s (collection: 0.684s, learning 0.134s)
               Value function loss: 0.0629
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.12
                Mean reward (task): 2.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 96.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0169
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0185
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1493
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.82s
                        Total time: 830.86s
                               ETA: 621 mins 22.6 s

################################################################################
                     Learning iteration 1090/50000                      

                       Computation: 150137 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.02
                Mean reward (task): 2.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0182
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0240
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0299
 Mean episode rew_tracking_lin_vel: 0.1447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.65s
                        Total time: 831.51s
                               ETA: 621 mins 17.0 s

################################################################################
                     Learning iteration 1091/50000                      

                       Computation: 146050 steps/s (collection: 0.551s, learning 0.122s)
               Value function loss: 0.0636
                    Surrogate loss: 0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.79
                Mean reward (task): 2.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0164
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0163
       Mean episode rew_smoothness: -0.0236
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.67s
                        Total time: 832.18s
                               ETA: 621 mins 12.2 s

################################################################################
                     Learning iteration 1092/50000                      

                       Computation: 148182 steps/s (collection: 0.541s, learning 0.122s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.50
                Mean reward (task): 2.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0252
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0316
 Mean episode rew_tracking_lin_vel: 0.1539
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.66s
                        Total time: 832.85s
                               ETA: 621 mins 7.0 s

################################################################################
                     Learning iteration 1093/50000                      

                       Computation: 127864 steps/s (collection: 0.646s, learning 0.123s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.45
                Mean reward (task): 2.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0266
   Mean episode rew_dof_pos_limits: -0.0189
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0242
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0309
 Mean episode rew_tracking_lin_vel: 0.1550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.77s
                        Total time: 833.62s
                               ETA: 621 mins 6.6 s

################################################################################
                     Learning iteration 1094/50000                      

                       Computation: 147680 steps/s (collection: 0.544s, learning 0.122s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.43
                Mean reward (task): 2.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0311
 Mean episode rew_tracking_lin_vel: 0.1533
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.67s
                        Total time: 834.28s
                               ETA: 621 mins 1.5 s

################################################################################
                     Learning iteration 1095/50000                      

                       Computation: 140554 steps/s (collection: 0.576s, learning 0.123s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.32
                Mean reward (task): 2.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0162
       Mean episode rew_smoothness: -0.0235
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0298
 Mean episode rew_tracking_lin_vel: 0.1422
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.70s
                        Total time: 834.98s
                               ETA: 620 mins 58.0 s

################################################################################
                     Learning iteration 1096/50000                      

                       Computation: 155024 steps/s (collection: 0.512s, learning 0.122s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.65
                Mean reward (task): 1.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 80.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0174
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0157
       Mean episode rew_smoothness: -0.0229
          Mean episode rew_torques: -0.0046
 Mean episode rew_tracking_ang_vel: 0.0296
 Mean episode rew_tracking_lin_vel: 0.1398
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.63s
                        Total time: 835.61s
                               ETA: 620 mins 51.5 s

################################################################################
                     Learning iteration 1097/50000                      

                       Computation: 136424 steps/s (collection: 0.571s, learning 0.149s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.27
                Mean reward (task): 2.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0247
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.72s
                        Total time: 836.34s
                               ETA: 620 mins 48.9 s

################################################################################
                     Learning iteration 1098/50000                      

                       Computation: 132255 steps/s (collection: 0.609s, learning 0.134s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.03
                Mean reward (task): 2.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0160
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0176
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0157
       Mean episode rew_smoothness: -0.0231
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0293
 Mean episode rew_tracking_lin_vel: 0.1405
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.74s
                        Total time: 837.08s
                               ETA: 620 mins 47.3 s

################################################################################
                     Learning iteration 1099/50000                      

                       Computation: 144541 steps/s (collection: 0.557s, learning 0.123s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.90
                Mean reward (task): 1.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0164
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0162
       Mean episode rew_smoothness: -0.0236
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0298
 Mean episode rew_tracking_lin_vel: 0.1419
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.68s
                        Total time: 837.76s
                               ETA: 620 mins 43.0 s

################################################################################
                     Learning iteration 1100/50000                      

                       Computation: 144634 steps/s (collection: 0.556s, learning 0.124s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.98
                Mean reward (task): 1.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0192
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0250
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0307
 Mean episode rew_tracking_lin_vel: 0.1553
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.68s
                        Total time: 838.44s
                               ETA: 620 mins 38.6 s

################################################################################
                     Learning iteration 1101/50000                      

                       Computation: 151311 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0643
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.80
                Mean reward (task): 1.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0277
   Mean episode rew_dof_pos_limits: -0.0192
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0174
       Mean episode rew_smoothness: -0.0251
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0312
 Mean episode rew_tracking_lin_vel: 0.1563
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.65s
                        Total time: 839.09s
                               ETA: 620 mins 32.8 s

################################################################################
                     Learning iteration 1102/50000                      

                       Computation: 137468 steps/s (collection: 0.592s, learning 0.123s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.67
                Mean reward (task): 2.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0168
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0244
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0315
 Mean episode rew_tracking_lin_vel: 0.1539
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.72s
                        Total time: 839.80s
                               ETA: 620 mins 30.0 s

################################################################################
                     Learning iteration 1103/50000                      

                       Computation: 150640 steps/s (collection: 0.529s, learning 0.123s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.55
                Mean reward (task): 2.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0167
       Mean episode rew_smoothness: -0.0241
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0310
 Mean episode rew_tracking_lin_vel: 0.1521
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.65s
                        Total time: 840.46s
                               ETA: 620 mins 24.4 s

################################################################################
                     Learning iteration 1104/50000                      

                       Computation: 151642 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.86
                Mean reward (task): 2.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0169
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0189
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0246
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0311
 Mean episode rew_tracking_lin_vel: 0.1564
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.65s
                        Total time: 841.10s
                               ETA: 620 mins 18.7 s

################################################################################
                     Learning iteration 1105/50000                      

                       Computation: 132424 steps/s (collection: 0.605s, learning 0.137s)
               Value function loss: 0.0709
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0161
       Mean episode rew_ang_vel_xy: -0.0138
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0253
   Mean episode rew_dof_pos_limits: -0.0179
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0160
       Mean episode rew_smoothness: -0.0234
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0292
 Mean episode rew_tracking_lin_vel: 0.1469
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.74s
                        Total time: 841.85s
                               ETA: 620 mins 17.1 s

################################################################################
                     Learning iteration 1106/50000                      

                       Computation: 136489 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.89
                Mean reward (task): 1.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 83.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0167
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0169
       Mean episode rew_smoothness: -0.0243
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0309
 Mean episode rew_tracking_lin_vel: 0.1522
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.72s
                        Total time: 842.57s
                               ETA: 620 mins 14.5 s

################################################################################
                     Learning iteration 1107/50000                      

                       Computation: 154741 steps/s (collection: 0.513s, learning 0.122s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.03
                Mean reward (task): 2.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0267
   Mean episode rew_dof_pos_limits: -0.0183
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0237
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0309
 Mean episode rew_tracking_lin_vel: 0.1472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.64s
                        Total time: 843.20s
                               ETA: 620 mins 8.2 s

################################################################################
                     Learning iteration 1108/50000                      

                       Computation: 137261 steps/s (collection: 0.566s, learning 0.150s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.78
                Mean reward (task): 2.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0159
       Mean episode rew_ang_vel_xy: -0.0140
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0177
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0156
       Mean episode rew_smoothness: -0.0230
          Mean episode rew_torques: -0.0047
 Mean episode rew_tracking_ang_vel: 0.0286
 Mean episode rew_tracking_lin_vel: 0.1431
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.72s
                        Total time: 843.92s
                               ETA: 620 mins 5.5 s

################################################################################
                     Learning iteration 1109/50000                      

                       Computation: 127720 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.20
                Mean reward (task): 2.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0163
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0184
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0163
       Mean episode rew_smoothness: -0.0239
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0303
 Mean episode rew_tracking_lin_vel: 0.1486
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.77s
                        Total time: 844.69s
                               ETA: 620 mins 5.1 s

################################################################################
                     Learning iteration 1110/50000                      

                       Computation: 134472 steps/s (collection: 0.600s, learning 0.131s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.64
                Mean reward (task): 2.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0174
       Mean episode rew_smoothness: -0.0251
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0310
 Mean episode rew_tracking_lin_vel: 0.1593
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.73s
                        Total time: 845.42s
                               ETA: 620 mins 3.0 s

################################################################################
                     Learning iteration 1111/50000                      

                       Computation: 154207 steps/s (collection: 0.514s, learning 0.123s)
               Value function loss: 0.0651
                    Surrogate loss: 0.0101
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.39
                Mean reward (task): 3.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0162
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0178
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0162
       Mean episode rew_smoothness: -0.0237
          Mean episode rew_torques: -0.0048
 Mean episode rew_tracking_ang_vel: 0.0293
 Mean episode rew_tracking_lin_vel: 0.1470
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.64s
                        Total time: 846.06s
                               ETA: 619 mins 56.8 s

################################################################################
                     Learning iteration 1112/50000                      

                       Computation: 149565 steps/s (collection: 0.533s, learning 0.124s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.32
                Mean reward (task): 2.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0145
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0184
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0164
       Mean episode rew_smoothness: -0.0241
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0297
 Mean episode rew_tracking_lin_vel: 0.1476
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.66s
                        Total time: 846.71s
                               ETA: 619 mins 51.5 s

################################################################################
                     Learning iteration 1113/50000                      

                       Computation: 154073 steps/s (collection: 0.514s, learning 0.124s)
               Value function loss: 0.0639
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.21
                Mean reward (task): 2.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0166
       Mean episode rew_ang_vel_xy: -0.0141
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0182
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0163
       Mean episode rew_smoothness: -0.0240
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0300
 Mean episode rew_tracking_lin_vel: 0.1460
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.64s
                        Total time: 847.35s
                               ETA: 619 mins 45.4 s

################################################################################
                     Learning iteration 1114/50000                      

                       Computation: 153948 steps/s (collection: 0.515s, learning 0.124s)
               Value function loss: 0.0653
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.23
                Mean reward (task): 2.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0176
       Mean episode rew_smoothness: -0.0253
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0324
 Mean episode rew_tracking_lin_vel: 0.1623
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.64s
                        Total time: 847.99s
                               ETA: 619 mins 39.2 s

################################################################################
                     Learning iteration 1115/50000                      

                       Computation: 129999 steps/s (collection: 0.610s, learning 0.146s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.69
                Mean reward (task): 2.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0191
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0250
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0309
 Mean episode rew_tracking_lin_vel: 0.1577
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.76s
                        Total time: 848.75s
                               ETA: 619 mins 38.3 s

################################################################################
                     Learning iteration 1116/50000                      

                       Computation: 119374 steps/s (collection: 0.668s, learning 0.156s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.83
                Mean reward (task): 2.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0168
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0183
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0165
       Mean episode rew_smoothness: -0.0245
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0303
 Mean episode rew_tracking_lin_vel: 0.1455
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.82s
                        Total time: 849.57s
                               ETA: 619 mins 40.3 s

################################################################################
                     Learning iteration 1117/50000                      

                       Computation: 120216 steps/s (collection: 0.643s, learning 0.175s)
               Value function loss: 0.0693
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.13
                Mean reward (task): 2.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 92.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0167
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0247
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0307
 Mean episode rew_tracking_lin_vel: 0.1542
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.82s
                        Total time: 850.39s
                               ETA: 619 mins 42.0 s

################################################################################
                     Learning iteration 1118/50000                      

                       Computation: 126926 steps/s (collection: 0.626s, learning 0.149s)
               Value function loss: 0.0692
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.86
                Mean reward (task): 2.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0177
       Mean episode rew_smoothness: -0.0258
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0326
 Mean episode rew_tracking_lin_vel: 0.1634
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.77s
                        Total time: 851.16s
                               ETA: 619 mins 41.9 s

################################################################################
                     Learning iteration 1119/50000                      

                       Computation: 143470 steps/s (collection: 0.546s, learning 0.140s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.84
                Mean reward (task): 1.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0165
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0186
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0166
       Mean episode rew_smoothness: -0.0243
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0308
 Mean episode rew_tracking_lin_vel: 0.1513
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.69s
                        Total time: 851.85s
                               ETA: 619 mins 37.8 s

################################################################################
                     Learning iteration 1120/50000                      

                       Computation: 140848 steps/s (collection: 0.575s, learning 0.123s)
               Value function loss: 0.0698
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.88
                Mean reward (task): 2.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0247
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0312
 Mean episode rew_tracking_lin_vel: 0.1567
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.70s
                        Total time: 852.55s
                               ETA: 619 mins 34.3 s

################################################################################
                     Learning iteration 1121/50000                      

                       Computation: 143122 steps/s (collection: 0.558s, learning 0.129s)
               Value function loss: 0.0655
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.47
                Mean reward (task): 2.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0166
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0168
       Mean episode rew_smoothness: -0.0242
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0310
 Mean episode rew_tracking_lin_vel: 0.1506
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.69s
                        Total time: 853.23s
                               ETA: 619 mins 30.4 s

################################################################################
                     Learning iteration 1122/50000                      

                       Computation: 142638 steps/s (collection: 0.567s, learning 0.122s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.84
                Mean reward (task): 1.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0166
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0259
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0168
       Mean episode rew_smoothness: -0.0244
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0316
 Mean episode rew_tracking_lin_vel: 0.1530
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.69s
                        Total time: 853.92s
                               ETA: 619 mins 26.5 s

################################################################################
                     Learning iteration 1123/50000                      

                       Computation: 133904 steps/s (collection: 0.597s, learning 0.137s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.00
                Mean reward (task): 2.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0199
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0321
 Mean episode rew_tracking_lin_vel: 0.1613
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.73s
                        Total time: 854.66s
                               ETA: 619 mins 24.6 s

################################################################################
                     Learning iteration 1124/50000                      

                       Computation: 136806 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0663
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.53
                Mean reward (task): 2.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0258
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0323
 Mean episode rew_tracking_lin_vel: 0.1634
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.72s
                        Total time: 855.37s
                               ETA: 619 mins 22.0 s

################################################################################
                     Learning iteration 1125/50000                      

                       Computation: 154108 steps/s (collection: 0.515s, learning 0.123s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0177
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0200
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0182
       Mean episode rew_smoothness: -0.0264
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0334
 Mean episode rew_tracking_lin_vel: 0.1655
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.64s
                        Total time: 856.01s
                               ETA: 619 mins 15.9 s

################################################################################
                     Learning iteration 1126/50000                      

                       Computation: 144433 steps/s (collection: 0.557s, learning 0.124s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.91
                Mean reward (task): 1.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0169
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0188
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0170
       Mean episode rew_smoothness: -0.0249
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0305
 Mean episode rew_tracking_lin_vel: 0.1527
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.68s
                        Total time: 856.69s
                               ETA: 619 mins 11.7 s

################################################################################
                     Learning iteration 1127/50000                      

                       Computation: 135896 steps/s (collection: 0.586s, learning 0.137s)
               Value function loss: 0.0685
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0167
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0186
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0168
       Mean episode rew_smoothness: -0.0244
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0305
 Mean episode rew_tracking_lin_vel: 0.1511
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.72s
                        Total time: 857.42s
                               ETA: 619 mins 9.4 s

################################################################################
                     Learning iteration 1128/50000                      

                       Computation: 145702 steps/s (collection: 0.553s, learning 0.122s)
               Value function loss: 0.0655
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.71
                Mean reward (task): 2.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0181
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0265
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0323
 Mean episode rew_tracking_lin_vel: 0.1624
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.67s
                        Total time: 858.09s
                               ETA: 619 mins 4.9 s

################################################################################
                     Learning iteration 1129/50000                      

                       Computation: 144280 steps/s (collection: 0.547s, learning 0.134s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.33
                Mean reward (task): 2.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0195
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0176
       Mean episode rew_smoothness: -0.0256
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0331
 Mean episode rew_tracking_lin_vel: 0.1608
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.68s
                        Total time: 858.77s
                               ETA: 619 mins 0.7 s

################################################################################
                     Learning iteration 1130/50000                      

                       Computation: 153828 steps/s (collection: 0.515s, learning 0.124s)
               Value function loss: 0.0696
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.75
                Mean reward (task): 2.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0186
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0194
       Mean episode rew_smoothness: -0.0277
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0346
 Mean episode rew_tracking_lin_vel: 0.1791
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.64s
                        Total time: 859.41s
                               ETA: 618 mins 54.8 s

################################################################################
                     Learning iteration 1131/50000                      

                       Computation: 125106 steps/s (collection: 0.639s, learning 0.147s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.33
                Mean reward (task): 3.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0167
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0182
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0166
       Mean episode rew_smoothness: -0.0244
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0302
 Mean episode rew_tracking_lin_vel: 0.1481
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.79s
                        Total time: 860.20s
                               ETA: 618 mins 55.1 s

################################################################################
                     Learning iteration 1132/50000                      

                       Computation: 141742 steps/s (collection: 0.570s, learning 0.124s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.20
                Mean reward (task): 2.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0200
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0260
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0327
 Mean episode rew_tracking_lin_vel: 0.1583
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.69s
                        Total time: 860.89s
                               ETA: 618 mins 51.5 s

################################################################################
                     Learning iteration 1133/50000                      

                       Computation: 137725 steps/s (collection: 0.583s, learning 0.130s)
               Value function loss: 0.0672
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.23
                Mean reward (task): 3.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0259
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0338
 Mean episode rew_tracking_lin_vel: 0.1635
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.71s
                        Total time: 861.60s
                               ETA: 618 mins 48.8 s

################################################################################
                     Learning iteration 1134/50000                      

                       Computation: 135207 steps/s (collection: 0.606s, learning 0.121s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.15
                Mean reward (task): 2.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 91.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0181
       Mean episode rew_smoothness: -0.0262
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0334
 Mean episode rew_tracking_lin_vel: 0.1672
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.73s
                        Total time: 862.33s
                               ETA: 618 mins 46.6 s

################################################################################
                     Learning iteration 1135/50000                      

                       Computation: 121774 steps/s (collection: 0.664s, learning 0.143s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.99
                Mean reward (task): 2.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0183
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0207
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0189
       Mean episode rew_smoothness: -0.0272
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0341
 Mean episode rew_tracking_lin_vel: 0.1725
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.81s
                        Total time: 863.14s
                               ETA: 618 mins 47.9 s

################################################################################
                     Learning iteration 1136/50000                      

                       Computation: 154208 steps/s (collection: 0.513s, learning 0.124s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.14
                Mean reward (task): 2.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 90.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0168
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0189
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0171
       Mean episode rew_smoothness: -0.0250
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0317
 Mean episode rew_tracking_lin_vel: 0.1564
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.64s
                        Total time: 863.78s
                               ETA: 618 mins 41.9 s

################################################################################
                     Learning iteration 1137/50000                      

                       Computation: 133707 steps/s (collection: 0.607s, learning 0.128s)
               Value function loss: 0.0674
                    Surrogate loss: 0.0092
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.69
                Mean reward (task): 3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0175
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0280
   Mean episode rew_dof_pos_limits: -0.0192
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0177
       Mean episode rew_smoothness: -0.0259
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0322
 Mean episode rew_tracking_lin_vel: 0.1621
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.74s
                        Total time: 864.51s
                               ETA: 618 mins 40.0 s

################################################################################
                     Learning iteration 1138/50000                      

                       Computation: 143075 steps/s (collection: 0.561s, learning 0.126s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.92
                Mean reward (task): 1.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0180
       Mean episode rew_ang_vel_xy: -0.0157
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0204
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0186
       Mean episode rew_smoothness: -0.0271
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0345
 Mean episode rew_tracking_lin_vel: 0.1708
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.69s
                        Total time: 865.20s
                               ETA: 618 mins 36.2 s

################################################################################
                     Learning iteration 1139/50000                      

                       Computation: 145531 steps/s (collection: 0.525s, learning 0.151s)
               Value function loss: 0.0648
                    Surrogate loss: 0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.00
                Mean reward (task): 3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0146
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0195
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0327
 Mean episode rew_tracking_lin_vel: 0.1642
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.68s
                        Total time: 865.87s
                               ETA: 618 mins 31.8 s

################################################################################
                     Learning iteration 1140/50000                      

                       Computation: 126005 steps/s (collection: 0.649s, learning 0.131s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.65
                Mean reward (task): 1.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0202
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0184
       Mean episode rew_smoothness: -0.0264
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0326
 Mean episode rew_tracking_lin_vel: 0.1700
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.78s
                        Total time: 866.65s
                               ETA: 618 mins 31.9 s

################################################################################
                     Learning iteration 1141/50000                      

                       Computation: 130574 steps/s (collection: 0.611s, learning 0.142s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.98
                Mean reward (task): 1.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 89.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0149
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0189
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0172
       Mean episode rew_smoothness: -0.0252
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0313
 Mean episode rew_tracking_lin_vel: 0.1542
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.75s
                        Total time: 867.41s
                               ETA: 618 mins 30.9 s

################################################################################
                     Learning iteration 1142/50000                      

                       Computation: 132374 steps/s (collection: 0.611s, learning 0.132s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 1.92
                Mean reward (task): 1.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0181
       Mean episode rew_smoothness: -0.0261
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0333
 Mean episode rew_tracking_lin_vel: 0.1603
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.74s
                        Total time: 868.15s
                               ETA: 618 mins 29.4 s

################################################################################
                     Learning iteration 1143/50000                      

                       Computation: 154339 steps/s (collection: 0.516s, learning 0.121s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.47
                Mean reward (task): 2.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0268
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0335
 Mean episode rew_tracking_lin_vel: 0.1594
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.64s
                        Total time: 868.79s
                               ETA: 618 mins 23.4 s

################################################################################
                     Learning iteration 1144/50000                      

                       Computation: 145585 steps/s (collection: 0.552s, learning 0.123s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.69
                Mean reward (task): 3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0196
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0218
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0205
       Mean episode rew_smoothness: -0.0295
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0376
 Mean episode rew_tracking_lin_vel: 0.1885
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.68s
                        Total time: 869.46s
                               ETA: 618 mins 19.0 s

################################################################################
                     Learning iteration 1145/50000                      

                       Computation: 116983 steps/s (collection: 0.698s, learning 0.142s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.46
                Mean reward (task): 2.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0178
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0188
       Mean episode rew_smoothness: -0.0269
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0332
 Mean episode rew_tracking_lin_vel: 0.1667
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.84s
                        Total time: 870.30s
                               ETA: 618 mins 21.7 s

################################################################################
                     Learning iteration 1146/50000                      

                       Computation: 131172 steps/s (collection: 0.604s, learning 0.146s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.64
                Mean reward (task): 2.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0181
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0268
   Mean episode rew_dof_pos_limits: -0.0213
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0196
       Mean episode rew_smoothness: -0.0274
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0354
 Mean episode rew_tracking_lin_vel: 0.1803
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.75s
                        Total time: 871.05s
                               ETA: 618 mins 20.6 s

################################################################################
                     Learning iteration 1147/50000                      

                       Computation: 140411 steps/s (collection: 0.548s, learning 0.152s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.08
                Mean reward (task): 2.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0195
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0258
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0328
 Mean episode rew_tracking_lin_vel: 0.1569
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.70s
                        Total time: 871.75s
                               ETA: 618 mins 17.3 s

################################################################################
                     Learning iteration 1148/50000                      

                       Computation: 133803 steps/s (collection: 0.595s, learning 0.140s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.51
                Mean reward (task): 2.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0180
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0202
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0185
       Mean episode rew_smoothness: -0.0271
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0340
 Mean episode rew_tracking_lin_vel: 0.1691
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.73s
                        Total time: 872.49s
                               ETA: 618 mins 15.5 s

################################################################################
                     Learning iteration 1149/50000                      

                       Computation: 149839 steps/s (collection: 0.517s, learning 0.139s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 2.15
                Mean reward (task): 2.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0181
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0204
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0190
       Mean episode rew_smoothness: -0.0272
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0349
 Mean episode rew_tracking_lin_vel: 0.1725
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.66s
                        Total time: 873.14s
                               ETA: 618 mins 10.3 s

################################################################################
                     Learning iteration 1150/50000                      

                       Computation: 125266 steps/s (collection: 0.623s, learning 0.162s)
               Value function loss: 0.0671
                    Surrogate loss: 0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.57
               Mean reward (total): 3.09
                Mean reward (task): 3.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0260
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0329
 Mean episode rew_tracking_lin_vel: 0.1622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.78s
                        Total time: 873.93s
                               ETA: 618 mins 10.6 s

################################################################################
                     Learning iteration 1151/50000                      

                       Computation: 138084 steps/s (collection: 0.571s, learning 0.141s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.64
                Mean reward (task): 2.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0259
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0324
 Mean episode rew_tracking_lin_vel: 0.1597
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.71s
                        Total time: 874.64s
                               ETA: 618 mins 7.9 s

################################################################################
                     Learning iteration 1152/50000                      

                       Computation: 135348 steps/s (collection: 0.570s, learning 0.157s)
               Value function loss: 0.0711
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.03
                Mean reward (task): 3.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0175
       Mean episode rew_ang_vel_xy: -0.0151
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0195
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0260
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0320
 Mean episode rew_tracking_lin_vel: 0.1640
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.73s
                        Total time: 875.37s
                               ETA: 618 mins 5.7 s

################################################################################
                     Learning iteration 1153/50000                      

                       Computation: 130096 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0181
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0209
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0192
       Mean episode rew_smoothness: -0.0273
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0348
 Mean episode rew_tracking_lin_vel: 0.1749
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.76s
                        Total time: 876.12s
                               ETA: 618 mins 4.8 s

################################################################################
                     Learning iteration 1154/50000                      

                       Computation: 157358 steps/s (collection: 0.502s, learning 0.122s)
               Value function loss: 0.0795
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.28
                Mean reward (task): 2.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 95.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0148
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0177
       Mean episode rew_smoothness: -0.0255
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0325
 Mean episode rew_tracking_lin_vel: 0.1584
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.62s
                        Total time: 876.75s
                               ETA: 617 mins 58.4 s

################################################################################
                     Learning iteration 1155/50000                      

                       Computation: 137619 steps/s (collection: 0.588s, learning 0.127s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.58
                Mean reward (task): 2.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0171
       Mean episode rew_ang_vel_xy: -0.0147
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0258
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0176
       Mean episode rew_smoothness: -0.0256
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0330
 Mean episode rew_tracking_lin_vel: 0.1578
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.71s
                        Total time: 877.46s
                               ETA: 617 mins 55.7 s

################################################################################
                     Learning iteration 1156/50000                      

                       Computation: 126477 steps/s (collection: 0.635s, learning 0.142s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.91
                Mean reward (task): 2.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0153
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0183
       Mean episode rew_smoothness: -0.0264
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0333
 Mean episode rew_tracking_lin_vel: 0.1664
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.78s
                        Total time: 878.24s
                               ETA: 617 mins 55.7 s

################################################################################
                     Learning iteration 1157/50000                      

                       Computation: 152191 steps/s (collection: 0.506s, learning 0.140s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.50
                Mean reward (task): 2.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0178
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0202
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0183
       Mean episode rew_smoothness: -0.0266
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0326
 Mean episode rew_tracking_lin_vel: 0.1681
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.65s
                        Total time: 878.88s
                               ETA: 617 mins 50.2 s

################################################################################
                     Learning iteration 1158/50000                      

                       Computation: 120297 steps/s (collection: 0.658s, learning 0.159s)
               Value function loss: 0.0636
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.93
                Mean reward (task): 1.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 87.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0169
       Mean episode rew_ang_vel_xy: -0.0143
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0276
   Mean episode rew_dof_pos_limits: -0.0186
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0166
       Mean episode rew_smoothness: -0.0249
          Mean episode rew_torques: -0.0049
 Mean episode rew_tracking_ang_vel: 0.0312
 Mean episode rew_tracking_lin_vel: 0.1509
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.82s
                        Total time: 879.70s
                               ETA: 617 mins 51.9 s

################################################################################
                     Learning iteration 1159/50000                      

                       Computation: 150794 steps/s (collection: 0.509s, learning 0.143s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.08
                Mean reward (task): 2.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0176
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0179
       Mean episode rew_smoothness: -0.0262
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0330
 Mean episode rew_tracking_lin_vel: 0.1613
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.65s
                        Total time: 880.35s
                               ETA: 617 mins 46.6 s

################################################################################
                     Learning iteration 1160/50000                      

                       Computation: 135944 steps/s (collection: 0.562s, learning 0.161s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 1.77
                Mean reward (task): 1.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 82.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0176
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0318
 Mean episode rew_tracking_lin_vel: 0.1580
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.72s
                        Total time: 881.08s
                               ETA: 617 mins 44.3 s

################################################################################
                     Learning iteration 1161/50000                      

                       Computation: 153249 steps/s (collection: 0.518s, learning 0.124s)
               Value function loss: 0.0667
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.25
                Mean reward (task): 2.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0173
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0192
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0175
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0051
 Mean episode rew_tracking_ang_vel: 0.0319
 Mean episode rew_tracking_lin_vel: 0.1546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.64s
                        Total time: 881.72s
                               ETA: 617 mins 38.7 s

################################################################################
                     Learning iteration 1162/50000                      

                       Computation: 148274 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.68
                Mean reward (task): 2.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0172
       Mean episode rew_ang_vel_xy: -0.0142
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0264
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0196
           Mean episode rew_no_fly: 0.0178
       Mean episode rew_smoothness: -0.0257
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0324
 Mean episode rew_tracking_lin_vel: 0.1591
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.66s
                        Total time: 882.38s
                               ETA: 617 mins 33.9 s

################################################################################
                     Learning iteration 1163/50000                      

                       Computation: 149680 steps/s (collection: 0.522s, learning 0.135s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.67
                Mean reward (task): 2.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0170
       Mean episode rew_ang_vel_xy: -0.0144
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0263
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0181
           Mean episode rew_no_fly: 0.0175
       Mean episode rew_smoothness: -0.0256
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0321
 Mean episode rew_tracking_lin_vel: 0.1564
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.66s
                        Total time: 883.04s
                               ETA: 617 mins 28.8 s

################################################################################
                     Learning iteration 1164/50000                      

                       Computation: 136933 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0644
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.89
                Mean reward (task): 2.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0178
       Mean episode rew_ang_vel_xy: -0.0154
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0201
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0184
       Mean episode rew_smoothness: -0.0267
          Mean episode rew_torques: -0.0053
 Mean episode rew_tracking_ang_vel: 0.0336
 Mean episode rew_tracking_lin_vel: 0.1635
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.72s
                        Total time: 883.75s
                               ETA: 617 mins 26.4 s

################################################################################
                     Learning iteration 1165/50000                      

                       Computation: 127035 steps/s (collection: 0.632s, learning 0.142s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.27
                Mean reward (task): 2.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0192
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0221
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0203
       Mean episode rew_smoothness: -0.0290
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0367
 Mean episode rew_tracking_lin_vel: 0.1858
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.77s
                        Total time: 884.53s
                               ETA: 617 mins 26.2 s

################################################################################
                     Learning iteration 1166/50000                      

                       Computation: 153811 steps/s (collection: 0.518s, learning 0.122s)
               Value function loss: 0.0665
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.76
                Mean reward (task): 2.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0181
       Mean episode rew_ang_vel_xy: -0.0155
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0204
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0186
       Mean episode rew_smoothness: -0.0271
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0345
 Mean episode rew_tracking_lin_vel: 0.1657
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.64s
                        Total time: 885.17s
                               ETA: 617 mins 20.5 s

################################################################################
                     Learning iteration 1167/50000                      

                       Computation: 151241 steps/s (collection: 0.525s, learning 0.125s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.47
                Mean reward (task): 2.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0178
       Mean episode rew_ang_vel_xy: -0.0150
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0202
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0181
       Mean episode rew_smoothness: -0.0263
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0329
 Mean episode rew_tracking_lin_vel: 0.1639
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.65s
                        Total time: 885.82s
                               ETA: 617 mins 15.2 s

################################################################################
                     Learning iteration 1168/50000                      

                       Computation: 127749 steps/s (collection: 0.629s, learning 0.140s)
               Value function loss: 0.0754
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0189
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0213
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0199
       Mean episode rew_smoothness: -0.0285
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0358
 Mean episode rew_tracking_lin_vel: 0.1746
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.77s
                        Total time: 886.59s
                               ETA: 617 mins 14.9 s

################################################################################
                     Learning iteration 1169/50000                      

                       Computation: 134178 steps/s (collection: 0.595s, learning 0.137s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.28
                Mean reward (task): 2.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0174
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0191
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0176
       Mean episode rew_smoothness: -0.0262
          Mean episode rew_torques: -0.0050
 Mean episode rew_tracking_ang_vel: 0.0326
 Mean episode rew_tracking_lin_vel: 0.1540
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.73s
                        Total time: 887.32s
                               ETA: 617 mins 13.1 s

################################################################################
                     Learning iteration 1170/50000                      

                       Computation: 154642 steps/s (collection: 0.512s, learning 0.124s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.30
                Mean reward (task): 2.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0185
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0204
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0191
       Mean episode rew_smoothness: -0.0278
          Mean episode rew_torques: -0.0054
 Mean episode rew_tracking_ang_vel: 0.0341
 Mean episode rew_tracking_lin_vel: 0.1686
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.64s
                        Total time: 887.95s
                               ETA: 617 mins 7.2 s

################################################################################
                     Learning iteration 1171/50000                      

                       Computation: 120315 steps/s (collection: 0.667s, learning 0.151s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.02
                Mean reward (task): 4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0189
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0201
       Mean episode rew_smoothness: -0.0287
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0358
 Mean episode rew_tracking_lin_vel: 0.1820
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.82s
                        Total time: 888.77s
                               ETA: 617 mins 8.9 s

################################################################################
                     Learning iteration 1172/50000                      

                       Computation: 155301 steps/s (collection: 0.510s, learning 0.123s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.81
                Mean reward (task): 2.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0185
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0194
       Mean episode rew_smoothness: -0.0279
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0356
 Mean episode rew_tracking_lin_vel: 0.1721
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.63s
                        Total time: 889.40s
                               ETA: 617 mins 2.9 s

################################################################################
                     Learning iteration 1173/50000                      

                       Computation: 128105 steps/s (collection: 0.617s, learning 0.150s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.63
                Mean reward (task): 2.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0191
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0219
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0202
       Mean episode rew_smoothness: -0.0288
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1829
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.77s
                        Total time: 890.17s
                               ETA: 617 mins 2.5 s

################################################################################
                     Learning iteration 1174/50000                      

                       Computation: 137016 steps/s (collection: 0.595s, learning 0.122s)
               Value function loss: 0.0738
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.13
                Mean reward (task): 2.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0187
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0191
       Mean episode rew_smoothness: -0.0279
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0346
 Mean episode rew_tracking_lin_vel: 0.1706
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.72s
                        Total time: 890.89s
                               ETA: 617 mins 0.1 s

################################################################################
                     Learning iteration 1175/50000                      

                       Computation: 146919 steps/s (collection: 0.546s, learning 0.123s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.63
                Mean reward (task): 2.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0189
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0196
       Mean episode rew_smoothness: -0.0284
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0358
 Mean episode rew_tracking_lin_vel: 0.1747
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.67s
                        Total time: 891.56s
                               ETA: 616 mins 55.6 s

################################################################################
                     Learning iteration 1176/50000                      

                       Computation: 146125 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.80
                Mean reward (task): 2.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0186
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0215
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0198
       Mean episode rew_smoothness: -0.0280
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0356
 Mean episode rew_tracking_lin_vel: 0.1788
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.67s
                        Total time: 892.23s
                               ETA: 616 mins 51.3 s

################################################################################
                     Learning iteration 1177/50000                      

                       Computation: 148049 steps/s (collection: 0.539s, learning 0.125s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0182
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0185
       Mean episode rew_smoothness: -0.0271
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0328
 Mean episode rew_tracking_lin_vel: 0.1625
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.66s
                        Total time: 892.90s
                               ETA: 616 mins 46.7 s

################################################################################
                     Learning iteration 1178/50000                      

                       Computation: 153285 steps/s (collection: 0.517s, learning 0.124s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.71
                Mean reward (task): 2.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0183
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0272
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0192
       Mean episode rew_smoothness: -0.0275
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0348
 Mean episode rew_tracking_lin_vel: 0.1716
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.64s
                        Total time: 893.54s
                               ETA: 616 mins 41.1 s

################################################################################
                     Learning iteration 1179/50000                      

                       Computation: 146285 steps/s (collection: 0.548s, learning 0.124s)
               Value function loss: 0.0659
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.38
                Mean reward (task): 2.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 94.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0217
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0199
       Mean episode rew_smoothness: -0.0284
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0357
 Mean episode rew_tracking_lin_vel: 0.1836
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.67s
                        Total time: 894.21s
                               ETA: 616 mins 36.8 s

################################################################################
                     Learning iteration 1180/50000                      

                       Computation: 135430 steps/s (collection: 0.589s, learning 0.137s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.31
                Mean reward (task): 2.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0182
       Mean episode rew_ang_vel_xy: -0.0156
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0274
   Mean episode rew_dof_pos_limits: -0.0207
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0190
       Mean episode rew_smoothness: -0.0272
          Mean episode rew_torques: -0.0056
 Mean episode rew_tracking_ang_vel: 0.0340
 Mean episode rew_tracking_lin_vel: 0.1687
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.73s
                        Total time: 894.93s
                               ETA: 616 mins 34.7 s

################################################################################
                     Learning iteration 1181/50000                      

                       Computation: 130484 steps/s (collection: 0.606s, learning 0.148s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0217
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0196
       Mean episode rew_smoothness: -0.0287
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0347
 Mean episode rew_tracking_lin_vel: 0.1774
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.75s
                        Total time: 895.69s
                               ETA: 616 mins 33.7 s

################################################################################
                     Learning iteration 1182/50000                      

                       Computation: 133762 steps/s (collection: 0.594s, learning 0.141s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.14
                Mean reward (task): 2.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 88.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0217
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0201
       Mean episode rew_smoothness: -0.0285
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0366
 Mean episode rew_tracking_lin_vel: 0.1801
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.73s
                        Total time: 896.42s
                               ETA: 616 mins 32.0 s

################################################################################
                     Learning iteration 1183/50000                      

                       Computation: 150069 steps/s (collection: 0.515s, learning 0.140s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.20
                Mean reward (task): 2.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0219
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0202
       Mean episode rew_smoothness: -0.0288
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0358
 Mean episode rew_tracking_lin_vel: 0.1816
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.66s
                        Total time: 897.08s
                               ETA: 616 mins 27.0 s

################################################################################
                     Learning iteration 1184/50000                      

                       Computation: 136393 steps/s (collection: 0.580s, learning 0.141s)
               Value function loss: 0.0691
                    Surrogate loss: 0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0188
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0213
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0197
       Mean episode rew_smoothness: -0.0282
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0355
 Mean episode rew_tracking_lin_vel: 0.1758
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.72s
                        Total time: 897.80s
                               ETA: 616 mins 24.8 s

################################################################################
                     Learning iteration 1185/50000                      

                       Computation: 154573 steps/s (collection: 0.514s, learning 0.122s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.14
                Mean reward (task): 2.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 93.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0185
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0269
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0193
       Mean episode rew_smoothness: -0.0276
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0348
 Mean episode rew_tracking_lin_vel: 0.1763
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.64s
                        Total time: 898.43s
                               ETA: 616 mins 19.0 s

################################################################################
                     Learning iteration 1186/50000                      

                       Computation: 132484 steps/s (collection: 0.620s, learning 0.122s)
               Value function loss: 0.0686
                    Surrogate loss: 0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.98
                Mean reward (task): 2.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0192
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0199
       Mean episode rew_smoothness: -0.0291
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0359
 Mean episode rew_tracking_lin_vel: 0.1811
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.74s
                        Total time: 899.18s
                               ETA: 616 mins 17.6 s

################################################################################
                     Learning iteration 1187/50000                      

                       Computation: 141170 steps/s (collection: 0.557s, learning 0.139s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.39
                Mean reward (task): 3.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0218
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0202
       Mean episode rew_smoothness: -0.0291
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0362
 Mean episode rew_tracking_lin_vel: 0.1838
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.70s
                        Total time: 899.87s
                               ETA: 616 mins 14.3 s

################################################################################
                     Learning iteration 1188/50000                      

                       Computation: 135966 steps/s (collection: 0.594s, learning 0.129s)
               Value function loss: 0.0648
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.46
                Mean reward (task): 2.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 98.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0180
       Mean episode rew_ang_vel_xy: -0.0152
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0275
   Mean episode rew_dof_pos_limits: -0.0202
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0184
       Mean episode rew_smoothness: -0.0267
          Mean episode rew_torques: -0.0055
 Mean episode rew_tracking_ang_vel: 0.0332
 Mean episode rew_tracking_lin_vel: 0.1646
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.72s
                        Total time: 900.60s
                               ETA: 616 mins 12.2 s

################################################################################
                     Learning iteration 1189/50000                      

                       Computation: 152216 steps/s (collection: 0.522s, learning 0.124s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0196
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0227
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0211
       Mean episode rew_smoothness: -0.0297
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0369
 Mean episode rew_tracking_lin_vel: 0.1919
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.65s
                        Total time: 901.24s
                               ETA: 616 mins 6.8 s

################################################################################
                     Learning iteration 1190/50000                      

                       Computation: 135813 steps/s (collection: 0.583s, learning 0.141s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0198
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0228
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0209
       Mean episode rew_smoothness: -0.0299
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0361
 Mean episode rew_tracking_lin_vel: 0.1880
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.72s
                        Total time: 901.97s
                               ETA: 616 mins 4.7 s

################################################################################
                     Learning iteration 1191/50000                      

                       Computation: 138913 steps/s (collection: 0.584s, learning 0.123s)
               Value function loss: 0.0821
                    Surrogate loss: 0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.55
                Mean reward (task): 2.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0191
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0220
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0199
       Mean episode rew_smoothness: -0.0286
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0358
 Mean episode rew_tracking_lin_vel: 0.1777
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.71s
                        Total time: 902.67s
                               ETA: 616 mins 1.9 s

################################################################################
                     Learning iteration 1192/50000                      

                       Computation: 142239 steps/s (collection: 0.566s, learning 0.125s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.81
                Mean reward (task): 2.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0206
       Mean episode rew_smoothness: -0.0292
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1891
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.69s
                        Total time: 903.36s
                               ETA: 615 mins 58.4 s

################################################################################
                     Learning iteration 1193/50000                      

                       Computation: 147077 steps/s (collection: 0.544s, learning 0.124s)
               Value function loss: 0.0771
                    Surrogate loss: 0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.88
                Mean reward (task): 2.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0225
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0207
       Mean episode rew_smoothness: -0.0294
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0366
 Mean episode rew_tracking_lin_vel: 0.1882
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.67s
                        Total time: 904.03s
                               ETA: 615 mins 54.1 s

################################################################################
                     Learning iteration 1194/50000                      

                       Computation: 137349 steps/s (collection: 0.584s, learning 0.132s)
               Value function loss: 0.0772
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.66
                Mean reward (task): 2.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0194
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0204
       Mean episode rew_smoothness: -0.0291
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0354
 Mean episode rew_tracking_lin_vel: 0.1836
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.72s
                        Total time: 904.75s
                               ETA: 615 mins 51.6 s

################################################################################
                     Learning iteration 1195/50000                      

                       Computation: 122939 steps/s (collection: 0.658s, learning 0.142s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.13
                Mean reward (task): 3.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0214
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0201
       Mean episode rew_smoothness: -0.0287
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0356
 Mean episode rew_tracking_lin_vel: 0.1836
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.80s
                        Total time: 905.55s
                               ETA: 615 mins 52.6 s

################################################################################
                     Learning iteration 1196/50000                      

                       Computation: 133857 steps/s (collection: 0.586s, learning 0.148s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.47
                Mean reward (task): 2.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0187
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0197
       Mean episode rew_smoothness: -0.0284
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0345
 Mean episode rew_tracking_lin_vel: 0.1778
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.73s
                        Total time: 906.28s
                               ETA: 615 mins 50.9 s

################################################################################
                     Learning iteration 1197/50000                      

                       Computation: 156750 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.92
                Mean reward (task): 2.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0186
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0270
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0197
       Mean episode rew_smoothness: -0.0282
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0353
 Mean episode rew_tracking_lin_vel: 0.1759
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.63s
                        Total time: 906.91s
                               ETA: 615 mins 44.8 s

################################################################################
                     Learning iteration 1198/50000                      

                       Computation: 156586 steps/s (collection: 0.506s, learning 0.122s)
               Value function loss: 0.0733
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0158
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0228
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0204
       Mean episode rew_smoothness: -0.0290
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0352
 Mean episode rew_tracking_lin_vel: 0.1834
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.63s
                        Total time: 907.54s
                               ETA: 615 mins 38.8 s

################################################################################
                     Learning iteration 1199/50000                      

                       Computation: 135271 steps/s (collection: 0.604s, learning 0.123s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.35
                Mean reward (task): 3.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0197
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0224
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0206
       Mean episode rew_smoothness: -0.0296
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0364
 Mean episode rew_tracking_lin_vel: 0.1846
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.73s
                        Total time: 908.26s
                               ETA: 615 mins 36.8 s

################################################################################
                     Learning iteration 1200/50000                      

                       Computation: 156348 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.62
                Mean reward (task): 2.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0189
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0214
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0199
       Mean episode rew_smoothness: -0.0288
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0357
 Mean episode rew_tracking_lin_vel: 0.1792
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.63s
                        Total time: 908.89s
                               ETA: 615 mins 30.9 s

################################################################################
                     Learning iteration 1201/50000                      

                       Computation: 137675 steps/s (collection: 0.563s, learning 0.151s)
               Value function loss: 0.0765
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.12
                Mean reward (task): 3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0197
       Mean episode rew_smoothness: -0.0284
          Mean episode rew_torques: -0.0058
 Mean episode rew_tracking_ang_vel: 0.0350
 Mean episode rew_tracking_lin_vel: 0.1735
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.71s
                        Total time: 909.61s
                               ETA: 615 mins 28.4 s

################################################################################
                     Learning iteration 1202/50000                      

                       Computation: 137838 steps/s (collection: 0.572s, learning 0.141s)
               Value function loss: 0.0797
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.41
                Mean reward (task): 3.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0207
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0242
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0316
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.71s
                        Total time: 910.32s
                               ETA: 615 mins 25.9 s

################################################################################
                     Learning iteration 1203/50000                      

                       Computation: 139665 steps/s (collection: 0.572s, learning 0.132s)
               Value function loss: 0.0773
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.83
                Mean reward (task): 2.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0226
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0209
       Mean episode rew_smoothness: -0.0295
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0366
 Mean episode rew_tracking_lin_vel: 0.1909
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.70s
                        Total time: 911.02s
                               ETA: 615 mins 23.0 s

################################################################################
                     Learning iteration 1204/50000                      

                       Computation: 135271 steps/s (collection: 0.598s, learning 0.129s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.53
                Mean reward (task): 2.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0205
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0236
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0219
       Mean episode rew_smoothness: -0.0310
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0380
 Mean episode rew_tracking_lin_vel: 0.1990
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.73s
                        Total time: 911.75s
                               ETA: 615 mins 21.0 s

################################################################################
                     Learning iteration 1205/50000                      

                       Computation: 150493 steps/s (collection: 0.529s, learning 0.125s)
               Value function loss: 0.0758
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0189
       Mean episode rew_ang_vel_xy: -0.0159
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0218
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0201
       Mean episode rew_smoothness: -0.0283
          Mean episode rew_torques: -0.0059
 Mean episode rew_tracking_ang_vel: 0.0348
 Mean episode rew_tracking_lin_vel: 0.1768
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.65s
                        Total time: 912.40s
                               ETA: 615 mins 16.1 s

################################################################################
                     Learning iteration 1206/50000                      

                       Computation: 155707 steps/s (collection: 0.508s, learning 0.123s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.10
                Mean reward (task): 3.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0194
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0220
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0202
       Mean episode rew_smoothness: -0.0289
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0355
 Mean episode rew_tracking_lin_vel: 0.1849
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.63s
                        Total time: 913.04s
                               ETA: 615 mins 10.2 s

################################################################################
                     Learning iteration 1207/50000                      

                       Computation: 149923 steps/s (collection: 0.533s, learning 0.123s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.61
                Mean reward (task): 2.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 103.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0208
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0243
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0227
       Mean episode rew_smoothness: -0.0315
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0391
 Mean episode rew_tracking_lin_vel: 0.2064
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.66s
                        Total time: 913.69s
                               ETA: 615 mins 5.4 s

################################################################################
                     Learning iteration 1208/50000                      

                       Computation: 157499 steps/s (collection: 0.500s, learning 0.124s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.06
                Mean reward (task): 4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0215
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0253
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0330
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.2169
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.62s
                        Total time: 914.32s
                               ETA: 614 mins 59.3 s

################################################################################
                     Learning iteration 1209/50000                      

                       Computation: 143781 steps/s (collection: 0.561s, learning 0.122s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.05
                Mean reward (task): 3.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0205
       Mean episode rew_smoothness: -0.0291
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0370
 Mean episode rew_tracking_lin_vel: 0.1826
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.68s
                        Total time: 915.00s
                               ETA: 614 mins 55.6 s

################################################################################
                     Learning iteration 1210/50000                      

                       Computation: 154530 steps/s (collection: 0.514s, learning 0.122s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.27
                Mean reward (task): 3.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0213
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0396
 Mean episode rew_tracking_lin_vel: 0.2069
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.64s
                        Total time: 915.64s
                               ETA: 614 mins 50.0 s

################################################################################
                     Learning iteration 1211/50000                      

                       Computation: 127627 steps/s (collection: 0.638s, learning 0.133s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.44
                Mean reward (task): 3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0202
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0235
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0217
       Mean episode rew_smoothness: -0.0303
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0380
 Mean episode rew_tracking_lin_vel: 0.1949
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.77s
                        Total time: 916.41s
                               ETA: 614 mins 49.9 s

################################################################################
                     Learning iteration 1212/50000                      

                       Computation: 125743 steps/s (collection: 0.643s, learning 0.138s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.57
                Mean reward (task): 2.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0203
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0233
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0217
       Mean episode rew_smoothness: -0.0305
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0380
 Mean episode rew_tracking_lin_vel: 0.1960
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.78s
                        Total time: 917.19s
                               ETA: 614 mins 50.1 s

################################################################################
                     Learning iteration 1213/50000                      

                       Computation: 131302 steps/s (collection: 0.613s, learning 0.135s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.28
                Mean reward (task): 3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0195
       Mean episode rew_ang_vel_xy: -0.0162
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0273
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0207
       Mean episode rew_smoothness: -0.0293
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0370
 Mean episode rew_tracking_lin_vel: 0.1877
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.75s
                        Total time: 917.94s
                               ETA: 614 mins 49.1 s

################################################################################
                     Learning iteration 1214/50000                      

                       Computation: 128733 steps/s (collection: 0.642s, learning 0.122s)
               Value function loss: 0.0736
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.64
                Mean reward (task): 2.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0206
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0243
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0310
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0390
 Mean episode rew_tracking_lin_vel: 0.2058
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.76s
                        Total time: 918.70s
                               ETA: 614 mins 48.6 s

################################################################################
                     Learning iteration 1215/50000                      

                       Computation: 140443 steps/s (collection: 0.565s, learning 0.135s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.75
                Mean reward (task): 2.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0210
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0244
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0315
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0391
 Mean episode rew_tracking_lin_vel: 0.2054
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.70s
                        Total time: 919.40s
                               ETA: 614 mins 45.6 s

################################################################################
                     Learning iteration 1216/50000                      

                       Computation: 138262 steps/s (collection: 0.590s, learning 0.121s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.25
                Mean reward (task): 2.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0206
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0245
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0227
       Mean episode rew_smoothness: -0.0313
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.2082
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.71s
                        Total time: 920.11s
                               ETA: 614 mins 43.0 s

################################################################################
                     Learning iteration 1217/50000                      

                       Computation: 125706 steps/s (collection: 0.648s, learning 0.134s)
               Value function loss: 0.0715
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.78
                Mean reward (task): 2.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0193
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0226
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0206
       Mean episode rew_smoothness: -0.0289
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0367
 Mean episode rew_tracking_lin_vel: 0.1861
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.78s
                        Total time: 920.89s
                               ETA: 614 mins 43.3 s

################################################################################
                     Learning iteration 1218/50000                      

                       Computation: 127620 steps/s (collection: 0.621s, learning 0.150s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.86
                Mean reward (task): 2.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0190
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0211
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0195
       Mean episode rew_smoothness: -0.0283
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0353
 Mean episode rew_tracking_lin_vel: 0.1747
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.77s
                        Total time: 921.66s
                               ETA: 614 mins 43.1 s

################################################################################
                     Learning iteration 1219/50000                      

                       Computation: 154063 steps/s (collection: 0.506s, learning 0.132s)
               Value function loss: 0.0776
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.10
                Mean reward (task): 3.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0204
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0240
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0221
       Mean episode rew_smoothness: -0.0308
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.1974
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.64s
                        Total time: 922.30s
                               ETA: 614 mins 37.7 s

################################################################################
                     Learning iteration 1220/50000                      

                       Computation: 154186 steps/s (collection: 0.497s, learning 0.141s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0210
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0240
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0226
       Mean episode rew_smoothness: -0.0316
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0386
 Mean episode rew_tracking_lin_vel: 0.2025
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.64s
                        Total time: 922.94s
                               ETA: 614 mins 32.2 s

################################################################################
                     Learning iteration 1221/50000                      

                       Computation: 123582 steps/s (collection: 0.644s, learning 0.151s)
               Value function loss: 0.0751
                    Surrogate loss: 0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.84
                Mean reward (task): 2.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0255
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0234
       Mean episode rew_smoothness: -0.0330
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0403
 Mean episode rew_tracking_lin_vel: 0.2134
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.80s
                        Total time: 923.73s
                               ETA: 614 mins 33.0 s

################################################################################
                     Learning iteration 1222/50000                      

                       Computation: 152486 steps/s (collection: 0.507s, learning 0.138s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.93
                Mean reward (task): 2.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0207
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0241
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0222
       Mean episode rew_smoothness: -0.0314
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.1970
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.64s
                        Total time: 924.38s
                               ETA: 614 mins 27.8 s

################################################################################
                     Learning iteration 1223/50000                      

                       Computation: 148236 steps/s (collection: 0.523s, learning 0.140s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.87
                Mean reward (task): 2.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0205
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0233
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0216
       Mean episode rew_smoothness: -0.0304
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.1932
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.66s
                        Total time: 925.04s
                               ETA: 614 mins 23.4 s

################################################################################
                     Learning iteration 1224/50000                      

                       Computation: 127166 steps/s (collection: 0.617s, learning 0.156s)
               Value function loss: 0.0737
                    Surrogate loss: 0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.57
                Mean reward (task): 3.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0207
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0234
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0217
       Mean episode rew_smoothness: -0.0308
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0374
 Mean episode rew_tracking_lin_vel: 0.1905
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.77s
                        Total time: 925.81s
                               ETA: 614 mins 23.3 s

################################################################################
                     Learning iteration 1225/50000                      

                       Computation: 132634 steps/s (collection: 0.600s, learning 0.141s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.60
                Mean reward (task): 3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0207
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0237
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0220
       Mean episode rew_smoothness: -0.0311
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0377
 Mean episode rew_tracking_lin_vel: 0.1940
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.74s
                        Total time: 926.56s
                               ETA: 614 mins 22.0 s

################################################################################
                     Learning iteration 1226/50000                      

                       Computation: 136028 steps/s (collection: 0.582s, learning 0.141s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.41
                Mean reward (task): 2.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0197
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0220
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0203
       Mean episode rew_smoothness: -0.0292
          Mean episode rew_torques: -0.0060
 Mean episode rew_tracking_ang_vel: 0.0357
 Mean episode rew_tracking_lin_vel: 0.1743
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.72s
                        Total time: 927.28s
                               ETA: 614 mins 19.9 s

################################################################################
                     Learning iteration 1227/50000                      

                       Computation: 133288 steps/s (collection: 0.593s, learning 0.145s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.25
                Mean reward (task): 2.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0204
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0227
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0213
       Mean episode rew_smoothness: -0.0304
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0368
 Mean episode rew_tracking_lin_vel: 0.1896
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.74s
                        Total time: 928.02s
                               ETA: 614 mins 18.4 s

################################################################################
                     Learning iteration 1228/50000                      

                       Computation: 154369 steps/s (collection: 0.513s, learning 0.124s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.04
                Mean reward (task): 3.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0207
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0235
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0216
       Mean episode rew_smoothness: -0.0307
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1885
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.64s
                        Total time: 928.65s
                               ETA: 614 mins 12.9 s

################################################################################
                     Learning iteration 1229/50000                      

                       Computation: 134317 steps/s (collection: 0.597s, learning 0.135s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.31
                Mean reward (task): 4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0243
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0319
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0406
 Mean episode rew_tracking_lin_vel: 0.2070
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.73s
                        Total time: 929.38s
                               ETA: 614 mins 11.2 s

################################################################################
                     Learning iteration 1230/50000                      

                       Computation: 151512 steps/s (collection: 0.525s, learning 0.124s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.01
                Mean reward (task): 3.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0239
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0222
       Mean episode rew_smoothness: -0.0316
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0377
 Mean episode rew_tracking_lin_vel: 0.1921
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.65s
                        Total time: 930.03s
                               ETA: 614 mins 6.3 s

################################################################################
                     Learning iteration 1231/50000                      

                       Computation: 157121 steps/s (collection: 0.502s, learning 0.123s)
               Value function loss: 0.0712
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.39
                Mean reward (task): 3.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0215
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0243
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0389
 Mean episode rew_tracking_lin_vel: 0.2075
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.63s
                        Total time: 930.66s
                               ETA: 614 mins 0.4 s

################################################################################
                     Learning iteration 1232/50000                      

                       Computation: 149677 steps/s (collection: 0.525s, learning 0.132s)
               Value function loss: 0.0698
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.14
                Mean reward (task): 3.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0246
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0327
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2012
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.66s
                        Total time: 931.32s
                               ETA: 613 mins 55.7 s

################################################################################
                     Learning iteration 1233/50000                      

                       Computation: 131274 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.01
                Mean reward (task): 3.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0246
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0316
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0412
 Mean episode rew_tracking_lin_vel: 0.1994
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.75s
                        Total time: 932.06s
                               ETA: 613 mins 54.7 s

################################################################################
                     Learning iteration 1234/50000                      

                       Computation: 150439 steps/s (collection: 0.530s, learning 0.123s)
               Value function loss: 0.0687
                    Surrogate loss: 0.0134
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.69
                Mean reward (task): 2.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0247
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0403
 Mean episode rew_tracking_lin_vel: 0.2033
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.65s
                        Total time: 932.72s
                               ETA: 613 mins 49.9 s

################################################################################
                     Learning iteration 1235/50000                      

                       Computation: 143397 steps/s (collection: 0.563s, learning 0.122s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.12
                Mean reward (task): 3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0250
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0234
       Mean episode rew_smoothness: -0.0326
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2029
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.69s
                        Total time: 933.40s
                               ETA: 613 mins 46.4 s

################################################################################
                     Learning iteration 1236/50000                      

                       Computation: 151625 steps/s (collection: 0.525s, learning 0.123s)
               Value function loss: 0.0685
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.76
                Mean reward (task): 2.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0202
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0227
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0211
       Mean episode rew_smoothness: -0.0298
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0372
 Mean episode rew_tracking_lin_vel: 0.1826
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.65s
                        Total time: 934.05s
                               ETA: 613 mins 41.4 s

################################################################################
                     Learning iteration 1237/50000                      

                       Computation: 148643 steps/s (collection: 0.540s, learning 0.122s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.54
                Mean reward (task): 4.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0240
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0224
       Mean episode rew_smoothness: -0.0310
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.2014
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.66s
                        Total time: 934.71s
                               ETA: 613 mins 37.0 s

################################################################################
                     Learning iteration 1238/50000                      

                       Computation: 132854 steps/s (collection: 0.608s, learning 0.132s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.12
                Mean reward (task): 4.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0249
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0323
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2025
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.74s
                        Total time: 935.45s
                               ETA: 613 mins 35.6 s

################################################################################
                     Learning iteration 1239/50000                      

                       Computation: 152923 steps/s (collection: 0.520s, learning 0.123s)
               Value function loss: 0.0727
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.13
                Mean reward (task): 3.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0199
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0224
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0203
       Mean episode rew_smoothness: -0.0292
          Mean episode rew_torques: -0.0062
 Mean episode rew_tracking_ang_vel: 0.0361
 Mean episode rew_tracking_lin_vel: 0.1759
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.64s
                        Total time: 936.10s
                               ETA: 613 mins 30.5 s

################################################################################
                     Learning iteration 1240/50000                      

                       Computation: 125053 steps/s (collection: 0.649s, learning 0.137s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.23
                Mean reward (task): 2.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0199
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0205
       Mean episode rew_smoothness: -0.0297
          Mean episode rew_torques: -0.0061
 Mean episode rew_tracking_ang_vel: 0.0361
 Mean episode rew_tracking_lin_vel: 0.1770
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.79s
                        Total time: 936.88s
                               ETA: 613 mins 30.9 s

################################################################################
                     Learning iteration 1241/50000                      

                       Computation: 142535 steps/s (collection: 0.566s, learning 0.124s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0220
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0325
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0397
 Mean episode rew_tracking_lin_vel: 0.2045
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.69s
                        Total time: 937.57s
                               ETA: 613 mins 27.6 s

################################################################################
                     Learning iteration 1242/50000                      

                       Computation: 131262 steps/s (collection: 0.607s, learning 0.142s)
               Value function loss: 0.0728
                    Surrogate loss: 0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.73
                Mean reward (task): 2.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0243
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0227
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2040
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.75s
                        Total time: 938.32s
                               ETA: 613 mins 26.6 s

################################################################################
                     Learning iteration 1243/50000                      

                       Computation: 149934 steps/s (collection: 0.533s, learning 0.122s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.97
                Mean reward (task): 2.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0206
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0235
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0212
       Mean episode rew_smoothness: -0.0304
          Mean episode rew_torques: -0.0065
 Mean episode rew_tracking_ang_vel: 0.0381
 Mean episode rew_tracking_lin_vel: 0.1848
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.66s
                        Total time: 938.98s
                               ETA: 613 mins 22.0 s

################################################################################
                     Learning iteration 1244/50000                      

                       Computation: 141779 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0695
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.98
                Mean reward (task): 2.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0213
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0244
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0223
       Mean episode rew_smoothness: -0.0317
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0379
 Mean episode rew_tracking_lin_vel: 0.1948
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.69s
                        Total time: 939.67s
                               ETA: 613 mins 18.8 s

################################################################################
                     Learning iteration 1245/50000                      

                       Computation: 145479 steps/s (collection: 0.550s, learning 0.125s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0210
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0245
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0224
       Mean episode rew_smoothness: -0.0317
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0393
 Mean episode rew_tracking_lin_vel: 0.2019
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.68s
                        Total time: 940.35s
                               ETA: 613 mins 15.0 s

################################################################################
                     Learning iteration 1246/50000                      

                       Computation: 154175 steps/s (collection: 0.514s, learning 0.124s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.83
                Mean reward (task): 2.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0254
       Mean episode rew_smoothness: -0.0358
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0427
 Mean episode rew_tracking_lin_vel: 0.2301
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.64s
                        Total time: 940.98s
                               ETA: 613 mins 9.7 s

################################################################################
                     Learning iteration 1247/50000                      

                       Computation: 135146 steps/s (collection: 0.590s, learning 0.137s)
               Value function loss: 0.0720
                    Surrogate loss: 0.0137
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.12
                Mean reward (task): 2.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 99.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0196
       Mean episode rew_ang_vel_xy: -0.0161
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0211
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0195
       Mean episode rew_smoothness: -0.0287
          Mean episode rew_torques: -0.0057
 Mean episode rew_tracking_ang_vel: 0.0349
 Mean episode rew_tracking_lin_vel: 0.1704
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.73s
                        Total time: 941.71s
                               ETA: 613 mins 7.8 s

################################################################################
                     Learning iteration 1248/50000                      

                       Computation: 133988 steps/s (collection: 0.589s, learning 0.144s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0209
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0239
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0221
       Mean episode rew_smoothness: -0.0313
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0387
 Mean episode rew_tracking_lin_vel: 0.2016
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.73s
                        Total time: 942.44s
                               ETA: 613 mins 6.3 s

################################################################################
                     Learning iteration 1249/50000                      

                       Computation: 140410 steps/s (collection: 0.550s, learning 0.150s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.91
                Mean reward (task): 2.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0250
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0327
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0401
 Mean episode rew_tracking_lin_vel: 0.2016
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.70s
                        Total time: 943.14s
                               ETA: 613 mins 3.4 s

################################################################################
                     Learning iteration 1250/50000                      

                       Computation: 144085 steps/s (collection: 0.558s, learning 0.124s)
               Value function loss: 0.0786
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.27
                Mean reward (task): 4.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0249
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0323
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0396
 Mean episode rew_tracking_lin_vel: 0.2077
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.68s
                        Total time: 943.83s
                               ETA: 612 mins 59.8 s

################################################################################
                     Learning iteration 1251/50000                      

                       Computation: 135128 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.09
                Mean reward (task): 3.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0210
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0242
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0221
       Mean episode rew_smoothness: -0.0314
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0391
 Mean episode rew_tracking_lin_vel: 0.1970
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.73s
                        Total time: 944.55s
                               ETA: 612 mins 58.0 s

################################################################################
                     Learning iteration 1252/50000                      

                       Computation: 149504 steps/s (collection: 0.519s, learning 0.139s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.38
                Mean reward (task): 3.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2011
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.66s
                        Total time: 945.21s
                               ETA: 612 mins 53.5 s

################################################################################
                     Learning iteration 1253/50000                      

                       Computation: 138974 steps/s (collection: 0.583s, learning 0.125s)
               Value function loss: 0.0847
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0227
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0257
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0239
       Mean episode rew_smoothness: -0.0339
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0404
 Mean episode rew_tracking_lin_vel: 0.2173
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.71s
                        Total time: 945.92s
                               ETA: 612 mins 50.9 s

################################################################################
                     Learning iteration 1254/50000                      

                       Computation: 135886 steps/s (collection: 0.593s, learning 0.131s)
               Value function loss: 0.0885
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.44
                Mean reward (task): 4.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0231
       Mean episode rew_smoothness: -0.0324
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2086
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.72s
                        Total time: 946.64s
                               ETA: 612 mins 49.0 s

################################################################################
                     Learning iteration 1255/50000                      

                       Computation: 140319 steps/s (collection: 0.571s, learning 0.130s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.06
                Mean reward (task): 2.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0202
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0224
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0206
       Mean episode rew_smoothness: -0.0297
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0363
 Mean episode rew_tracking_lin_vel: 0.1812
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.70s
                        Total time: 947.34s
                               ETA: 612 mins 46.1 s

################################################################################
                     Learning iteration 1256/50000                      

                       Computation: 129324 steps/s (collection: 0.613s, learning 0.147s)
               Value function loss: 0.0793
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.75
                Mean reward (task): 2.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0199
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0224
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0205
       Mean episode rew_smoothness: -0.0297
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0353
 Mean episode rew_tracking_lin_vel: 0.1765
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.76s
                        Total time: 948.10s
                               ETA: 612 mins 45.6 s

################################################################################
                     Learning iteration 1257/50000                      

                       Computation: 141004 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0797
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.98
                Mean reward (task): 2.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0261
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0337
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0414
 Mean episode rew_tracking_lin_vel: 0.2128
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.70s
                        Total time: 948.80s
                               ETA: 612 mins 42.6 s

################################################################################
                     Learning iteration 1258/50000                      

                       Computation: 135183 steps/s (collection: 0.590s, learning 0.138s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.00
                Mean reward (task): 3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0339
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0418
 Mean episode rew_tracking_lin_vel: 0.2164
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.73s
                        Total time: 949.53s
                               ETA: 612 mins 40.8 s

################################################################################
                     Learning iteration 1259/50000                      

                       Computation: 148206 steps/s (collection: 0.528s, learning 0.135s)
               Value function loss: 0.0722
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.89
                Mean reward (task): 2.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0244
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0226
       Mean episode rew_smoothness: -0.0322
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0393
 Mean episode rew_tracking_lin_vel: 0.2065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.66s
                        Total time: 950.19s
                               ETA: 612 mins 36.5 s

################################################################################
                     Learning iteration 1260/50000                      

                       Computation: 138958 steps/s (collection: 0.583s, learning 0.124s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0220
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0253
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0234
       Mean episode rew_smoothness: -0.0328
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.2080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.71s
                        Total time: 950.90s
                               ETA: 612 mins 34.0 s

################################################################################
                     Learning iteration 1261/50000                      

                       Computation: 156684 steps/s (collection: 0.503s, learning 0.124s)
               Value function loss: 0.0708
                    Surrogate loss: 0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.23
                Mean reward (task): 2.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0208
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0235
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0216
       Mean episode rew_smoothness: -0.0307
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0374
 Mean episode rew_tracking_lin_vel: 0.1907
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.63s
                        Total time: 951.53s
                               ETA: 612 mins 28.3 s

################################################################################
                     Learning iteration 1262/50000                      

                       Computation: 153806 steps/s (collection: 0.518s, learning 0.121s)
               Value function loss: 0.0749
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.11
                Mean reward (task): 3.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0223
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0331
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.2079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.64s
                        Total time: 952.16s
                               ETA: 612 mins 23.2 s

################################################################################
                     Learning iteration 1263/50000                      

                       Computation: 133721 steps/s (collection: 0.600s, learning 0.135s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.93
                Mean reward (task): 3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0235
       Mean episode rew_smoothness: -0.0337
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0396
 Mean episode rew_tracking_lin_vel: 0.2159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.74s
                        Total time: 952.90s
                               ETA: 612 mins 21.7 s

################################################################################
                     Learning iteration 1264/50000                      

                       Computation: 123582 steps/s (collection: 0.666s, learning 0.129s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.81
                Mean reward (task): 3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0263
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0243
       Mean episode rew_smoothness: -0.0342
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2231
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.80s
                        Total time: 953.70s
                               ETA: 612 mins 22.5 s

################################################################################
                     Learning iteration 1265/50000                      

                       Computation: 154501 steps/s (collection: 0.514s, learning 0.122s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.58
                Mean reward (task): 2.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0286
   Mean episode rew_dof_pos_limits: -0.0257
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0329
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0404
 Mean episode rew_tracking_lin_vel: 0.2106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.64s
                        Total time: 954.33s
                               ETA: 612 mins 17.2 s

################################################################################
                     Learning iteration 1266/50000                      

                       Computation: 151583 steps/s (collection: 0.526s, learning 0.122s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0270
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0354
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0429
 Mean episode rew_tracking_lin_vel: 0.2279
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.65s
                        Total time: 954.98s
                               ETA: 612 mins 12.4 s

################################################################################
                     Learning iteration 1267/50000                      

                       Computation: 155569 steps/s (collection: 0.511s, learning 0.121s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0250
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0232
       Mean episode rew_smoothness: -0.0328
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0400
 Mean episode rew_tracking_lin_vel: 0.2115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.63s
                        Total time: 955.61s
                               ETA: 612 mins 7.0 s

################################################################################
                     Learning iteration 1268/50000                      

                       Computation: 126787 steps/s (collection: 0.642s, learning 0.133s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.82
                Mean reward (task): 2.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0222
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0235
       Mean episode rew_smoothness: -0.0336
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0414
 Mean episode rew_tracking_lin_vel: 0.2120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.78s
                        Total time: 956.39s
                               ETA: 612 mins 7.1 s

################################################################################
                     Learning iteration 1269/50000                      

                       Computation: 146900 steps/s (collection: 0.535s, learning 0.134s)
               Value function loss: 0.0755
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.38
                Mean reward (task): 3.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0227
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0339
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0406
 Mean episode rew_tracking_lin_vel: 0.2081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.67s
                        Total time: 957.06s
                               ETA: 612 mins 3.1 s

################################################################################
                     Learning iteration 1270/50000                      

                       Computation: 155265 steps/s (collection: 0.496s, learning 0.137s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.05
                Mean reward (task): 3.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0215
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0245
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0226
       Mean episode rew_smoothness: -0.0321
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0389
 Mean episode rew_tracking_lin_vel: 0.2032
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.63s
                        Total time: 957.69s
                               ETA: 611 mins 57.7 s

################################################################################
                     Learning iteration 1271/50000                      

                       Computation: 158169 steps/s (collection: 0.500s, learning 0.121s)
               Value function loss: 0.0763
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.36
                Mean reward (task): 3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0214
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0242
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0317
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2016
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.62s
                        Total time: 958.31s
                               ETA: 611 mins 51.9 s

################################################################################
                     Learning iteration 1272/50000                      

                       Computation: 136234 steps/s (collection: 0.588s, learning 0.134s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0214
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0249
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0228
       Mean episode rew_smoothness: -0.0319
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.72s
                        Total time: 959.03s
                               ETA: 611 mins 49.9 s

################################################################################
                     Learning iteration 1273/50000                      

                       Computation: 156739 steps/s (collection: 0.502s, learning 0.126s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.69
                Mean reward (task): 3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0230
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0267
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0247
       Mean episode rew_smoothness: -0.0347
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.63s
                        Total time: 959.66s
                               ETA: 611 mins 44.4 s

################################################################################
                     Learning iteration 1274/50000                      

                       Computation: 139850 steps/s (collection: 0.573s, learning 0.130s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.56
                Mean reward (task): 2.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0238
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0222
       Mean episode rew_smoothness: -0.0318
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0376
 Mean episode rew_tracking_lin_vel: 0.1969
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.70s
                        Total time: 960.36s
                               ETA: 611 mins 41.7 s

################################################################################
                     Learning iteration 1275/50000                      

                       Computation: 157491 steps/s (collection: 0.502s, learning 0.122s)
               Value function loss: 0.0736
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0238
       Mean episode rew_smoothness: -0.0337
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0423
 Mean episode rew_tracking_lin_vel: 0.2145
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.62s
                        Total time: 960.99s
                               ETA: 611 mins 36.0 s

################################################################################
                     Learning iteration 1276/50000                      

                       Computation: 147146 steps/s (collection: 0.536s, learning 0.132s)
               Value function loss: 0.0723
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.89
                Mean reward (task): 2.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0220
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0254
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0330
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2157
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.67s
                        Total time: 961.66s
                               ETA: 611 mins 32.0 s

################################################################################
                     Learning iteration 1277/50000                      

                       Computation: 149515 steps/s (collection: 0.535s, learning 0.123s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.56
                Mean reward (task): 3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0220
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0246
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0330
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2103
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.66s
                        Total time: 962.31s
                               ETA: 611 mins 27.6 s

################################################################################
                     Learning iteration 1278/50000                      

                       Computation: 141680 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.25
                Mean reward (task): 3.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0227
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0260
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0341
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2147
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.69s
                        Total time: 963.01s
                               ETA: 611 mins 24.6 s

################################################################################
                     Learning iteration 1279/50000                      

                       Computation: 151163 steps/s (collection: 0.528s, learning 0.122s)
               Value function loss: 0.0735
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.27
                Mean reward (task): 2.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 97.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0214
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0238
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0223
       Mean episode rew_smoothness: -0.0321
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0392
 Mean episode rew_tracking_lin_vel: 0.1998
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.65s
                        Total time: 963.66s
                               ETA: 611 mins 19.9 s

################################################################################
                     Learning iteration 1280/50000                      

                       Computation: 133653 steps/s (collection: 0.601s, learning 0.135s)
               Value function loss: 0.0744
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.41
                Mean reward (task): 2.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0272
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0356
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2349
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.74s
                        Total time: 964.39s
                               ETA: 611 mins 18.5 s

################################################################################
                     Learning iteration 1281/50000                      

                       Computation: 141631 steps/s (collection: 0.556s, learning 0.139s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.63
                Mean reward (task): 3.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0252
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0227
       Mean episode rew_smoothness: -0.0320
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0384
 Mean episode rew_tracking_lin_vel: 0.2054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.69s
                        Total time: 965.09s
                               ETA: 611 mins 15.5 s

################################################################################
                     Learning iteration 1282/50000                      

                       Computation: 133839 steps/s (collection: 0.612s, learning 0.123s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.68
                Mean reward (task): 2.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0204
       Mean episode rew_ang_vel_xy: -0.0164
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0284
   Mean episode rew_dof_pos_limits: -0.0229
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0209
       Mean episode rew_smoothness: -0.0306
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0372
 Mean episode rew_tracking_lin_vel: 0.1847
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.73s
                        Total time: 965.82s
                               ETA: 611 mins 14.1 s

################################################################################
                     Learning iteration 1283/50000                      

                       Computation: 156301 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.29
                Mean reward (task): 2.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 102.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0293
   Mean episode rew_dof_pos_limits: -0.0240
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0221
       Mean episode rew_smoothness: -0.0316
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0383
 Mean episode rew_tracking_lin_vel: 0.1955
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.63s
                        Total time: 966.45s
                               ETA: 611 mins 8.6 s

################################################################################
                     Learning iteration 1284/50000                      

                       Computation: 145650 steps/s (collection: 0.538s, learning 0.137s)
               Value function loss: 0.0785
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.66
                Mean reward (task): 2.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0203
       Mean episode rew_ang_vel_xy: -0.0163
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0224
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0210
       Mean episode rew_smoothness: -0.0304
          Mean episode rew_torques: -0.0063
 Mean episode rew_tracking_ang_vel: 0.0364
 Mean episode rew_tracking_lin_vel: 0.1867
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.67s
                        Total time: 967.12s
                               ETA: 611 mins 4.9 s

################################################################################
                     Learning iteration 1285/50000                      

                       Computation: 126850 steps/s (collection: 0.652s, learning 0.123s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.63
                Mean reward (task): 2.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0220
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0250
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0232
       Mean episode rew_smoothness: -0.0328
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0390
 Mean episode rew_tracking_lin_vel: 0.2052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.77s
                        Total time: 967.90s
                               ETA: 611 mins 5.0 s

################################################################################
                     Learning iteration 1286/50000                      

                       Computation: 143241 steps/s (collection: 0.563s, learning 0.123s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.90
                Mean reward (task): 4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0270
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0255
       Mean episode rew_smoothness: -0.0359
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2354
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.69s
                        Total time: 968.59s
                               ETA: 611 mins 1.8 s

################################################################################
                     Learning iteration 1287/50000                      

                       Computation: 144787 steps/s (collection: 0.555s, learning 0.124s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.32
                Mean reward (task): 3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0219
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0252
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0232
       Mean episode rew_smoothness: -0.0331
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.68s
                        Total time: 969.27s
                               ETA: 610 mins 58.2 s

################################################################################
                     Learning iteration 1288/50000                      

                       Computation: 131998 steps/s (collection: 0.610s, learning 0.135s)
               Value function loss: 0.0816
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.42
                Mean reward (task): 4.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0237
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0269
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0361
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0437
 Mean episode rew_tracking_lin_vel: 0.2345
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.74s
                        Total time: 970.01s
                               ETA: 610 mins 57.2 s

################################################################################
                     Learning iteration 1289/50000                      

                       Computation: 124324 steps/s (collection: 0.660s, learning 0.131s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.93
                Mean reward (task): 3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0221
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0252
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0234
       Mean episode rew_smoothness: -0.0332
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.2121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.79s
                        Total time: 970.80s
                               ETA: 610 mins 57.9 s

################################################################################
                     Learning iteration 1290/50000                      

                       Computation: 154722 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.10
                Mean reward (task): 3.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0210
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0283
   Mean episode rew_dof_pos_limits: -0.0236
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0217
       Mean episode rew_smoothness: -0.0315
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0383
 Mean episode rew_tracking_lin_vel: 0.1996
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.64s
                        Total time: 971.44s
                               ETA: 610 mins 52.7 s

################################################################################
                     Learning iteration 1291/50000                      

                       Computation: 142267 steps/s (collection: 0.551s, learning 0.140s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.39
                Mean reward (task): 2.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 105.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0212
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0237
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0219
       Mean episode rew_smoothness: -0.0318
          Mean episode rew_torques: -0.0066
 Mean episode rew_tracking_ang_vel: 0.0377
 Mean episode rew_tracking_lin_vel: 0.1968
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.69s
                        Total time: 972.13s
                               ETA: 610 mins 49.6 s

################################################################################
                     Learning iteration 1292/50000                      

                       Computation: 148715 steps/s (collection: 0.535s, learning 0.126s)
               Value function loss: 0.0766
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.59
                Mean reward (task): 3.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0267
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0251
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2231
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.66s
                        Total time: 972.79s
                               ETA: 610 mins 45.4 s

################################################################################
                     Learning iteration 1293/50000                      

                       Computation: 138157 steps/s (collection: 0.572s, learning 0.140s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.18
                Mean reward (task): 4.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0226
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0253
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0237
       Mean episode rew_smoothness: -0.0338
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2194
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.71s
                        Total time: 973.50s
                               ETA: 610 mins 43.1 s

################################################################################
                     Learning iteration 1294/50000                      

                       Computation: 153056 steps/s (collection: 0.521s, learning 0.121s)
               Value function loss: 0.0766
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.35
                Mean reward (task): 2.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0267
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0250
       Mean episode rew_smoothness: -0.0355
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0428
 Mean episode rew_tracking_lin_vel: 0.2262
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.64s
                        Total time: 974.14s
                               ETA: 610 mins 38.3 s

################################################################################
                     Learning iteration 1295/50000                      

                       Computation: 153914 steps/s (collection: 0.515s, learning 0.123s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.06
                Mean reward (task): 4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0263
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0251
       Mean episode rew_smoothness: -0.0350
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0423
 Mean episode rew_tracking_lin_vel: 0.2258
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.64s
                        Total time: 974.78s
                               ETA: 610 mins 33.2 s

################################################################################
                     Learning iteration 1296/50000                      

                       Computation: 141032 steps/s (collection: 0.573s, learning 0.124s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.95
                Mean reward (task): 2.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0266
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0249
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2253
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.70s
                        Total time: 975.48s
                               ETA: 610 mins 30.4 s

################################################################################
                     Learning iteration 1297/50000                      

                       Computation: 139901 steps/s (collection: 0.567s, learning 0.136s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0261
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0346
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0412
 Mean episode rew_tracking_lin_vel: 0.2243
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.70s
                        Total time: 976.18s
                               ETA: 610 mins 27.8 s

################################################################################
                     Learning iteration 1298/50000                      

                       Computation: 141500 steps/s (collection: 0.572s, learning 0.122s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.20
                Mean reward (task): 4.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0237
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0361
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2351
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.69s
                        Total time: 976.87s
                               ETA: 610 mins 24.9 s

################################################################################
                     Learning iteration 1299/50000                      

                       Computation: 143430 steps/s (collection: 0.560s, learning 0.125s)
               Value function loss: 0.0717
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0213
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0281
   Mean episode rew_dof_pos_limits: -0.0247
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0323
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0389
 Mean episode rew_tracking_lin_vel: 0.2060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.69s
                        Total time: 977.56s
                               ETA: 610 mins 21.7 s

################################################################################
                     Learning iteration 1300/50000                      

                       Computation: 154154 steps/s (collection: 0.514s, learning 0.123s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.57
                Mean reward (task): 3.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0211
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0241
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0321
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0382
 Mean episode rew_tracking_lin_vel: 0.1950
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.64s
                        Total time: 978.20s
                               ETA: 610 mins 16.6 s

################################################################################
                     Learning iteration 1301/50000                      

                       Computation: 152607 steps/s (collection: 0.513s, learning 0.132s)
               Value function loss: 0.0793
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.01
                Mean reward (task): 3.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0247
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0326
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0388
 Mean episode rew_tracking_lin_vel: 0.2056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.64s
                        Total time: 978.84s
                               ETA: 610 mins 11.8 s

################################################################################
                     Learning iteration 1302/50000                      

                       Computation: 138880 steps/s (collection: 0.571s, learning 0.136s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.85
                Mean reward (task): 3.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0231
       Mean episode rew_smoothness: -0.0327
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0392
 Mean episode rew_tracking_lin_vel: 0.2070
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.71s
                        Total time: 979.55s
                               ETA: 610 mins 9.5 s

################################################################################
                     Learning iteration 1303/50000                      

                       Computation: 153730 steps/s (collection: 0.516s, learning 0.124s)
               Value function loss: 0.0763
                    Surrogate loss: 0.0098
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0245
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0287
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0270
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0455
 Mean episode rew_tracking_lin_vel: 0.2456
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.64s
                        Total time: 980.19s
                               ETA: 610 mins 4.5 s

################################################################################
                     Learning iteration 1304/50000                      

                       Computation: 157458 steps/s (collection: 0.502s, learning 0.122s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.72
                Mean reward (task): 3.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0248
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0425
 Mean episode rew_tracking_lin_vel: 0.2280
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.62s
                        Total time: 980.81s
                               ETA: 609 mins 59.0 s

################################################################################
                     Learning iteration 1305/50000                      

                       Computation: 142888 steps/s (collection: 0.563s, learning 0.125s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.00
                Mean reward (task): 4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0245
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0266
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2324
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.69s
                        Total time: 981.50s
                               ETA: 609 mins 55.9 s

################################################################################
                     Learning iteration 1306/50000                      

                       Computation: 154799 steps/s (collection: 0.501s, learning 0.135s)
               Value function loss: 0.0714
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.49
                Mean reward (task): 2.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0226
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0259
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0241
       Mean episode rew_smoothness: -0.0341
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0413
 Mean episode rew_tracking_lin_vel: 0.2153
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.64s
                        Total time: 982.14s
                               ETA: 609 mins 50.8 s

################################################################################
                     Learning iteration 1307/50000                      

                       Computation: 122128 steps/s (collection: 0.656s, learning 0.149s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.72
                Mean reward (task): 2.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 114.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0261
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0243
       Mean episode rew_smoothness: -0.0346
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0403
 Mean episode rew_tracking_lin_vel: 0.2134
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.80s
                        Total time: 982.94s
                               ETA: 609 mins 52.0 s

################################################################################
                     Learning iteration 1308/50000                      

                       Computation: 138468 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.07
                Mean reward (task): 4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0264
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0250
       Mean episode rew_smoothness: -0.0355
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0421
 Mean episode rew_tracking_lin_vel: 0.2308
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.71s
                        Total time: 983.65s
                               ETA: 609 mins 49.7 s

################################################################################
                     Learning iteration 1309/50000                      

                       Computation: 156220 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0691
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.08
                Mean reward (task): 3.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0222
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0332
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2070
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.63s
                        Total time: 984.28s
                               ETA: 609 mins 44.4 s

################################################################################
                     Learning iteration 1310/50000                      

                       Computation: 131504 steps/s (collection: 0.589s, learning 0.159s)
               Value function loss: 0.0696
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.47
                Mean reward (task): 2.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0257
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0342
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0401
 Mean episode rew_tracking_lin_vel: 0.2126
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.75s
                        Total time: 985.03s
                               ETA: 609 mins 43.5 s

################################################################################
                     Learning iteration 1311/50000                      

                       Computation: 140234 steps/s (collection: 0.561s, learning 0.140s)
               Value function loss: 0.0735
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.26
                Mean reward (task): 4.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0249
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0264
       Mean episode rew_smoothness: -0.0371
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2347
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.70s
                        Total time: 985.73s
                               ETA: 609 mins 40.9 s

################################################################################
                     Learning iteration 1312/50000                      

                       Computation: 137538 steps/s (collection: 0.576s, learning 0.139s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.35
                Mean reward (task): 2.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 106.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0269
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0346
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.71s
                        Total time: 986.44s
                               ETA: 609 mins 38.8 s

################################################################################
                     Learning iteration 1313/50000                      

                       Computation: 153568 steps/s (collection: 0.502s, learning 0.138s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.94
                Mean reward (task): 2.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0223
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0256
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0232
       Mean episode rew_smoothness: -0.0332
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0392
 Mean episode rew_tracking_lin_vel: 0.2034
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.64s
                        Total time: 987.08s
                               ETA: 609 mins 33.9 s

################################################################################
                     Learning iteration 1314/50000                      

                       Computation: 136263 steps/s (collection: 0.599s, learning 0.123s)
               Value function loss: 0.0816
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.02
                Mean reward (task): 4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0262
       Mean episode rew_smoothness: -0.0365
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0446
 Mean episode rew_tracking_lin_vel: 0.2366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.72s
                        Total time: 987.81s
                               ETA: 609 mins 32.1 s

################################################################################
                     Learning iteration 1315/50000                      

                       Computation: 140019 steps/s (collection: 0.565s, learning 0.137s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.12
                Mean reward (task): 3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0259
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0239
       Mean episode rew_smoothness: -0.0340
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.70s
                        Total time: 988.51s
                               ETA: 609 mins 29.5 s

################################################################################
                     Learning iteration 1316/50000                      

                       Computation: 142094 steps/s (collection: 0.570s, learning 0.122s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.45
                Mean reward (task): 3.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0239
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0361
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2317
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.69s
                        Total time: 989.20s
                               ETA: 609 mins 26.6 s

################################################################################
                     Learning iteration 1317/50000                      

                       Computation: 155982 steps/s (collection: 0.508s, learning 0.122s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.44
                Mean reward (task): 3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0267
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0247
       Mean episode rew_smoothness: -0.0350
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2280
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.63s
                        Total time: 989.83s
                               ETA: 609 mins 21.4 s

################################################################################
                     Learning iteration 1318/50000                      

                       Computation: 142073 steps/s (collection: 0.559s, learning 0.133s)
               Value function loss: 0.0750
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.75
                Mean reward (task): 2.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0244
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0224
       Mean episode rew_smoothness: -0.0329
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0379
 Mean episode rew_tracking_lin_vel: 0.2012
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.69s
                        Total time: 990.52s
                               ETA: 609 mins 18.4 s

################################################################################
                     Learning iteration 1319/50000                      

                       Computation: 137118 steps/s (collection: 0.596s, learning 0.121s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.98
                Mean reward (task): 2.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0259
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0336
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0395
 Mean episode rew_tracking_lin_vel: 0.2097
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.72s
                        Total time: 991.24s
                               ETA: 609 mins 16.4 s

################################################################################
                     Learning iteration 1320/50000                      

                       Computation: 142341 steps/s (collection: 0.552s, learning 0.139s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.87
                Mean reward (task): 2.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0253
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0329
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0378
 Mean episode rew_tracking_lin_vel: 0.2020
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.69s
                        Total time: 991.93s
                               ETA: 609 mins 13.5 s

################################################################################
                     Learning iteration 1321/50000                      

                       Computation: 137804 steps/s (collection: 0.574s, learning 0.140s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.36
                Mean reward (task): 4.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0248
       Mean episode rew_smoothness: -0.0353
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.2259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.71s
                        Total time: 992.64s
                               ETA: 609 mins 11.3 s

################################################################################
                     Learning iteration 1322/50000                      

                       Computation: 140741 steps/s (collection: 0.550s, learning 0.149s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0253
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0375
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.70s
                        Total time: 993.34s
                               ETA: 609 mins 8.6 s

################################################################################
                     Learning iteration 1323/50000                      

                       Computation: 155513 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.48
                Mean reward (task): 3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0240
       Mean episode rew_smoothness: -0.0340
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0402
 Mean episode rew_tracking_lin_vel: 0.2131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.63s
                        Total time: 993.97s
                               ETA: 609 mins 3.5 s

################################################################################
                     Learning iteration 1324/50000                      

                       Computation: 155432 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.81
                Mean reward (task): 2.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0257
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0237
       Mean episode rew_smoothness: -0.0339
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.63s
                        Total time: 994.61s
                               ETA: 608 mins 58.4 s

################################################################################
                     Learning iteration 1325/50000                      

                       Computation: 152565 steps/s (collection: 0.520s, learning 0.124s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.86
                Mean reward (task): 3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0285
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0360
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2295
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.64s
                        Total time: 995.25s
                               ETA: 608 mins 53.8 s

################################################################################
                     Learning iteration 1326/50000                      

                       Computation: 153003 steps/s (collection: 0.520s, learning 0.122s)
               Value function loss: 0.0692
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.84
                Mean reward (task): 3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0352
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0420
 Mean episode rew_tracking_lin_vel: 0.2311
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.64s
                        Total time: 995.89s
                               ETA: 608 mins 49.1 s

################################################################################
                     Learning iteration 1327/50000                      

                       Computation: 152300 steps/s (collection: 0.522s, learning 0.124s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.74
                Mean reward (task): 2.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 120.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0290
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0375
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2462
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.65s
                        Total time: 996.54s
                               ETA: 608 mins 44.5 s

################################################################################
                     Learning iteration 1328/50000                      

                       Computation: 139455 steps/s (collection: 0.559s, learning 0.146s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0217
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0242
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0221
       Mean episode rew_smoothness: -0.0320
          Mean episode rew_torques: -0.0068
 Mean episode rew_tracking_ang_vel: 0.0379
 Mean episode rew_tracking_lin_vel: 0.2009
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.70s
                        Total time: 997.24s
                               ETA: 608 mins 42.0 s

################################################################################
                     Learning iteration 1329/50000                      

                       Computation: 134805 steps/s (collection: 0.551s, learning 0.179s)
               Value function loss: 0.0738
                    Surrogate loss: 0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.91
                Mean reward (task): 2.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0216
       Mean episode rew_ang_vel_xy: -0.0160
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0278
   Mean episode rew_dof_pos_limits: -0.0252
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0324
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0378
 Mean episode rew_tracking_lin_vel: 0.2125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.73s
                        Total time: 997.97s
                               ETA: 608 mins 40.5 s

################################################################################
                     Learning iteration 1330/50000                      

                       Computation: 134722 steps/s (collection: 0.590s, learning 0.140s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.49
                Mean reward (task): 3.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0246
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0365
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2269
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.73s
                        Total time: 998.70s
                               ETA: 608 mins 39.0 s

################################################################################
                     Learning iteration 1331/50000                      

                       Computation: 138821 steps/s (collection: 0.568s, learning 0.141s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.78
                Mean reward (task): 2.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0254
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0232
       Mean episode rew_smoothness: -0.0338
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0393
 Mean episode rew_tracking_lin_vel: 0.2090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.71s
                        Total time: 999.41s
                               ETA: 608 mins 36.7 s

################################################################################
                     Learning iteration 1332/50000                      

                       Computation: 126313 steps/s (collection: 0.643s, learning 0.135s)
               Value function loss: 0.0768
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.58
                Mean reward (task): 3.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0236
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0269
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0248
       Mean episode rew_smoothness: -0.0349
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0417
 Mean episode rew_tracking_lin_vel: 0.2225
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.78s
                        Total time: 1000.19s
                               ETA: 608 mins 37.0 s

################################################################################
                     Learning iteration 1333/50000                      

                       Computation: 146210 steps/s (collection: 0.547s, learning 0.125s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.28
                Mean reward (task): 3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0221
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0282
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0229
       Mean episode rew_smoothness: -0.0323
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0380
 Mean episode rew_tracking_lin_vel: 0.2046
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 0.67s
                        Total time: 1000.86s
                               ETA: 608 mins 33.4 s

################################################################################
                     Learning iteration 1334/50000                      

                       Computation: 140118 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.25
                Mean reward (task): 3.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0251
       Mean episode rew_smoothness: -0.0355
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0425
 Mean episode rew_tracking_lin_vel: 0.2268
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 0.70s
                        Total time: 1001.56s
                               ETA: 608 mins 30.9 s

################################################################################
                     Learning iteration 1335/50000                      

                       Computation: 128683 steps/s (collection: 0.642s, learning 0.122s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.22
                Mean reward (task): 3.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0252
       Mean episode rew_smoothness: -0.0356
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0430
 Mean episode rew_tracking_lin_vel: 0.2294
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 0.76s
                        Total time: 1002.33s
                               ETA: 608 mins 30.6 s

################################################################################
                     Learning iteration 1336/50000                      

                       Computation: 137590 steps/s (collection: 0.569s, learning 0.145s)
               Value function loss: 0.0796
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.84
                Mean reward (task): 3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0224
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0251
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0328
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0385
 Mean episode rew_tracking_lin_vel: 0.2011
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 0.71s
                        Total time: 1003.04s
                               ETA: 608 mins 28.6 s

################################################################################
                     Learning iteration 1337/50000                      

                       Computation: 157986 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0106
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.89
                Mean reward (task): 2.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0291
   Mean episode rew_dof_pos_limits: -0.0268
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0248
       Mean episode rew_smoothness: -0.0347
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0406
 Mean episode rew_tracking_lin_vel: 0.2226
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 0.62s
                        Total time: 1003.66s
                               ETA: 608 mins 23.2 s

################################################################################
                     Learning iteration 1338/50000                      

                       Computation: 132900 steps/s (collection: 0.592s, learning 0.148s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.10
                Mean reward (task): 4.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0273
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0303
       Mean episode rew_smoothness: -0.0415
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0514
 Mean episode rew_tracking_lin_vel: 0.2838
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 0.74s
                        Total time: 1004.40s
                               ETA: 608 mins 22.0 s

################################################################################
                     Learning iteration 1339/50000                      

                       Computation: 130014 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.58
                Mean reward (task): 3.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0261
       Mean episode rew_smoothness: -0.0362
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0452
 Mean episode rew_tracking_lin_vel: 0.2402
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 0.76s
                        Total time: 1005.16s
                               ETA: 608 mins 21.5 s

################################################################################
                     Learning iteration 1340/50000                      

                       Computation: 147697 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0724
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.78
                Mean reward (task): 3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0263
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0243
       Mean episode rew_smoothness: -0.0345
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 0.67s
                        Total time: 1005.82s
                               ETA: 608 mins 17.7 s

################################################################################
                     Learning iteration 1341/50000                      

                       Computation: 140015 steps/s (collection: 0.564s, learning 0.138s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.85
                Mean reward (task): 4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0270
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0248
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2214
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 0.70s
                        Total time: 1006.53s
                               ETA: 608 mins 15.2 s

################################################################################
                     Learning iteration 1342/50000                      

                       Computation: 135850 steps/s (collection: 0.600s, learning 0.124s)
               Value function loss: 0.0746
                    Surrogate loss: 0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.90
                Mean reward (task): 2.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0226
       Mean episode rew_ang_vel_xy: -0.0168
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0260
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0237
       Mean episode rew_smoothness: -0.0337
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0398
 Mean episode rew_tracking_lin_vel: 0.2134
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.72s
                        Total time: 1007.25s
                               ETA: 608 mins 13.5 s

################################################################################
                     Learning iteration 1343/50000                      

                       Computation: 158385 steps/s (collection: 0.499s, learning 0.122s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0358
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2297
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.62s
                        Total time: 1007.87s
                               ETA: 608 mins 8.1 s

################################################################################
                     Learning iteration 1344/50000                      

                       Computation: 147102 steps/s (collection: 0.530s, learning 0.138s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.51
                Mean reward (task): 3.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0266
        Mean episode rew_lin_vel_z: -0.0286
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0349
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0416
 Mean episode rew_tracking_lin_vel: 0.2259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.67s
                        Total time: 1008.54s
                               ETA: 608 mins 4.4 s

################################################################################
                     Learning iteration 1345/50000                      

                       Computation: 142017 steps/s (collection: 0.570s, learning 0.123s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.51
                Mean reward (task): 2.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 104.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0360
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0428
 Mean episode rew_tracking_lin_vel: 0.2372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.69s
                        Total time: 1009.23s
                               ETA: 608 mins 1.5 s

################################################################################
                     Learning iteration 1346/50000                      

                       Computation: 152825 steps/s (collection: 0.513s, learning 0.130s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.39
                Mean reward (task): 3.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0257
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0241
       Mean episode rew_smoothness: -0.0344
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.64s
                        Total time: 1009.87s
                               ETA: 607 mins 56.9 s

################################################################################
                     Learning iteration 1347/50000                      

                       Computation: 144636 steps/s (collection: 0.556s, learning 0.124s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.73
                Mean reward (task): 3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0429
 Mean episode rew_tracking_lin_vel: 0.2373
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.68s
                        Total time: 1010.55s
                               ETA: 607 mins 53.6 s

################################################################################
                     Learning iteration 1348/50000                      

                       Computation: 140522 steps/s (collection: 0.552s, learning 0.147s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.17
                Mean reward (task): 3.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0362
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0438
 Mean episode rew_tracking_lin_vel: 0.2312
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.70s
                        Total time: 1011.25s
                               ETA: 607 mins 51.1 s

################################################################################
                     Learning iteration 1349/50000                      

                       Computation: 155954 steps/s (collection: 0.506s, learning 0.125s)
               Value function loss: 0.0811
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.00
                Mean reward (task): 3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0272
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0353
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0438
 Mean episode rew_tracking_lin_vel: 0.2253
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.63s
                        Total time: 1011.88s
                               ETA: 607 mins 46.0 s

################################################################################
                     Learning iteration 1350/50000                      

                       Computation: 145354 steps/s (collection: 0.548s, learning 0.128s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0268
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0251
       Mean episode rew_smoothness: -0.0360
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0416
 Mean episode rew_tracking_lin_vel: 0.2252
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.68s
                        Total time: 1012.56s
                               ETA: 607 mins 42.6 s

################################################################################
                     Learning iteration 1351/50000                      

                       Computation: 136600 steps/s (collection: 0.593s, learning 0.127s)
               Value function loss: 0.0755
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.19
                Mean reward (task): 3.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0260
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0344
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0409
 Mean episode rew_tracking_lin_vel: 0.2179
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.72s
                        Total time: 1013.28s
                               ETA: 607 mins 40.8 s

################################################################################
                     Learning iteration 1352/50000                      

                       Computation: 158127 steps/s (collection: 0.498s, learning 0.123s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.13
                Mean reward (task): 3.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0246
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0263
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0447
 Mean episode rew_tracking_lin_vel: 0.2396
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.62s
                        Total time: 1013.90s
                               ETA: 607 mins 35.5 s

################################################################################
                     Learning iteration 1353/50000                      

                       Computation: 135117 steps/s (collection: 0.606s, learning 0.122s)
               Value function loss: 0.0770
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0299
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0388
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0471
 Mean episode rew_tracking_lin_vel: 0.2611
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.73s
                        Total time: 1014.63s
                               ETA: 607 mins 33.9 s

################################################################################
                     Learning iteration 1354/50000                      

                       Computation: 156246 steps/s (collection: 0.505s, learning 0.124s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.36
                Mean reward (task): 3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0249
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2237
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.63s
                        Total time: 1015.26s
                               ETA: 607 mins 28.9 s

################################################################################
                     Learning iteration 1355/50000                      

                       Computation: 132979 steps/s (collection: 0.607s, learning 0.133s)
               Value function loss: 0.0803
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.57
                Mean reward (task): 5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0363
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2356
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.74s
                        Total time: 1016.00s
                               ETA: 607 mins 27.8 s

################################################################################
                     Learning iteration 1356/50000                      

                       Computation: 156776 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0253
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0239
       Mean episode rew_smoothness: -0.0344
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0409
 Mean episode rew_tracking_lin_vel: 0.2200
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.63s
                        Total time: 1016.62s
                               ETA: 607 mins 22.6 s

################################################################################
                     Learning iteration 1357/50000                      

                       Computation: 139351 steps/s (collection: 0.583s, learning 0.122s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.89
                Mean reward (task): 3.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0264
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0342
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0418
 Mean episode rew_tracking_lin_vel: 0.2170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.71s
                        Total time: 1017.33s
                               ETA: 607 mins 20.3 s

################################################################################
                     Learning iteration 1358/50000                      

                       Computation: 143467 steps/s (collection: 0.561s, learning 0.124s)
               Value function loss: 0.0739
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.11
                Mean reward (task): 2.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 107.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0246
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0261
       Mean episode rew_smoothness: -0.0366
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0435
 Mean episode rew_tracking_lin_vel: 0.2321
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.69s
                        Total time: 1018.01s
                               ETA: 607 mins 17.3 s

################################################################################
                     Learning iteration 1359/50000                      

                       Computation: 158540 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.19
                Mean reward (task): 4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0270
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0254
       Mean episode rew_smoothness: -0.0360
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0429
 Mean episode rew_tracking_lin_vel: 0.2299
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.62s
                        Total time: 1018.63s
                               ETA: 607 mins 11.9 s

################################################################################
                     Learning iteration 1360/50000                      

                       Computation: 150283 steps/s (collection: 0.532s, learning 0.122s)
               Value function loss: 0.0780
                    Surrogate loss: 0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.97
                Mean reward (task): 2.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0230
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0255
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0339
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0386
 Mean episode rew_tracking_lin_vel: 0.2109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.65s
                        Total time: 1019.29s
                               ETA: 607 mins 7.8 s

################################################################################
                     Learning iteration 1361/50000                      

                       Computation: 140190 steps/s (collection: 0.578s, learning 0.124s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.48
                Mean reward (task): 3.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0167
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0285
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0197
           Mean episode rew_no_fly: 0.0243
       Mean episode rew_smoothness: -0.0342
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0421
 Mean episode rew_tracking_lin_vel: 0.2262
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.70s
                        Total time: 1019.99s
                               ETA: 607 mins 5.3 s

################################################################################
                     Learning iteration 1362/50000                      

                       Computation: 142772 steps/s (collection: 0.565s, learning 0.123s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.44
                Mean reward (task): 3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 128.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0271
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0359
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0431
 Mean episode rew_tracking_lin_vel: 0.2280
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.69s
                        Total time: 1020.68s
                               ETA: 607 mins 2.4 s

################################################################################
                     Learning iteration 1363/50000                      

                       Computation: 142096 steps/s (collection: 0.568s, learning 0.124s)
               Value function loss: 0.0769
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.33
                Mean reward (task): 4.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0264
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0300
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0472
 Mean episode rew_tracking_lin_vel: 0.2658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.69s
                        Total time: 1021.37s
                               ETA: 606 mins 59.6 s

################################################################################
                     Learning iteration 1364/50000                      

                       Computation: 146156 steps/s (collection: 0.544s, learning 0.129s)
               Value function loss: 0.0778
                    Surrogate loss: 0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.62
                Mean reward (task): 3.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0357
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2287
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.67s
                        Total time: 1022.04s
                               ETA: 606 mins 56.2 s

################################################################################
                     Learning iteration 1365/50000                      

                       Computation: 149940 steps/s (collection: 0.517s, learning 0.139s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.22
                Mean reward (task): 3.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0276
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0363
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0427
 Mean episode rew_tracking_lin_vel: 0.2328
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.66s
                        Total time: 1022.70s
                               ETA: 606 mins 52.1 s

################################################################################
                     Learning iteration 1366/50000                      

                       Computation: 149465 steps/s (collection: 0.517s, learning 0.140s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0260
       Mean episode rew_smoothness: -0.0363
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0436
 Mean episode rew_tracking_lin_vel: 0.2366
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.66s
                        Total time: 1023.36s
                               ETA: 606 mins 48.1 s

################################################################################
                     Learning iteration 1367/50000                      

                       Computation: 141711 steps/s (collection: 0.569s, learning 0.125s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.64
                Mean reward (task): 3.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0273
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0362
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2334
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.69s
                        Total time: 1024.05s
                               ETA: 606 mins 45.4 s

################################################################################
                     Learning iteration 1368/50000                      

                       Computation: 135787 steps/s (collection: 0.602s, learning 0.122s)
               Value function loss: 0.0706
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.38
                Mean reward (task): 2.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 109.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0254
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0235
       Mean episode rew_smoothness: -0.0337
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0394
 Mean episode rew_tracking_lin_vel: 0.2090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.72s
                        Total time: 1024.77s
                               ETA: 606 mins 43.8 s

################################################################################
                     Learning iteration 1369/50000                      

                       Computation: 144681 steps/s (collection: 0.557s, learning 0.123s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.05
                Mean reward (task): 3.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0246
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0230
       Mean episode rew_smoothness: -0.0334
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0383
 Mean episode rew_tracking_lin_vel: 0.2097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.68s
                        Total time: 1025.45s
                               ETA: 606 mins 40.6 s

################################################################################
                     Learning iteration 1370/50000                      

                       Computation: 129867 steps/s (collection: 0.635s, learning 0.122s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.61
                Mean reward (task): 3.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0289
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0376
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2452
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.76s
                        Total time: 1026.21s
                               ETA: 606 mins 40.2 s

################################################################################
                     Learning iteration 1371/50000                      

                       Computation: 157449 steps/s (collection: 0.501s, learning 0.124s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.11
                Mean reward (task): 4.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0241
       Mean episode rew_smoothness: -0.0347
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.62s
                        Total time: 1026.83s
                               ETA: 606 mins 35.0 s

################################################################################
                     Learning iteration 1372/50000                      

                       Computation: 155415 steps/s (collection: 0.509s, learning 0.124s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.03
                Mean reward (task): 3.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0225
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0247
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0233
       Mean episode rew_smoothness: -0.0333
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0399
 Mean episode rew_tracking_lin_vel: 0.2062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.63s
                        Total time: 1027.47s
                               ETA: 606 mins 30.2 s

################################################################################
                     Learning iteration 1373/50000                      

                       Computation: 157132 steps/s (collection: 0.504s, learning 0.121s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.32
                Mean reward (task): 4.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0235
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0249
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.63s
                        Total time: 1028.09s
                               ETA: 606 mins 25.1 s

################################################################################
                     Learning iteration 1374/50000                      

                       Computation: 137643 steps/s (collection: 0.575s, learning 0.139s)
               Value function loss: 0.0722
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.44
                Mean reward (task): 4.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0266
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.71s
                        Total time: 1028.81s
                               ETA: 606 mins 23.1 s

################################################################################
                     Learning iteration 1375/50000                      

                       Computation: 140911 steps/s (collection: 0.557s, learning 0.141s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.81
                Mean reward (task): 2.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0247
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0282
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0263
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2386
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.70s
                        Total time: 1029.50s
                               ETA: 606 mins 20.6 s

################################################################################
                     Learning iteration 1376/50000                      

                       Computation: 151843 steps/s (collection: 0.506s, learning 0.141s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.29
                Mean reward (task): 4.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0264
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0447
 Mean episode rew_tracking_lin_vel: 0.2336
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.65s
                        Total time: 1030.15s
                               ETA: 606 mins 16.3 s

################################################################################
                     Learning iteration 1377/50000                      

                       Computation: 130207 steps/s (collection: 0.594s, learning 0.161s)
               Value function loss: 0.0699
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.11
                Mean reward (task): 3.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0260
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0352
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0411
 Mean episode rew_tracking_lin_vel: 0.2192
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.75s
                        Total time: 1030.91s
                               ETA: 606 mins 15.8 s

################################################################################
                     Learning iteration 1378/50000                      

                       Computation: 134532 steps/s (collection: 0.569s, learning 0.162s)
               Value function loss: 0.0760
                    Surrogate loss: 0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.32
                Mean reward (task): 3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0242
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0365
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2307
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.73s
                        Total time: 1031.64s
                               ETA: 606 mins 14.4 s

################################################################################
                     Learning iteration 1379/50000                      

                       Computation: 134820 steps/s (collection: 0.562s, learning 0.167s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.28
                Mean reward (task): 3.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0247
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0272
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0263
       Mean episode rew_smoothness: -0.0369
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0445
 Mean episode rew_tracking_lin_vel: 0.2329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.73s
                        Total time: 1032.37s
                               ETA: 606 mins 13.0 s

################################################################################
                     Learning iteration 1380/50000                      

                       Computation: 125854 steps/s (collection: 0.610s, learning 0.172s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.90
                Mean reward (task): 2.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 119.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0267
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0357
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0433
 Mean episode rew_tracking_lin_vel: 0.2297
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.78s
                        Total time: 1033.15s
                               ETA: 606 mins 13.4 s

################################################################################
                     Learning iteration 1381/50000                      

                       Computation: 145991 steps/s (collection: 0.544s, learning 0.129s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.01
                Mean reward (task): 4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0248
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0342
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.67s
                        Total time: 1033.82s
                               ETA: 606 mins 10.0 s

################################################################################
                     Learning iteration 1382/50000                      

                       Computation: 152789 steps/s (collection: 0.519s, learning 0.124s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.14
                Mean reward (task): 3.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0239
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0271
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0356
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0432
 Mean episode rew_tracking_lin_vel: 0.2213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.64s
                        Total time: 1034.46s
                               ETA: 606 mins 5.6 s

################################################################################
                     Learning iteration 1383/50000                      

                       Computation: 140612 steps/s (collection: 0.562s, learning 0.137s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.56
                Mean reward (task): 3.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0264
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2328
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.70s
                        Total time: 1035.16s
                               ETA: 606 mins 3.1 s

################################################################################
                     Learning iteration 1384/50000                      

                       Computation: 140860 steps/s (collection: 0.576s, learning 0.122s)
               Value function loss: 0.0762
                    Surrogate loss: 0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.21
                Mean reward (task): 3.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2338
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.70s
                        Total time: 1035.86s
                               ETA: 606 mins 0.6 s

################################################################################
                     Learning iteration 1385/50000                      

                       Computation: 141478 steps/s (collection: 0.561s, learning 0.134s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.40
                Mean reward (task): 5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0282
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0271
       Mean episode rew_smoothness: -0.0380
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0447
 Mean episode rew_tracking_lin_vel: 0.2446
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.69s
                        Total time: 1036.56s
                               ETA: 605 mins 58.0 s

################################################################################
                     Learning iteration 1386/50000                      

                       Computation: 148672 steps/s (collection: 0.521s, learning 0.140s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.33
                Mean reward (task): 3.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0276
       Mean episode rew_smoothness: -0.0382
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.66s
                        Total time: 1037.22s
                               ETA: 605 mins 54.2 s

################################################################################
                     Learning iteration 1387/50000                      

                       Computation: 139303 steps/s (collection: 0.555s, learning 0.151s)
               Value function loss: 0.0704
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0218
       Mean episode rew_ang_vel_xy: -0.0165
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0241
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0225
       Mean episode rew_smoothness: -0.0323
          Mean episode rew_torques: -0.0067
 Mean episode rew_tracking_ang_vel: 0.0362
 Mean episode rew_tracking_lin_vel: 0.2010
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.71s
                        Total time: 1037.92s
                               ETA: 605 mins 52.0 s

################################################################################
                     Learning iteration 1388/50000                      

                       Computation: 136379 steps/s (collection: 0.589s, learning 0.131s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.40
                Mean reward (task): 2.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0249
       Mean episode rew_smoothness: -0.0358
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0427
 Mean episode rew_tracking_lin_vel: 0.2217
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.72s
                        Total time: 1038.64s
                               ETA: 605 mins 50.3 s

################################################################################
                     Learning iteration 1389/50000                      

                       Computation: 147736 steps/s (collection: 0.543s, learning 0.122s)
               Value function loss: 0.0646
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.41
                Mean reward (task): 3.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0231
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0264
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0247
       Mean episode rew_smoothness: -0.0341
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0405
 Mean episode rew_tracking_lin_vel: 0.2138
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.67s
                        Total time: 1039.31s
                               ETA: 605 mins 46.7 s

################################################################################
                     Learning iteration 1390/50000                      

                       Computation: 151807 steps/s (collection: 0.524s, learning 0.123s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.55
                Mean reward (task): 3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0257
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2461
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.65s
                        Total time: 1039.96s
                               ETA: 605 mins 42.4 s

################################################################################
                     Learning iteration 1391/50000                      

                       Computation: 136194 steps/s (collection: 0.576s, learning 0.146s)
               Value function loss: 0.0743
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.02
                Mean reward (task): 3.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0228
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0296
   Mean episode rew_dof_pos_limits: -0.0250
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0236
       Mean episode rew_smoothness: -0.0336
          Mean episode rew_torques: -0.0071
 Mean episode rew_tracking_ang_vel: 0.0400
 Mean episode rew_tracking_lin_vel: 0.2013
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.72s
                        Total time: 1040.68s
                               ETA: 605 mins 40.8 s

################################################################################
                     Learning iteration 1392/50000                      

                       Computation: 144554 steps/s (collection: 0.518s, learning 0.162s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.00
                Mean reward (task): 3.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 125.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0374
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2395
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.68s
                        Total time: 1041.36s
                               ETA: 605 mins 37.7 s

################################################################################
                     Learning iteration 1393/50000                      

                       Computation: 131711 steps/s (collection: 0.605s, learning 0.141s)
               Value function loss: 0.0746
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.85
                Mean reward (task): 2.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0166
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0279
   Mean episode rew_dof_pos_limits: -0.0264
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0252
       Mean episode rew_smoothness: -0.0346
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0418
 Mean episode rew_tracking_lin_vel: 0.2201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.75s
                        Total time: 1042.11s
                               ETA: 605 mins 36.9 s

################################################################################
                     Learning iteration 1394/50000                      

                       Computation: 137429 steps/s (collection: 0.586s, learning 0.129s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.78
                Mean reward (task): 3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 142.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0284
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0261
       Mean episode rew_smoothness: -0.0357
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2333
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.72s
                        Total time: 1042.82s
                               ETA: 605 mins 35.0 s

################################################################################
                     Learning iteration 1395/50000                      

                       Computation: 156020 steps/s (collection: 0.508s, learning 0.122s)
               Value function loss: 0.0740
                    Surrogate loss: 0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.04
                Mean reward (task): 3.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0232
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0246
       Mean episode rew_smoothness: -0.0344
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0409
 Mean episode rew_tracking_lin_vel: 0.2122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.63s
                        Total time: 1043.45s
                               ETA: 605 mins 30.2 s

################################################################################
                     Learning iteration 1396/50000                      

                       Computation: 148336 steps/s (collection: 0.537s, learning 0.126s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.13
                Mean reward (task): 3.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0247
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0265
           Mean episode rew_no_fly: 0.0262
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2372
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.66s
                        Total time: 1044.11s
                               ETA: 605 mins 26.5 s

################################################################################
                     Learning iteration 1397/50000                      

                       Computation: 128606 steps/s (collection: 0.638s, learning 0.126s)
               Value function loss: 0.0698
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.71
                Mean reward (task): 3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0263
       Mean episode rew_smoothness: -0.0371
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2322
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.76s
                        Total time: 1044.88s
                               ETA: 605 mins 26.3 s

################################################################################
                     Learning iteration 1398/50000                      

                       Computation: 151490 steps/s (collection: 0.526s, learning 0.123s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.58
                Mean reward (task): 3.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0222
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0246
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0231
       Mean episode rew_smoothness: -0.0328
          Mean episode rew_torques: -0.0069
 Mean episode rew_tracking_ang_vel: 0.0391
 Mean episode rew_tracking_lin_vel: 0.2065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.65s
                        Total time: 1045.53s
                               ETA: 605 mins 22.2 s

################################################################################
                     Learning iteration 1399/50000                      

                       Computation: 152527 steps/s (collection: 0.521s, learning 0.124s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.58
                Mean reward (task): 2.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 101.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0234
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0243
       Mean episode rew_smoothness: -0.0346
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0416
 Mean episode rew_tracking_lin_vel: 0.2155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.64s
                        Total time: 1046.17s
                               ETA: 605 mins 17.8 s

################################################################################
                     Learning iteration 1400/50000                      

                       Computation: 134571 steps/s (collection: 0.608s, learning 0.123s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.59
                Mean reward (task): 2.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0276
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0355
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2336
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.73s
                        Total time: 1046.90s
                               ETA: 605 mins 16.5 s

################################################################################
                     Learning iteration 1401/50000                      

                       Computation: 151777 steps/s (collection: 0.524s, learning 0.124s)
               Value function loss: 0.0696
                    Surrogate loss: 0.0100
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.94
                Mean reward (task): 2.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0237
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0269
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0252
       Mean episode rew_smoothness: -0.0352
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0420
 Mean episode rew_tracking_lin_vel: 0.2275
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.65s
                        Total time: 1047.55s
                               ETA: 605 mins 12.3 s

################################################################################
                     Learning iteration 1402/50000                      

                       Computation: 136181 steps/s (collection: 0.601s, learning 0.121s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.95
                Mean reward (task): 3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0289
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0273
       Mean episode rew_smoothness: -0.0374
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2479
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.72s
                        Total time: 1048.27s
                               ETA: 605 mins 10.7 s

################################################################################
                     Learning iteration 1403/50000                      

                       Computation: 141425 steps/s (collection: 0.563s, learning 0.133s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.20
                Mean reward (task): 4.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0239
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0273
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0356
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0436
 Mean episode rew_tracking_lin_vel: 0.2319
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.70s
                        Total time: 1048.97s
                               ETA: 605 mins 8.1 s

################################################################################
                     Learning iteration 1404/50000                      

                       Computation: 129541 steps/s (collection: 0.621s, learning 0.138s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.75
                Mean reward (task): 3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0357
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2296
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.76s
                        Total time: 1049.73s
                               ETA: 605 mins 7.8 s

################################################################################
                     Learning iteration 1405/50000                      

                       Computation: 141220 steps/s (collection: 0.569s, learning 0.127s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.34
                Mean reward (task): 2.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0233
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0252
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0242
       Mean episode rew_smoothness: -0.0343
          Mean episode rew_torques: -0.0070
 Mean episode rew_tracking_ang_vel: 0.0412
 Mean episode rew_tracking_lin_vel: 0.2155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.70s
                        Total time: 1050.42s
                               ETA: 605 mins 5.3 s

################################################################################
                     Learning iteration 1406/50000                      

                       Computation: 157154 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.12
                Mean reward (task): 3.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0249
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0282
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0267
       Mean episode rew_smoothness: -0.0372
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2325
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.63s
                        Total time: 1051.05s
                               ETA: 605 mins 0.3 s

################################################################################
                     Learning iteration 1407/50000                      

                       Computation: 126732 steps/s (collection: 0.647s, learning 0.128s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.17
                Mean reward (task): 3.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0227
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0288
   Mean episode rew_dof_pos_limits: -0.0255
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0237
       Mean episode rew_smoothness: -0.0334
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0411
 Mean episode rew_tracking_lin_vel: 0.2040
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.78s
                        Total time: 1051.82s
                               ETA: 605 mins 0.6 s

################################################################################
                     Learning iteration 1408/50000                      

                       Computation: 157721 steps/s (collection: 0.500s, learning 0.123s)
               Value function loss: 0.0694
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.79
                Mean reward (task): 4.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0244
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0270
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0363
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0415
 Mean episode rew_tracking_lin_vel: 0.2275
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.62s
                        Total time: 1052.45s
                               ETA: 604 mins 55.6 s

################################################################################
                     Learning iteration 1409/50000                      

                       Computation: 151051 steps/s (collection: 0.510s, learning 0.140s)
               Value function loss: 0.0688
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.14
                Mean reward (task): 4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0288
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2407
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.65s
                        Total time: 1053.10s
                               ETA: 604 mins 51.5 s

################################################################################
                     Learning iteration 1410/50000                      

                       Computation: 144615 steps/s (collection: 0.553s, learning 0.126s)
               Value function loss: 0.0738
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.62
                Mean reward (task): 3.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0245
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0361
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0419
 Mean episode rew_tracking_lin_vel: 0.2310
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.68s
                        Total time: 1053.78s
                               ETA: 604 mins 48.5 s

################################################################################
                     Learning iteration 1411/50000                      

                       Computation: 138722 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0708
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.47
                Mean reward (task): 3.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0237
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0263
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0249
       Mean episode rew_smoothness: -0.0350
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0409
 Mean episode rew_tracking_lin_vel: 0.2213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.71s
                        Total time: 1054.49s
                               ETA: 604 mins 46.4 s

################################################################################
                     Learning iteration 1412/50000                      

                       Computation: 142402 steps/s (collection: 0.562s, learning 0.128s)
               Value function loss: 0.0776
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.23
                Mean reward (task): 3.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0285
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0270
       Mean episode rew_smoothness: -0.0380
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2457
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.69s
                        Total time: 1055.18s
                               ETA: 604 mins 43.7 s

################################################################################
                     Learning iteration 1413/50000                      

                       Computation: 142506 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0687
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 6.08
                Mean reward (task): 6.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 213.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0307
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0295
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0482
 Mean episode rew_tracking_lin_vel: 0.2649
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.69s
                        Total time: 1055.87s
                               ETA: 604 mins 41.0 s

################################################################################
                     Learning iteration 1414/50000                      

                       Computation: 139912 steps/s (collection: 0.579s, learning 0.124s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.88
                Mean reward (task): 2.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 115.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0260
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0250
       Mean episode rew_smoothness: -0.0356
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0408
 Mean episode rew_tracking_lin_vel: 0.2277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.70s
                        Total time: 1056.57s
                               ETA: 604 mins 38.7 s

################################################################################
                     Learning iteration 1415/50000                      

                       Computation: 145281 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.90
                Mean reward (task): 3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0298
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0285
       Mean episode rew_smoothness: -0.0392
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2577
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.68s
                        Total time: 1057.24s
                               ETA: 604 mins 35.6 s

################################################################################
                     Learning iteration 1416/50000                      

                       Computation: 138455 steps/s (collection: 0.566s, learning 0.144s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.95
                Mean reward (task): 3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0260
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0293
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0276
       Mean episode rew_smoothness: -0.0382
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0449
 Mean episode rew_tracking_lin_vel: 0.2465
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.71s
                        Total time: 1057.95s
                               ETA: 604 mins 33.6 s

################################################################################
                     Learning iteration 1417/50000                      

                       Computation: 141818 steps/s (collection: 0.551s, learning 0.142s)
               Value function loss: 0.0765
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.22
                Mean reward (task): 4.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0229
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0290
   Mean episode rew_dof_pos_limits: -0.0261
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0240
       Mean episode rew_smoothness: -0.0338
          Mean episode rew_torques: -0.0072
 Mean episode rew_tracking_ang_vel: 0.0381
 Mean episode rew_tracking_lin_vel: 0.2179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.69s
                        Total time: 1058.65s
                               ETA: 604 mins 31.0 s

################################################################################
                     Learning iteration 1418/50000                      

                       Computation: 153060 steps/s (collection: 0.508s, learning 0.134s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.60
                Mean reward (task): 4.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0249
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0261
       Mean episode rew_smoothness: -0.0365
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2326
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.64s
                        Total time: 1059.29s
                               ETA: 604 mins 26.7 s

################################################################################
                     Learning iteration 1419/50000                      

                       Computation: 150427 steps/s (collection: 0.523s, learning 0.131s)
               Value function loss: 0.0708
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.19
                Mean reward (task): 4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0307
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0403
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0476
 Mean episode rew_tracking_lin_vel: 0.2690
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.65s
                        Total time: 1059.94s
                               ETA: 604 mins 22.8 s

################################################################################
                     Learning iteration 1420/50000                      

                       Computation: 152424 steps/s (collection: 0.522s, learning 0.123s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.04
                Mean reward (task): 4.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0257
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0381
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0437
 Mean episode rew_tracking_lin_vel: 0.2448
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.64s
                        Total time: 1060.59s
                               ETA: 604 mins 18.5 s

################################################################################
                     Learning iteration 1421/50000                      

                       Computation: 132084 steps/s (collection: 0.614s, learning 0.131s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.28
                Mean reward (task): 2.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 110.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0271
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0253
       Mean episode rew_smoothness: -0.0359
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0426
 Mean episode rew_tracking_lin_vel: 0.2180
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.74s
                        Total time: 1061.33s
                               ETA: 604 mins 17.7 s

################################################################################
                     Learning iteration 1422/50000                      

                       Computation: 153286 steps/s (collection: 0.519s, learning 0.122s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.07
                Mean reward (task): 4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0266
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0394
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2499
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.64s
                        Total time: 1061.97s
                               ETA: 604 mins 13.4 s

################################################################################
                     Learning iteration 1423/50000                      

                       Computation: 154928 steps/s (collection: 0.511s, learning 0.124s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.88
                Mean reward (task): 4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0256
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0297
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0473
 Mean episode rew_tracking_lin_vel: 0.2557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.63s
                        Total time: 1062.61s
                               ETA: 604 mins 8.8 s

################################################################################
                     Learning iteration 1424/50000                      

                       Computation: 136210 steps/s (collection: 0.582s, learning 0.139s)
               Value function loss: 0.0760
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.26
                Mean reward (task): 3.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0301
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0398
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0484
 Mean episode rew_tracking_lin_vel: 0.2604
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.72s
                        Total time: 1063.33s
                               ETA: 604 mins 7.3 s

################################################################################
                     Learning iteration 1425/50000                      

                       Computation: 147336 steps/s (collection: 0.535s, learning 0.132s)
               Value function loss: 0.0691
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.61
                Mean reward (task): 3.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0250
       Mean episode rew_smoothness: -0.0357
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0433
 Mean episode rew_tracking_lin_vel: 0.2237
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.67s
                        Total time: 1064.00s
                               ETA: 604 mins 3.8 s

################################################################################
                     Learning iteration 1426/50000                      

                       Computation: 149595 steps/s (collection: 0.533s, learning 0.124s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.47
                Mean reward (task): 3.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0272
       Mean episode rew_smoothness: -0.0377
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2444
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.66s
                        Total time: 1064.65s
                               ETA: 604 mins 0.0 s

################################################################################
                     Learning iteration 1427/50000                      

                       Computation: 157400 steps/s (collection: 0.502s, learning 0.123s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.98
                Mean reward (task): 3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0260
       Mean episode rew_smoothness: -0.0363
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.62s
                        Total time: 1065.28s
                               ETA: 603 mins 55.2 s

################################################################################
                     Learning iteration 1428/50000                      

                       Computation: 155484 steps/s (collection: 0.509s, learning 0.123s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0259
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0292
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0389
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0461
 Mean episode rew_tracking_lin_vel: 0.2446
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.63s
                        Total time: 1065.91s
                               ETA: 603 mins 50.5 s

################################################################################
                     Learning iteration 1429/50000                      

                       Computation: 157627 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.40
                Mean reward (task): 3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0245
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0289
   Mean episode rew_dof_pos_limits: -0.0280
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0267
       Mean episode rew_smoothness: -0.0369
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2306
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.62s
                        Total time: 1066.54s
                               ETA: 603 mins 45.6 s

################################################################################
                     Learning iteration 1430/50000                      

                       Computation: 123814 steps/s (collection: 0.654s, learning 0.140s)
               Value function loss: 0.0693
                    Surrogate loss: 0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0271
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0304
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0290
       Mean episode rew_smoothness: -0.0405
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0487
 Mean episode rew_tracking_lin_vel: 0.2532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.79s
                        Total time: 1067.33s
                               ETA: 603 mins 46.5 s

################################################################################
                     Learning iteration 1431/50000                      

                       Computation: 136460 steps/s (collection: 0.577s, learning 0.143s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0265
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0303
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0398
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0483
 Mean episode rew_tracking_lin_vel: 0.2600
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.72s
                        Total time: 1068.05s
                               ETA: 603 mins 44.9 s

################################################################################
                     Learning iteration 1432/50000                      

                       Computation: 153100 steps/s (collection: 0.520s, learning 0.122s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.50
                Mean reward (task): 3.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0244
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0284
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0264
       Mean episode rew_smoothness: -0.0369
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0449
 Mean episode rew_tracking_lin_vel: 0.2357
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.64s
                        Total time: 1068.69s
                               ETA: 603 mins 40.7 s

################################################################################
                     Learning iteration 1433/50000                      

                       Computation: 156586 steps/s (collection: 0.505s, learning 0.123s)
               Value function loss: 0.0712
                    Surrogate loss: 0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.34
                Mean reward (task): 4.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0300
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0403
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2592
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.63s
                        Total time: 1069.32s
                               ETA: 603 mins 35.9 s

################################################################################
                     Learning iteration 1434/50000                      

                       Computation: 143158 steps/s (collection: 0.553s, learning 0.133s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0389
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2424
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.69s
                        Total time: 1070.01s
                               ETA: 603 mins 33.2 s

################################################################################
                     Learning iteration 1435/50000                      

                       Computation: 141340 steps/s (collection: 0.558s, learning 0.138s)
               Value function loss: 0.0783
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.24
                Mean reward (task): 3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0389
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0465
 Mean episode rew_tracking_lin_vel: 0.2561
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.70s
                        Total time: 1070.70s
                               ETA: 603 mins 30.7 s

################################################################################
                     Learning iteration 1436/50000                      

                       Computation: 150068 steps/s (collection: 0.523s, learning 0.132s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.84
                Mean reward (task): 3.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0276
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0270
       Mean episode rew_smoothness: -0.0382
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2429
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.66s
                        Total time: 1071.36s
                               ETA: 603 mins 26.9 s

################################################################################
                     Learning iteration 1437/50000                      

                       Computation: 131837 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0374
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2436
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.75s
                        Total time: 1072.10s
                               ETA: 603 mins 26.2 s

################################################################################
                     Learning iteration 1438/50000                      

                       Computation: 146087 steps/s (collection: 0.551s, learning 0.122s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.01
                Mean reward (task): 3.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0275
       Mean episode rew_smoothness: -0.0378
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2380
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.67s
                        Total time: 1072.78s
                               ETA: 603 mins 23.0 s

################################################################################
                     Learning iteration 1439/50000                      

                       Computation: 152624 steps/s (collection: 0.521s, learning 0.123s)
               Value function loss: 0.0726
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0298
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0384
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0463
 Mean episode rew_tracking_lin_vel: 0.2512
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.64s
                        Total time: 1073.42s
                               ETA: 603 mins 18.8 s

################################################################################
                     Learning iteration 1440/50000                      

                       Computation: 154509 steps/s (collection: 0.513s, learning 0.123s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.86
                Mean reward (task): 3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0282
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0264
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0450
 Mean episode rew_tracking_lin_vel: 0.2320
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.64s
                        Total time: 1074.06s
                               ETA: 603 mins 14.4 s

################################################################################
                     Learning iteration 1441/50000                      

                       Computation: 140621 steps/s (collection: 0.561s, learning 0.138s)
               Value function loss: 0.0758
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.90
                Mean reward (task): 3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0306
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0410
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0501
 Mean episode rew_tracking_lin_vel: 0.2594
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.70s
                        Total time: 1074.75s
                               ETA: 603 mins 12.1 s

################################################################################
                     Learning iteration 1442/50000                      

                       Computation: 137044 steps/s (collection: 0.573s, learning 0.144s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.52
                Mean reward (task): 4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0262
       Mean episode rew_smoothness: -0.0375
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0422
 Mean episode rew_tracking_lin_vel: 0.2403
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.72s
                        Total time: 1075.47s
                               ETA: 603 mins 10.4 s

################################################################################
                     Learning iteration 1443/50000                      

                       Computation: 119929 steps/s (collection: 0.651s, learning 0.169s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.62
                Mean reward (task): 5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0289
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0272
       Mean episode rew_smoothness: -0.0376
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0447
 Mean episode rew_tracking_lin_vel: 0.2455
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.82s
                        Total time: 1076.29s
                               ETA: 603 mins 12.2 s

################################################################################
                     Learning iteration 1444/50000                      

                       Computation: 142558 steps/s (collection: 0.550s, learning 0.139s)
               Value function loss: 0.0649
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.21
                Mean reward (task): 3.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0253
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0280
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0438
 Mean episode rew_tracking_lin_vel: 0.2338
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.69s
                        Total time: 1076.98s
                               ETA: 603 mins 9.5 s

################################################################################
                     Learning iteration 1445/50000                      

                       Computation: 140820 steps/s (collection: 0.566s, learning 0.132s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.45
                Mean reward (task): 4.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0303
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0400
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.70s
                        Total time: 1077.68s
                               ETA: 603 mins 7.2 s

################################################################################
                     Learning iteration 1446/50000                      

                       Computation: 125073 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.78
                Mean reward (task): 3.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0269
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0251
       Mean episode rew_smoothness: -0.0355
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0414
 Mean episode rew_tracking_lin_vel: 0.2199
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.79s
                        Total time: 1078.47s
                               ETA: 603 mins 7.8 s

################################################################################
                     Learning iteration 1447/50000                      

                       Computation: 156629 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.32
                Mean reward (task): 3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0255
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0380
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2350
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.63s
                        Total time: 1079.09s
                               ETA: 603 mins 3.1 s

################################################################################
                     Learning iteration 1448/50000                      

                       Computation: 149072 steps/s (collection: 0.538s, learning 0.122s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.55
                Mean reward (task): 3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0260
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2419
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.66s
                        Total time: 1079.75s
                               ETA: 602 mins 59.5 s

################################################################################
                     Learning iteration 1449/50000                      

                       Computation: 149007 steps/s (collection: 0.524s, learning 0.136s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.03
                Mean reward (task): 4.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0297
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0395
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2585
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.66s
                        Total time: 1080.41s
                               ETA: 602 mins 55.9 s

################################################################################
                     Learning iteration 1450/50000                      

                       Computation: 125118 steps/s (collection: 0.657s, learning 0.129s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.22
                Mean reward (task): 4.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0266
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0399
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0467
 Mean episode rew_tracking_lin_vel: 0.2643
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.79s
                        Total time: 1081.20s
                               ETA: 602 mins 56.5 s

################################################################################
                     Learning iteration 1451/50000                      

                       Computation: 152098 steps/s (collection: 0.524s, learning 0.123s)
               Value function loss: 0.0713
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.92
                Mean reward (task): 4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 183.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0287
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0270
       Mean episode rew_smoothness: -0.0386
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0438
 Mean episode rew_tracking_lin_vel: 0.2470
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.65s
                        Total time: 1081.84s
                               ETA: 602 mins 52.5 s

################################################################################
                     Learning iteration 1452/50000                      

                       Computation: 136873 steps/s (collection: 0.572s, learning 0.146s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0256
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0382
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2425
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.72s
                        Total time: 1082.56s
                               ETA: 602 mins 50.8 s

################################################################################
                     Learning iteration 1453/50000                      

                       Computation: 150217 steps/s (collection: 0.532s, learning 0.122s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.25
                Mean reward (task): 4.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0308
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0285
       Mean episode rew_smoothness: -0.0400
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0478
 Mean episode rew_tracking_lin_vel: 0.2644
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.65s
                        Total time: 1083.22s
                               ETA: 602 mins 47.1 s

################################################################################
                     Learning iteration 1454/50000                      

                       Computation: 157773 steps/s (collection: 0.502s, learning 0.121s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.26
                Mean reward (task): 3.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0260
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0427
 Mean episode rew_tracking_lin_vel: 0.2333
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.62s
                        Total time: 1083.84s
                               ETA: 602 mins 42.3 s

################################################################################
                     Learning iteration 1455/50000                      

                       Computation: 141722 steps/s (collection: 0.571s, learning 0.123s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.29
                Mean reward (task): 4.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0297
        Mean episode rew_lin_vel_z: -0.0264
           Mean episode rew_no_fly: 0.0284
       Mean episode rew_smoothness: -0.0394
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2572
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.69s
                        Total time: 1084.53s
                               ETA: 602 mins 39.8 s

################################################################################
                     Learning iteration 1456/50000                      

                       Computation: 149918 steps/s (collection: 0.535s, learning 0.121s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.37
                Mean reward (task): 3.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0254
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0377
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2412
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.66s
                        Total time: 1085.19s
                               ETA: 602 mins 36.1 s

################################################################################
                     Learning iteration 1457/50000                      

                       Computation: 155095 steps/s (collection: 0.512s, learning 0.122s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.58
                Mean reward (task): 2.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0275
       Mean episode rew_smoothness: -0.0384
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0452
 Mean episode rew_tracking_lin_vel: 0.2489
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.63s
                        Total time: 1085.82s
                               ETA: 602 mins 31.6 s

################################################################################
                     Learning iteration 1458/50000                      

                       Computation: 158625 steps/s (collection: 0.497s, learning 0.122s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.83
                Mean reward (task): 4.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0253
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0491
 Mean episode rew_tracking_lin_vel: 0.2740
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.62s
                        Total time: 1086.44s
                               ETA: 602 mins 26.7 s

################################################################################
                     Learning iteration 1459/50000                      

                       Computation: 142638 steps/s (collection: 0.562s, learning 0.127s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.68
                Mean reward (task): 3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0277
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2277
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.69s
                        Total time: 1087.13s
                               ETA: 602 mins 24.2 s

################################################################################
                     Learning iteration 1460/50000                      

                       Computation: 145323 steps/s (collection: 0.540s, learning 0.137s)
               Value function loss: 0.0685
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.54
                Mean reward (task): 2.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 111.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0238
       Mean episode rew_ang_vel_xy: -0.0172
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0265
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0349
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0400
 Mean episode rew_tracking_lin_vel: 0.2228
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.68s
                        Total time: 1087.81s
                               ETA: 602 mins 21.1 s

################################################################################
                     Learning iteration 1461/50000                      

                       Computation: 153099 steps/s (collection: 0.504s, learning 0.138s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.19
                Mean reward (task): 4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0258
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0288
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0276
       Mean episode rew_smoothness: -0.0386
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2578
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.64s
                        Total time: 1088.45s
                               ETA: 602 mins 17.0 s

################################################################################
                     Learning iteration 1462/50000                      

                       Computation: 139860 steps/s (collection: 0.568s, learning 0.135s)
               Value function loss: 0.0711
                    Surrogate loss: 0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.07
                Mean reward (task): 4.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0250
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0259
       Mean episode rew_smoothness: -0.0369
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0427
 Mean episode rew_tracking_lin_vel: 0.2293
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.70s
                        Total time: 1089.15s
                               ETA: 602 mins 14.9 s

################################################################################
                     Learning iteration 1463/50000                      

                       Computation: 152060 steps/s (collection: 0.524s, learning 0.123s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.04
                Mean reward (task): 4.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0259
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0388
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2562
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.65s
                        Total time: 1089.80s
                               ETA: 602 mins 10.9 s

################################################################################
                     Learning iteration 1464/50000                      

                       Computation: 141578 steps/s (collection: 0.571s, learning 0.123s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.56
                Mean reward (task): 2.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 116.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0304
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0393
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2546
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.69s
                        Total time: 1090.49s
                               ETA: 602 mins 8.5 s

################################################################################
                     Learning iteration 1465/50000                      

                       Computation: 130091 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0720
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.36
                Mean reward (task): 3.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0254
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0432
 Mean episode rew_tracking_lin_vel: 0.2415
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.76s
                        Total time: 1091.25s
                               ETA: 602 mins 8.1 s

################################################################################
                     Learning iteration 1466/50000                      

                       Computation: 151568 steps/s (collection: 0.510s, learning 0.138s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.03
                Mean reward (task): 5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 181.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0271
       Mean episode rew_smoothness: -0.0374
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2438
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.65s
                        Total time: 1091.90s
                               ETA: 602 mins 4.2 s

################################################################################
                     Learning iteration 1467/50000                      

                       Computation: 137895 steps/s (collection: 0.583s, learning 0.130s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.72
                Mean reward (task): 4.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0293
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0392
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.71s
                        Total time: 1092.61s
                               ETA: 602 mins 2.4 s

################################################################################
                     Learning iteration 1468/50000                      

                       Computation: 141170 steps/s (collection: 0.572s, learning 0.124s)
               Value function loss: 0.0751
                    Surrogate loss: 0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.96
                Mean reward (task): 3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2548
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.70s
                        Total time: 1093.31s
                               ETA: 602 mins 0.1 s

################################################################################
                     Learning iteration 1469/50000                      

                       Computation: 154549 steps/s (collection: 0.513s, learning 0.123s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.52
                Mean reward (task): 3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0248
       Mean episode rew_ang_vel_xy: -0.0174
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0271
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0258
       Mean episode rew_smoothness: -0.0368
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2344
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.64s
                        Total time: 1093.94s
                               ETA: 601 mins 55.8 s

################################################################################
                     Learning iteration 1470/50000                      

                       Computation: 135174 steps/s (collection: 0.606s, learning 0.121s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.63
                Mean reward (task): 3.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0298
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0399
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0463
 Mean episode rew_tracking_lin_vel: 0.2657
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.73s
                        Total time: 1094.67s
                               ETA: 601 mins 54.5 s

################################################################################
                     Learning iteration 1471/50000                      

                       Computation: 151668 steps/s (collection: 0.517s, learning 0.131s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.52
                Mean reward (task): 3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0371
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0430
 Mean episode rew_tracking_lin_vel: 0.2390
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.65s
                        Total time: 1095.32s
                               ETA: 601 mins 50.5 s

################################################################################
                     Learning iteration 1472/50000                      

                       Computation: 131030 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.44
                Mean reward (task): 3.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0253
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0272
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0257
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0438
 Mean episode rew_tracking_lin_vel: 0.2346
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.75s
                        Total time: 1096.07s
                               ETA: 601 mins 50.0 s

################################################################################
                     Learning iteration 1473/50000                      

                       Computation: 146195 steps/s (collection: 0.549s, learning 0.124s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.40
                Mean reward (task): 3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0373
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0435
 Mean episode rew_tracking_lin_vel: 0.2369
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.67s
                        Total time: 1096.74s
                               ETA: 601 mins 46.9 s

################################################################################
                     Learning iteration 1474/50000                      

                       Computation: 144507 steps/s (collection: 0.558s, learning 0.122s)
               Value function loss: 0.0709
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.94
                Mean reward (task): 2.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0292
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0275
       Mean episode rew_smoothness: -0.0390
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0444
 Mean episode rew_tracking_lin_vel: 0.2530
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.68s
                        Total time: 1097.42s
                               ETA: 601 mins 44.1 s

################################################################################
                     Learning iteration 1475/50000                      

                       Computation: 158291 steps/s (collection: 0.499s, learning 0.122s)
               Value function loss: 0.0757
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.66
                Mean reward (task): 4.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0306
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0486
 Mean episode rew_tracking_lin_vel: 0.2914
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.62s
                        Total time: 1098.04s
                               ETA: 601 mins 39.3 s

################################################################################
                     Learning iteration 1476/50000                      

                       Computation: 141071 steps/s (collection: 0.574s, learning 0.123s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.32
                Mean reward (task): 5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0506
 Mean episode rew_tracking_lin_vel: 0.2708
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.70s
                        Total time: 1098.74s
                               ETA: 601 mins 37.0 s

################################################################################
                     Learning iteration 1477/50000                      

                       Computation: 133194 steps/s (collection: 0.598s, learning 0.140s)
               Value function loss: 0.0752
                    Surrogate loss: 0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.72
                Mean reward (task): 4.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 181.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0264
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0292
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2615
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.74s
                        Total time: 1099.48s
                               ETA: 601 mins 36.0 s

################################################################################
                     Learning iteration 1478/50000                      

                       Computation: 147073 steps/s (collection: 0.535s, learning 0.134s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.48
                Mean reward (task): 2.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 108.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0266
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0274
       Mean episode rew_smoothness: -0.0390
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2472
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.67s
                        Total time: 1100.15s
                               ETA: 601 mins 32.8 s

################################################################################
                     Learning iteration 1479/50000                      

                       Computation: 138309 steps/s (collection: 0.553s, learning 0.158s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.47
                Mean reward (task): 4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2558
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.71s
                        Total time: 1100.86s
                               ETA: 601 mins 31.0 s

################################################################################
                     Learning iteration 1480/50000                      

                       Computation: 143822 steps/s (collection: 0.560s, learning 0.123s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.96
                Mean reward (task): 3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0265
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0393
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2524
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.68s
                        Total time: 1101.54s
                               ETA: 601 mins 28.3 s

################################################################################
                     Learning iteration 1481/50000                      

                       Computation: 147519 steps/s (collection: 0.543s, learning 0.123s)
               Value function loss: 0.0733
                    Surrogate loss: 0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.48
                Mean reward (task): 4.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0276
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0461
 Mean episode rew_tracking_lin_vel: 0.2517
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.67s
                        Total time: 1102.21s
                               ETA: 601 mins 25.0 s

################################################################################
                     Learning iteration 1482/50000                      

                       Computation: 134248 steps/s (collection: 0.600s, learning 0.132s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0266
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0467
 Mean episode rew_tracking_lin_vel: 0.2522
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.73s
                        Total time: 1102.94s
                               ETA: 601 mins 23.9 s

################################################################################
                     Learning iteration 1483/50000                      

                       Computation: 157225 steps/s (collection: 0.502s, learning 0.123s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0274
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0407
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2628
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.63s
                        Total time: 1103.56s
                               ETA: 601 mins 19.3 s

################################################################################
                     Learning iteration 1484/50000                      

                       Computation: 141420 steps/s (collection: 0.572s, learning 0.123s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.35
                Mean reward (task): 4.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0414
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2777
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.70s
                        Total time: 1104.26s
                               ETA: 601 mins 16.9 s

################################################################################
                     Learning iteration 1485/50000                      

                       Computation: 148535 steps/s (collection: 0.538s, learning 0.124s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.14
                Mean reward (task): 4.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0297
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2637
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.66s
                        Total time: 1104.92s
                               ETA: 601 mins 13.5 s

################################################################################
                     Learning iteration 1486/50000                      

                       Computation: 157910 steps/s (collection: 0.499s, learning 0.123s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.40
                Mean reward (task): 4.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0295
       Mean episode rew_smoothness: -0.0406
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0484
 Mean episode rew_tracking_lin_vel: 0.2695
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.62s
                        Total time: 1105.54s
                               ETA: 601 mins 8.8 s

################################################################################
                     Learning iteration 1487/50000                      

                       Computation: 140069 steps/s (collection: 0.564s, learning 0.138s)
               Value function loss: 0.0708
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.74
                Mean reward (task): 4.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0290
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2549
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.70s
                        Total time: 1106.25s
                               ETA: 601 mins 6.7 s

################################################################################
                     Learning iteration 1488/50000                      

                       Computation: 122714 steps/s (collection: 0.645s, learning 0.156s)
               Value function loss: 0.0775
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.31
                Mean reward (task): 5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 186.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0297
       Mean episode rew_smoothness: -0.0411
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0498
 Mean episode rew_tracking_lin_vel: 0.2788
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.80s
                        Total time: 1107.05s
                               ETA: 601 mins 7.9 s

################################################################################
                     Learning iteration 1489/50000                      

                       Computation: 128935 steps/s (collection: 0.639s, learning 0.123s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.70
                Mean reward (task): 2.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0241
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0256
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0244
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0073
 Mean episode rew_tracking_ang_vel: 0.0407
 Mean episode rew_tracking_lin_vel: 0.2123
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.76s
                        Total time: 1107.81s
                               ETA: 601 mins 7.7 s

################################################################################
                     Learning iteration 1490/50000                      

                       Computation: 159693 steps/s (collection: 0.495s, learning 0.121s)
               Value function loss: 0.0701
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.16
                Mean reward (task): 4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0282
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0272
       Mean episode rew_smoothness: -0.0383
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0463
 Mean episode rew_tracking_lin_vel: 0.2441
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.62s
                        Total time: 1108.42s
                               ETA: 601 mins 2.8 s

################################################################################
                     Learning iteration 1491/50000                      

                       Computation: 133896 steps/s (collection: 0.609s, learning 0.125s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.80
                Mean reward (task): 2.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0249
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0285
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0266
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2303
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.73s
                        Total time: 1109.16s
                               ETA: 601 mins 1.8 s

################################################################################
                     Learning iteration 1492/50000                      

                       Computation: 150289 steps/s (collection: 0.531s, learning 0.123s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.58
                Mean reward (task): 3.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0384
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0439
 Mean episode rew_tracking_lin_vel: 0.2402
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.65s
                        Total time: 1109.81s
                               ETA: 600 mins 58.1 s

################################################################################
                     Learning iteration 1493/50000                      

                       Computation: 155522 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0732
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.48
                Mean reward (task): 4.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0253
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0285
        Mean episode rew_lin_vel_z: -0.0198
           Mean episode rew_no_fly: 0.0271
       Mean episode rew_smoothness: -0.0375
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0458
 Mean episode rew_tracking_lin_vel: 0.2453
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.63s
                        Total time: 1110.44s
                               ETA: 600 mins 53.8 s

################################################################################
                     Learning iteration 1494/50000                      

                       Computation: 145475 steps/s (collection: 0.551s, learning 0.125s)
               Value function loss: 0.0682
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.49
                Mean reward (task): 3.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0169
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0258
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0245
       Mean episode rew_smoothness: -0.0351
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0402
 Mean episode rew_tracking_lin_vel: 0.2324
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.68s
                        Total time: 1111.12s
                               ETA: 600 mins 50.9 s

################################################################################
                     Learning iteration 1495/50000                      

                       Computation: 147136 steps/s (collection: 0.523s, learning 0.145s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.99
                Mean reward (task): 2.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0297
       Mean episode rew_smoothness: -0.0410
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2670
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.67s
                        Total time: 1111.79s
                               ETA: 600 mins 47.7 s

################################################################################
                     Learning iteration 1496/50000                      

                       Computation: 159254 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.66
                Mean reward (task): 4.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0255
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0280
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0268
       Mean episode rew_smoothness: -0.0377
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2382
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.62s
                        Total time: 1112.41s
                               ETA: 600 mins 42.9 s

################################################################################
                     Learning iteration 1497/50000                      

                       Computation: 156479 steps/s (collection: 0.505s, learning 0.123s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.82
                Mean reward (task): 3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0269
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0308
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0398
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0491
 Mean episode rew_tracking_lin_vel: 0.2605
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.63s
                        Total time: 1113.03s
                               ETA: 600 mins 38.4 s

################################################################################
                     Learning iteration 1498/50000                      

                       Computation: 146029 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.77
                Mean reward (task): 3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0274
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0303
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2775
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.67s
                        Total time: 1113.71s
                               ETA: 600 mins 35.4 s

################################################################################
                     Learning iteration 1499/50000                      

                       Computation: 143141 steps/s (collection: 0.547s, learning 0.140s)
               Value function loss: 0.0751
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.76
                Mean reward (task): 2.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 118.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0275
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0262
       Mean episode rew_smoothness: -0.0370
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0436
 Mean episode rew_tracking_lin_vel: 0.2387
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.69s
                        Total time: 1114.39s
                               ETA: 600 mins 32.8 s

################################################################################
                     Learning iteration 1500/50000                      

                       Computation: 144000 steps/s (collection: 0.543s, learning 0.140s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.02
                Mean reward (task): 4.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0260
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0274
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0455
 Mean episode rew_tracking_lin_vel: 0.2477
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.68s
                        Total time: 1115.08s
                               ETA: 600 mins 30.1 s

################################################################################
                     Learning iteration 1501/50000                      

                       Computation: 160374 steps/s (collection: 0.489s, learning 0.124s)
               Value function loss: 0.0681
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.65
                Mean reward (task): 5.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0478
 Mean episode rew_tracking_lin_vel: 0.2662
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.61s
                        Total time: 1115.69s
                               ETA: 600 mins 25.2 s

################################################################################
                     Learning iteration 1502/50000                      

                       Computation: 128132 steps/s (collection: 0.636s, learning 0.131s)
               Value function loss: 0.0694
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.16
                Mean reward (task): 4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0283
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2800
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.77s
                        Total time: 1116.46s
                               ETA: 600 mins 25.2 s

################################################################################
                     Learning iteration 1503/50000                      

                       Computation: 138084 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0672
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.27
                Mean reward (task): 3.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0251
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0305
   Mean episode rew_dof_pos_limits: -0.0274
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0256
       Mean episode rew_smoothness: -0.0367
          Mean episode rew_torques: -0.0077
 Mean episode rew_tracking_ang_vel: 0.0412
 Mean episode rew_tracking_lin_vel: 0.2302
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.71s
                        Total time: 1117.17s
                               ETA: 600 mins 23.5 s

################################################################################
                     Learning iteration 1504/50000                      

                       Computation: 144626 steps/s (collection: 0.550s, learning 0.130s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.93
                Mean reward (task): 3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0320
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2749
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.68s
                        Total time: 1117.85s
                               ETA: 600 mins 20.7 s

################################################################################
                     Learning iteration 1505/50000                      

                       Computation: 155383 steps/s (collection: 0.510s, learning 0.123s)
               Value function loss: 0.0763
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.31
                Mean reward (task): 3.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 127.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0243
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0262
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0246
       Mean episode rew_smoothness: -0.0354
          Mean episode rew_torques: -0.0074
 Mean episode rew_tracking_ang_vel: 0.0402
 Mean episode rew_tracking_lin_vel: 0.2208
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.63s
                        Total time: 1118.48s
                               ETA: 600 mins 16.4 s

################################################################################
                     Learning iteration 1506/50000                      

                       Computation: 158994 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.77
                Mean reward (task): 3.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0254
       Mean episode rew_ang_vel_xy: -0.0176
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0267
       Mean episode rew_smoothness: -0.0371
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0450
 Mean episode rew_tracking_lin_vel: 0.2352
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.62s
                        Total time: 1119.10s
                               ETA: 600 mins 11.7 s

################################################################################
                     Learning iteration 1507/50000                      

                       Computation: 157820 steps/s (collection: 0.501s, learning 0.122s)
               Value function loss: 0.0729
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.66
                Mean reward (task): 3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0310
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0298
       Mean episode rew_smoothness: -0.0410
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2605
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.62s
                        Total time: 1119.72s
                               ETA: 600 mins 7.1 s

################################################################################
                     Learning iteration 1508/50000                      

                       Computation: 160505 steps/s (collection: 0.491s, learning 0.122s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.97
                Mean reward (task): 3.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0260
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0293
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0277
       Mean episode rew_smoothness: -0.0384
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2544
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.61s
                        Total time: 1120.34s
                               ETA: 600 mins 2.2 s

################################################################################
                     Learning iteration 1509/50000                      

                       Computation: 158642 steps/s (collection: 0.496s, learning 0.123s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.41
                Mean reward (task): 5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0306
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0402
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0483
 Mean episode rew_tracking_lin_vel: 0.2660
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.62s
                        Total time: 1120.95s
                               ETA: 599 mins 57.5 s

################################################################################
                     Learning iteration 1510/50000                      

                       Computation: 159370 steps/s (collection: 0.492s, learning 0.125s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.28
                Mean reward (task): 4.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0437
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0511
 Mean episode rew_tracking_lin_vel: 0.2935
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.62s
                        Total time: 1121.57s
                               ETA: 599 mins 52.7 s

################################################################################
                     Learning iteration 1511/50000                      

                       Computation: 159646 steps/s (collection: 0.494s, learning 0.122s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0269
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0462
 Mean episode rew_tracking_lin_vel: 0.2517
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.62s
                        Total time: 1122.19s
                               ETA: 599 mins 47.9 s

################################################################################
                     Learning iteration 1512/50000                      

                       Computation: 159912 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0710
                    Surrogate loss: 0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0309
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0289
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0475
 Mean episode rew_tracking_lin_vel: 0.2601
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.61s
                        Total time: 1122.80s
                               ETA: 599 mins 43.1 s

################################################################################
                     Learning iteration 1513/50000                      

                       Computation: 160174 steps/s (collection: 0.491s, learning 0.122s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.19
                Mean reward (task): 4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0310
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0296
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.2616
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.61s
                        Total time: 1123.42s
                               ETA: 599 mins 38.2 s

################################################################################
                     Learning iteration 1514/50000                      

                       Computation: 157924 steps/s (collection: 0.500s, learning 0.123s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.76
                Mean reward (task): 5.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0301
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0415
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0465
 Mean episode rew_tracking_lin_vel: 0.2615
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.62s
                        Total time: 1124.04s
                               ETA: 599 mins 33.7 s

################################################################################
                     Learning iteration 1515/50000                      

                       Computation: 159059 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.59
                Mean reward (task): 3.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0256
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0276
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0265
       Mean episode rew_smoothness: -0.0376
          Mean episode rew_torques: -0.0078
 Mean episode rew_tracking_ang_vel: 0.0440
 Mean episode rew_tracking_lin_vel: 0.2333
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.62s
                        Total time: 1124.66s
                               ETA: 599 mins 29.0 s

################################################################################
                     Learning iteration 1516/50000                      

                       Computation: 158921 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 6.01
                Mean reward (task): 6.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 211.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0331
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0434
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0528
 Mean episode rew_tracking_lin_vel: 0.2861
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.62s
                        Total time: 1125.27s
                               ETA: 599 mins 24.3 s

################################################################################
                     Learning iteration 1517/50000                      

                       Computation: 159951 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0780
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.86
                Mean reward (task): 5.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 218.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0317
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2799
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.61s
                        Total time: 1125.89s
                               ETA: 599 mins 19.5 s

################################################################################
                     Learning iteration 1518/50000                      

                       Computation: 157340 steps/s (collection: 0.502s, learning 0.122s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0412
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0504
 Mean episode rew_tracking_lin_vel: 0.2700
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.62s
                        Total time: 1126.51s
                               ETA: 599 mins 15.0 s

################################################################################
                     Learning iteration 1519/50000                      

                       Computation: 160217 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.16
                Mean reward (task): 5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0317
       Mean episode rew_smoothness: -0.0434
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0516
 Mean episode rew_tracking_lin_vel: 0.2838
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.61s
                        Total time: 1127.13s
                               ETA: 599 mins 10.2 s

################################################################################
                     Learning iteration 1520/50000                      

                       Computation: 157539 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.22
                Mean reward (task): 5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0299
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2663
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.62s
                        Total time: 1127.75s
                               ETA: 599 mins 5.7 s

################################################################################
                     Learning iteration 1521/50000                      

                       Computation: 159579 steps/s (collection: 0.492s, learning 0.124s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.04
                Mean reward (task): 3.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0283
       Mean episode rew_smoothness: -0.0387
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0462
 Mean episode rew_tracking_lin_vel: 0.2538
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.62s
                        Total time: 1128.37s
                               ETA: 599 mins 1.0 s

################################################################################
                     Learning iteration 1522/50000                      

                       Computation: 158445 steps/s (collection: 0.499s, learning 0.121s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.64
                Mean reward (task): 4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0281
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0274
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0432
 Mean episode rew_tracking_lin_vel: 0.2377
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.62s
                        Total time: 1128.99s
                               ETA: 598 mins 56.4 s

################################################################################
                     Learning iteration 1523/50000                      

                       Computation: 159220 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0750
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0388
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0464
 Mean episode rew_tracking_lin_vel: 0.2532
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.62s
                        Total time: 1129.61s
                               ETA: 598 mins 51.7 s

################################################################################
                     Learning iteration 1524/50000                      

                       Computation: 157719 steps/s (collection: 0.500s, learning 0.124s)
               Value function loss: 0.0724
                    Surrogate loss: 0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.74
                Mean reward (task): 4.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0425
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2791
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.62s
                        Total time: 1130.23s
                               ETA: 598 mins 47.2 s

################################################################################
                     Learning iteration 1525/50000                      

                       Computation: 160792 steps/s (collection: 0.488s, learning 0.123s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.79
                Mean reward (task): 3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0280
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0313
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0412
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2637
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.61s
                        Total time: 1130.84s
                               ETA: 598 mins 42.3 s

################################################################################
                     Learning iteration 1526/50000                      

                       Computation: 157763 steps/s (collection: 0.501s, learning 0.122s)
               Value function loss: 0.0806
                    Surrogate loss: 0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.52
                Mean reward (task): 3.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0333
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0317
       Mean episode rew_smoothness: -0.0435
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0513
 Mean episode rew_tracking_lin_vel: 0.2881
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.62s
                        Total time: 1131.46s
                               ETA: 598 mins 37.8 s

################################################################################
                     Learning iteration 1527/50000                      

                       Computation: 158219 steps/s (collection: 0.498s, learning 0.124s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.30
                Mean reward (task): 5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0356
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0344
       Mean episode rew_smoothness: -0.0456
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0565
 Mean episode rew_tracking_lin_vel: 0.3117
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.62s
                        Total time: 1132.08s
                               ETA: 598 mins 33.3 s

################################################################################
                     Learning iteration 1528/50000                      

                       Computation: 158902 steps/s (collection: 0.496s, learning 0.123s)
               Value function loss: 0.0705
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.47
                Mean reward (task): 4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2529
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.62s
                        Total time: 1132.70s
                               ETA: 598 mins 28.7 s

################################################################################
                     Learning iteration 1529/50000                      

                       Computation: 158175 steps/s (collection: 0.497s, learning 0.124s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.92
                Mean reward (task): 3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0283
       Mean episode rew_smoothness: -0.0400
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0458
 Mean episode rew_tracking_lin_vel: 0.2501
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.62s
                        Total time: 1133.32s
                               ETA: 598 mins 24.2 s

################################################################################
                     Learning iteration 1530/50000                      

                       Computation: 158603 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0754
                    Surrogate loss: 0.0105
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.06
                Mean reward (task): 4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0295
   Mean episode rew_dof_pos_limits: -0.0292
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0382
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0452
 Mean episode rew_tracking_lin_vel: 0.2483
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.62s
                        Total time: 1133.94s
                               ETA: 598 mins 19.6 s

################################################################################
                     Learning iteration 1531/50000                      

                       Computation: 158082 steps/s (collection: 0.500s, learning 0.122s)
               Value function loss: 0.0742
                    Surrogate loss: 0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.80
                Mean reward (task): 5.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 196.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0308
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0299
       Mean episode rew_smoothness: -0.0415
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0495
 Mean episode rew_tracking_lin_vel: 0.2769
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.62s
                        Total time: 1134.57s
                               ETA: 598 mins 15.1 s

################################################################################
                     Learning iteration 1532/50000                      

                       Computation: 156821 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.34
                Mean reward (task): 2.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 112.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0314
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0299
       Mean episode rew_smoothness: -0.0412
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0468
 Mean episode rew_tracking_lin_vel: 0.2653
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.63s
                        Total time: 1135.19s
                               ETA: 598 mins 10.8 s

################################################################################
                     Learning iteration 1533/50000                      

                       Computation: 160027 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0712
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.90
                Mean reward (task): 2.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 122.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0277
       Mean episode rew_smoothness: -0.0386
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2540
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.61s
                        Total time: 1135.81s
                               ETA: 598 mins 6.0 s

################################################################################
                     Learning iteration 1534/50000                      

                       Computation: 159914 steps/s (collection: 0.492s, learning 0.122s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.90
                Mean reward (task): 3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0286
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0394
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2542
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.61s
                        Total time: 1136.42s
                               ETA: 598 mins 1.3 s

################################################################################
                     Learning iteration 1535/50000                      

                       Computation: 157520 steps/s (collection: 0.502s, learning 0.122s)
               Value function loss: 0.0770
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.54
                Mean reward (task): 3.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0299
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0387
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0461
 Mean episode rew_tracking_lin_vel: 0.2551
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.62s
                        Total time: 1137.05s
                               ETA: 597 mins 56.9 s

################################################################################
                     Learning iteration 1536/50000                      

                       Computation: 160786 steps/s (collection: 0.489s, learning 0.123s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.22
                Mean reward (task): 4.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0297
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0434
 Mean episode rew_tracking_lin_vel: 0.2577
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.61s
                        Total time: 1137.66s
                               ETA: 597 mins 52.1 s

################################################################################
                     Learning iteration 1537/50000                      

                       Computation: 157196 steps/s (collection: 0.502s, learning 0.124s)
               Value function loss: 0.0814
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.51
                Mean reward (task): 4.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0333
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0443
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0537
 Mean episode rew_tracking_lin_vel: 0.2979
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.63s
                        Total time: 1138.28s
                               ETA: 597 mins 47.8 s

################################################################################
                     Learning iteration 1538/50000                      

                       Computation: 155649 steps/s (collection: 0.507s, learning 0.124s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.12
                Mean reward (task): 4.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0269
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0393
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0457
 Mean episode rew_tracking_lin_vel: 0.2499
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.63s
                        Total time: 1138.91s
                               ETA: 597 mins 43.6 s

################################################################################
                     Learning iteration 1539/50000                      

                       Computation: 159611 steps/s (collection: 0.492s, learning 0.124s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.54
                Mean reward (task): 4.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0423
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0495
 Mean episode rew_tracking_lin_vel: 0.2840
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.62s
                        Total time: 1139.53s
                               ETA: 597 mins 39.0 s

################################################################################
                     Learning iteration 1540/50000                      

                       Computation: 158322 steps/s (collection: 0.492s, learning 0.129s)
               Value function loss: 0.0736
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.06
                Mean reward (task): 3.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 121.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0252
       Mean episode rew_ang_vel_xy: -0.0170
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0299
   Mean episode rew_dof_pos_limits: -0.0278
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0269
       Mean episode rew_smoothness: -0.0372
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0441
 Mean episode rew_tracking_lin_vel: 0.2417
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.62s
                        Total time: 1140.15s
                               ETA: 597 mins 34.5 s

################################################################################
                     Learning iteration 1541/50000                      

                       Computation: 154338 steps/s (collection: 0.496s, learning 0.141s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.28
                Mean reward (task): 5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0430
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0501
 Mean episode rew_tracking_lin_vel: 0.2819
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.64s
                        Total time: 1140.79s
                               ETA: 597 mins 30.5 s

################################################################################
                     Learning iteration 1542/50000                      

                       Computation: 152570 steps/s (collection: 0.506s, learning 0.139s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.20
                Mean reward (task): 4.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0284
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0274
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2438
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.64s
                        Total time: 1141.43s
                               ETA: 597 mins 26.8 s

################################################################################
                     Learning iteration 1543/50000                      

                       Computation: 159131 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.74
                Mean reward (task): 5.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0335
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2987
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.62s
                        Total time: 1142.05s
                               ETA: 597 mins 22.2 s

################################################################################
                     Learning iteration 1544/50000                      

                       Computation: 140575 steps/s (collection: 0.577s, learning 0.123s)
               Value function loss: 0.0793
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.51
                Mean reward (task): 3.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0263
       Mean episode rew_ang_vel_xy: -0.0173
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0290
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0390
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0457
 Mean episode rew_tracking_lin_vel: 0.2509
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.70s
                        Total time: 1142.75s
                               ETA: 597 mins 20.2 s

################################################################################
                     Learning iteration 1545/50000                      

                       Computation: 156390 steps/s (collection: 0.505s, learning 0.124s)
               Value function loss: 0.0831
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.71
                Mean reward (task): 4.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2787
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.63s
                        Total time: 1143.38s
                               ETA: 597 mins 16.0 s

################################################################################
                     Learning iteration 1546/50000                      

                       Computation: 141923 steps/s (collection: 0.570s, learning 0.122s)
               Value function loss: 0.0827
                    Surrogate loss: 0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.79
                Mean reward (task): 4.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0328
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0314
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0506
 Mean episode rew_tracking_lin_vel: 0.2923
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.69s
                        Total time: 1144.07s
                               ETA: 597 mins 13.8 s

################################################################################
                     Learning iteration 1547/50000                      

                       Computation: 160201 steps/s (collection: 0.491s, learning 0.123s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.93
                Mean reward (task): 3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0303
        Mean episode rew_lin_vel_z: -0.0260
           Mean episode rew_no_fly: 0.0290
       Mean episode rew_smoothness: -0.0407
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0482
 Mean episode rew_tracking_lin_vel: 0.2690
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.61s
                        Total time: 1144.68s
                               ETA: 597 mins 9.1 s

################################################################################
                     Learning iteration 1548/50000                      

                       Computation: 158242 steps/s (collection: 0.498s, learning 0.123s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.09
                Mean reward (task): 5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0281
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0327
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0416
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0515
 Mean episode rew_tracking_lin_vel: 0.2901
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.62s
                        Total time: 1145.31s
                               ETA: 597 mins 4.6 s

################################################################################
                     Learning iteration 1549/50000                      

                       Computation: 156152 steps/s (collection: 0.508s, learning 0.121s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.27
                Mean reward (task): 3.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0271
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0284
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0464
 Mean episode rew_tracking_lin_vel: 0.2551
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.63s
                        Total time: 1145.94s
                               ETA: 597 mins 0.5 s

################################################################################
                     Learning iteration 1550/50000                      

                       Computation: 149265 steps/s (collection: 0.536s, learning 0.123s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.69
                Mean reward (task): 3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0302
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0290
       Mean episode rew_smoothness: -0.0403
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2556
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.66s
                        Total time: 1146.59s
                               ETA: 596 mins 57.2 s

################################################################################
                     Learning iteration 1551/50000                      

                       Computation: 125769 steps/s (collection: 0.641s, learning 0.140s)
               Value function loss: 0.0757
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.82
                Mean reward (task): 3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 140.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0298
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0482
 Mean episode rew_tracking_lin_vel: 0.2693
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.78s
                        Total time: 1147.38s
                               ETA: 596 mins 57.8 s

################################################################################
                     Learning iteration 1552/50000                      

                       Computation: 150024 steps/s (collection: 0.532s, learning 0.123s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.06
                Mean reward (task): 5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0284
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0270
       Mean episode rew_smoothness: -0.0383
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0430
 Mean episode rew_tracking_lin_vel: 0.2409
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.66s
                        Total time: 1148.03s
                               ETA: 596 mins 54.4 s

################################################################################
                     Learning iteration 1553/50000                      

                       Computation: 159133 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0393
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.2582
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.62s
                        Total time: 1148.65s
                               ETA: 596 mins 49.9 s

################################################################################
                     Learning iteration 1554/50000                      

                       Computation: 149351 steps/s (collection: 0.516s, learning 0.142s)
               Value function loss: 0.0749
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.99
                Mean reward (task): 4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0314
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0510
 Mean episode rew_tracking_lin_vel: 0.2910
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.66s
                        Total time: 1149.31s
                               ETA: 596 mins 46.6 s

################################################################################
                     Learning iteration 1555/50000                      

                       Computation: 140782 steps/s (collection: 0.573s, learning 0.125s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0273
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0293
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0399
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0458
 Mean episode rew_tracking_lin_vel: 0.2521
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.70s
                        Total time: 1150.01s
                               ETA: 596 mins 44.6 s

################################################################################
                     Learning iteration 1556/50000                      

                       Computation: 151142 steps/s (collection: 0.505s, learning 0.145s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.18
                Mean reward (task): 3.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 131.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0302
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0288
       Mean episode rew_smoothness: -0.0394
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0477
 Mean episode rew_tracking_lin_vel: 0.2621
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.65s
                        Total time: 1150.66s
                               ETA: 596 mins 41.1 s

################################################################################
                     Learning iteration 1557/50000                      

                       Computation: 143145 steps/s (collection: 0.558s, learning 0.128s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.66
                Mean reward (task): 3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0285
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0514
 Mean episode rew_tracking_lin_vel: 0.2832
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.69s
                        Total time: 1151.34s
                               ETA: 596 mins 38.8 s

################################################################################
                     Learning iteration 1558/50000                      

                       Computation: 132374 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0799
                    Surrogate loss: 0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.32
                Mean reward (task): 4.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0334
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0326
       Mean episode rew_smoothness: -0.0446
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0525
 Mean episode rew_tracking_lin_vel: 0.2993
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.74s
                        Total time: 1152.08s
                               ETA: 596 mins 38.1 s

################################################################################
                     Learning iteration 1559/50000                      

                       Computation: 150526 steps/s (collection: 0.531s, learning 0.122s)
               Value function loss: 0.0815
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.37
                Mean reward (task): 4.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2890
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.65s
                        Total time: 1152.74s
                               ETA: 596 mins 34.7 s

################################################################################
                     Learning iteration 1560/50000                      

                       Computation: 134945 steps/s (collection: 0.601s, learning 0.127s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.43
                Mean reward (task): 4.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0316
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0422
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0501
 Mean episode rew_tracking_lin_vel: 0.2765
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.73s
                        Total time: 1153.47s
                               ETA: 596 mins 33.7 s

################################################################################
                     Learning iteration 1561/50000                      

                       Computation: 154705 steps/s (collection: 0.513s, learning 0.122s)
               Value function loss: 0.0853
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.30
                Mean reward (task): 5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0314
       Mean episode rew_smoothness: -0.0437
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2888
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.64s
                        Total time: 1154.10s
                               ETA: 596 mins 29.7 s

################################################################################
                     Learning iteration 1562/50000                      

                       Computation: 132904 steps/s (collection: 0.609s, learning 0.130s)
               Value function loss: 0.0752
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.62
                Mean reward (task): 3.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0292
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0279
       Mean episode rew_smoothness: -0.0387
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2450
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.74s
                        Total time: 1154.84s
                               ETA: 596 mins 29.0 s

################################################################################
                     Learning iteration 1563/50000                      

                       Computation: 153865 steps/s (collection: 0.499s, learning 0.140s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.67
                Mean reward (task): 2.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 113.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0309
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0294
       Mean episode rew_smoothness: -0.0401
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2577
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.64s
                        Total time: 1155.48s
                               ETA: 596 mins 25.2 s

################################################################################
                     Learning iteration 1564/50000                      

                       Computation: 150049 steps/s (collection: 0.515s, learning 0.140s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.16
                Mean reward (task): 5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0318
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2771
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.66s
                        Total time: 1156.14s
                               ETA: 596 mins 21.8 s

################################################################################
                     Learning iteration 1565/50000                      

                       Computation: 154943 steps/s (collection: 0.494s, learning 0.140s)
               Value function loss: 0.0741
                    Surrogate loss: 0.0091
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.07
                Mean reward (task): 5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0312
       Mean episode rew_smoothness: -0.0425
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0508
 Mean episode rew_tracking_lin_vel: 0.2877
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.63s
                        Total time: 1156.77s
                               ETA: 596 mins 17.9 s

################################################################################
                     Learning iteration 1566/50000                      

                       Computation: 128246 steps/s (collection: 0.626s, learning 0.140s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.36
                Mean reward (task): 4.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0456
 Mean episode rew_tracking_lin_vel: 0.2523
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.77s
                        Total time: 1157.54s
                               ETA: 596 mins 18.0 s

################################################################################
                     Learning iteration 1567/50000                      

                       Computation: 156639 steps/s (collection: 0.491s, learning 0.137s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.53
                Mean reward (task): 4.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0388
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0467
 Mean episode rew_tracking_lin_vel: 0.2513
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.63s
                        Total time: 1158.16s
                               ETA: 596 mins 13.8 s

################################################################################
                     Learning iteration 1568/50000                      

                       Computation: 120787 steps/s (collection: 0.649s, learning 0.165s)
               Value function loss: 0.0736
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.00
                Mean reward (task): 4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0309
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0483
 Mean episode rew_tracking_lin_vel: 0.2578
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.81s
                        Total time: 1158.98s
                               ETA: 596 mins 15.4 s

################################################################################
                     Learning iteration 1569/50000                      

                       Computation: 138090 steps/s (collection: 0.571s, learning 0.141s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.76
                Mean reward (task): 4.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0284
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0421
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0486
 Mean episode rew_tracking_lin_vel: 0.2815
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.71s
                        Total time: 1159.69s
                               ETA: 596 mins 13.8 s

################################################################################
                     Learning iteration 1570/50000                      

                       Computation: 145443 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0825
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.57
                Mean reward (task): 5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0333
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0441
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0518
 Mean episode rew_tracking_lin_vel: 0.2978
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.68s
                        Total time: 1160.37s
                               ETA: 596 mins 11.2 s

################################################################################
                     Learning iteration 1571/50000                      

                       Computation: 144259 steps/s (collection: 0.540s, learning 0.141s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.55
                Mean reward (task): 3.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0271
           Mean episode rew_no_fly: 0.0299
       Mean episode rew_smoothness: -0.0405
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2692
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.68s
                        Total time: 1161.05s
                               ETA: 596 mins 8.7 s

################################################################################
                     Learning iteration 1572/50000                      

                       Computation: 144298 steps/s (collection: 0.552s, learning 0.129s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.32
                Mean reward (task): 4.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0417
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0498
 Mean episode rew_tracking_lin_vel: 0.2797
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.68s
                        Total time: 1161.73s
                               ETA: 596 mins 6.2 s

################################################################################
                     Learning iteration 1573/50000                      

                       Computation: 138830 steps/s (collection: 0.560s, learning 0.149s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.06
                Mean reward (task): 5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0298
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0337
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0441
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0547
 Mean episode rew_tracking_lin_vel: 0.3054
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.71s
                        Total time: 1162.44s
                               ETA: 596 mins 4.5 s

################################################################################
                     Learning iteration 1574/50000                      

                       Computation: 139509 steps/s (collection: 0.578s, learning 0.127s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 2.97
                Mean reward (task): 2.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 126.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0302
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0405
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0474
 Mean episode rew_tracking_lin_vel: 0.2567
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.70s
                        Total time: 1163.14s
                               ETA: 596 mins 2.7 s

################################################################################
                     Learning iteration 1575/50000                      

                       Computation: 153622 steps/s (collection: 0.517s, learning 0.123s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.28
                Mean reward (task): 5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0284
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2778
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.64s
                        Total time: 1163.78s
                               ETA: 595 mins 58.9 s

################################################################################
                     Learning iteration 1576/50000                      

                       Computation: 148207 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0748
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.59
                Mean reward (task): 4.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0311
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0518
 Mean episode rew_tracking_lin_vel: 0.2842
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.66s
                        Total time: 1164.44s
                               ETA: 595 mins 55.9 s

################################################################################
                     Learning iteration 1577/50000                      

                       Computation: 152508 steps/s (collection: 0.521s, learning 0.124s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.11
                Mean reward (task): 3.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 123.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0262
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0287
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0385
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0465
 Mean episode rew_tracking_lin_vel: 0.2500
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.64s
                        Total time: 1165.09s
                               ETA: 595 mins 52.3 s

################################################################################
                     Learning iteration 1578/50000                      

                       Computation: 144239 steps/s (collection: 0.559s, learning 0.122s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.10
                Mean reward (task): 5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 199.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0302
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0337
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0330
       Mean episode rew_smoothness: -0.0445
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0542
 Mean episode rew_tracking_lin_vel: 0.2908
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.68s
                        Total time: 1165.77s
                               ETA: 595 mins 49.8 s

################################################################################
                     Learning iteration 1579/50000                      

                       Computation: 140172 steps/s (collection: 0.578s, learning 0.124s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.89
                Mean reward (task): 4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 195.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0297
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0329
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0438
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2795
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.70s
                        Total time: 1166.47s
                               ETA: 595 mins 47.9 s

################################################################################
                     Learning iteration 1580/50000                      

                       Computation: 146522 steps/s (collection: 0.533s, learning 0.138s)
               Value function loss: 0.0770
                    Surrogate loss: 0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.53
                Mean reward (task): 3.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0330
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0422
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0508
 Mean episode rew_tracking_lin_vel: 0.2724
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.67s
                        Total time: 1167.14s
                               ETA: 595 mins 45.1 s

################################################################################
                     Learning iteration 1581/50000                      

                       Computation: 139102 steps/s (collection: 0.564s, learning 0.143s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.23
                Mean reward (task): 3.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0302
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0340
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0329
       Mean episode rew_smoothness: -0.0445
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0517
 Mean episode rew_tracking_lin_vel: 0.3055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.71s
                        Total time: 1167.85s
                               ETA: 595 mins 43.4 s

################################################################################
                     Learning iteration 1582/50000                      

                       Computation: 151561 steps/s (collection: 0.521s, learning 0.127s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.18
                Mean reward (task): 4.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0415
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0485
 Mean episode rew_tracking_lin_vel: 0.2712
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.65s
                        Total time: 1168.50s
                               ETA: 595 mins 39.9 s

################################################################################
                     Learning iteration 1583/50000                      

                       Computation: 138067 steps/s (collection: 0.589s, learning 0.123s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.67
                Mean reward (task): 3.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0317
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2884
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.71s
                        Total time: 1169.21s
                               ETA: 595 mins 38.4 s

################################################################################
                     Learning iteration 1584/50000                      

                       Computation: 150716 steps/s (collection: 0.529s, learning 0.123s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.96
                Mean reward (task): 3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0259
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0276
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0266
       Mean episode rew_smoothness: -0.0378
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0447
 Mean episode rew_tracking_lin_vel: 0.2443
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.65s
                        Total time: 1169.86s
                               ETA: 595 mins 35.0 s

################################################################################
                     Learning iteration 1585/50000                      

                       Computation: 137762 steps/s (collection: 0.589s, learning 0.125s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.12
                Mean reward (task): 4.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0302
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0402
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0479
 Mean episode rew_tracking_lin_vel: 0.2622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.71s
                        Total time: 1170.58s
                               ETA: 595 mins 33.6 s

################################################################################
                     Learning iteration 1586/50000                      

                       Computation: 156306 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.60
                Mean reward (task): 3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0283
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0272
       Mean episode rew_smoothness: -0.0381
          Mean episode rew_torques: -0.0081
 Mean episode rew_tracking_ang_vel: 0.0443
 Mean episode rew_tracking_lin_vel: 0.2532
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 0.63s
                        Total time: 1171.20s
                               ETA: 595 mins 29.5 s

################################################################################
                     Learning iteration 1587/50000                      

                       Computation: 148321 steps/s (collection: 0.540s, learning 0.123s)
               Value function loss: 0.0773
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 3.81
                Mean reward (task): 3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0314
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0300
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0503
 Mean episode rew_tracking_lin_vel: 0.2763
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 0.66s
                        Total time: 1171.87s
                               ETA: 595 mins 26.5 s

################################################################################
                     Learning iteration 1588/50000                      

                       Computation: 152143 steps/s (collection: 0.523s, learning 0.124s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 2.87
                Mean reward (task): 2.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 117.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0266
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0391
          Mean episode rew_torques: -0.0079
 Mean episode rew_tracking_ang_vel: 0.0463
 Mean episode rew_tracking_lin_vel: 0.2533
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 0.65s
                        Total time: 1172.51s
                               ETA: 595 mins 22.9 s

################################################################################
                     Learning iteration 1589/50000                      

                       Computation: 136536 steps/s (collection: 0.572s, learning 0.148s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.74
                Mean reward (task): 5.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0313
       Mean episode rew_smoothness: -0.0424
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2846
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 0.72s
                        Total time: 1173.23s
                               ETA: 595 mins 21.6 s

################################################################################
                     Learning iteration 1590/50000                      

                       Computation: 157079 steps/s (collection: 0.492s, learning 0.134s)
               Value function loss: 0.0744
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.65
                Mean reward (task): 4.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0283
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0504
 Mean episode rew_tracking_lin_vel: 0.2743
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.63s
                        Total time: 1173.86s
                               ETA: 595 mins 17.5 s

################################################################################
                     Learning iteration 1591/50000                      

                       Computation: 144507 steps/s (collection: 0.542s, learning 0.138s)
               Value function loss: 0.0800
                    Surrogate loss: 0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.32
                Mean reward (task): 4.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0482
 Mean episode rew_tracking_lin_vel: 0.2698
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 0.68s
                        Total time: 1174.54s
                               ETA: 595 mins 15.0 s

################################################################################
                     Learning iteration 1592/50000                      

                       Computation: 132083 steps/s (collection: 0.617s, learning 0.128s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.59
                Mean reward (task): 5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 195.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0275
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0414
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2680
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.74s
                        Total time: 1175.28s
                               ETA: 595 mins 14.5 s

################################################################################
                     Learning iteration 1593/50000                      

                       Computation: 150520 steps/s (collection: 0.526s, learning 0.127s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.88
                Mean reward (task): 5.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 202.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0280
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0299
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0411
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0448
 Mean episode rew_tracking_lin_vel: 0.2634
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.65s
                        Total time: 1175.94s
                               ETA: 595 mins 11.2 s

################################################################################
                     Learning iteration 1594/50000                      

                       Computation: 134080 steps/s (collection: 0.608s, learning 0.125s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.04
                Mean reward (task): 4.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0285
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0320
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0420
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0494
 Mean episode rew_tracking_lin_vel: 0.2822
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 0.73s
                        Total time: 1176.67s
                               ETA: 595 mins 10.3 s

################################################################################
                     Learning iteration 1595/50000                      

                       Computation: 149115 steps/s (collection: 0.537s, learning 0.122s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.31
                Mean reward (task): 4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0282
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0298
       Mean episode rew_smoothness: -0.0414
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2749
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 0.66s
                        Total time: 1177.33s
                               ETA: 595 mins 7.2 s

################################################################################
                     Learning iteration 1596/50000                      

                       Computation: 132570 steps/s (collection: 0.617s, learning 0.125s)
               Value function loss: 0.0779
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.56
                Mean reward (task): 4.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0291
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0394
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0464
 Mean episode rew_tracking_lin_vel: 0.2617
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.74s
                        Total time: 1178.07s
                               ETA: 595 mins 6.5 s

################################################################################
                     Learning iteration 1597/50000                      

                       Computation: 131021 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.58
                Mean reward (task): 4.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0279
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0313
   Mean episode rew_dof_pos_limits: -0.0313
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0411
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2710
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 0.75s
                        Total time: 1178.82s
                               ETA: 595 mins 6.2 s

################################################################################
                     Learning iteration 1598/50000                      

                       Computation: 146794 steps/s (collection: 0.549s, learning 0.121s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0271
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0298
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0285
       Mean episode rew_smoothness: -0.0400
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0466
 Mean episode rew_tracking_lin_vel: 0.2593
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.67s
                        Total time: 1179.49s
                               ETA: 595 mins 3.4 s

################################################################################
                     Learning iteration 1599/50000                      

                       Computation: 124043 steps/s (collection: 0.650s, learning 0.142s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0295
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0335
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0528
 Mean episode rew_tracking_lin_vel: 0.2901
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 0.79s
                        Total time: 1180.28s
                               ETA: 595 mins 4.3 s

################################################################################
                     Learning iteration 1600/50000                      

                       Computation: 145786 steps/s (collection: 0.535s, learning 0.140s)
               Value function loss: 0.0747
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.21
                Mean reward (task): 5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 207.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0300
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0327
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0532
 Mean episode rew_tracking_lin_vel: 0.2959
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 0.67s
                        Total time: 1180.96s
                               ETA: 595 mins 1.7 s

################################################################################
                     Learning iteration 1601/50000                      

                       Computation: 148462 steps/s (collection: 0.540s, learning 0.122s)
               Value function loss: 0.0834
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.82
                Mean reward (task): 4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0285
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0316
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0423
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2772
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.66s
                        Total time: 1181.62s
                               ETA: 594 mins 58.6 s

################################################################################
                     Learning iteration 1602/50000                      

                       Computation: 161649 steps/s (collection: 0.486s, learning 0.122s)
               Value function loss: 0.0818
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.80
                Mean reward (task): 3.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0271
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0285
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0276
       Mean episode rew_smoothness: -0.0393
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.2517
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.61s
                        Total time: 1182.23s
                               ETA: 594 mins 54.0 s

################################################################################
                     Learning iteration 1603/50000                      

                       Computation: 155707 steps/s (collection: 0.491s, learning 0.140s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.84
                Mean reward (task): 4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0284
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0303
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0481
 Mean episode rew_tracking_lin_vel: 0.2738
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 0.63s
                        Total time: 1182.86s
                               ETA: 594 mins 50.1 s

################################################################################
                     Learning iteration 1604/50000                      

                       Computation: 151110 steps/s (collection: 0.508s, learning 0.143s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.40
                Mean reward (task): 4.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0431
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2713
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.65s
                        Total time: 1183.51s
                               ETA: 594 mins 46.7 s

################################################################################
                     Learning iteration 1605/50000                      

                       Computation: 143341 steps/s (collection: 0.525s, learning 0.160s)
               Value function loss: 0.0733
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.98
                Mean reward (task): 3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 153.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0314
       Mean episode rew_smoothness: -0.0431
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0524
 Mean episode rew_tracking_lin_vel: 0.2860
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 0.69s
                        Total time: 1184.20s
                               ETA: 594 mins 44.4 s

################################################################################
                     Learning iteration 1606/50000                      

                       Computation: 154689 steps/s (collection: 0.497s, learning 0.138s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.32
                Mean reward (task): 3.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0303
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0330
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0448
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0530
 Mean episode rew_tracking_lin_vel: 0.2891
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 0.64s
                        Total time: 1184.83s
                               ETA: 594 mins 40.6 s

################################################################################
                     Learning iteration 1607/50000                      

                       Computation: 136714 steps/s (collection: 0.597s, learning 0.122s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.92
                Mean reward (task): 4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 193.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0281
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0417
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0490
 Mean episode rew_tracking_lin_vel: 0.2690
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.72s
                        Total time: 1185.55s
                               ETA: 594 mins 39.3 s

################################################################################
                     Learning iteration 1608/50000                      

                       Computation: 158123 steps/s (collection: 0.499s, learning 0.123s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.45
                Mean reward (task): 4.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0178
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0412
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2726
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.62s
                        Total time: 1186.17s
                               ETA: 594 mins 35.1 s

################################################################################
                     Learning iteration 1609/50000                      

                       Computation: 141046 steps/s (collection: 0.555s, learning 0.142s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.79
                Mean reward (task): 3.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0300
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0326
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0320
       Mean episode rew_smoothness: -0.0443
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0518
 Mean episode rew_tracking_lin_vel: 0.2887
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 0.70s
                        Total time: 1186.87s
                               ETA: 594 mins 33.2 s

################################################################################
                     Learning iteration 1610/50000                      

                       Computation: 126915 steps/s (collection: 0.629s, learning 0.146s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.19
                Mean reward (task): 3.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0318
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0423
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0494
 Mean episode rew_tracking_lin_vel: 0.2651
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 0.77s
                        Total time: 1187.64s
                               ETA: 594 mins 33.5 s

################################################################################
                     Learning iteration 1611/50000                      

                       Computation: 138539 steps/s (collection: 0.563s, learning 0.147s)
               Value function loss: 0.0673
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.49
                Mean reward (task): 4.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0343
       Mean episode rew_smoothness: -0.0458
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0561
 Mean episode rew_tracking_lin_vel: 0.3205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 0.71s
                        Total time: 1188.35s
                               ETA: 594 mins 32.0 s

################################################################################
                     Learning iteration 1612/50000                      

                       Computation: 151659 steps/s (collection: 0.512s, learning 0.136s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.27
                Mean reward (task): 4.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0308
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0354
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0453
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0535
 Mean episode rew_tracking_lin_vel: 0.2955
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 0.65s
                        Total time: 1189.00s
                               ETA: 594 mins 28.6 s

################################################################################
                     Learning iteration 1613/50000                      

                       Computation: 131350 steps/s (collection: 0.621s, learning 0.127s)
               Value function loss: 0.0768
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.56
                Mean reward (task): 4.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0296
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0340
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0436
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0523
 Mean episode rew_tracking_lin_vel: 0.2900
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 0.75s
                        Total time: 1189.75s
                               ETA: 594 mins 28.2 s

################################################################################
                     Learning iteration 1614/50000                      

                       Computation: 156200 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0770
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.87
                Mean reward (task): 4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0296
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0438
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2885
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.63s
                        Total time: 1190.38s
                               ETA: 594 mins 24.2 s

################################################################################
                     Learning iteration 1615/50000                      

                       Computation: 143790 steps/s (collection: 0.560s, learning 0.124s)
               Value function loss: 0.0768
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.24
                Mean reward (task): 4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2769
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 0.68s
                        Total time: 1191.06s
                               ETA: 594 mins 21.9 s

################################################################################
                     Learning iteration 1616/50000                      

                       Computation: 135026 steps/s (collection: 0.594s, learning 0.134s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.12
                Mean reward (task): 5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0180
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0302
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0399
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0473
 Mean episode rew_tracking_lin_vel: 0.2547
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.73s
                        Total time: 1191.79s
                               ETA: 594 mins 20.9 s

################################################################################
                     Learning iteration 1617/50000                      

                       Computation: 159629 steps/s (collection: 0.491s, learning 0.124s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.58
                Mean reward (task): 4.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0426
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0510
 Mean episode rew_tracking_lin_vel: 0.2827
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.62s
                        Total time: 1192.41s
                               ETA: 594 mins 16.5 s

################################################################################
                     Learning iteration 1618/50000                      

                       Computation: 147458 steps/s (collection: 0.545s, learning 0.121s)
               Value function loss: 0.0719
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.23
                Mean reward (task): 4.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0181
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0294
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0281
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0453
 Mean episode rew_tracking_lin_vel: 0.2557
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.67s
                        Total time: 1193.07s
                               ETA: 594 mins 13.7 s

################################################################################
                     Learning iteration 1619/50000                      

                       Computation: 152960 steps/s (collection: 0.520s, learning 0.123s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0274
       Mean episode rew_ang_vel_xy: -0.0179
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0288
       Mean episode rew_smoothness: -0.0398
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2587
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.64s
                        Total time: 1193.72s
                               ETA: 594 mins 10.1 s

################################################################################
                     Learning iteration 1620/50000                      

                       Computation: 157821 steps/s (collection: 0.500s, learning 0.123s)
               Value function loss: 0.0772
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.11
                Mean reward (task): 4.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0331
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0433
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0527
 Mean episode rew_tracking_lin_vel: 0.2888
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 0.62s
                        Total time: 1194.34s
                               ETA: 594 mins 6.0 s

################################################################################
                     Learning iteration 1621/50000                      

                       Computation: 157173 steps/s (collection: 0.502s, learning 0.124s)
               Value function loss: 0.0737
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0336
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0323
       Mean episode rew_smoothness: -0.0438
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0535
 Mean episode rew_tracking_lin_vel: 0.2786
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.63s
                        Total time: 1194.96s
                               ETA: 594 mins 1.9 s

################################################################################
                     Learning iteration 1622/50000                      

                       Computation: 157617 steps/s (collection: 0.499s, learning 0.125s)
               Value function loss: 0.0819
                    Surrogate loss: 0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.98
                Mean reward (task): 4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0363
        Mean episode rew_lin_vel_z: -0.0209
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0579
 Mean episode rew_tracking_lin_vel: 0.3159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.62s
                        Total time: 1195.59s
                               ETA: 593 mins 57.8 s

################################################################################
                     Learning iteration 1623/50000                      

                       Computation: 145407 steps/s (collection: 0.538s, learning 0.138s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0324
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0503
 Mean episode rew_tracking_lin_vel: 0.2719
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.68s
                        Total time: 1196.26s
                               ETA: 593 mins 55.3 s

################################################################################
                     Learning iteration 1624/50000                      

                       Computation: 130073 steps/s (collection: 0.606s, learning 0.149s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.40
                Mean reward (task): 4.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 160.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0314
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0300
       Mean episode rew_smoothness: -0.0406
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0504
 Mean episode rew_tracking_lin_vel: 0.2683
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.76s
                        Total time: 1197.02s
                               ETA: 593 mins 55.1 s

################################################################################
                     Learning iteration 1625/50000                      

                       Computation: 119032 steps/s (collection: 0.685s, learning 0.141s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.07
                Mean reward (task): 5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 197.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0342
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0524
 Mean episode rew_tracking_lin_vel: 0.2698
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 0.83s
                        Total time: 1197.85s
                               ETA: 593 mins 57.0 s

################################################################################
                     Learning iteration 1626/50000                      

                       Computation: 133469 steps/s (collection: 0.595s, learning 0.142s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.68
                Mean reward (task): 3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0308
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2655
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.74s
                        Total time: 1198.58s
                               ETA: 593 mins 56.3 s

################################################################################
                     Learning iteration 1627/50000                      

                       Computation: 131522 steps/s (collection: 0.607s, learning 0.141s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0281
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0298
       Mean episode rew_smoothness: -0.0412
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2643
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 0.75s
                        Total time: 1199.33s
                               ETA: 593 mins 55.9 s

################################################################################
                     Learning iteration 1628/50000                      

                       Computation: 134847 steps/s (collection: 0.577s, learning 0.152s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.72
                Mean reward (task): 4.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0321
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0297
       Mean episode rew_smoothness: -0.0409
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0512
 Mean episode rew_tracking_lin_vel: 0.2674
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.73s
                        Total time: 1200.06s
                               ETA: 593 mins 54.9 s

################################################################################
                     Learning iteration 1629/50000                      

                       Computation: 149451 steps/s (collection: 0.535s, learning 0.123s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.66
                Mean reward (task): 4.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0333
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0319
       Mean episode rew_smoothness: -0.0434
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2866
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.66s
                        Total time: 1200.72s
                               ETA: 593 mins 51.8 s

################################################################################
                     Learning iteration 1630/50000                      

                       Computation: 148644 steps/s (collection: 0.538s, learning 0.124s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.56
                Mean reward (task): 4.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0346
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0444
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0532
 Mean episode rew_tracking_lin_vel: 0.2854
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 0.66s
                        Total time: 1201.38s
                               ETA: 593 mins 48.8 s

################################################################################
                     Learning iteration 1631/50000                      

                       Computation: 150536 steps/s (collection: 0.529s, learning 0.124s)
               Value function loss: 0.0771
                    Surrogate loss: 0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.92
                Mean reward (task): 3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 158.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0316
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0295
       Mean episode rew_smoothness: -0.0407
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2605
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 0.65s
                        Total time: 1202.03s
                               ETA: 593 mins 45.6 s

################################################################################
                     Learning iteration 1632/50000                      

                       Computation: 157056 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.23
                Mean reward (task): 5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0301
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0395
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2541
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 0.63s
                        Total time: 1202.66s
                               ETA: 593 mins 41.6 s

################################################################################
                     Learning iteration 1633/50000                      

                       Computation: 158379 steps/s (collection: 0.496s, learning 0.125s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.24
                Mean reward (task): 4.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0307
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0294
       Mean episode rew_smoothness: -0.0410
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2617
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.62s
                        Total time: 1203.28s
                               ETA: 593 mins 37.4 s

################################################################################
                     Learning iteration 1634/50000                      

                       Computation: 158059 steps/s (collection: 0.499s, learning 0.123s)
               Value function loss: 0.0775
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.30
                Mean reward (task): 3.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 138.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0423
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2707
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 0.62s
                        Total time: 1203.90s
                               ETA: 593 mins 33.3 s

################################################################################
                     Learning iteration 1635/50000                      

                       Computation: 158608 steps/s (collection: 0.496s, learning 0.124s)
               Value function loss: 0.0794
                    Surrogate loss: 0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.93
                Mean reward (task): 3.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0302
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0345
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0327
       Mean episode rew_smoothness: -0.0445
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0542
 Mean episode rew_tracking_lin_vel: 0.2938
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 0.62s
                        Total time: 1204.52s
                               ETA: 593 mins 29.1 s

################################################################################
                     Learning iteration 1636/50000                      

                       Computation: 159071 steps/s (collection: 0.496s, learning 0.122s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 2.79
                Mean reward (task): 2.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0280
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0297
       Mean episode rew_smoothness: -0.0411
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2493
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 0.62s
                        Total time: 1205.14s
                               ETA: 593 mins 24.9 s

################################################################################
                     Learning iteration 1637/50000                      

                       Computation: 158498 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0802
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.58
                Mean reward (task): 4.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0334
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0443
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0524
 Mean episode rew_tracking_lin_vel: 0.2855
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 0.62s
                        Total time: 1205.76s
                               ETA: 593 mins 20.8 s

################################################################################
                     Learning iteration 1638/50000                      

                       Computation: 159277 steps/s (collection: 0.495s, learning 0.122s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0300
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0486
 Mean episode rew_tracking_lin_vel: 0.2731
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.62s
                        Total time: 1206.37s
                               ETA: 593 mins 16.5 s

################################################################################
                     Learning iteration 1639/50000                      

                       Computation: 154887 steps/s (collection: 0.500s, learning 0.135s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.01
                Mean reward (task): 5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0421
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0506
 Mean episode rew_tracking_lin_vel: 0.2774
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.63s
                        Total time: 1207.01s
                               ETA: 593 mins 12.8 s

################################################################################
                     Learning iteration 1640/50000                      

                       Computation: 139946 steps/s (collection: 0.579s, learning 0.123s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.58
               Mean reward (total): 5.11
                Mean reward (task): 5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 198.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0363
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0340
       Mean episode rew_smoothness: -0.0457
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0536
 Mean episode rew_tracking_lin_vel: 0.2973
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.70s
                        Total time: 1207.71s
                               ETA: 593 mins 11.1 s

################################################################################
                     Learning iteration 1641/50000                      

                       Computation: 150410 steps/s (collection: 0.530s, learning 0.124s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 2.97
                Mean reward (task): 2.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 134.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0269
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0298
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0278
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0442
 Mean episode rew_tracking_lin_vel: 0.2405
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.65s
                        Total time: 1208.37s
                               ETA: 593 mins 7.9 s

################################################################################
                     Learning iteration 1642/50000                      

                       Computation: 145335 steps/s (collection: 0.553s, learning 0.124s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.19
                Mean reward (task): 4.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0271
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0301
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0282
       Mean episode rew_smoothness: -0.0397
          Mean episode rew_torques: -0.0084
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.2611
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 0.68s
                        Total time: 1209.04s
                               ETA: 593 mins 5.4 s

################################################################################
                     Learning iteration 1643/50000                      

                       Computation: 143736 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0719
                    Surrogate loss: 0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.29
                Mean reward (task): 3.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 133.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0281
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0313
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0295
       Mean episode rew_smoothness: -0.0413
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0479
 Mean episode rew_tracking_lin_vel: 0.2596
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.68s
                        Total time: 1209.73s
                               ETA: 593 mins 3.1 s

################################################################################
                     Learning iteration 1644/50000                      

                       Computation: 159076 steps/s (collection: 0.494s, learning 0.124s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.73
                Mean reward (task): 3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0336
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2922
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.62s
                        Total time: 1210.34s
                               ETA: 592 mins 58.9 s

################################################################################
                     Learning iteration 1645/50000                      

                       Computation: 155119 steps/s (collection: 0.511s, learning 0.123s)
               Value function loss: 0.0689
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.16
                Mean reward (task): 4.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0300
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0344
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0441
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2927
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.63s
                        Total time: 1210.98s
                               ETA: 592 mins 55.2 s

################################################################################
                     Learning iteration 1646/50000                      

                       Computation: 160379 steps/s (collection: 0.491s, learning 0.122s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.90
                Mean reward (task): 4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 181.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0311
   Mean episode rew_dof_pos_limits: -0.0316
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0299
       Mean episode rew_smoothness: -0.0410
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0497
 Mean episode rew_tracking_lin_vel: 0.2622
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.61s
                        Total time: 1211.59s
                               ETA: 592 mins 50.9 s

################################################################################
                     Learning iteration 1647/50000                      

                       Computation: 133132 steps/s (collection: 0.604s, learning 0.134s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.88
                Mean reward (task): 3.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0259
       Mean episode rew_ang_vel_xy: -0.0182
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0279
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0266
       Mean episode rew_smoothness: -0.0379
          Mean episode rew_torques: -0.0076
 Mean episode rew_tracking_ang_vel: 0.0436
 Mean episode rew_tracking_lin_vel: 0.2329
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.74s
                        Total time: 1212.33s
                               ETA: 592 mins 50.2 s

################################################################################
                     Learning iteration 1648/50000                      

                       Computation: 157063 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.43
                Mean reward (task): 3.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 141.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0295
       Mean episode rew_smoothness: -0.0408
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0478
 Mean episode rew_tracking_lin_vel: 0.2545
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.63s
                        Total time: 1212.95s
                               ETA: 592 mins 46.3 s

################################################################################
                     Learning iteration 1649/50000                      

                       Computation: 132146 steps/s (collection: 0.603s, learning 0.141s)
               Value function loss: 0.0692
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.73
                Mean reward (task): 3.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0357
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0328
       Mean episode rew_smoothness: -0.0450
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.2889
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.74s
                        Total time: 1213.70s
                               ETA: 592 mins 45.8 s

################################################################################
                     Learning iteration 1650/50000                      

                       Computation: 152054 steps/s (collection: 0.525s, learning 0.122s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.01
                Mean reward (task): 5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0365
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0461
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0562
 Mean episode rew_tracking_lin_vel: 0.3056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.65s
                        Total time: 1214.34s
                               ETA: 592 mins 42.4 s

################################################################################
                     Learning iteration 1651/50000                      

                       Computation: 160891 steps/s (collection: 0.489s, learning 0.122s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.31
                Mean reward (task): 4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0298
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0339
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0315
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2742
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.61s
                        Total time: 1214.96s
                               ETA: 592 mins 38.0 s

################################################################################
                     Learning iteration 1652/50000                      

                       Computation: 147381 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0717
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.00
                Mean reward (task): 4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0283
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0303
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0497
 Mean episode rew_tracking_lin_vel: 0.2585
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.67s
                        Total time: 1215.62s
                               ETA: 592 mins 35.3 s

################################################################################
                     Learning iteration 1653/50000                      

                       Computation: 156702 steps/s (collection: 0.489s, learning 0.138s)
               Value function loss: 0.0709
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.06
                Mean reward (task): 4.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0337
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2889
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 0.63s
                        Total time: 1216.25s
                               ETA: 592 mins 31.4 s

################################################################################
                     Learning iteration 1654/50000                      

                       Computation: 125441 steps/s (collection: 0.640s, learning 0.143s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.65
                Mean reward (task): 4.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0295
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0341
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0435
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0546
 Mean episode rew_tracking_lin_vel: 0.2915
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 0.78s
                        Total time: 1217.03s
                               ETA: 592 mins 32.1 s

################################################################################
                     Learning iteration 1655/50000                      

                       Computation: 149122 steps/s (collection: 0.536s, learning 0.123s)
               Value function loss: 0.0697
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.22
                Mean reward (task): 4.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0284
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0320
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0417
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0498
 Mean episode rew_tracking_lin_vel: 0.2751
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 0.66s
                        Total time: 1217.69s
                               ETA: 592 mins 29.1 s

################################################################################
                     Learning iteration 1656/50000                      

                       Computation: 153412 steps/s (collection: 0.518s, learning 0.123s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.59
                Mean reward (task): 3.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0261
       Mean episode rew_ang_vel_xy: -0.0175
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0289
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0273
       Mean episode rew_smoothness: -0.0380
          Mean episode rew_torques: -0.0080
 Mean episode rew_tracking_ang_vel: 0.0458
 Mean episode rew_tracking_lin_vel: 0.2404
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 0.64s
                        Total time: 1218.33s
                               ETA: 592 mins 25.6 s

################################################################################
                     Learning iteration 1657/50000                      

                       Computation: 145478 steps/s (collection: 0.553s, learning 0.122s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.62
                Mean reward (task): 5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 203.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0326
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0418
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0494
 Mean episode rew_tracking_lin_vel: 0.2714
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.68s
                        Total time: 1219.01s
                               ETA: 592 mins 23.2 s

################################################################################
                     Learning iteration 1658/50000                      

                       Computation: 157178 steps/s (collection: 0.503s, learning 0.123s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.13
                Mean reward (task): 4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0308
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0292
       Mean episode rew_smoothness: -0.0404
          Mean episode rew_torques: -0.0087
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2474
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 0.63s
                        Total time: 1219.63s
                               ETA: 592 mins 19.2 s

################################################################################
                     Learning iteration 1659/50000                      

                       Computation: 148595 steps/s (collection: 0.539s, learning 0.123s)
               Value function loss: 0.0775
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.40
                Mean reward (task): 3.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0426
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0516
 Mean episode rew_tracking_lin_vel: 0.2702
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.66s
                        Total time: 1220.30s
                               ETA: 592 mins 16.4 s

################################################################################
                     Learning iteration 1660/50000                      

                       Computation: 152486 steps/s (collection: 0.508s, learning 0.137s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.82
                Mean reward (task): 3.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0270
       Mean episode rew_ang_vel_xy: -0.0184
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0295
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0280
       Mean episode rew_smoothness: -0.0396
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2447
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 0.64s
                        Total time: 1220.94s
                               ETA: 592 mins 13.0 s

################################################################################
                     Learning iteration 1661/50000                      

                       Computation: 154932 steps/s (collection: 0.511s, learning 0.123s)
               Value function loss: 0.0752
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.22
                Mean reward (task): 5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0350
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0335
       Mean episode rew_smoothness: -0.0455
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0540
 Mean episode rew_tracking_lin_vel: 0.3041
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 0.63s
                        Total time: 1221.58s
                               ETA: 592 mins 9.3 s

################################################################################
                     Learning iteration 1662/50000                      

                       Computation: 155964 steps/s (collection: 0.504s, learning 0.126s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.23
                Mean reward (task): 3.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0307
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0336
       Mean episode rew_smoothness: -0.0451
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0532
 Mean episode rew_tracking_lin_vel: 0.3008
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.63s
                        Total time: 1222.21s
                               ETA: 592 mins 5.5 s

################################################################################
                     Learning iteration 1663/50000                      

                       Computation: 150566 steps/s (collection: 0.503s, learning 0.150s)
               Value function loss: 0.0722
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.17
                Mean reward (task): 5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0337
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0451
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0530
 Mean episode rew_tracking_lin_vel: 0.2903
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 0.65s
                        Total time: 1222.86s
                               ETA: 592 mins 2.4 s

################################################################################
                     Learning iteration 1664/50000                      

                       Computation: 156643 steps/s (collection: 0.504s, learning 0.124s)
               Value function loss: 0.0760
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.81
                Mean reward (task): 3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 154.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0274
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0404
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0494
 Mean episode rew_tracking_lin_vel: 0.2611
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 0.63s
                        Total time: 1223.49s
                               ETA: 591 mins 58.6 s

################################################################################
                     Learning iteration 1665/50000                      

                       Computation: 159223 steps/s (collection: 0.492s, learning 0.126s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.98
                Mean reward (task): 3.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0283
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0300
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0416
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2639
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.62s
                        Total time: 1224.10s
                               ETA: 591 mins 54.4 s

################################################################################
                     Learning iteration 1666/50000                      

                       Computation: 156862 steps/s (collection: 0.505s, learning 0.122s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.46
                Mean reward (task): 4.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0329
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0430
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0511
 Mean episode rew_tracking_lin_vel: 0.2764
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 0.63s
                        Total time: 1224.73s
                               ETA: 591 mins 50.6 s

################################################################################
                     Learning iteration 1667/50000                      

                       Computation: 135441 steps/s (collection: 0.591s, learning 0.135s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.15
                Mean reward (task): 3.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 124.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0267
       Mean episode rew_ang_vel_xy: -0.0171
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0307
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0289
       Mean episode rew_smoothness: -0.0392
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2565
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 0.73s
                        Total time: 1225.46s
                               ETA: 591 mins 49.6 s

################################################################################
                     Learning iteration 1668/50000                      

                       Computation: 158909 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.47
                Mean reward (task): 4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0353
        Mean episode rew_lin_vel_z: -0.0203
           Mean episode rew_no_fly: 0.0340
       Mean episode rew_smoothness: -0.0457
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.2992
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 0.62s
                        Total time: 1226.07s
                               ETA: 591 mins 45.5 s

################################################################################
                     Learning iteration 1669/50000                      

                       Computation: 157702 steps/s (collection: 0.501s, learning 0.123s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.10
                Mean reward (task): 4.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0296
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0404
          Mean episode rew_torques: -0.0083
 Mean episode rew_tracking_ang_vel: 0.0485
 Mean episode rew_tracking_lin_vel: 0.2658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 0.62s
                        Total time: 1226.70s
                               ETA: 591 mins 41.5 s

################################################################################
                     Learning iteration 1670/50000                      

                       Computation: 144693 steps/s (collection: 0.556s, learning 0.123s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.13
                Mean reward (task): 4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0214
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0503
 Mean episode rew_tracking_lin_vel: 0.2747
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 0.68s
                        Total time: 1227.38s
                               ETA: 591 mins 39.2 s

################################################################################
                     Learning iteration 1671/50000                      

                       Computation: 153693 steps/s (collection: 0.511s, learning 0.128s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0424
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0501
 Mean episode rew_tracking_lin_vel: 0.2843
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 0.64s
                        Total time: 1228.02s
                               ETA: 591 mins 35.7 s

################################################################################
                     Learning iteration 1672/50000                      

                       Computation: 138219 steps/s (collection: 0.576s, learning 0.136s)
               Value function loss: 0.0702
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.13
                Mean reward (task): 4.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0303
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0489
 Mean episode rew_tracking_lin_vel: 0.2625
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 0.71s
                        Total time: 1228.73s
                               ETA: 591 mins 34.3 s

################################################################################
                     Learning iteration 1673/50000                      

                       Computation: 154821 steps/s (collection: 0.513s, learning 0.122s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.69
                Mean reward (task): 3.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0319
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0306
       Mean episode rew_smoothness: -0.0422
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0499
 Mean episode rew_tracking_lin_vel: 0.2778
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 0.63s
                        Total time: 1229.36s
                               ETA: 591 mins 30.7 s

################################################################################
                     Learning iteration 1674/50000                      

                       Computation: 152860 steps/s (collection: 0.521s, learning 0.122s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.72
                Mean reward (task): 5.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0348
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0433
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0536
 Mean episode rew_tracking_lin_vel: 0.2894
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 0.64s
                        Total time: 1230.01s
                               ETA: 591 mins 27.3 s

################################################################################
                     Learning iteration 1675/50000                      

                       Computation: 158155 steps/s (collection: 0.497s, learning 0.124s)
               Value function loss: 0.0765
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.91
                Mean reward (task): 4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0326
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0434
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2818
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 0.62s
                        Total time: 1230.63s
                               ETA: 591 mins 23.4 s

################################################################################
                     Learning iteration 1676/50000                      

                       Computation: 155376 steps/s (collection: 0.510s, learning 0.123s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.35
                Mean reward (task): 5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0272
       Mean episode rew_ang_vel_xy: -0.0177
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0292
   Mean episode rew_dof_pos_limits: -0.0317
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0293
       Mean episode rew_smoothness: -0.0399
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0469
 Mean episode rew_tracking_lin_vel: 0.2575
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 0.63s
                        Total time: 1231.26s
                               ETA: 591 mins 19.7 s

################################################################################
                     Learning iteration 1677/50000                      

                       Computation: 151354 steps/s (collection: 0.527s, learning 0.123s)
               Value function loss: 0.0767
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.82
                Mean reward (task): 4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0298
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0338
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0514
 Mean episode rew_tracking_lin_vel: 0.2854
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 0.65s
                        Total time: 1231.91s
                               ETA: 591 mins 16.5 s

################################################################################
                     Learning iteration 1678/50000                      

                       Computation: 146082 steps/s (collection: 0.550s, learning 0.123s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.00
                Mean reward (task): 4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0337
        Mean episode rew_lin_vel_z: -0.0201
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0446
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0517
 Mean episode rew_tracking_lin_vel: 0.2921
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 0.67s
                        Total time: 1232.58s
                               ETA: 591 mins 14.0 s

################################################################################
                     Learning iteration 1679/50000                      

                       Computation: 143633 steps/s (collection: 0.562s, learning 0.122s)
               Value function loss: 0.0724
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.20
                Mean reward (task): 4.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0286
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0324
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0421
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0505
 Mean episode rew_tracking_lin_vel: 0.2721
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 0.68s
                        Total time: 1233.27s
                               ETA: 591 mins 11.9 s

################################################################################
                     Learning iteration 1680/50000                      

                       Computation: 142400 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.19
                Mean reward (task): 6.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 232.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0315
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0465
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.3051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.69s
                        Total time: 1233.96s
                               ETA: 591 mins 9.9 s

################################################################################
                     Learning iteration 1681/50000                      

                       Computation: 156522 steps/s (collection: 0.506s, learning 0.122s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.16
                Mean reward (task): 5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0325
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0387
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0367
       Mean episode rew_smoothness: -0.0486
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0603
 Mean episode rew_tracking_lin_vel: 0.3156
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.63s
                        Total time: 1234.59s
                               ETA: 591 mins 6.1 s

################################################################################
                     Learning iteration 1682/50000                      

                       Computation: 159464 steps/s (collection: 0.494s, learning 0.123s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.68
                Mean reward (task): 3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0335
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0441
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0525
 Mean episode rew_tracking_lin_vel: 0.2789
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.62s
                        Total time: 1235.20s
                               ETA: 591 mins 2.0 s

################################################################################
                     Learning iteration 1683/50000                      

                       Computation: 145967 steps/s (collection: 0.540s, learning 0.133s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 147.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0284
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0315
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0298
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0480
 Mean episode rew_tracking_lin_vel: 0.2623
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.67s
                        Total time: 1235.88s
                               ETA: 590 mins 59.5 s

################################################################################
                     Learning iteration 1684/50000                      

                       Computation: 139297 steps/s (collection: 0.583s, learning 0.123s)
               Value function loss: 0.0747
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.55
                Mean reward (task): 4.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 186.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0352
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0339
       Mean episode rew_smoothness: -0.0462
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0546
 Mean episode rew_tracking_lin_vel: 0.2935
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.71s
                        Total time: 1236.58s
                               ETA: 590 mins 58.0 s

################################################################################
                     Learning iteration 1685/50000                      

                       Computation: 147961 steps/s (collection: 0.542s, learning 0.123s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.95
                Mean reward (task): 3.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0303
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0329
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0445
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0519
 Mean episode rew_tracking_lin_vel: 0.2842
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.66s
                        Total time: 1237.25s
                               ETA: 590 mins 55.2 s

################################################################################
                     Learning iteration 1686/50000                      

                       Computation: 141636 steps/s (collection: 0.556s, learning 0.138s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.54
                Mean reward (task): 5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0500
 Mean episode rew_tracking_lin_vel: 0.2788
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.69s
                        Total time: 1237.94s
                               ETA: 590 mins 53.4 s

################################################################################
                     Learning iteration 1687/50000                      

                       Computation: 136326 steps/s (collection: 0.566s, learning 0.155s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.29
                Mean reward (task): 4.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0308
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0344
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0455
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0538
 Mean episode rew_tracking_lin_vel: 0.3053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.72s
                        Total time: 1238.66s
                               ETA: 590 mins 52.3 s

################################################################################
                     Learning iteration 1688/50000                      

                       Computation: 140805 steps/s (collection: 0.557s, learning 0.141s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.60
                Mean reward (task): 3.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0316
   Mean episode rew_dof_pos_limits: -0.0320
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0431
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0506
 Mean episode rew_tracking_lin_vel: 0.2580
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.70s
                        Total time: 1239.36s
                               ETA: 590 mins 50.5 s

################################################################################
                     Learning iteration 1689/50000                      

                       Computation: 153384 steps/s (collection: 0.500s, learning 0.141s)
               Value function loss: 0.0686
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.51
                Mean reward (task): 4.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0281
       Mean episode rew_ang_vel_xy: -0.0187
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0304
   Mean episode rew_dof_pos_limits: -0.0313
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0299
       Mean episode rew_smoothness: -0.0416
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0475
 Mean episode rew_tracking_lin_vel: 0.2623
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.64s
                        Total time: 1240.00s
                               ETA: 590 mins 47.1 s

################################################################################
                     Learning iteration 1690/50000                      

                       Computation: 140632 steps/s (collection: 0.548s, learning 0.151s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.05
                Mean reward (task): 4.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 157.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0292
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0327
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0311
       Mean episode rew_smoothness: -0.0433
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0504
 Mean episode rew_tracking_lin_vel: 0.2713
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.70s
                        Total time: 1240.70s
                               ETA: 590 mins 45.4 s

################################################################################
                     Learning iteration 1691/50000                      

                       Computation: 145992 steps/s (collection: 0.531s, learning 0.142s)
               Value function loss: 0.0808
                    Surrogate loss: 0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.68
                Mean reward (task): 3.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0341
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0525
 Mean episode rew_tracking_lin_vel: 0.3061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.67s
                        Total time: 1241.37s
                               ETA: 590 mins 42.9 s

################################################################################
                     Learning iteration 1692/50000                      

                       Computation: 157214 steps/s (collection: 0.501s, learning 0.124s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0320
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0348
       Mean episode rew_smoothness: -0.0476
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0564
 Mean episode rew_tracking_lin_vel: 0.3068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 0.63s
                        Total time: 1242.00s
                               ETA: 590 mins 39.1 s

################################################################################
                     Learning iteration 1693/50000                      

                       Computation: 145455 steps/s (collection: 0.553s, learning 0.123s)
               Value function loss: 0.0804
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.52
                Mean reward (task): 5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0295
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0348
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0323
       Mean episode rew_smoothness: -0.0438
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2769
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 0.68s
                        Total time: 1242.67s
                               ETA: 590 mins 36.7 s

################################################################################
                     Learning iteration 1694/50000                      

                       Computation: 154932 steps/s (collection: 0.512s, learning 0.122s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.80
                Mean reward (task): 5.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 211.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0204
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0466
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0563
 Mean episode rew_tracking_lin_vel: 0.3272
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 0.63s
                        Total time: 1243.31s
                               ETA: 590 mins 33.2 s

################################################################################
                     Learning iteration 1695/50000                      

                       Computation: 136842 steps/s (collection: 0.586s, learning 0.133s)
               Value function loss: 0.0710
                    Surrogate loss: 0.0111
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.64
                Mean reward (task): 4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0183
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0301
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0286
       Mean episode rew_smoothness: -0.0407
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0462
 Mean episode rew_tracking_lin_vel: 0.2531
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.72s
                        Total time: 1244.03s
                               ETA: 590 mins 32.0 s

################################################################################
                     Learning iteration 1696/50000                      

                       Computation: 159761 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.10
                Mean reward (task): 3.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 130.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0268
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0294
   Mean episode rew_dof_pos_limits: -0.0293
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0275
       Mean episode rew_smoothness: -0.0392
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0451
 Mean episode rew_tracking_lin_vel: 0.2412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 0.62s
                        Total time: 1244.64s
                               ETA: 590 mins 27.9 s

################################################################################
                     Learning iteration 1697/50000                      

                       Computation: 146008 steps/s (collection: 0.551s, learning 0.122s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.32
                Mean reward (task): 4.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0345
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0331
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0535
 Mean episode rew_tracking_lin_vel: 0.2869
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 0.67s
                        Total time: 1245.32s
                               ETA: 590 mins 25.5 s

################################################################################
                     Learning iteration 1698/50000                      

                       Computation: 152186 steps/s (collection: 0.522s, learning 0.124s)
               Value function loss: 0.0865
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.96
                Mean reward (task): 5.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 205.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0369
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0558
 Mean episode rew_tracking_lin_vel: 0.3071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.65s
                        Total time: 1245.96s
                               ETA: 590 mins 22.3 s

################################################################################
                     Learning iteration 1699/50000                      

                       Computation: 136092 steps/s (collection: 0.584s, learning 0.139s)
               Value function loss: 0.0742
                    Surrogate loss: 0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.92
                Mean reward (task): 5.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0332
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0380
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0364
       Mean episode rew_smoothness: -0.0492
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0589
 Mean episode rew_tracking_lin_vel: 0.3275
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.72s
                        Total time: 1246.68s
                               ETA: 590 mins 21.2 s

################################################################################
                     Learning iteration 1700/50000                      

                       Computation: 154947 steps/s (collection: 0.494s, learning 0.141s)
               Value function loss: 0.0828
                    Surrogate loss: 0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.44
                Mean reward (task): 4.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0357
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0341
       Mean episode rew_smoothness: -0.0463
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0535
 Mean episode rew_tracking_lin_vel: 0.3102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.63s
                        Total time: 1247.32s
                               ETA: 590 mins 17.7 s

################################################################################
                     Learning iteration 1701/50000                      

                       Computation: 135629 steps/s (collection: 0.584s, learning 0.141s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.56
                Mean reward (task): 4.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 186.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0323
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2728
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.72s
                        Total time: 1248.04s
                               ETA: 590 mins 16.7 s

################################################################################
                     Learning iteration 1702/50000                      

                       Computation: 147617 steps/s (collection: 0.525s, learning 0.141s)
               Value function loss: 0.0781
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.67
                Mean reward (task): 3.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0288
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0325
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0493
 Mean episode rew_tracking_lin_vel: 0.2665
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.67s
                        Total time: 1248.71s
                               ETA: 590 mins 14.1 s

################################################################################
                     Learning iteration 1703/50000                      

                       Computation: 145646 steps/s (collection: 0.534s, learning 0.141s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.49
                Mean reward (task): 3.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 135.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0354
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0340
       Mean episode rew_smoothness: -0.0462
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.3164
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.67s
                        Total time: 1249.38s
                               ETA: 590 mins 11.7 s

################################################################################
                     Learning iteration 1704/50000                      

                       Computation: 130360 steps/s (collection: 0.609s, learning 0.145s)
               Value function loss: 0.0715
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.83
                Mean reward (task): 5.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 212.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0373
   Mean episode rew_dof_pos_limits: -0.0362
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0349
       Mean episode rew_smoothness: -0.0479
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0576
 Mean episode rew_tracking_lin_vel: 0.3163
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 0.75s
                        Total time: 1250.14s
                               ETA: 590 mins 11.5 s

################################################################################
                     Learning iteration 1705/50000                      

                       Computation: 135016 steps/s (collection: 0.606s, learning 0.122s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.34
                Mean reward (task): 5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0319
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2933
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 0.73s
                        Total time: 1250.87s
                               ETA: 590 mins 10.6 s

################################################################################
                     Learning iteration 1706/50000                      

                       Computation: 153875 steps/s (collection: 0.516s, learning 0.123s)
               Value function loss: 0.0750
                    Surrogate loss: 0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.49
                Mean reward (task): 4.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0361
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0456
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2970
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.64s
                        Total time: 1251.50s
                               ETA: 590 mins 7.2 s

################################################################################
                     Learning iteration 1707/50000                      

                       Computation: 159134 steps/s (collection: 0.495s, learning 0.123s)
               Value function loss: 0.0827
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.19
                Mean reward (task): 5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 193.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0293
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0318
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0301
       Mean episode rew_smoothness: -0.0425
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0470
 Mean episode rew_tracking_lin_vel: 0.2710
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.62s
                        Total time: 1252.12s
                               ETA: 590 mins 3.2 s

################################################################################
                     Learning iteration 1708/50000                      

                       Computation: 149217 steps/s (collection: 0.536s, learning 0.123s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.05
                Mean reward (task): 5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0328
        Mean episode rew_lin_vel_z: -0.0247
           Mean episode rew_no_fly: 0.0317
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2924
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.66s
                        Total time: 1252.78s
                               ETA: 590 mins 0.4 s

################################################################################
                     Learning iteration 1709/50000                      

                       Computation: 136543 steps/s (collection: 0.592s, learning 0.128s)
               Value function loss: 0.0696
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.00
                Mean reward (task): 6.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 213.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0331
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0379
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0362
       Mean episode rew_smoothness: -0.0488
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0595
 Mean episode rew_tracking_lin_vel: 0.3341
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.72s
                        Total time: 1253.50s
                               ETA: 589 mins 59.3 s

################################################################################
                     Learning iteration 1710/50000                      

                       Computation: 152764 steps/s (collection: 0.522s, learning 0.122s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.48
                Mean reward (task): 4.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0332
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0382
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0366
       Mean episode rew_smoothness: -0.0491
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0601
 Mean episode rew_tracking_lin_vel: 0.3263
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.64s
                        Total time: 1254.14s
                               ETA: 589 mins 56.0 s

################################################################################
                     Learning iteration 1711/50000                      

                       Computation: 131357 steps/s (collection: 0.612s, learning 0.136s)
               Value function loss: 0.0724
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.91
                Mean reward (task): 3.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0307
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0353
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0331
       Mean episode rew_smoothness: -0.0449
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2908
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.75s
                        Total time: 1254.89s
                               ETA: 589 mins 55.8 s

################################################################################
                     Learning iteration 1712/50000                      

                       Computation: 149663 steps/s (collection: 0.530s, learning 0.127s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 2.96
                Mean reward (task): 2.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 132.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0285
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0311
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0296
       Mean episode rew_smoothness: -0.0414
          Mean episode rew_torques: -0.0091
 Mean episode rew_tracking_ang_vel: 0.0473
 Mean episode rew_tracking_lin_vel: 0.2637
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.66s
                        Total time: 1255.55s
                               ETA: 589 mins 52.9 s

################################################################################
                     Learning iteration 1713/50000                      

                       Computation: 154012 steps/s (collection: 0.503s, learning 0.136s)
               Value function loss: 0.0784
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0188
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0322
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0425
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0478
 Mean episode rew_tracking_lin_vel: 0.2816
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.64s
                        Total time: 1256.19s
                               ETA: 589 mins 49.5 s

################################################################################
                     Learning iteration 1714/50000                      

                       Computation: 154662 steps/s (collection: 0.496s, learning 0.140s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.64
                Mean reward (task): 4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0350
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0336
       Mean episode rew_smoothness: -0.0465
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0546
 Mean episode rew_tracking_lin_vel: 0.3003
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.64s
                        Total time: 1256.82s
                               ETA: 589 mins 46.0 s

################################################################################
                     Learning iteration 1715/50000                      

                       Computation: 142247 steps/s (collection: 0.550s, learning 0.141s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.37
                Mean reward (task): 4.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 163.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0502
 Mean episode rew_tracking_lin_vel: 0.2732
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.69s
                        Total time: 1257.51s
                               ETA: 589 mins 44.1 s

################################################################################
                     Learning iteration 1716/50000                      

                       Computation: 155505 steps/s (collection: 0.494s, learning 0.138s)
               Value function loss: 0.0710
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.98
                Mean reward (task): 5.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0309
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0357
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0336
       Mean episode rew_smoothness: -0.0456
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0550
 Mean episode rew_tracking_lin_vel: 0.2979
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.63s
                        Total time: 1258.15s
                               ETA: 589 mins 40.5 s

################################################################################
                     Learning iteration 1717/50000                      

                       Computation: 147713 steps/s (collection: 0.526s, learning 0.140s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.38
                Mean reward (task): 5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0359
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0341
       Mean episode rew_smoothness: -0.0464
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0573
 Mean episode rew_tracking_lin_vel: 0.3102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.67s
                        Total time: 1258.81s
                               ETA: 589 mins 37.9 s

################################################################################
                     Learning iteration 1718/50000                      

                       Computation: 154832 steps/s (collection: 0.512s, learning 0.123s)
               Value function loss: 0.0734
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.61
                Mean reward (task): 3.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 145.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0289
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0326
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0424
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0467
 Mean episode rew_tracking_lin_vel: 0.2731
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.63s
                        Total time: 1259.45s
                               ETA: 589 mins 34.4 s

################################################################################
                     Learning iteration 1719/50000                      

                       Computation: 160631 steps/s (collection: 0.489s, learning 0.123s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.15
                Mean reward (task): 5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 199.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0300
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0330
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0320
       Mean episode rew_smoothness: -0.0440
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2877
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.61s
                        Total time: 1260.06s
                               ETA: 589 mins 30.3 s

################################################################################
                     Learning iteration 1720/50000                      

                       Computation: 136424 steps/s (collection: 0.582s, learning 0.138s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.92
                Mean reward (task): 4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0193
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0353
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0331
       Mean episode rew_smoothness: -0.0449
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.2978
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.72s
                        Total time: 1260.78s
                               ETA: 589 mins 29.2 s

################################################################################
                     Learning iteration 1721/50000                      

                       Computation: 162241 steps/s (collection: 0.484s, learning 0.122s)
               Value function loss: 0.0754
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.98
                Mean reward (task): 4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0314
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0461
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.3007
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.61s
                        Total time: 1261.39s
                               ETA: 589 mins 24.9 s

################################################################################
                     Learning iteration 1722/50000                      

                       Computation: 152389 steps/s (collection: 0.523s, learning 0.122s)
               Value function loss: 0.0718
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.24
                Mean reward (task): 5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0348
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0329
       Mean episode rew_smoothness: -0.0451
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0525
 Mean episode rew_tracking_lin_vel: 0.3041
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.65s
                        Total time: 1262.03s
                               ETA: 589 mins 21.8 s

################################################################################
                     Learning iteration 1723/50000                      

                       Computation: 160968 steps/s (collection: 0.488s, learning 0.122s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0195
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0354
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0334
       Mean episode rew_smoothness: -0.0448
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.3089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.61s
                        Total time: 1262.64s
                               ETA: 589 mins 17.6 s

################################################################################
                     Learning iteration 1724/50000                      

                       Computation: 149826 steps/s (collection: 0.534s, learning 0.122s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.50
                Mean reward (task): 3.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0291
       Mean episode rew_ang_vel_xy: -0.0189
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0327
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0311
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2735
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.66s
                        Total time: 1263.30s
                               ETA: 589 mins 14.8 s

################################################################################
                     Learning iteration 1725/50000                      

                       Computation: 160559 steps/s (collection: 0.491s, learning 0.121s)
               Value function loss: 0.0772
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.70
                Mean reward (task): 3.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0287
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0306
   Mean episode rew_dof_pos_limits: -0.0318
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0423
          Mean episode rew_torques: -0.0089
 Mean episode rew_tracking_ang_vel: 0.0507
 Mean episode rew_tracking_lin_vel: 0.2701
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.61s
                        Total time: 1263.91s
                               ETA: 589 mins 10.7 s

################################################################################
                     Learning iteration 1726/50000                      

                       Computation: 154779 steps/s (collection: 0.514s, learning 0.121s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.31
                Mean reward (task): 4.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0325
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0353
       Mean episode rew_smoothness: -0.0482
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0571
 Mean episode rew_tracking_lin_vel: 0.3211
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.64s
                        Total time: 1264.55s
                               ETA: 589 mins 7.2 s

################################################################################
                     Learning iteration 1727/50000                      

                       Computation: 149402 steps/s (collection: 0.526s, learning 0.132s)
               Value function loss: 0.0739
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.42
                Mean reward (task): 3.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 136.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0300
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0347
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2899
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.66s
                        Total time: 1265.20s
                               ETA: 589 mins 4.4 s

################################################################################
                     Learning iteration 1728/50000                      

                       Computation: 157810 steps/s (collection: 0.500s, learning 0.123s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.87
                Mean reward (task): 5.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 202.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0352
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0337
       Mean episode rew_smoothness: -0.0465
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0552
 Mean episode rew_tracking_lin_vel: 0.3051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.62s
                        Total time: 1265.83s
                               ETA: 589 mins 0.6 s

################################################################################
                     Learning iteration 1729/50000                      

                       Computation: 156277 steps/s (collection: 0.506s, learning 0.123s)
               Value function loss: 0.0791
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.17
                Mean reward (task): 3.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 129.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0285
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0302
   Mean episode rew_dof_pos_limits: -0.0326
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0302
       Mean episode rew_smoothness: -0.0416
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0487
 Mean episode rew_tracking_lin_vel: 0.2658
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.63s
                        Total time: 1266.45s
                               ETA: 588 mins 57.0 s

################################################################################
                     Learning iteration 1730/50000                      

                       Computation: 150454 steps/s (collection: 0.530s, learning 0.123s)
               Value function loss: 0.0758
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.27
                Mean reward (task): 5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0296
       Mean episode rew_ang_vel_xy: -0.0185
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0342
        Mean episode rew_lin_vel_z: -0.0200
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0440
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0529
 Mean episode rew_tracking_lin_vel: 0.2968
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.65s
                        Total time: 1267.11s
                               ETA: 588 mins 54.1 s

################################################################################
                     Learning iteration 1731/50000                      

                       Computation: 143831 steps/s (collection: 0.558s, learning 0.125s)
               Value function loss: 0.0702
                    Surrogate loss: 0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.77
                Mean reward (task): 4.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0347
   Mean episode rew_dof_pos_limits: -0.0353
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0341
       Mean episode rew_smoothness: -0.0472
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0533
 Mean episode rew_tracking_lin_vel: 0.3143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.68s
                        Total time: 1267.79s
                               ETA: 588 mins 52.0 s

################################################################################
                     Learning iteration 1732/50000                      

                       Computation: 142493 steps/s (collection: 0.545s, learning 0.145s)
               Value function loss: 0.0761
                    Surrogate loss: 0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.85
                Mean reward (task): 4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0190
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0307
       Mean episode rew_smoothness: -0.0427
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0486
 Mean episode rew_tracking_lin_vel: 0.2749
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.69s
                        Total time: 1268.48s
                               ETA: 588 mins 50.1 s

################################################################################
                     Learning iteration 1733/50000                      

                       Computation: 154851 steps/s (collection: 0.511s, learning 0.124s)
               Value function loss: 0.0835
                    Surrogate loss: 0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 7.13
                Mean reward (task): 7.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 239.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0330
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0387
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0365
       Mean episode rew_smoothness: -0.0489
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0594
 Mean episode rew_tracking_lin_vel: 0.3374
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.63s
                        Total time: 1269.12s
                               ETA: 588 mins 46.7 s

################################################################################
                     Learning iteration 1734/50000                      

                       Computation: 158491 steps/s (collection: 0.497s, learning 0.123s)
               Value function loss: 0.0792
                    Surrogate loss: 0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.46
                Mean reward (task): 6.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 207.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0318
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0357
       Mean episode rew_smoothness: -0.0480
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0564
 Mean episode rew_tracking_lin_vel: 0.3364
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.62s
                        Total time: 1269.74s
                               ETA: 588 mins 42.8 s

################################################################################
                     Learning iteration 1735/50000                      

                       Computation: 140709 steps/s (collection: 0.576s, learning 0.122s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.60
                Mean reward (task): 5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 199.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0352
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0335
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0525
 Mean episode rew_tracking_lin_vel: 0.3055
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.70s
                        Total time: 1270.44s
                               ETA: 588 mins 41.2 s

################################################################################
                     Learning iteration 1736/50000                      

                       Computation: 148449 steps/s (collection: 0.522s, learning 0.140s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.24
                Mean reward (task): 3.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 139.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0276
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0283
       Mean episode rew_smoothness: -0.0404
          Mean episode rew_torques: -0.0088
 Mean episode rew_tracking_ang_vel: 0.0459
 Mean episode rew_tracking_lin_vel: 0.2534
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.66s
                        Total time: 1271.10s
                               ETA: 588 mins 38.5 s

################################################################################
                     Learning iteration 1737/50000                      

                       Computation: 126213 steps/s (collection: 0.611s, learning 0.168s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 137.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0277
       Mean episode rew_ang_vel_xy: -0.0186
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0305
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0287
       Mean episode rew_smoothness: -0.0407
          Mean episode rew_torques: -0.0086
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.2604
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.78s
                        Total time: 1271.88s
                               ETA: 588 mins 39.1 s

################################################################################
                     Learning iteration 1738/50000                      

                       Computation: 145840 steps/s (collection: 0.534s, learning 0.140s)
               Value function loss: 0.0746
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.71
                Mean reward (task): 3.71
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 144.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0328
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0309
       Mean episode rew_smoothness: -0.0429
          Mean episode rew_torques: -0.0092
 Mean episode rew_tracking_ang_vel: 0.0516
 Mean episode rew_tracking_lin_vel: 0.2786
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.67s
                        Total time: 1272.55s
                               ETA: 588 mins 36.8 s

################################################################################
                     Learning iteration 1739/50000                      

                       Computation: 126261 steps/s (collection: 0.641s, learning 0.138s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.57
                Mean reward (task): 4.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 161.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0194
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0350
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0458
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0533
 Mean episode rew_tracking_lin_vel: 0.3118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.78s
                        Total time: 1273.33s
                               ETA: 588 mins 37.3 s

################################################################################
                     Learning iteration 1740/50000                      

                       Computation: 140371 steps/s (collection: 0.555s, learning 0.145s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.83
                Mean reward (task): 5.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0362
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0475
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0552
 Mean episode rew_tracking_lin_vel: 0.3163
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.70s
                        Total time: 1274.03s
                               ETA: 588 mins 35.7 s

################################################################################
                     Learning iteration 1741/50000                      

                       Computation: 154019 steps/s (collection: 0.499s, learning 0.140s)
               Value function loss: 0.0854
                    Surrogate loss: 0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.74
                Mean reward (task): 5.74
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0346
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0408
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0387
       Mean episode rew_smoothness: -0.0513
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0623
 Mean episode rew_tracking_lin_vel: 0.3559
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.64s
                        Total time: 1274.67s
                               ETA: 588 mins 32.4 s

################################################################################
                     Learning iteration 1742/50000                      

                       Computation: 131229 steps/s (collection: 0.607s, learning 0.142s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.26
                Mean reward (task): 4.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0324
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0349
       Mean episode rew_smoothness: -0.0476
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0559
 Mean episode rew_tracking_lin_vel: 0.3177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.75s
                        Total time: 1275.42s
                               ETA: 588 mins 32.1 s

################################################################################
                     Learning iteration 1743/50000                      

                       Computation: 135921 steps/s (collection: 0.601s, learning 0.123s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.36
                Mean reward (task): 6.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0326
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0348
       Mean episode rew_smoothness: -0.0476
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0588
 Mean episode rew_tracking_lin_vel: 0.3211
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.72s
                        Total time: 1276.14s
                               ETA: 588 mins 31.2 s

################################################################################
                     Learning iteration 1744/50000                      

                       Computation: 151534 steps/s (collection: 0.522s, learning 0.126s)
               Value function loss: 0.0692
                    Surrogate loss: 0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.47
                Mean reward (task): 4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0324
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0531
 Mean episode rew_tracking_lin_vel: 0.2831
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.65s
                        Total time: 1276.79s
                               ETA: 588 mins 28.1 s

################################################################################
                     Learning iteration 1745/50000                      

                       Computation: 159829 steps/s (collection: 0.492s, learning 0.123s)
               Value function loss: 0.0735
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.86
                Mean reward (task): 3.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0351
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0332
       Mean episode rew_smoothness: -0.0458
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0541
 Mean episode rew_tracking_lin_vel: 0.2998
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.62s
                        Total time: 1277.40s
                               ETA: 588 mins 24.2 s

################################################################################
                     Learning iteration 1746/50000                      

                       Computation: 151640 steps/s (collection: 0.514s, learning 0.134s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.94
                Mean reward (task): 3.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0311
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0354
        Mean episode rew_lin_vel_z: -0.0268
           Mean episode rew_no_fly: 0.0335
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0550
 Mean episode rew_tracking_lin_vel: 0.3071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.65s
                        Total time: 1278.05s
                               ETA: 588 mins 21.2 s

################################################################################
                     Learning iteration 1747/50000                      

                       Computation: 147245 steps/s (collection: 0.540s, learning 0.127s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.20
                Mean reward (task): 4.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0338
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0315
       Mean episode rew_smoothness: -0.0435
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0530
 Mean episode rew_tracking_lin_vel: 0.2913
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.67s
                        Total time: 1278.72s
                               ETA: 588 mins 18.7 s

################################################################################
                     Learning iteration 1748/50000                      

                       Computation: 139935 steps/s (collection: 0.573s, learning 0.130s)
               Value function loss: 0.0701
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.37
                Mean reward (task): 3.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0294
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0335
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0310
       Mean episode rew_smoothness: -0.0432
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0495
 Mean episode rew_tracking_lin_vel: 0.2705
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.70s
                        Total time: 1279.42s
                               ETA: 588 mins 17.1 s

################################################################################
                     Learning iteration 1749/50000                      

                       Computation: 151089 steps/s (collection: 0.527s, learning 0.124s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.66
                Mean reward (task): 5.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0336
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0382
        Mean episode rew_lin_vel_z: -0.0269
           Mean episode rew_no_fly: 0.0365
       Mean episode rew_smoothness: -0.0493
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0584
 Mean episode rew_tracking_lin_vel: 0.3381
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.65s
                        Total time: 1280.07s
                               ETA: 588 mins 14.2 s

################################################################################
                     Learning iteration 1750/50000                      

                       Computation: 148556 steps/s (collection: 0.538s, learning 0.124s)
               Value function loss: 0.0760
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.55
                Mean reward (task): 5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 221.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0381
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0356
       Mean episode rew_smoothness: -0.0481
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.3092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.66s
                        Total time: 1280.73s
                               ETA: 588 mins 11.5 s

################################################################################
                     Learning iteration 1751/50000                      

                       Computation: 161376 steps/s (collection: 0.487s, learning 0.122s)
               Value function loss: 0.0807
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.21
                Mean reward (task): 5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0342
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0404
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0377
       Mean episode rew_smoothness: -0.0504
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0616
 Mean episode rew_tracking_lin_vel: 0.3353
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.61s
                        Total time: 1281.34s
                               ETA: 588 mins 7.4 s

################################################################################
                     Learning iteration 1752/50000                      

                       Computation: 156394 steps/s (collection: 0.505s, learning 0.123s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.21
                Mean reward (task): 4.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 162.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0365
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0344
       Mean episode rew_smoothness: -0.0464
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.3020
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.63s
                        Total time: 1281.97s
                               ETA: 588 mins 3.9 s

################################################################################
                     Learning iteration 1753/50000                      

                       Computation: 157100 steps/s (collection: 0.502s, learning 0.124s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.96
                Mean reward (task): 3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 150.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0297
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0332
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0432
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0509
 Mean episode rew_tracking_lin_vel: 0.2821
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.63s
                        Total time: 1282.60s
                               ETA: 588 mins 0.2 s

################################################################################
                     Learning iteration 1754/50000                      

                       Computation: 160543 steps/s (collection: 0.490s, learning 0.122s)
               Value function loss: 0.0784
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.81
                Mean reward (task): 3.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0343
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0447
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0522
 Mean episode rew_tracking_lin_vel: 0.2955
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 0.61s
                        Total time: 1283.21s
                               ETA: 587 mins 56.2 s

################################################################################
                     Learning iteration 1755/50000                      

                       Computation: 161128 steps/s (collection: 0.484s, learning 0.126s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.52
                Mean reward (task): 4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0329
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0305
       Mean episode rew_smoothness: -0.0432
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0497
 Mean episode rew_tracking_lin_vel: 0.2713
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 0.61s
                        Total time: 1283.82s
                               ETA: 587 mins 52.2 s

################################################################################
                     Learning iteration 1756/50000                      

                       Computation: 138093 steps/s (collection: 0.570s, learning 0.142s)
               Value function loss: 0.0777
                    Surrogate loss: 0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.29
                Mean reward (task): 6.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 219.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0319
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0372
        Mean episode rew_lin_vel_z: -0.0249
           Mean episode rew_no_fly: 0.0348
       Mean episode rew_smoothness: -0.0475
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0571
 Mean episode rew_tracking_lin_vel: 0.3250
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.71s
                        Total time: 1284.53s
                               ETA: 587 mins 50.9 s

################################################################################
                     Learning iteration 1757/50000                      

                       Computation: 159607 steps/s (collection: 0.492s, learning 0.124s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.17
                Mean reward (task): 4.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0343
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0322
       Mean episode rew_smoothness: -0.0450
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0526
 Mean episode rew_tracking_lin_vel: 0.2822
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.62s
                        Total time: 1285.15s
                               ETA: 587 mins 47.0 s

################################################################################
                     Learning iteration 1758/50000                      

                       Computation: 129974 steps/s (collection: 0.623s, learning 0.133s)
               Value function loss: 0.0764
                    Surrogate loss: 0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.77
                Mean reward (task): 4.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 182.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0343
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0400
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0379
       Mean episode rew_smoothness: -0.0503
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0598
 Mean episode rew_tracking_lin_vel: 0.3488
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.76s
                        Total time: 1285.90s
                               ETA: 587 mins 47.0 s

################################################################################
                     Learning iteration 1759/50000                      

                       Computation: 155556 steps/s (collection: 0.510s, learning 0.122s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.03
                Mean reward (task): 6.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0334
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0387
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0364
       Mean episode rew_smoothness: -0.0491
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0580
 Mean episode rew_tracking_lin_vel: 0.3323
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.63s
                        Total time: 1286.54s
                               ETA: 587 mins 43.5 s

################################################################################
                     Learning iteration 1760/50000                      

                       Computation: 160752 steps/s (collection: 0.489s, learning 0.122s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.31
                Mean reward (task): 5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0334
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0560
 Mean episode rew_tracking_lin_vel: 0.3040
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.61s
                        Total time: 1287.15s
                               ETA: 587 mins 39.5 s

################################################################################
                     Learning iteration 1761/50000                      

                       Computation: 128596 steps/s (collection: 0.632s, learning 0.133s)
               Value function loss: 0.0717
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.31
                Mean reward (task): 3.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0343
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0556
 Mean episode rew_tracking_lin_vel: 0.3083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.76s
                        Total time: 1287.91s
                               ETA: 587 mins 39.7 s

################################################################################
                     Learning iteration 1762/50000                      

                       Computation: 134023 steps/s (collection: 0.612s, learning 0.121s)
               Value function loss: 0.0735
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.57
                Mean reward (task): 5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 195.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0228
           Mean episode rew_no_fly: 0.0345
       Mean episode rew_smoothness: -0.0468
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0557
 Mean episode rew_tracking_lin_vel: 0.3168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.73s
                        Total time: 1288.65s
                               ETA: 587 mins 39.0 s

################################################################################
                     Learning iteration 1763/50000                      

                       Computation: 130743 steps/s (collection: 0.617s, learning 0.135s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.82
                Mean reward (task): 4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0349
   Mean episode rew_dof_pos_limits: -0.0358
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.3058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.75s
                        Total time: 1289.40s
                               ETA: 587 mins 38.9 s

################################################################################
                     Learning iteration 1764/50000                      

                       Computation: 153244 steps/s (collection: 0.521s, learning 0.121s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.79
                Mean reward (task): 5.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 208.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0386
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0356
       Mean episode rew_smoothness: -0.0475
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0560
 Mean episode rew_tracking_lin_vel: 0.3202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.64s
                        Total time: 1290.04s
                               ETA: 587 mins 35.7 s

################################################################################
                     Learning iteration 1765/50000                      

                       Computation: 140286 steps/s (collection: 0.551s, learning 0.149s)
               Value function loss: 0.0810
                    Surrogate loss: 0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.06
                Mean reward (task): 5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0295
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0343
        Mean episode rew_lin_vel_z: -0.0205
           Mean episode rew_no_fly: 0.0312
       Mean episode rew_smoothness: -0.0428
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0513
 Mean episode rew_tracking_lin_vel: 0.2793
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 0.70s
                        Total time: 1290.74s
                               ETA: 587 mins 34.2 s

################################################################################
                     Learning iteration 1766/50000                      

                       Computation: 152423 steps/s (collection: 0.498s, learning 0.147s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.92
                Mean reward (task): 4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0297
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0333
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0308
       Mean episode rew_smoothness: -0.0432
          Mean episode rew_torques: -0.0096
 Mean episode rew_tracking_ang_vel: 0.0484
 Mean episode rew_tracking_lin_vel: 0.2737
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 0.64s
                        Total time: 1291.38s
                               ETA: 587 mins 31.1 s

################################################################################
                     Learning iteration 1767/50000                      

                       Computation: 150579 steps/s (collection: 0.520s, learning 0.133s)
               Value function loss: 0.0788
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.38
                Mean reward (task): 5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 216.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0344
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0497
 Mean episode rew_tracking_lin_vel: 0.2742
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.65s
                        Total time: 1292.04s
                               ETA: 587 mins 28.2 s

################################################################################
                     Learning iteration 1768/50000                      

                       Computation: 131310 steps/s (collection: 0.606s, learning 0.143s)
               Value function loss: 0.0748
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.92
                Mean reward (task): 4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 197.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0377
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0332
       Mean episode rew_smoothness: -0.0464
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0539
 Mean episode rew_tracking_lin_vel: 0.2966
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.75s
                        Total time: 1292.79s
                               ETA: 587 mins 28.0 s

################################################################################
                     Learning iteration 1769/50000                      

                       Computation: 140220 steps/s (collection: 0.556s, learning 0.145s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.76
                Mean reward (task): 5.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 199.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0343
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0408
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0374
       Mean episode rew_smoothness: -0.0500
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0597
 Mean episode rew_tracking_lin_vel: 0.3526
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.70s
                        Total time: 1293.49s
                               ETA: 587 mins 26.4 s

################################################################################
                     Learning iteration 1770/50000                      

                       Computation: 150078 steps/s (collection: 0.531s, learning 0.124s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.12
                Mean reward (task): 5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 181.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0303
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0346
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0318
       Mean episode rew_smoothness: -0.0442
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2854
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 0.66s
                        Total time: 1294.14s
                               ETA: 587 mins 23.6 s

################################################################################
                     Learning iteration 1771/50000                      

                       Computation: 134560 steps/s (collection: 0.597s, learning 0.134s)
               Value function loss: 0.0787
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.20
                Mean reward (task): 6.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0381
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0359
       Mean episode rew_smoothness: -0.0486
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0600
 Mean episode rew_tracking_lin_vel: 0.3237
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 0.73s
                        Total time: 1294.87s
                               ETA: 587 mins 22.9 s

################################################################################
                     Learning iteration 1772/50000                      

                       Computation: 143623 steps/s (collection: 0.550s, learning 0.134s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0013
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.75
                Mean reward (task): 3.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0319
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0333
       Mean episode rew_smoothness: -0.0457
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0546
 Mean episode rew_tracking_lin_vel: 0.2900
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 0.68s
                        Total time: 1295.56s
                               ETA: 587 mins 20.9 s

################################################################################
                     Learning iteration 1773/50000                      

                       Computation: 128157 steps/s (collection: 0.618s, learning 0.149s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.60
                Mean reward (task): 4.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0351
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0323
       Mean episode rew_smoothness: -0.0438
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0530
 Mean episode rew_tracking_lin_vel: 0.2858
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 0.77s
                        Total time: 1296.32s
                               ETA: 587 mins 21.2 s

################################################################################
                     Learning iteration 1774/50000                      

                       Computation: 131130 steps/s (collection: 0.620s, learning 0.130s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.00
                Mean reward (task): 4.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 159.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0319
   Mean episode rew_dof_pos_limits: -0.0341
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0315
       Mean episode rew_smoothness: -0.0435
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0515
 Mean episode rew_tracking_lin_vel: 0.2785
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 0.75s
                        Total time: 1297.07s
                               ETA: 587 mins 21.0 s

################################################################################
                     Learning iteration 1775/50000                      

                       Computation: 153712 steps/s (collection: 0.512s, learning 0.127s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.29
                Mean reward (task): 6.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 216.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0314
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0362
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0334
       Mean episode rew_smoothness: -0.0455
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0551
 Mean episode rew_tracking_lin_vel: 0.2896
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.64s
                        Total time: 1297.71s
                               ETA: 587 mins 17.8 s

################################################################################
                     Learning iteration 1776/50000                      

                       Computation: 154748 steps/s (collection: 0.510s, learning 0.125s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.10
                Mean reward (task): 4.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0301
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0297
   Mean episode rew_dof_pos_limits: -0.0354
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0326
       Mean episode rew_smoothness: -0.0440
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0534
 Mean episode rew_tracking_lin_vel: 0.2848
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.64s
                        Total time: 1298.35s
                               ETA: 587 mins 14.4 s

################################################################################
                     Learning iteration 1777/50000                      

                       Computation: 155389 steps/s (collection: 0.509s, learning 0.124s)
               Value function loss: 0.0796
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.54
                Mean reward (task): 5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0196
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0339
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0439
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0495
 Mean episode rew_tracking_lin_vel: 0.2754
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.63s
                        Total time: 1298.98s
                               ETA: 587 mins 11.0 s

################################################################################
                     Learning iteration 1778/50000                      

                       Computation: 156875 steps/s (collection: 0.504s, learning 0.123s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.75
                Mean reward (task): 4.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0323
   Mean episode rew_dof_pos_limits: -0.0360
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0334
       Mean episode rew_smoothness: -0.0455
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0533
 Mean episode rew_tracking_lin_vel: 0.3110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.63s
                        Total time: 1299.61s
                               ETA: 587 mins 7.5 s

################################################################################
                     Learning iteration 1779/50000                      

                       Computation: 147402 steps/s (collection: 0.532s, learning 0.135s)
               Value function loss: 0.0742
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.04
                Mean reward (task): 6.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 208.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0299
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0303
   Mean episode rew_dof_pos_limits: -0.0346
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0325
       Mean episode rew_smoothness: -0.0436
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0542
 Mean episode rew_tracking_lin_vel: 0.2882
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.67s
                        Total time: 1300.28s
                               ETA: 587 mins 5.0 s

################################################################################
                     Learning iteration 1780/50000                      

                       Computation: 36696 steps/s (collection: 2.535s, learning 0.144s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.66
                Mean reward (task): 4.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0309
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0344
       Mean episode rew_smoothness: -0.0459
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0563
 Mean episode rew_tracking_lin_vel: 0.3135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.68s
                        Total time: 1302.95s
                               ETA: 587 mins 57.1 s

################################################################################
                     Learning iteration 1781/50000                      

                       Computation: 129968 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.0828
                    Surrogate loss: 0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.52
                Mean reward (task): 4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0461
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0556
 Mean episode rew_tracking_lin_vel: 0.3010
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.76s
                        Total time: 1303.71s
                               ETA: 587 mins 57.0 s

################################################################################
                     Learning iteration 1782/50000                      

                       Computation: 127684 steps/s (collection: 0.630s, learning 0.140s)
               Value function loss: 0.0779
                    Surrogate loss: 0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.08
                Mean reward (task): 6.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0372
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0480
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0570
 Mean episode rew_tracking_lin_vel: 0.3207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.77s
                        Total time: 1304.48s
                               ETA: 587 mins 57.3 s

################################################################################
                     Learning iteration 1783/50000                      

                       Computation: 133344 steps/s (collection: 0.609s, learning 0.128s)
               Value function loss: 0.0804
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.21
                Mean reward (task): 5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0376
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0352
       Mean episode rew_smoothness: -0.0473
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0565
 Mean episode rew_tracking_lin_vel: 0.3266
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.74s
                        Total time: 1305.22s
                               ETA: 587 mins 56.7 s

################################################################################
                     Learning iteration 1784/50000                      

                       Computation: 156844 steps/s (collection: 0.504s, learning 0.122s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.29
                Mean reward (task): 5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0207
           Mean episode rew_no_fly: 0.0348
       Mean episode rew_smoothness: -0.0464
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0562
 Mean episode rew_tracking_lin_vel: 0.3171
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.63s
                        Total time: 1305.84s
                               ETA: 587 mins 53.2 s

################################################################################
                     Learning iteration 1785/50000                      

                       Computation: 134344 steps/s (collection: 0.603s, learning 0.129s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.21
                Mean reward (task): 4.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0363
   Mean episode rew_dof_pos_limits: -0.0343
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0454
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2790
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.73s
                        Total time: 1306.58s
                               ETA: 587 mins 52.4 s

################################################################################
                     Learning iteration 1786/50000                      

                       Computation: 135345 steps/s (collection: 0.578s, learning 0.148s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.49
                Mean reward (task): 4.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0315
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0211
           Mean episode rew_no_fly: 0.0357
       Mean episode rew_smoothness: -0.0475
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0581
 Mean episode rew_tracking_lin_vel: 0.3186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.73s
                        Total time: 1307.30s
                               ETA: 587 mins 51.6 s

################################################################################
                     Learning iteration 1787/50000                      

                       Computation: 131239 steps/s (collection: 0.609s, learning 0.141s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.46
                Mean reward (task): 4.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0360
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0335
       Mean episode rew_smoothness: -0.0455
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0550
 Mean episode rew_tracking_lin_vel: 0.2871
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.75s
                        Total time: 1308.05s
                               ETA: 587 mins 51.3 s

################################################################################
                     Learning iteration 1788/50000                      

                       Computation: 146651 steps/s (collection: 0.531s, learning 0.139s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.54
                Mean reward (task): 4.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0310
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0330
       Mean episode rew_smoothness: -0.0450
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0562
 Mean episode rew_tracking_lin_vel: 0.2986
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.67s
                        Total time: 1308.72s
                               ETA: 587 mins 48.9 s

################################################################################
                     Learning iteration 1789/50000                      

                       Computation: 127657 steps/s (collection: 0.637s, learning 0.133s)
               Value function loss: 0.0785
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.57
                Mean reward (task): 4.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0313
       Mean episode rew_ang_vel_xy: -0.0191
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0376
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0346
       Mean episode rew_smoothness: -0.0461
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.3115
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.77s
                        Total time: 1309.49s
                               ETA: 587 mins 49.2 s

################################################################################
                     Learning iteration 1790/50000                      

                       Computation: 131988 steps/s (collection: 0.622s, learning 0.123s)
               Value function loss: 0.0777
                    Surrogate loss: 0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.24
                Mean reward (task): 5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0372
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0349
       Mean episode rew_smoothness: -0.0473
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0580
 Mean episode rew_tracking_lin_vel: 0.3132
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.74s
                        Total time: 1310.24s
                               ETA: 587 mins 48.8 s

################################################################################
                     Learning iteration 1791/50000                      

                       Computation: 138408 steps/s (collection: 0.583s, learning 0.127s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.41
                Mean reward (task): 5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 203.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0360
       Mean episode rew_smoothness: -0.0482
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0591
 Mean episode rew_tracking_lin_vel: 0.3308
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.71s
                        Total time: 1310.95s
                               ETA: 587 mins 47.5 s

################################################################################
                     Learning iteration 1792/50000                      

                       Computation: 135186 steps/s (collection: 0.600s, learning 0.127s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.47
                Mean reward (task): 3.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 143.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0305
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0312
   Mean episode rew_dof_pos_limits: -0.0351
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0443
          Mean episode rew_torques: -0.0101
 Mean episode rew_tracking_ang_vel: 0.0523
 Mean episode rew_tracking_lin_vel: 0.2896
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.73s
                        Total time: 1311.67s
                               ETA: 587 mins 46.7 s

################################################################################
                     Learning iteration 1793/50000                      

                       Computation: 135452 steps/s (collection: 0.593s, learning 0.133s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 7.85
                Mean reward (task): 7.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 252.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0350
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0370
   Mean episode rew_dof_pos_limits: -0.0411
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0388
       Mean episode rew_smoothness: -0.0519
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0645
 Mean episode rew_tracking_lin_vel: 0.3571
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 0.73s
                        Total time: 1312.40s
                               ETA: 587 mins 45.8 s

################################################################################
                     Learning iteration 1794/50000                      

                       Computation: 152271 steps/s (collection: 0.501s, learning 0.145s)
               Value function loss: 0.0755
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.68
                Mean reward (task): 4.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0315
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0335
   Mean episode rew_dof_pos_limits: -0.0367
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0458
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0538
 Mean episode rew_tracking_lin_vel: 0.2886
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.65s
                        Total time: 1313.05s
                               ETA: 587 mins 42.8 s

################################################################################
                     Learning iteration 1795/50000                      

                       Computation: 124628 steps/s (collection: 0.653s, learning 0.136s)
               Value function loss: 0.0754
                    Surrogate loss: 0.0029
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.79
                Mean reward (task): 4.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 181.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0335
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0441
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0521
 Mean episode rew_tracking_lin_vel: 0.2840
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.79s
                        Total time: 1313.83s
                               ETA: 587 mins 43.6 s

################################################################################
                     Learning iteration 1796/50000                      

                       Computation: 155075 steps/s (collection: 0.509s, learning 0.125s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.56
                Mean reward (task): 5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0344
       Mean episode rew_smoothness: -0.0469
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0553
 Mean episode rew_tracking_lin_vel: 0.3089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.63s
                        Total time: 1314.47s
                               ETA: 587 mins 40.2 s

################################################################################
                     Learning iteration 1797/50000                      

                       Computation: 137275 steps/s (collection: 0.583s, learning 0.133s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.38
                Mean reward (task): 5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 196.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0343
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0475
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.3140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.72s
                        Total time: 1315.18s
                               ETA: 587 mins 39.1 s

################################################################################
                     Learning iteration 1798/50000                      

                       Computation: 151552 steps/s (collection: 0.511s, learning 0.137s)
               Value function loss: 0.0741
                    Surrogate loss: 0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.64
                Mean reward (task): 4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0317
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0321
       Mean episode rew_smoothness: -0.0443
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0521
 Mean episode rew_tracking_lin_vel: 0.2893
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.65s
                        Total time: 1315.83s
                               ETA: 587 mins 36.1 s

################################################################################
                     Learning iteration 1799/50000                      

                       Computation: 128951 steps/s (collection: 0.621s, learning 0.141s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.29
                Mean reward (task): 6.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 230.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0326
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0373
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0346
       Mean episode rew_smoothness: -0.0474
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0567
 Mean episode rew_tracking_lin_vel: 0.3073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.76s
                        Total time: 1316.60s
                               ETA: 587 mins 36.2 s

################################################################################
                     Learning iteration 1800/50000                      

                       Computation: 144103 steps/s (collection: 0.558s, learning 0.124s)
               Value function loss: 0.0748
                    Surrogate loss: 0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.65
                Mean reward (task): 3.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0296
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0331
   Mean episode rew_dof_pos_limits: -0.0328
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0424
          Mean episode rew_torques: -0.0094
 Mean episode rew_tracking_ang_vel: 0.0496
 Mean episode rew_tracking_lin_vel: 0.2761
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.68s
                        Total time: 1317.28s
                               ETA: 587 mins 34.2 s

################################################################################
                     Learning iteration 1801/50000                      

                       Computation: 130992 steps/s (collection: 0.599s, learning 0.152s)
               Value function loss: 0.0769
                    Surrogate loss: 0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.98
                Mean reward (task): 4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0335
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0389
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0360
       Mean episode rew_smoothness: -0.0489
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0585
 Mean episode rew_tracking_lin_vel: 0.3236
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.75s
                        Total time: 1318.03s
                               ETA: 587 mins 34.0 s

################################################################################
                     Learning iteration 1802/50000                      

                       Computation: 133444 steps/s (collection: 0.605s, learning 0.132s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.72
                Mean reward (task): 5.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0327
       Mean episode rew_smoothness: -0.0447
          Mean episode rew_torques: -0.0100
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2963
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.74s
                        Total time: 1318.76s
                               ETA: 587 mins 33.4 s

################################################################################
                     Learning iteration 1803/50000                      

                       Computation: 138429 steps/s (collection: 0.587s, learning 0.123s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.98
                Mean reward (task): 5.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0323
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0365
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0469
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0556
 Mean episode rew_tracking_lin_vel: 0.3140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.71s
                        Total time: 1319.47s
                               ETA: 587 mins 32.1 s

################################################################################
                     Learning iteration 1804/50000                      

                       Computation: 126905 steps/s (collection: 0.640s, learning 0.135s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.58
                Mean reward (task): 5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 210.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0343
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0410
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0383
       Mean episode rew_smoothness: -0.0507
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0633
 Mean episode rew_tracking_lin_vel: 0.3550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.77s
                        Total time: 1320.25s
                               ETA: 587 mins 32.5 s

################################################################################
                     Learning iteration 1805/50000                      

                       Computation: 136607 steps/s (collection: 0.595s, learning 0.124s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.27
                Mean reward (task): 4.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 172.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0344
        Mean episode rew_lin_vel_z: -0.0251
           Mean episode rew_no_fly: 0.0324
       Mean episode rew_smoothness: -0.0456
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0521
 Mean episode rew_tracking_lin_vel: 0.2919
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.72s
                        Total time: 1320.97s
                               ETA: 587 mins 31.4 s

################################################################################
                     Learning iteration 1806/50000                      

                       Computation: 145666 steps/s (collection: 0.551s, learning 0.124s)
               Value function loss: 0.0815
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0342
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0399
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0378
       Mean episode rew_smoothness: -0.0502
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0615
 Mean episode rew_tracking_lin_vel: 0.3513
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.67s
                        Total time: 1321.64s
                               ETA: 587 mins 29.2 s

################################################################################
                     Learning iteration 1807/50000                      

                       Computation: 152153 steps/s (collection: 0.522s, learning 0.124s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.50
                Mean reward (task): 5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 200.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0384
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0362
       Mean episode rew_smoothness: -0.0485
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0600
 Mean episode rew_tracking_lin_vel: 0.3238
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.65s
                        Total time: 1322.29s
                               ETA: 587 mins 26.2 s

################################################################################
                     Learning iteration 1808/50000                      

                       Computation: 128108 steps/s (collection: 0.636s, learning 0.132s)
               Value function loss: 0.0765
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.34
                Mean reward (task): 3.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 148.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0329
   Mean episode rew_dof_pos_limits: -0.0358
        Mean episode rew_lin_vel_z: -0.0237
           Mean episode rew_no_fly: 0.0332
       Mean episode rew_smoothness: -0.0453
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0534
 Mean episode rew_tracking_lin_vel: 0.3033
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.77s
                        Total time: 1323.06s
                               ETA: 587 mins 26.4 s

################################################################################
                     Learning iteration 1809/50000                      

                       Computation: 156043 steps/s (collection: 0.508s, learning 0.122s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.14
                Mean reward (task): 6.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 228.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0382
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0357
       Mean episode rew_smoothness: -0.0486
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0586
 Mean episode rew_tracking_lin_vel: 0.3262
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.63s
                        Total time: 1323.69s
                               ETA: 587 mins 23.0 s

################################################################################
                     Learning iteration 1810/50000                      

                       Computation: 157741 steps/s (collection: 0.501s, learning 0.122s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.95
                Mean reward (task): 4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0331
       Mean episode rew_ang_vel_xy: -0.0201
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0393
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0371
       Mean episode rew_smoothness: -0.0490
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0601
 Mean episode rew_tracking_lin_vel: 0.3390
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.62s
                        Total time: 1324.31s
                               ETA: 587 mins 19.4 s

################################################################################
                     Learning iteration 1811/50000                      

                       Computation: 138157 steps/s (collection: 0.573s, learning 0.138s)
               Value function loss: 0.0795
                    Surrogate loss: 0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0462
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0550
 Mean episode rew_tracking_lin_vel: 0.2829
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.71s
                        Total time: 1325.02s
                               ETA: 587 mins 18.1 s

################################################################################
                     Learning iteration 1812/50000                      

                       Computation: 129490 steps/s (collection: 0.621s, learning 0.138s)
               Value function loss: 0.0753
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.41
                Mean reward (task): 4.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 169.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0375
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0346
       Mean episode rew_smoothness: -0.0474
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0574
 Mean episode rew_tracking_lin_vel: 0.3108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.76s
                        Total time: 1325.78s
                               ETA: 587 mins 18.1 s

################################################################################
                     Learning iteration 1813/50000                      

                       Computation: 136384 steps/s (collection: 0.574s, learning 0.146s)
               Value function loss: 0.0735
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.99
                Mean reward (task): 5.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 222.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0329
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0356
       Mean episode rew_smoothness: -0.0476
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0584
 Mean episode rew_tracking_lin_vel: 0.3150
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.72s
                        Total time: 1326.50s
                               ETA: 587 mins 17.1 s

################################################################################
                     Learning iteration 1814/50000                      

                       Computation: 132842 steps/s (collection: 0.598s, learning 0.142s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.93
                Mean reward (task): 4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0346
       Mean episode rew_smoothness: -0.0472
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0560
 Mean episode rew_tracking_lin_vel: 0.3133
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.74s
                        Total time: 1327.24s
                               ETA: 587 mins 16.6 s

################################################################################
                     Learning iteration 1815/50000                      

                       Computation: 128265 steps/s (collection: 0.615s, learning 0.152s)
               Value function loss: 0.0710
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.35
                Mean reward (task): 5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 180.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0326
       Mean episode rew_ang_vel_xy: -0.0208
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0360
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0344
       Mean episode rew_smoothness: -0.0471
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0556
 Mean episode rew_tracking_lin_vel: 0.3161
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.77s
                        Total time: 1328.01s
                               ETA: 587 mins 16.8 s

################################################################################
                     Learning iteration 1816/50000                      

                       Computation: 124355 steps/s (collection: 0.637s, learning 0.154s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.77
                Mean reward (task): 5.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 212.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0359
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0406
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0392
       Mean episode rew_smoothness: -0.0522
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0645
 Mean episode rew_tracking_lin_vel: 0.3535
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.79s
                        Total time: 1328.80s
                               ETA: 587 mins 17.7 s

################################################################################
                     Learning iteration 1817/50000                      

                       Computation: 141765 steps/s (collection: 0.563s, learning 0.130s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.28
                Mean reward (task): 5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 196.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0326
       Mean episode rew_ang_vel_xy: -0.0205
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0386
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0355
       Mean episode rew_smoothness: -0.0476
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0582
 Mean episode rew_tracking_lin_vel: 0.3143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.69s
                        Total time: 1329.49s
                               ETA: 587 mins 15.9 s

################################################################################
                     Learning iteration 1818/50000                      

                       Computation: 144191 steps/s (collection: 0.545s, learning 0.137s)
               Value function loss: 0.0833
                    Surrogate loss: 0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0353
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0351
   Mean episode rew_dof_pos_limits: -0.0417
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0389
       Mean episode rew_smoothness: -0.0510
          Mean episode rew_torques: -0.0121
 Mean episode rew_tracking_ang_vel: 0.0642
 Mean episode rew_tracking_lin_vel: 0.3421
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.68s
                        Total time: 1330.17s
                               ETA: 587 mins 13.9 s

################################################################################
                     Learning iteration 1819/50000                      

                       Computation: 140295 steps/s (collection: 0.573s, learning 0.127s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.92
                Mean reward (task): 3.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0198
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0344
   Mean episode rew_dof_pos_limits: -0.0355
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0467
          Mean episode rew_torques: -0.0097
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.2895
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.70s
                        Total time: 1330.87s
                               ETA: 587 mins 12.3 s

################################################################################
                     Learning iteration 1820/50000                      

                       Computation: 145452 steps/s (collection: 0.552s, learning 0.124s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.83
                Mean reward (task): 5.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 217.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0317
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0336
   Mean episode rew_dof_pos_limits: -0.0349
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0332
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.2907
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.68s
                        Total time: 1331.55s
                               ETA: 587 mins 10.1 s

################################################################################
                     Learning iteration 1821/50000                      

                       Computation: 142513 steps/s (collection: 0.567s, learning 0.123s)
               Value function loss: 0.0775
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.64
                Mean reward (task): 4.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0349
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0389
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0375
       Mean episode rew_smoothness: -0.0506
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0600
 Mean episode rew_tracking_lin_vel: 0.3335
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.69s
                        Total time: 1332.24s
                               ETA: 587 mins 8.3 s

################################################################################
                     Learning iteration 1822/50000                      

                       Computation: 143707 steps/s (collection: 0.560s, learning 0.124s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.47
                Mean reward (task): 4.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 183.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0375
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0352
       Mean episode rew_smoothness: -0.0477
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0556
 Mean episode rew_tracking_lin_vel: 0.2968
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.68s
                        Total time: 1332.92s
                               ETA: 587 mins 6.3 s

################################################################################
                     Learning iteration 1823/50000                      

                       Computation: 147504 steps/s (collection: 0.544s, learning 0.123s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.28
                Mean reward (task): 4.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 175.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0340
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0361
   Mean episode rew_dof_pos_limits: -0.0388
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0365
       Mean episode rew_smoothness: -0.0492
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0588
 Mean episode rew_tracking_lin_vel: 0.3181
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.67s
                        Total time: 1333.59s
                               ETA: 587 mins 3.9 s

################################################################################
                     Learning iteration 1824/50000                      

                       Computation: 144662 steps/s (collection: 0.534s, learning 0.145s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.69
                Mean reward (task): 5.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 224.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0356
       Mean episode rew_ang_vel_xy: -0.0225
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0368
   Mean episode rew_dof_pos_limits: -0.0411
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0387
       Mean episode rew_smoothness: -0.0519
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0632
 Mean episode rew_tracking_lin_vel: 0.3351
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.68s
                        Total time: 1334.27s
                               ETA: 587 mins 1.8 s

################################################################################
                     Learning iteration 1825/50000                      

                       Computation: 142284 steps/s (collection: 0.569s, learning 0.122s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.66
                Mean reward (task): 3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 151.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0206
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0353
   Mean episode rew_dof_pos_limits: -0.0361
        Mean episode rew_lin_vel_z: -0.0256
           Mean episode rew_no_fly: 0.0331
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0102
 Mean episode rew_tracking_ang_vel: 0.0534
 Mean episode rew_tracking_lin_vel: 0.2840
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.69s
                        Total time: 1334.96s
                               ETA: 587 mins 0.0 s

################################################################################
                     Learning iteration 1826/50000                      

                       Computation: 132915 steps/s (collection: 0.599s, learning 0.140s)
               Value function loss: 0.0830
                    Surrogate loss: 0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.21
                Mean reward (task): 5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0342
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0356
   Mean episode rew_dof_pos_limits: -0.0385
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0361
       Mean episode rew_smoothness: -0.0494
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0571
 Mean episode rew_tracking_lin_vel: 0.3304
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.74s
                        Total time: 1335.70s
                               ETA: 586 mins 59.5 s

################################################################################
                     Learning iteration 1827/50000                      

                       Computation: 131578 steps/s (collection: 0.579s, learning 0.168s)
               Value function loss: 0.0752
                    Surrogate loss: 0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.90
                Mean reward (task): 3.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 149.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0312
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0345
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0327
       Mean episode rew_smoothness: -0.0452
          Mean episode rew_torques: -0.0095
 Mean episode rew_tracking_ang_vel: 0.0523
 Mean episode rew_tracking_lin_vel: 0.2893
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.75s
                        Total time: 1336.45s
                               ETA: 586 mins 59.2 s

################################################################################
                     Learning iteration 1828/50000                      

                       Computation: 129687 steps/s (collection: 0.623s, learning 0.135s)
               Value function loss: 0.0699
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.67
                Mean reward (task): 4.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 198.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0366
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0345
       Mean episode rew_smoothness: -0.0474
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0552
 Mean episode rew_tracking_lin_vel: 0.2995
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.76s
                        Total time: 1337.21s
                               ETA: 586 mins 59.2 s

################################################################################
                     Learning iteration 1829/50000                      

                       Computation: 142008 steps/s (collection: 0.551s, learning 0.141s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.93
                Mean reward (task): 5.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 215.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0362
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0409
        Mean episode rew_lin_vel_z: -0.0226
           Mean episode rew_no_fly: 0.0391
       Mean episode rew_smoothness: -0.0524
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0614
 Mean episode rew_tracking_lin_vel: 0.3380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.69s
                        Total time: 1337.90s
                               ETA: 586 mins 57.4 s

################################################################################
                     Learning iteration 1830/50000                      

                       Computation: 19529 steps/s (collection: 4.911s, learning 0.123s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.40
                Mean reward (task): 4.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0351
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0400
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0377
       Mean episode rew_smoothness: -0.0509
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0623
 Mean episode rew_tracking_lin_vel: 0.3254
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 5.03s
                        Total time: 1342.93s
                               ETA: 588 mins 49.9 s

################################################################################
                     Learning iteration 1831/50000                      

                       Computation: 86161 steps/s (collection: 1.019s, learning 0.122s)
               Value function loss: 0.0756
                    Surrogate loss: 0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.52
                Mean reward (task): 5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 205.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0348
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0405
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0380
       Mean episode rew_smoothness: -0.0506
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0619
 Mean episode rew_tracking_lin_vel: 0.3361
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 1.14s
                        Total time: 1344.07s
                               ETA: 588 mins 59.9 s

################################################################################
                     Learning iteration 1832/50000                      

                       Computation: 83806 steps/s (collection: 1.043s, learning 0.130s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.63
                Mean reward (task): 4.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0354
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0388
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0368
       Mean episode rew_smoothness: -0.0507
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0579
 Mean episode rew_tracking_lin_vel: 0.3236
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 1.17s
                        Total time: 1345.25s
                               ETA: 589 mins 10.7 s

################################################################################
                     Learning iteration 1833/50000                      

                       Computation: 83020 steps/s (collection: 1.058s, learning 0.126s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.87
                Mean reward (task): 4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 203.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0350
       Mean episode rew_ang_vel_xy: -0.0234
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0392
        Mean episode rew_lin_vel_z: -0.0259
           Mean episode rew_no_fly: 0.0372
       Mean episode rew_smoothness: -0.0507
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0595
 Mean episode rew_tracking_lin_vel: 0.3205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.18s
                        Total time: 1346.43s
                               ETA: 589 mins 21.8 s

################################################################################
                     Learning iteration 1834/50000                      

                       Computation: 86611 steps/s (collection: 1.010s, learning 0.125s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.40
                Mean reward (task): 5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0352
       Mean episode rew_ang_vel_xy: -0.0224
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0417
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0390
       Mean episode rew_smoothness: -0.0514
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0623
 Mean episode rew_tracking_lin_vel: 0.3380
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.14s
                        Total time: 1347.56s
                               ETA: 589 mins 31.6 s

################################################################################
                     Learning iteration 1835/50000                      

                       Computation: 92272 steps/s (collection: 0.927s, learning 0.138s)
               Value function loss: 0.0747
                    Surrogate loss: 0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.39
                Mean reward (task): 5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 206.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0345
       Mean episode rew_smoothness: -0.0467
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0541
 Mean episode rew_tracking_lin_vel: 0.2962
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.07s
                        Total time: 1348.63s
                               ETA: 589 mins 39.5 s

################################################################################
                     Learning iteration 1836/50000                      

                       Computation: 82780 steps/s (collection: 1.040s, learning 0.148s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0022
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.94
                Mean reward (task): 5.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 239.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0338
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0395
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0361
       Mean episode rew_smoothness: -0.0490
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0583
 Mean episode rew_tracking_lin_vel: 0.3048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.19s
                        Total time: 1349.82s
                               ETA: 589 mins 50.6 s

################################################################################
                     Learning iteration 1837/50000                      

                       Computation: 79313 steps/s (collection: 1.095s, learning 0.144s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.48
                Mean reward (task): 4.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 171.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0346
        Mean episode rew_lin_vel_z: -0.0245
           Mean episode rew_no_fly: 0.0327
       Mean episode rew_smoothness: -0.0460
          Mean episode rew_torques: -0.0098
 Mean episode rew_tracking_ang_vel: 0.0547
 Mean episode rew_tracking_lin_vel: 0.2823
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.24s
                        Total time: 1351.06s
                               ETA: 590 mins 3.1 s

################################################################################
                     Learning iteration 1838/50000                      

                       Computation: 86597 steps/s (collection: 0.989s, learning 0.146s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.96
                Mean reward (task): 4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 192.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0341
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0386
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0360
       Mean episode rew_smoothness: -0.0487
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0578
 Mean episode rew_tracking_lin_vel: 0.3143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 1.14s
                        Total time: 1352.19s
                               ETA: 590 mins 12.9 s

################################################################################
                     Learning iteration 1839/50000                      

                       Computation: 79486 steps/s (collection: 1.096s, learning 0.141s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.69
                Mean reward (task): 5.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 225.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0336
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0375
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0354
       Mean episode rew_smoothness: -0.0485
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0554
 Mean episode rew_tracking_lin_vel: 0.3089
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 1.24s
                        Total time: 1353.43s
                               ETA: 590 mins 25.3 s

################################################################################
                     Learning iteration 1840/50000                      

                       Computation: 82652 steps/s (collection: 1.040s, learning 0.150s)
               Value function loss: 0.0733
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.48
                Mean reward (task): 4.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0290
       Mean episode rew_ang_vel_xy: -0.0192
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0321
   Mean episode rew_dof_pos_limits: -0.0312
        Mean episode rew_lin_vel_z: -0.0220
           Mean episode rew_no_fly: 0.0291
       Mean episode rew_smoothness: -0.0416
          Mean episode rew_torques: -0.0085
 Mean episode rew_tracking_ang_vel: 0.0458
 Mean episode rew_tracking_lin_vel: 0.2590
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 1.19s
                        Total time: 1354.62s
                               ETA: 590 mins 36.4 s

################################################################################
                     Learning iteration 1841/50000                      

                       Computation: 92684 steps/s (collection: 0.920s, learning 0.141s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.73
                Mean reward (task): 5.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 202.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0352
       Mean episode rew_ang_vel_xy: -0.0216
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0433
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0397
       Mean episode rew_smoothness: -0.0513
          Mean episode rew_torques: -0.0125
 Mean episode rew_tracking_ang_vel: 0.0641
 Mean episode rew_tracking_lin_vel: 0.3642
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.06s
                        Total time: 1355.68s
                               ETA: 590 mins 44.2 s

################################################################################
                     Learning iteration 1842/50000                      

                       Computation: 80029 steps/s (collection: 1.061s, learning 0.167s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.23
                Mean reward (task): 4.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0348
       Mean episode rew_ang_vel_xy: -0.0218
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0393
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0372
       Mean episode rew_smoothness: -0.0507
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0618
 Mean episode rew_tracking_lin_vel: 0.3289
        Mean episode terrain_level: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 1.23s
                        Total time: 1356.91s
                               ETA: 590 mins 56.3 s

################################################################################
                     Learning iteration 1843/50000                      

                       Computation: 81396 steps/s (collection: 1.052s, learning 0.155s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.98
                Mean reward (task): 4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0368
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0431
        Mean episode rew_lin_vel_z: -0.0257
           Mean episode rew_no_fly: 0.0406
       Mean episode rew_smoothness: -0.0534
          Mean episode rew_torques: -0.0124
 Mean episode rew_tracking_ang_vel: 0.0649
 Mean episode rew_tracking_lin_vel: 0.3512
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.21s
                        Total time: 1358.12s
                               ETA: 591 mins 7.9 s

################################################################################
                     Learning iteration 1844/50000                      

                       Computation: 88235 steps/s (collection: 0.981s, learning 0.133s)
               Value function loss: 0.0748
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.01
                Mean reward (task): 5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0358
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0439
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0408
       Mean episode rew_smoothness: -0.0523
          Mean episode rew_torques: -0.0128
 Mean episode rew_tracking_ang_vel: 0.0653
 Mean episode rew_tracking_lin_vel: 0.3561
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 1.11s
                        Total time: 1359.23s
                               ETA: 591 mins 17.0 s

################################################################################
                     Learning iteration 1845/50000                      

                       Computation: 82325 steps/s (collection: 1.053s, learning 0.141s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.66
                Mean reward (task): 3.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 146.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0322
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0217
           Mean episode rew_no_fly: 0.0339
       Mean episode rew_smoothness: -0.0463
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0551
 Mean episode rew_tracking_lin_vel: 0.2989
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.19s
                        Total time: 1360.42s
                               ETA: 591 mins 28.2 s

################################################################################
                     Learning iteration 1846/50000                      

                       Computation: 87536 steps/s (collection: 0.982s, learning 0.141s)
               Value function loss: 0.0717
                    Surrogate loss: 0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.87
                Mean reward (task): 5.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 224.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0342
       Mean episode rew_ang_vel_xy: -0.0213
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0354
   Mean episode rew_dof_pos_limits: -0.0408
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0372
       Mean episode rew_smoothness: -0.0495
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0568
 Mean episode rew_tracking_lin_vel: 0.3189
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 1.12s
                        Total time: 1361.55s
                               ETA: 591 mins 37.5 s

################################################################################
                     Learning iteration 1847/50000                      

                       Computation: 81658 steps/s (collection: 1.064s, learning 0.140s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.09
                Mean reward (task): 5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 225.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0340
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0392
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0367
       Mean episode rew_smoothness: -0.0490
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0582
 Mean episode rew_tracking_lin_vel: 0.3159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.20s
                        Total time: 1362.75s
                               ETA: 591 mins 48.9 s

################################################################################
                     Learning iteration 1848/50000                      

                       Computation: 80880 steps/s (collection: 1.076s, learning 0.139s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.01
                Mean reward (task): 4.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 174.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0334
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0384
        Mean episode rew_lin_vel_z: -0.0224
           Mean episode rew_no_fly: 0.0359
       Mean episode rew_smoothness: -0.0480
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0571
 Mean episode rew_tracking_lin_vel: 0.3143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.22s
                        Total time: 1363.97s
                               ETA: 592 mins 0.6 s

################################################################################
                     Learning iteration 1849/50000                      

                       Computation: 87210 steps/s (collection: 0.985s, learning 0.142s)
               Value function loss: 0.0772
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.55
                Mean reward (task): 5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 209.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0332
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0339
   Mean episode rew_dof_pos_limits: -0.0383
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0353
       Mean episode rew_smoothness: -0.0477
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0558
 Mean episode rew_tracking_lin_vel: 0.3087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.13s
                        Total time: 1365.09s
                               ETA: 592 mins 10.0 s

################################################################################
                     Learning iteration 1850/50000                      

                       Computation: 82914 steps/s (collection: 1.047s, learning 0.139s)
               Value function loss: 0.0731
                    Surrogate loss: 0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.43
                Mean reward (task): 4.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 184.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0199
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0381
        Mean episode rew_lin_vel_z: -0.0213
           Mean episode rew_no_fly: 0.0353
       Mean episode rew_smoothness: -0.0473
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0563
 Mean episode rew_tracking_lin_vel: 0.3093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.19s
                        Total time: 1366.28s
                               ETA: 592 mins 21.0 s

################################################################################
                     Learning iteration 1851/50000                      

                       Computation: 86969 steps/s (collection: 0.988s, learning 0.142s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0008
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.38
                Mean reward (task): 6.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 213.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0321
       Mean episode rew_ang_vel_xy: -0.0204
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0376
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0463
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0553
 Mean episode rew_tracking_lin_vel: 0.3038
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 1.13s
                        Total time: 1367.41s
                               ETA: 592 mins 30.4 s

################################################################################
                     Learning iteration 1852/50000                      

                       Computation: 91964 steps/s (collection: 0.947s, learning 0.122s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.00
                Mean reward (task): 6.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 208.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0394
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0359
       Mean episode rew_smoothness: -0.0479
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0568
 Mean episode rew_tracking_lin_vel: 0.3259
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.07s
                        Total time: 1368.48s
                               ETA: 592 mins 38.3 s

################################################################################
                     Learning iteration 1853/50000                      

                       Computation: 89930 steps/s (collection: 0.968s, learning 0.125s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.11
                Mean reward (task): 4.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 164.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0340
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0341
   Mean episode rew_dof_pos_limits: -0.0395
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0369
       Mean episode rew_smoothness: -0.0491
          Mean episode rew_torques: -0.0115
 Mean episode rew_tracking_ang_vel: 0.0574
 Mean episode rew_tracking_lin_vel: 0.3217
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.09s
                        Total time: 1369.57s
                               ETA: 592 mins 46.7 s

################################################################################
                     Learning iteration 1854/50000                      

                       Computation: 85174 steps/s (collection: 1.031s, learning 0.123s)
               Value function loss: 0.0731
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.83
                Mean reward (task): 3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 168.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0304
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0314
   Mean episode rew_dof_pos_limits: -0.0352
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0316
       Mean episode rew_smoothness: -0.0433
          Mean episode rew_torques: -0.0099
 Mean episode rew_tracking_ang_vel: 0.0495
 Mean episode rew_tracking_lin_vel: 0.2741
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.15s
                        Total time: 1370.72s
                               ETA: 592 mins 56.8 s

################################################################################
                     Learning iteration 1855/50000                      

                       Computation: 88479 steps/s (collection: 0.978s, learning 0.133s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.96
                Mean reward (task): 3.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 165.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0332
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0373
        Mean episode rew_lin_vel_z: -0.0242
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0543
 Mean episode rew_tracking_lin_vel: 0.2946
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 1.11s
                        Total time: 1371.84s
                               ETA: 593 mins 5.7 s

################################################################################
                     Learning iteration 1856/50000                      

                       Computation: 86429 steps/s (collection: 0.992s, learning 0.145s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.59
                Mean reward (task): 4.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 176.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0324
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0326
   Mean episode rew_dof_pos_limits: -0.0372
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0465
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0551
 Mean episode rew_tracking_lin_vel: 0.3019
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 1.14s
                        Total time: 1372.97s
                               ETA: 593 mins 15.3 s

################################################################################
                     Learning iteration 1857/50000                      

                       Computation: 86759 steps/s (collection: 0.996s, learning 0.137s)
               Value function loss: 0.0734
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.14
                Mean reward (task): 5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0379
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0466
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.3080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 1.13s
                        Total time: 1374.11s
                               ETA: 593 mins 24.7 s

################################################################################
                     Learning iteration 1858/50000                      

                       Computation: 81248 steps/s (collection: 1.082s, learning 0.128s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.63
                Mean reward (task): 5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 204.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0327
   Mean episode rew_dof_pos_limits: -0.0379
        Mean episode rew_lin_vel_z: -0.0227
           Mean episode rew_no_fly: 0.0342
       Mean episode rew_smoothness: -0.0461
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.2988
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.21s
                        Total time: 1375.32s
                               ETA: 593 mins 36.2 s

################################################################################
                     Learning iteration 1859/50000                      

                       Computation: 88209 steps/s (collection: 0.984s, learning 0.130s)
               Value function loss: 0.0745
                    Surrogate loss: 0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.08
                Mean reward (task): 4.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0332
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0350
   Mean episode rew_dof_pos_limits: -0.0374
        Mean episode rew_lin_vel_z: -0.0233
           Mean episode rew_no_fly: 0.0349
       Mean episode rew_smoothness: -0.0477
          Mean episode rew_torques: -0.0106
 Mean episode rew_tracking_ang_vel: 0.0541
 Mean episode rew_tracking_lin_vel: 0.3032
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 1.11s
                        Total time: 1376.43s
                               ETA: 593 mins 45.1 s

################################################################################
                     Learning iteration 1860/50000                      

                       Computation: 84889 steps/s (collection: 1.005s, learning 0.153s)
               Value function loss: 0.0749
                    Surrogate loss: 0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.53
                Mean reward (task): 4.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 178.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0341
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0402
        Mean episode rew_lin_vel_z: -0.0254
           Mean episode rew_no_fly: 0.0365
       Mean episode rew_smoothness: -0.0494
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0581
 Mean episode rew_tracking_lin_vel: 0.3184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 1.16s
                        Total time: 1377.59s
                               ETA: 593 mins 55.2 s

################################################################################
                     Learning iteration 1861/50000                      

                       Computation: 82376 steps/s (collection: 1.055s, learning 0.139s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.79
                Mean reward (task): 4.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 190.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0339
       Mean episode rew_ang_vel_xy: -0.0230
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0365
   Mean episode rew_dof_pos_limits: -0.0400
        Mean episode rew_lin_vel_z: -0.0277
           Mean episode rew_no_fly: 0.0362
       Mean episode rew_smoothness: -0.0488
          Mean episode rew_torques: -0.0114
 Mean episode rew_tracking_ang_vel: 0.0583
 Mean episode rew_tracking_lin_vel: 0.3095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 1.19s
                        Total time: 1378.78s
                               ETA: 594 mins 6.2 s

################################################################################
                     Learning iteration 1862/50000                      

                       Computation: 84774 steps/s (collection: 1.021s, learning 0.138s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0018
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.83
                Mean reward (task): 3.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 152.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0295
       Mean episode rew_ang_vel_xy: -0.0197
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0307
   Mean episode rew_dof_pos_limits: -0.0343
        Mean episode rew_lin_vel_z: -0.0223
           Mean episode rew_no_fly: 0.0304
       Mean episode rew_smoothness: -0.0419
          Mean episode rew_torques: -0.0093
 Mean episode rew_tracking_ang_vel: 0.0473
 Mean episode rew_tracking_lin_vel: 0.2594
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 1.16s
                        Total time: 1379.94s
                               ETA: 594 mins 16.3 s

################################################################################
                     Learning iteration 1863/50000                      

                       Computation: 85325 steps/s (collection: 0.993s, learning 0.159s)
               Value function loss: 0.0779
                    Surrogate loss: 0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.73
                Mean reward (task): 5.73
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 196.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0362
       Mean episode rew_ang_vel_xy: -0.0222
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0359
   Mean episode rew_dof_pos_limits: -0.0419
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0391
       Mean episode rew_smoothness: -0.0526
          Mean episode rew_torques: -0.0117
 Mean episode rew_tracking_ang_vel: 0.0620
 Mean episode rew_tracking_lin_vel: 0.3592
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 1.15s
                        Total time: 1381.09s
                               ETA: 594 mins 26.2 s

################################################################################
                     Learning iteration 1864/50000                      

                       Computation: 87182 steps/s (collection: 1.002s, learning 0.126s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.51
                Mean reward (task): 4.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 179.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0339
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0386
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0360
       Mean episode rew_smoothness: -0.0486
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0583
 Mean episode rew_tracking_lin_vel: 0.3194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 1.13s
                        Total time: 1382.22s
                               ETA: 594 mins 35.4 s

################################################################################
                     Learning iteration 1865/50000                      

                       Computation: 83174 steps/s (collection: 1.035s, learning 0.147s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.34
                Mean reward (task): 5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 214.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0363
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0387
   Mean episode rew_dof_pos_limits: -0.0418
        Mean episode rew_lin_vel_z: -0.0248
           Mean episode rew_no_fly: 0.0387
       Mean episode rew_smoothness: -0.0523
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0606
 Mean episode rew_tracking_lin_vel: 0.3550
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.18s
                        Total time: 1383.40s
                               ETA: 594 mins 46.0 s

################################################################################
                     Learning iteration 1866/50000                      

                       Computation: 85125 steps/s (collection: 1.033s, learning 0.122s)
               Value function loss: 0.0760
                    Surrogate loss: 0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.33
                Mean reward (task): 4.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 156.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0328
       Mean episode rew_ang_vel_xy: -0.0210
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0373
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0340
       Mean episode rew_smoothness: -0.0470
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0557
 Mean episode rew_tracking_lin_vel: 0.3031
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 1.15s
                        Total time: 1384.56s
                               ETA: 594 mins 55.9 s

################################################################################
                     Learning iteration 1867/50000                      

                       Computation: 90165 steps/s (collection: 0.951s, learning 0.139s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.20
                Mean reward (task): 6.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 228.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0320
       Mean episode rew_ang_vel_xy: -0.0203
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0332
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0232
           Mean episode rew_no_fly: 0.0340
       Mean episode rew_smoothness: -0.0459
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0559
 Mean episode rew_tracking_lin_vel: 0.3029
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 1.09s
                        Total time: 1385.65s
                               ETA: 595 mins 4.2 s

################################################################################
                     Learning iteration 1868/50000                      

                       Computation: 93517 steps/s (collection: 0.913s, learning 0.138s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.63
                Mean reward (task): 4.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 166.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0334
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0340
   Mean episode rew_dof_pos_limits: -0.0375
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0350
       Mean episode rew_smoothness: -0.0479
          Mean episode rew_torques: -0.0105
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.3191
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 1.05s
                        Total time: 1386.70s
                               ETA: 595 mins 11.4 s

################################################################################
                     Learning iteration 1869/50000                      

                       Computation: 91664 steps/s (collection: 0.934s, learning 0.139s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.62
                Mean reward (task): 4.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 173.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0342
       Mean episode rew_ang_vel_xy: -0.0221
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0348
   Mean episode rew_dof_pos_limits: -0.0396
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0366
       Mean episode rew_smoothness: -0.0492
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0599
 Mean episode rew_tracking_lin_vel: 0.3195
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 1.07s
                        Total time: 1387.77s
                               ETA: 595 mins 19.2 s

################################################################################
                     Learning iteration 1870/50000                      

                       Computation: 83653 steps/s (collection: 1.034s, learning 0.142s)
               Value function loss: 0.0762
                    Surrogate loss: 0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 3.21
                Mean reward (task): 3.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 155.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0345
       Mean episode rew_ang_vel_xy: -0.0226
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0364
   Mean episode rew_dof_pos_limits: -0.0398
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0359
       Mean episode rew_smoothness: -0.0489
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0562
 Mean episode rew_tracking_lin_vel: 0.3160
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.18s
                        Total time: 1388.95s
                               ETA: 595 mins 29.6 s

################################################################################
                     Learning iteration 1871/50000                      

                       Computation: 80431 steps/s (collection: 1.082s, learning 0.140s)
               Value function loss: 0.0755
                    Surrogate loss: 0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.59
                Mean reward (task): 5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 201.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0345
       Mean episode rew_ang_vel_xy: -0.0220
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0346
   Mean episode rew_dof_pos_limits: -0.0403
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0373
       Mean episode rew_smoothness: -0.0499
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0606
 Mean episode rew_tracking_lin_vel: 0.3362
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.22s
                        Total time: 1390.17s
                               ETA: 595 mins 41.2 s

################################################################################
                     Learning iteration 1872/50000                      

                       Computation: 82723 steps/s (collection: 1.050s, learning 0.138s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 6.04
                Mean reward (task): 6.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 203.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0354
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0358
   Mean episode rew_dof_pos_limits: -0.0417
        Mean episode rew_lin_vel_z: -0.0235
           Mean episode rew_no_fly: 0.0380
       Mean episode rew_smoothness: -0.0509
          Mean episode rew_torques: -0.0116
 Mean episode rew_tracking_ang_vel: 0.0641
 Mean episode rew_tracking_lin_vel: 0.3411
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 1.19s
                        Total time: 1391.36s
                               ETA: 595 mins 51.9 s

################################################################################
                     Learning iteration 1873/50000                      

                       Computation: 89610 steps/s (collection: 0.959s, learning 0.138s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0016
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 5.21
                Mean reward (task): 5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 196.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0325
       Mean episode rew_ang_vel_xy: -0.0209
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0345
   Mean episode rew_dof_pos_limits: -0.0371
        Mean episode rew_lin_vel_z: -0.0221
           Mean episode rew_no_fly: 0.0337
       Mean episode rew_smoothness: -0.0467
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0520
 Mean episode rew_tracking_lin_vel: 0.2992
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.10s
                        Total time: 1392.45s
                               ETA: 596 mins 0.2 s

################################################################################
                     Learning iteration 1874/50000                      

                       Computation: 90754 steps/s (collection: 0.954s, learning 0.129s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.66
                Mean reward (task): 4.66
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 185.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0325
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0337
   Mean episode rew_dof_pos_limits: -0.0375
        Mean episode rew_lin_vel_z: -0.0252
           Mean episode rew_no_fly: 0.0338
       Mean episode rew_smoothness: -0.0458
          Mean episode rew_torques: -0.0104
 Mean episode rew_tracking_ang_vel: 0.0548
 Mean episode rew_tracking_lin_vel: 0.2931
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 1.08s
                        Total time: 1393.54s
                               ETA: 596 mins 8.2 s

################################################################################
                     Learning iteration 1875/50000                      

                       Computation: 82203 steps/s (collection: 1.073s, learning 0.123s)
               Value function loss: 0.0695
                    Surrogate loss: 0.0000
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.69
                Mean reward (task): 4.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 177.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0327
       Mean episode rew_ang_vel_xy: -0.0212
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0377
        Mean episode rew_lin_vel_z: -0.0218
           Mean episode rew_no_fly: 0.0343
       Mean episode rew_smoothness: -0.0467
          Mean episode rew_torques: -0.0107
 Mean episode rew_tracking_ang_vel: 0.0544
 Mean episode rew_tracking_lin_vel: 0.2962
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 1.20s
                        Total time: 1394.73s
                               ETA: 596 mins 19.1 s

################################################################################
                     Learning iteration 1876/50000                      

                       Computation: 85742 steps/s (collection: 1.006s, learning 0.140s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.76
                Mean reward (task): 4.76
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 183.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0337
       Mean episode rew_ang_vel_xy: -0.0217
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0392
        Mean episode rew_lin_vel_z: -0.0246
           Mean episode rew_no_fly: 0.0362
       Mean episode rew_smoothness: -0.0489
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0595
 Mean episode rew_tracking_lin_vel: 0.3186
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.15s
                        Total time: 1395.88s
                               ETA: 596 mins 28.7 s

################################################################################
                     Learning iteration 1877/50000                      

                       Computation: 88772 steps/s (collection: 0.965s, learning 0.142s)
               Value function loss: 0.0730
                    Surrogate loss: 0.0023
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.52
                Mean reward (task): 4.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 170.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0341
       Mean episode rew_ang_vel_xy: -0.0207
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0333
   Mean episode rew_dof_pos_limits: -0.0395
        Mean episode rew_lin_vel_z: -0.0208
           Mean episode rew_no_fly: 0.0362
       Mean episode rew_smoothness: -0.0489
          Mean episode rew_torques: -0.0112
 Mean episode rew_tracking_ang_vel: 0.0568
 Mean episode rew_tracking_lin_vel: 0.3272
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.11s
                        Total time: 1396.99s
                               ETA: 596 mins 37.2 s

################################################################################
                     Learning iteration 1878/50000                      

                       Computation: 86307 steps/s (collection: 1.001s, learning 0.138s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.59
               Mean reward (total): 4.38
                Mean reward (task): 4.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 167.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0331
       Mean episode rew_ang_vel_xy: -0.0202
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0328
   Mean episode rew_dof_pos_limits: -0.0383
        Mean episode rew_lin_vel_z: -0.0216
           Mean episode rew_no_fly: 0.0352
       Mean episode rew_smoothness: -0.0477
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0588
 Mean episode rew_tracking_lin_vel: 0.3041
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.14s
                        Total time: 1398.13s
                               ETA: 596 mins 46.6 s

################################################################################
                     Learning iteration 1879/50000                      

                       Computation: 77041 steps/s (collection: 1.107s, learning 0.169s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0007
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 4.81
                Mean reward (task): 4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 183.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0329
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0334
   Mean episode rew_dof_pos_limits: -0.0369
        Mean episode rew_lin_vel_z: -0.0230
           Mean episode rew_no_fly: 0.0345
       Mean episode rew_smoothness: -0.0471
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0527
 Mean episode rew_tracking_lin_vel: 0.3088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.28s
                        Total time: 1399.40s
                               ETA: 596 mins 59.5 s

################################################################################
                     Learning iteration 1880/50000                      

                       Computation: 87683 steps/s (collection: 0.980s, learning 0.141s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0009
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 4.62
                Mean reward (task): 4.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 191.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0343
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0338
   Mean episode rew_dof_pos_limits: -0.0405
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0368
       Mean episode rew_smoothness: -0.0495
          Mean episode rew_torques: -0.0113
 Mean episode rew_tracking_ang_vel: 0.0584
 Mean episode rew_tracking_lin_vel: 0.3132
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.12s
                        Total time: 1400.52s
                               ETA: 597 mins 8.4 s

################################################################################
                     Learning iteration 1881/50000                      

                       Computation: 87931 steps/s (collection: 0.983s, learning 0.135s)
               Value function loss: 0.0773
                    Surrogate loss: 0.0115
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 5.78
                Mean reward (task): 5.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0352
       Mean episode rew_ang_vel_xy: -0.0219
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0366
   Mean episode rew_dof_pos_limits: -0.0400
        Mean episode rew_lin_vel_z: -0.0258
           Mean episode rew_no_fly: 0.0372
       Mean episode rew_smoothness: -0.0510
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0598
 Mean episode rew_tracking_lin_vel: 0.3457
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.12s
                        Total time: 1401.64s
                               ETA: 597 mins 17.2 s

################################################################################
                     Learning iteration 1882/50000                      

                       Computation: 84196 steps/s (collection: 1.044s, learning 0.123s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 5.81
                Mean reward (task): 5.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 219.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0316
       Mean episode rew_ang_vel_xy: -0.0200
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0325
   Mean episode rew_dof_pos_limits: -0.0368
        Mean episode rew_lin_vel_z: -0.0238
           Mean episode rew_no_fly: 0.0332
       Mean episode rew_smoothness: -0.0456
          Mean episode rew_torques: -0.0103
 Mean episode rew_tracking_ang_vel: 0.0534
 Mean episode rew_tracking_lin_vel: 0.2815
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.17s
                        Total time: 1402.81s
                               ETA: 597 mins 27.3 s

################################################################################
                     Learning iteration 1883/50000                      

                       Computation: 91526 steps/s (collection: 0.951s, learning 0.124s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 6.79
                Mean reward (task): 6.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 232.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0360
       Mean episode rew_ang_vel_xy: -0.0228
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0372
   Mean episode rew_dof_pos_limits: -0.0425
        Mean episode rew_lin_vel_z: -0.0239
           Mean episode rew_no_fly: 0.0386
       Mean episode rew_smoothness: -0.0518
          Mean episode rew_torques: -0.0119
 Mean episode rew_tracking_ang_vel: 0.0597
 Mean episode rew_tracking_lin_vel: 0.3428
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.07s
                        Total time: 1403.88s
                               ETA: 597 mins 34.9 s

################################################################################
                     Learning iteration 1884/50000                      

                       Computation: 89643 steps/s (collection: 0.972s, learning 0.125s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0014
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 6.31
                Mean reward (task): 6.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 241.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0340
       Mean episode rew_ang_vel_xy: -0.0214
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0385
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0359
       Mean episode rew_smoothness: -0.0491
          Mean episode rew_torques: -0.0108
 Mean episode rew_tracking_ang_vel: 0.0576
 Mean episode rew_tracking_lin_vel: 0.3108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 1.10s
                        Total time: 1404.98s
                               ETA: 597 mins 43.1 s

################################################################################
                     Learning iteration 1885/50000                      

                       Computation: 89607 steps/s (collection: 0.974s, learning 0.123s)
               Value function loss: 0.0774
                    Surrogate loss: 0.0011
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 5.31
                Mean reward (task): 5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 189.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0352
       Mean episode rew_ang_vel_xy: -0.0215
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0342
   Mean episode rew_dof_pos_limits: -0.0413
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0381
       Mean episode rew_smoothness: -0.0509
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0615
 Mean episode rew_tracking_lin_vel: 0.3428
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.10s
                        Total time: 1406.08s
                               ETA: 597 mins 51.4 s

################################################################################
                     Learning iteration 1886/50000                      

                       Computation: 91899 steps/s (collection: 0.946s, learning 0.124s)
               Value function loss: 0.0707
                    Surrogate loss: 0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 4.95
                Mean reward (task): 4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0355
       Mean episode rew_ang_vel_xy: -0.0227
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0426
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0389
       Mean episode rew_smoothness: -0.0514
          Mean episode rew_torques: -0.0120
 Mean episode rew_tracking_ang_vel: 0.0609
 Mean episode rew_tracking_lin_vel: 0.3412
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 1.07s
                        Total time: 1407.15s
                               ETA: 597 mins 58.9 s

################################################################################
                     Learning iteration 1887/50000                      

                       Computation: 92168 steps/s (collection: 0.939s, learning 0.128s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 5.35
                Mean reward (task): 5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 194.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0386
       Mean episode rew_ang_vel_xy: -0.0236
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0390
   Mean episode rew_dof_pos_limits: -0.0453
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0421
       Mean episode rew_smoothness: -0.0562
          Mean episode rew_torques: -0.0130
 Mean episode rew_tracking_ang_vel: 0.0693
 Mean episode rew_tracking_lin_vel: 0.3663
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 1.07s
                        Total time: 1408.21s
                               ETA: 598 mins 6.3 s

################################################################################
                     Learning iteration 1888/50000                      

                       Computation: 94168 steps/s (collection: 0.914s, learning 0.130s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 5.03
                Mean reward (task): 5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 195.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0337
       Mean episode rew_ang_vel_xy: -0.0211
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0322
   Mean episode rew_dof_pos_limits: -0.0401
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0365
       Mean episode rew_smoothness: -0.0487
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0599
 Mean episode rew_tracking_lin_vel: 0.3225
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.04s
                        Total time: 1409.26s
                               ETA: 598 mins 13.2 s

################################################################################
                     Learning iteration 1889/50000                      

                       Computation: 81738 steps/s (collection: 1.054s, learning 0.149s)
               Value function loss: 0.0718
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.60
               Mean reward (total): 4.87
                Mean reward (task): 4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 188.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0388
       Mean episode rew_ang_vel_xy: -0.0232
    Mean episode rew_delta_torques: -0.0000
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0455
        Mean episode rew_lin_vel_z: -0.0229
           Mean episode rew_no_fly: 0.0422
       Mean episode rew_smoothness: -0.0566
          Mean episode rew_torques: -0.0129
 Mean episode rew_tracking_ang_vel: 0.0660
 Mean episode rew_tracking_lin_vel: 0.3794
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 1.20s
                        Total time: 1410.46s
                               ETA: 598 mins 24.0 s

swanlab:KeyboardInterrupt by user
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/4zo16fpi3g53rwu0j2f6k
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    